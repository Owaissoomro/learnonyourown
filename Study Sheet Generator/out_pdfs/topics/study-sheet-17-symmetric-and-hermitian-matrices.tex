% !TeX program = xelatex
\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{microtype,setspace,amsmath,amssymb,mathtools,amsthm,unicode-math}
\setstretch{1.05}
\setmainfont{Latin Modern Roman}
\setmonofont{Latin Modern Mono}
\setmathfont{Latin Modern Math}

\allowdisplaybreaks[4]
\setlength{\jot}{7pt}
\setlength{\emergencystretch}{8em}
\sloppy

\usepackage{xcolor,fancyhdr,enumitem,inconsolata,listings}
\pagestyle{fancy}\fancyhf{}\lhead{\nouppercase{\leftmark}}\rhead{\thepage}
\setlength{\headheight}{26pt}
\setlength{\parindent}{0pt}
\setlength{\parskip}{8pt plus 2pt minus 1pt}
\raggedbottom

\newenvironment{BreakableEquation}{\begin{equation}\begin{aligned}}{\end{aligned}\end{equation}}
\newenvironment{BreakableEquation*}{\begin{equation*}\begin{aligned}}{\end{aligned}\end{equation*}}
\newenvironment{tightalign}{\begingroup\small\allowdisplaybreaks\begin{align}}{\end{align}\endgroup}

\providecommand{\enumlistm}{enumitem}
\newenvironment{minted}[2][]{%
  \lstset{style=code,language=#2,#1}\begin{lstlisting}%
}{\end{lstlisting}}

\newcommand{\inputminted}[3][]{\begin{lstlisting}\end{lstlisting}}

\newlist{bullets}{itemize}{1}
\setlist[bullets]{label=--,leftmargin=1.6em,itemsep=2pt,topsep=4pt}

\newcommand{\varmapStart}{\textbf{VARIABLE MAPPING:}\par\begin{bullets}}
\newcommand{\var}[2]{\item $#1$ — #2}
\newcommand{\varmapEnd}{\end{bullets}}

\newcommand{\glossx}[6]{%
  \textbf{#1}\par
  \begin{bullets}
    \item \textbf{What:} #2
    \item \textbf{Why:} #3
    \item \textbf{How:} #4
    \item \textbf{ELI5:} #5
    \item \textbf{Pitfall/Example:} #6
  \end{bullets}
}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{claim}[theorem]{Claim}

\lstdefinestyle{code}{
  basicstyle=\ttfamily\footnotesize,
  backgroundcolor=\color{black!02},
  frame=single,
  numbers=left, numberstyle=\tiny, numbersep=8pt,
  breaklines=true, breakatwhitespace=true,
  tabsize=4, showstringspaces=false,
  upquote=true, keepspaces=true, columns=fullflexible,
  literate=
    {–}{{-}}1
    {—}{{-}}1
    {…}{{...}}1
    {≤}{{\ensuremath{\le}}}1
    {≥}{{\ensuremath{\ge}}}1
    {≠}{{\ensuremath{\ne}}}1
    {≈}{{\ensuremath{\approx}}}1
    {±}{{\ensuremath{\pm}}}1
    {→}{{\ensuremath{\to}}}1
    {←}{{\ensuremath{\leftarrow}}}1
    {∞}{{\ensuremath{\infty}}}1
    {√}{{\ensuremath{\sqrt{\ }}}}1
    {×}{{\ensuremath{\times}}}1
    {÷}{{\ensuremath{\div}}}1
}

\lstnewenvironment{codepy}[1][]%
  {\lstset{style=code,language=Python,#1}}%
  {}

\newcommand{\inlinecode}[1]{\lstinline[style=code]!#1!}

\newcommand{\LF}[2]{\par\noindent\textbf{#1:}~#2\par}
\newcommand{\WHAT}[1]{\LF{WHAT}{#1}}
\newcommand{\WHY}[1]{\LF{WHY}{#1}}
\newcommand{\HOW}[1]{\LF{HOW}{#1}}
\newcommand{\ELI}[1]{\LF{ELI5}{#1}}
\newcommand{\SCOPE}[1]{\LF{SCOPE}{#1}}
\newcommand{\CONFUSIONS}[1]{\LF{COMMON CONFUSIONS}{#1}}
\newcommand{\APPLICATIONS}[1]{\LF{APPLICATIONS}{#1}}
\newcommand{\FORMULA}[1]{\LF{FORMULA}{#1}}
\newcommand{\CANONICAL}[1]{\LF{CANONICAL FORM}{#1}}
\newcommand{\PRECONDS}[1]{\LF{PRECONDITIONS}{#1}}
\newcommand{\DERIVATION}[1]{\LF{DERIVATION}{#1}}
\newcommand{\EQUIV}[1]{\LF{EQUIVALENT FORMS}{#1}}
\newcommand{\LIMITS}[1]{\LF{LIMIT CASES}{#1}}
\newcommand{\INPUTS}[1]{\LF{INPUTS}{#1}}
\newcommand{\OUTPUTS}[1]{\LF{OUTPUTS}{#1}}
\newcommand{\RESULT}[1]{\LF{RESULT}{#1}}
\newcommand{\INTUITION}[1]{\LF{INTUITION}{#1}}
\newcommand{\PITFALLS}[1]{\LF{PITFALLS}{#1}}
\newcommand{\MODEL}[1]{\LF{CANONICAL MATH MODEL}{#1}}
\newcommand{\ASSUMPTIONS}[1]{\LF{ASSUMPTIONS}{#1}}
\newcommand{\WHICHFORMULA}[1]{\LF{WHICH FORMULA \& WHY}{#1}}
\newcommand{\GOVERN}[1]{\LF{GOVERNING EQUATION(S)}{#1}}
\newcommand{\UNITCHECK}[1]{\LF{UNIT CHECK}{#1}}
\newcommand{\EDGECASES}[1]{\LF{EDGE CASES}{#1}}
\newcommand{\ALTERNATE}[1]{\LF{ALTERNATE APPROACH (sketch)}{#1}}
\newcommand{\PROBLEM}[1]{\LF{PROBLEM}{#1}}
\newcommand{\API}[1]{\LF{API}{#1}}
\newcommand{\COMPLEXITY}[1]{\LF{COMPLEXITY}{#1}}
\newcommand{\FAILMODES}[1]{\LF{FAILURE MODES}{#1}}
\newcommand{\STABILITY}[1]{\LF{NUMERICAL STABILITY}{#1}}
\newcommand{\VALIDATION}[1]{\LF{VALIDATION}{#1}}
\newcommand{\EXPLANATION}[1]{\LF{EXPLANATION}{#1}}
\newcommand{\SCENARIO}[1]{\LF{SCENARIO}{#1}}
\newcommand{\PIPELINE}[1]{\LF{PIPELINE STEPS}{#1}}
\newcommand{\METRICS}[1]{\LF{METRICS}{#1}}
\newcommand{\INTERPRET}[1]{\LF{INTERPRETATION}{#1}}
\newcommand{\NEXTSTEPS}[1]{\LF{LIMITATIONS \& NEXT STEPS}{#1}}

\usepackage{titlesec}
\titleformat{\section}{\Large\bfseries}{\thesection}{0.6em}{}
\titlespacing*{\section}{0pt}{*2}{*1}
\usepackage{etoolbox}
\pretocmd{\section}{\clearpage}{}{}

\newcommand{\FormulaPage}[2]{%
  \clearpage
  \section*{Formula #1 — #2}%
  \addcontentsline{toc}{section}{Formula #1 — #2}%
}
\newcommand{\ProblemPage}[2]{%
  \clearpage
  \subsection*{Problem #1: #2}%
  \addcontentsline{toc}{subsection}{Problem #1: #2}%
}
\newcommand{\CodeDemoPage}[1]{%
  \clearpage
  \subsection*{Coding Demo: #1}%
  \addcontentsline{toc}{subsection}{Coding Demo: #1}%
}
\newcommand{\DomainPage}[1]{%
  \clearpage
  \subsection*{#1 (End-to-End)}%
  \addcontentsline{toc}{subsection}{#1 (End-to-End)}%
}

\begin{document}
\title{Comprehensive Study Sheet — Symmetric and Hermitian Matrices}
\date{\today}
\maketitle
\tableofcontents
\clearpage

\section{Concept Overview}
\WHAT{
A symmetric matrix $A\in\mathbb{R}^{n\times n}$ satisfies $A=A^\top$ with respect
to the standard Euclidean inner product on $\mathbb{R}^n$. A Hermitian matrix
$A\in\mathbb{C}^{n\times n}$ satisfies $A=A^\ast$ (conjugate transpose) with
respect to the standard Hermitian inner product
$\langle x,y\rangle = x^\ast y$ on $\mathbb{C}^n$.
They represent self-adjoint linear operators on finite dimensional inner product
spaces. Domain: $\mathbb{F}^n$ with $\mathbb{F}\in\{\mathbb{R},\mathbb{C}\}$.
Codomain: $\mathbb{F}^n$. Structure: inner product space, orthogonality, spectra.}

\WHY{
Symmetric and Hermitian matrices diagonalize by orthonormal bases and have real
eigenvalues. This yields stable computations, variational characterizations
of eigenvalues, quadratic form interpretations, and guarantees for optimization
and convexity. They encode energies (graph Laplacians, covariances, Hessians)
and underpin numerical linear algebra and spectral methods.}

\HOW{
1. Assume $A$ is symmetric or Hermitian (self-adjoint).
2. Use inner product identities $x^\ast Ay=\langle x,Ay\rangle
=\langle A^\ast x,y\rangle$ to deduce real spectra and orthogonal eigenvectors.
3. Apply Schur decomposition and normality to derive unitary diagonalization.
4. Obtain computational forms: spectral decomposition $A=U\Lambda U^\ast$,
Rayleigh quotient bounds, Cholesky for positive definiteness, and functional
calculus $f(A)=Uf(\Lambda)U^\ast$. Interpret each as energy, variance, or mode.}

\ELI{
Think of a symmetric or Hermitian matrix as a perfectly fair mirror: it treats
vectors the same from both sides. Because of this symmetry, it can be described
by independent directions that do not interfere (orthogonal eigenvectors),
and along each direction it just stretches by a real amount (eigenvalue).}

\SCOPE{
Valid for finite dimensional inner product spaces over $\mathbb{R}$ or
$\mathbb{C}$. Requires $A=A^\top$ or $A=A^\ast$. Edge cases: repeated
eigenvalues lead to nonunique eigenvectors but still allow orthonormal bases of
eigenspaces. Non Hermitian matrices can fail to diagonalize or may have complex
eigenvalues. Positive semidefinite vs. definite distinctions matter for invertibility.}

\CONFUSIONS{
Symmetric vs. positive semidefinite: symmetry alone does not imply nonnegative
quadratic form. Hermitian vs. normal: every Hermitian is normal, but not every
normal is Hermitian. Orthogonal vs. unitary: orthogonal for real, unitary for
complex. Cholesky exists only for positive definite, not for indefinite.}

\APPLICATIONS{
\begin{bullets}
\item Mathematical foundations: spectral theorem, orthogonal decompositions.
\item Computational modeling: PDE discretizations yield symmetric matrices.
\item Physical or economic interpretations: energies, covariances, Laplacians.
\item Statistical or algorithmic implications: PCA eigen decomposition of Gram matrices.
\end{bullets}
}

\textbf{ANALYTIC STRUCTURE.}
Self-adjointness implies normality and a complete set of orthonormal eigenvectors.
Quadratic form $x^\ast Ax$ is real. Convexity of $x\mapsto x^\ast Ax$ when $A$
is positive semidefinite. Spectral norm equals max absolute eigenvalue.

\textbf{CANONICAL LINKS.}
Spectral theorem enables functional calculus. Rayleigh quotient yields extremal
eigenvalues (Courant Fischer). Positive semidefinite equivalences enable Cholesky
and Gram factorizations. Interlacing binds spectra under principal submatrices.

\textbf{PROBLEM-TYPE RECOGNITION HEURISTICS.}
\begin{bullets}
\item Quadratic forms $x^\ast Ax$ with $A=A^\ast$.
\item Questions about real eigenvalues and orthogonal eigenvectors.
\item Decompositions $A=Q^\top\Lambda Q$ or $A=U\Lambda U^\ast$.
\item Covariance, Gram, Laplacian, Hessian matrices indicating PSD structure.
\end{bullets}

\textbf{SOLUTION STRATEGY BLUEPRINT.}
\begin{bullets}
\item Translate to $A=A^\top$ or $A=A^\ast$ and a quadratic form or eigenproblem.
\item Identify spectral theorem, Rayleigh quotient, or PSD equivalences.
\item Substitute known data; compute eigenpairs or Cholesky.
\item Interpret eigenvalues as energies or variances; eigenvectors as modes.
\item Validate via orthogonality, reconstruction, and sign checks.
\end{bullets}

\textbf{CONCEPTUAL INVARIANTS.}
Eigenvalues are real and invariant under unitary similarity. Trace equals sum
of eigenvalues; determinant equals product of eigenvalues. For PSD, $x^\ast Ax
\ge 0$ for all $x$.

\textbf{EDGE INTUITION.}
As entries scale by $\alpha$, eigenvalues scale by $\alpha$. As a PSD matrix
approaches singularity, the smallest eigenvalue tends to $0$ and directions
of near null energy appear. For large dimension, spectra cluster for structured
matrices like Toeplitz or Laplacians with mesh refinement.

\section{Glossary}
\glossx{Symmetric Matrix}{
A real square matrix with $A=A^\top$.}{
Diagonalizable by an orthogonal matrix with real eigenvalues; central to many
numerical and theoretical results.}{
Compute eigenpairs via orthogonal similarity $A=Q^\top\Lambda Q$.}{
Like a mirror that treats directions the same both ways.}{
Confusing symmetry with positive definiteness leads to incorrect sign claims.}

\glossx{Hermitian Matrix}{
A complex square matrix with $A=A^\ast$.}{
Guarantees real eigenvalues and unitary diagonalization; core of quantum and
signal processing models.}{
Use unitary $U$ so $A=U\Lambda U^\ast$ with real diagonal $\Lambda$.}{
A balanced see-saw that never tilts into complex directions.}{
Mistaking normal but non Hermitian matrices as Hermitian breaks reality of eigenvalues.}

\glossx{Rayleigh Quotient}{
$R_A(x)=\dfrac{x^\ast Ax}{x^\ast x}$ for $x\ne 0$.}{
Characterizes extremal eigenvalues and provides bounds and iterative methods.}{
Maximize or minimize $R_A$ over unit vectors to get $\lambda_{\max}$,
$\lambda_{\min}$.}{
Average energy per unit length seen through $A$.}{
For non Hermitian $A$, $R_A(x)$ can be complex and loses variational meaning.}

\glossx{Positive Semidefinite (PSD)}{
Hermitian $A$ with $x^\ast Ax\ge 0$ for all $x$.}{
Links to covariance, Gram, and energy; enables Cholesky like factorizations.}{
Check eigenvalues $\ge 0$ or factor $A=B^\ast B$.}{
A bowl that does not dip below the ground.}{
Numerically nearly PSD matrices can fail Cholesky without regularization.}

\section{Symbol Ledger}
\varmapStart
\var{A}{Symmetric or Hermitian matrix in $\mathbb{F}^{n\times n}$.}
\var{A^\top}{Transpose of $A$ (real).}
\var{A^\ast}{Conjugate transpose of $A$ (complex).}
\var{U,Q}{Unitary or orthogonal matrix with orthonormal columns.}
\var{\Lambda}{Diagonal matrix of eigenvalues.}
\var{\lambda_i}{Eigenvalues, real for Hermitian.}
\var{u_i}{Unit eigenvectors, orthonormal basis.}
\var{x}{Vector in $\mathbb{F}^n$, nonzero for Rayleigh quotient.}
\var{R_A(x)}{Rayleigh quotient $(x^\ast Ax)/(x^\ast x)$.}
\var{I}{Identity matrix.}
\var{B,R}{Factors in Gram or Cholesky factorizations.}
\var{f}{Scalar function applied in functional calculus.}
\var{\sigma(A)}{Spectrum of $A$.}
\var{L}{Graph Laplacian, symmetric PSD.}
\var{H}{Hessian, symmetric in multivariate calculus.}
\varmapEnd

\section{Formula Canon — One Formula Per Page}
\FormulaPage{1}{Spectral Theorem for Symmetric and Hermitian Matrices}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
For Hermitian $A\in\mathbb{C}^{n\times n}$ there exists unitary $U$ and real
diagonal $\Lambda$ such that $A=U\Lambda U^\ast$. For symmetric
$A\in\mathbb{R}^{n\times n}$, $A=Q^\top\Lambda Q$ with $Q$ orthogonal.

\WHAT{
Diagonalization by an orthonormal basis of eigenvectors with real eigenvalues.}

\WHY{
Enables simple representation, stable computation, and functional calculus,
and explains energy decomposition in orthogonal modes.}

\FORMULA{
\[
\exists\,U\ \text{unitary},\ \Lambda=\operatorname{diag}(\lambda_1,\dots,\lambda_n)
\ \text{real},\quad A=U\Lambda U^\ast,\quad U^\ast U=I.
\]}

\CANONICAL{
Hermitian or symmetric matrices on finite dimensional inner product spaces.
Eigenvalues are real; eigenvectors from distinct eigenvalues are orthogonal.}

\PRECONDS{
\begin{bullets}
\item $A=A^\ast$ (complex) or $A=A^\top$ (real).
\item Finite dimensional vector space with standard inner product.
\end{bullets}
}

\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
If $A=A^\ast$, then every eigenvalue $\lambda$ of $A$ is real and
eigenvectors from distinct eigenvalues are orthogonal.
\end{lemma}
\begin{proof}
Let $Au=\lambda u$ with $u\ne 0$. Then
$\lambda \|u\|^2=u^\ast(\lambda u)=u^\ast Au=(Au)^\ast u=(\lambda u)^\ast u
=\overline{\lambda}\|u\|^2$, so $\lambda=\overline{\lambda}$ and $\lambda\in\mathbb{R}$.
For $Au=\lambda u$ and $Av=\mu v$ with $\lambda\ne\mu$,
$\lambda \langle u,v\rangle=\langle Au,v\rangle=\langle u,Av\rangle
=\mu \langle u,v\rangle$, hence $(\lambda-\mu)\langle u,v\rangle=0$ so
$\langle u,v\rangle=0$. \qedhere
\end{proof}

\DERIVATION{
\begin{align*}
\text{Step 1:} &\ \text{By Schur, }A=UTU^\ast\text{ with }U\text{ unitary, }T
\text{ upper triangular}.\\
\text{Step 2:} &\ A\text{ normal since }A^\ast A=AA^\ast\text{ (self adjoint).}\\
\text{Step 3:} &\ \text{For normal }A,\ T\text{ must be diagonal.}\\
\text{Step 4:} &\ \text{By Lemma, diagonal entries are real eigenvalues.}\\
\text{Step 5:} &\ \text{Rename }T=\Lambda,\ \text{then }A=U\Lambda U^\ast.
\end{align*}
}

\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Check symmetry or Hermitian property.
\item Compute eigenpairs; form $U$ of orthonormal eigenvectors.
\item Build $\Lambda$; reconstruct $A=U\Lambda U^\ast$ to verify.
\item Use eigenbasis to simplify computations of $f(A)$ or powers.
\end{bullets}

\EQUIV{
\begin{bullets}
\item $A$ Hermitian $\iff$ $A$ normal with real spectrum $\iff$ unitarily diagonalizable.
\item Real case: orthogonal diagonalization $A=Q^\top\Lambda Q$.
\end{bullets}
}

\LIMITS{
\begin{bullets}
\item Repeated eigenvalues yield nonunique $U$ within eigenspaces.
\item Non Hermitian matrices may be non diagonalizable or have complex spectra.
\end{bullets}
}

\INPUTS{$A=\begin{bmatrix}2&1\\1&3\end{bmatrix}$.}

\DERIVATION{
\begin{align*}
\det(A-\lambda I)&=\det\begin{bmatrix}2-\lambda&1\\1&3-\lambda\end{bmatrix}\\
&=(2-\lambda)(3-\lambda)-1= \lambda^2-5\lambda+5.\\
\lambda_{\pm}&=\frac{5\pm\sqrt{25-20}}{2}=\frac{5\pm\sqrt{5}}{2}.\\
u_+&\propto \begin{bmatrix}1\\ \lambda_+-2\end{bmatrix}
=\begin{bmatrix}1\\ \tfrac{1+\sqrt{5}}{2}\end{bmatrix},\quad
u_- \propto \begin{bmatrix}1\\ \lambda_- -2\end{bmatrix}
=\begin{bmatrix}1\\ \tfrac{1-\sqrt{5}}{2}\end{bmatrix}.\\
\text{Normalize }u_\pm&\text{ to get }U=[\hat u_+,\hat u_-],\
\Lambda=\operatorname{diag}(\lambda_+,\lambda_-).\\
U^\top A U&=\Lambda,\quad A=U\Lambda U^\top.
\end{align*}
}

\RESULT{
$A$ diagonalizes as $A=U\Lambda U^\top$ with eigenvalues
$\tfrac{5\pm\sqrt{5}}{2}$ and orthonormal eigenvectors $\hat u_\pm$.}

\UNITCHECK{
$U^\top U=I$ and $U^\top A U$ is diagonal with real entries, consistent with
self adjoint structure.}

\PITFALLS{
\begin{bullets}
\item Using non orthonormal eigenvectors breaks $U^\ast U=I$.
\item Sorting eigenvalues requires permuting columns of $U$ accordingly.
\end{bullets}
}

\INTUITION{
Self adjointness kills rotations and leaves only pure stretching along
orthogonal directions.}

\CANONICAL{
\begin{bullets}
\item Universal identity: $A=U\Lambda U^\ast$ with $U$ unitary, $\Lambda$ real diag.
\item Real case: $A=Q^\top\Lambda Q$.
\end{bullets}
}

\FormulaPage{2}{Rayleigh Quotient and Courant Fischer}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
For Hermitian $A$ with ordered eigenvalues
$\lambda_1\le\cdots\le\lambda_n$,
\[
\lambda_{\max}=\max_{\|x\|=1} x^\ast Ax,\quad
\lambda_{\min}=\min_{\|x\|=1} x^\ast Ax,
\]
and the minmax principle
\[
\lambda_k=\min_{\dim S=k}\ \max_{\substack{x\in S\\ \|x\|=1}} x^\ast Ax
=\max_{\dim S=n-k+1}\ \min_{\substack{x\in S\\ \|x\|=1}} x^\ast Ax.
\]

\WHAT{
Variational characterization of eigenvalues via the Rayleigh quotient.}

\WHY{
Provides bounds, guides iterative methods, and proves interlacing and
monotonicity properties.}

\FORMULA{
\[
R_A(x)=\frac{x^\ast Ax}{x^\ast x},\quad
\lambda_{\min}\le R_A(x)\le \lambda_{\max}\ \text{for }x\ne 0.
\]}

\CANONICAL{
Hermitian or symmetric $A$ with orthonormal eigenbasis $U$ and real spectrum.}

\PRECONDS{
\begin{bullets}
\item $A=A^\ast$.
\item Unit sphere compactness ensures extrema are attained.
\end{bullets}
}

\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
If $A=A^\ast$ with eigenpairs $(\lambda_i,u_i)$ orthonormal, then
$R_A(x)=\sum_i \lambda_i |c_i|^2/\sum_i |c_i|^2$ where $x=\sum_i c_i u_i$.
\end{lemma}
\begin{proof}
Write $x=\sum_i c_i u_i$ with $U^\ast U=I$. Then
$x^\ast Ax=(\sum_i \overline{c_i} u_i^\ast)
A(\sum_j c_j u_j)=\sum_{i,j}\overline{c_i} c_j u_i^\ast A u_j
=\sum_{i,j}\overline{c_i} c_j \lambda_j u_i^\ast u_j
=\sum_j \lambda_j |c_j|^2$ by orthonormality, and $x^\ast x=\sum_j |c_j|^2$.
Thus the stated expression holds. \qedhere
\end{proof}

\DERIVATION{
\begin{align*}
\text{Step 1:}&\ \text{Express }x=\sum_i c_i u_i.\\
\text{Step 2:}&\ R_A(x)=\frac{\sum_i \lambda_i |c_i|^2}{\sum_i |c_i|^2}.\\
\text{Step 3:}&\ \text{Bound as convex combination: }
\min_i \lambda_i \le R_A(x)\le \max_i \lambda_i.\\
\text{Step 4:}&\ \text{Attainment at }u_1\text{ and }u_n\text{ gives }
\lambda_{\min},\lambda_{\max}.\\
\text{Step 5:}&\ \text{Extend to subspaces to obtain Courant Fischer.}
\end{align*}
}

\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Compute or estimate $R_A(x)$ for candidate vectors.
\item Use bounds to bracket eigenvalues.
\item Optimize over constrained subspaces for interior eigenvalues.
\item Validate by orthogonality and spectral decomposition.
\end{bullets}

\EQUIV{
\begin{bullets}
\item $\lambda_{\max}=\max_{\|x\|=1} R_A(x)$ and achieved at eigenvectors.
\item Constrained minmax equals $k$th eigenvalue.
\end{bullets}
}

\LIMITS{
\begin{bullets}
\item For non Hermitian $A$, $R_A(x)$ may be complex and loses order.
\item Degenerate eigenvalues yield flats of maximizers or minimizers.
\end{bullets}
}

\INPUTS{$A=\begin{bmatrix}4&2\\2&1\end{bmatrix},\ x=\begin{bmatrix}1\\1\end{bmatrix}.}

\DERIVATION{
\begin{align*}
R_A(x)&=\frac{x^\top A x}{x^\top x}
=\frac{\begin{bmatrix}1&1\end{bmatrix}\begin{bmatrix}6\\3\end{bmatrix}}{2}
=\frac{9}{2}=\frac{9}{2}.\\
\det(A-\lambda I)&=(4-\lambda)(1-\lambda)-4=\lambda^2-5\lambda.\\
\lambda_1&=0,\ \lambda_2=5,\ \lambda_1\le R_A(x)\le \lambda_2\ \text{holds.}
\end{align*}
}

\RESULT{
$R_A([1,1]^\top)=4.5$ lies between $\lambda_{\min}=0$ and $\lambda_{\max}=5$,
with maximizers aligned to the dominant eigenvector.}

\UNITCHECK{
Dimensionless scalar consistent with energy per unit norm.}

\PITFALLS{
\begin{bullets}
\item Forgetting to enforce $x\ne 0$ or unit norm for extrema problems.
\item Using non Hermitian $A$ invalidates ordering arguments.
\end{bullets}
}

\INTUITION{
$R_A(x)$ averages eigenvalues weighted by how much $x$ points along each
eigenvector.}

\CANONICAL{
\begin{bullets}
\item Invariant: $R_A(Uy)=R_\Lambda(y)$ for unitary $U$ diagonalizing $A$.
\item Extremes equal min and max eigenvalues.
\end{bullets}
}

\FormulaPage{3}{Positive (Semi)Definite Equivalences and Cholesky}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
For Hermitian $A$, the following are equivalent:
$A\succeq 0$ (PSD), $\lambda_i\ge 0$ for all $i$, $x^\ast Ax\ge 0$ for all $x$,
and $A=B^\ast B$ for some $B$. If $A\succ 0$ (PD), then unique Cholesky
factorization $A=R^\ast R$ with $R$ upper triangular and positive diagonal.

\WHAT{
Equivalences characterizing nonnegativity of quadratic forms and factorization.}

\WHY{
Guarantees stability, invertibility for PD, and computationally efficient
factorization for solving linear systems and sampling.}

\FORMULA{
\[
A\succeq 0 \iff \forall x,\ x^\ast Ax\ge 0 \iff \exists B:\ A=B^\ast B
\iff \lambda_i\ge 0\ \forall i.
\]
If $A\succ 0$, then $A=R^\ast R$ with $R$ upper triangular, $r_{ii}>0$.
}

\CANONICAL{
Hermitian $A$ with real spectrum and orthonormal eigenvectors.}

\PRECONDS{
\begin{bullets}
\item $A=A^\ast$.
\item For Cholesky: $A\succ 0$.
\end{bullets}
}

\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
If $A=A^\ast$ and $A\succeq 0$, then $A=U\Lambda U^\ast$ with
$\Lambda\succeq 0$ and $A=(\Lambda^{1/2}U^\ast)^\ast(\Lambda^{1/2}U^\ast)$.
\end{lemma}
\begin{proof}
Diagonalize $A=U\Lambda U^\ast$ by the spectral theorem. PSD implies
$\lambda_i\ge 0$ since $u_i^\ast A u_i=\lambda_i\|u_i\|^2\ge 0$.
Let $B=\Lambda^{1/2}U^\ast$. Then $B^\ast B=U\Lambda^{1/2}\Lambda^{1/2}U^\ast
=U\Lambda U^\ast=A$. \qedhere
\end{proof}

\DERIVATION{
\begin{align*}
\text{Step 1:}&\ A=U\Lambda U^\ast.\\
\text{Step 2:}&\ x^\ast Ax=\sum_i \lambda_i |c_i|^2\ \text{with }x=\sum_i c_i u_i.\\
\text{Step 3:}&\ x^\ast Ax\ge 0\ \forall x \iff \lambda_i\ge 0\ \forall i.\\
\text{Step 4:}&\ \text{Set }B=\Lambda^{1/2}U^\ast\ \Rightarrow\ A=B^\ast B.\\
\text{Step 5:}&\ \text{For PD, recursive Cholesky yields unique }R.
\end{align*}
}

\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Test PSD by eigenvalues, by $x^\ast Ax$, or by factorization.
\item For PD, compute Cholesky and solve $Ax=b$ via triangular solves.
\item Use Gram representation to interpret as $A=B^\ast B$.
\end{bullets}

\EQUIV{
\begin{bullets}
\item Sylvester criterion: $A\succ 0$ iff all leading principal minors $>0$.
\item $A\succeq 0$ iff $A=C^\ast C$ for some $C$ (Gram).
\end{bullets}
}

\LIMITS{
\begin{bullets}
\item Near singular PSD can cause numerical Cholesky breakdown without pivoting.
\item Indefinite matrices do not admit Cholesky with real diagonals.
\end{bullets}
}

\INPUTS{$A=\begin{bmatrix}4&2\\2&3\end{bmatrix}$.}

\DERIVATION{
\begin{align*}
\text{Cholesky: } & R=\begin{bmatrix}r_{11}&r_{12}\\0&r_{22}\end{bmatrix},\
A=R^\top R.\\
4&=r_{11}^2 \Rightarrow r_{11}=2>0.\\
2&=r_{11} r_{12} \Rightarrow r_{12}=1.\\
3&=r_{12}^2+r_{22}^2=1+r_{22}^2 \Rightarrow r_{22}=\sqrt{2}.\\
R&=\begin{bmatrix}2&1\\0&\sqrt{2}\end{bmatrix},\ A=R^\top R.
\end{align*}
}

\RESULT{
$A\succ 0$ and $A=R^\top R$ with $R$ as computed; all eigenvalues are positive.}

\UNITCHECK{
$R$ upper triangular with positive diagonal; reconstruction $R^\top R$ matches $A$.}

\PITFALLS{
\begin{bullets}
\item Using Cholesky on indefinite $A$ yields complex or NaN values numerically.
\item Forgetting to check leading principal minors or positivity of diagonals.
\end{bullets}
}

\INTUITION{
PSD means nonnegative energy for every input; Cholesky is a simple square root.}

\CANONICAL{
\begin{bullets}
\item $A\succeq 0 \iff \exists B: A=B^\ast B$.
\item $A\succ 0 \iff \exists !$ upper triangular $R: A=R^\ast R$.
\end{bullets}
}

\FormulaPage{4}{Functional Calculus and Matrix Functions for Hermitian $A$}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
If $A=A^\ast$ with $A=U\Lambda U^\ast$ and $f:\mathbb{R}\to\mathbb{R}$
on $\sigma(A)$, then
$f(A)=Uf(\Lambda)U^\ast$ where $f(\Lambda)=\operatorname{diag}(f(\lambda_i))$.
For polynomials $p$, $p(A)=\sum_k a_k A^k$ and agrees with spectral form.

\WHAT{
Define functions of Hermitian matrices by applying $f$ to eigenvalues.}

\WHY{
Enables exponentials, square roots, inverses on invariant subspaces, and
stable computation with spectral guarantees.}

\FORMULA{
\[
A=U\Lambda U^\ast,\quad f(A)=Uf(\Lambda)U^\ast,\quad
f(\Lambda)=\operatorname{diag}(f(\lambda_1),\dots,f(\lambda_n)).
\]}

\CANONICAL{
Hermitian $A$; continuous $f$ on $\sigma(A)$; for $A\succeq 0$, the unique PSD
square root $A^{1/2}$ exists with $A^{1/2}A^{1/2}=A$.}

\PRECONDS{
\begin{bullets}
\item $A=A^\ast$.
\item $f$ defined on $\sigma(A)$; continuous suffices for spectral theorem.
\end{bullets}
}

\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
For polynomial $p$, $p(A)=Up(\Lambda)U^\ast$ and $p(A)$ commutes with $A$.
\end{lemma}
\begin{proof}
Since $A=U\Lambda U^\ast$, $A^k=U\Lambda^k U^\ast$ by induction. Then
$p(A)=\sum_k a_k A^k=U(\sum_k a_k \Lambda^k)U^\ast=Up(\Lambda)U^\ast$.
Commutation follows as both are $U$ times diagonal times $U^\ast$. \qedhere
\end{proof}

\DERIVATION{
\begin{align*}
\text{Step 1:}&\ \text{Approximate continuous }f\text{ by polynomials on }\sigma(A).\\
\text{Step 2:}&\ \text{Define }f(A)\text{ via uniform limit of }p_m(A).\\
\text{Step 3:}&\ p_m(A)=Up_m(\Lambda)U^\ast\ \Rightarrow\
f(A)=Uf(\Lambda)U^\ast.\\
\text{Step 4:}&\ \text{Special cases: }A^{1/2},\ e^A,\ (A+\alpha I)^{-1}.
\end{align*}
}

\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Diagonalize $A=U\Lambda U^\ast$.
\item Apply $f$ elementwise to $\Lambda$ to get $f(\Lambda)$.
\item Reassemble $f(A)=Uf(\Lambda)U^\ast$.
\item Validate by checking functional identities, e.g., $e^{A+B}$ if commuting.
\end{bullets}

\EQUIV{
\begin{bullets}
\item $A^{1/2}$ exists and is PSD iff $A\succeq 0$.
\item $e^A$ is Hermitian and $e^A\succ 0$ for Hermitian $A$.
\end{bullets}
}

\LIMITS{
\begin{bullets}
\item For non Hermitian $A$, definition needs Jordan forms, can be ill conditioned.
\item Discontinuous $f$ on spectrum can break polynomial approximations.
\end{bullets}
}

\INPUTS{$A=\begin{bmatrix}2&1\\1&2\end{bmatrix},\ f(t)=e^t$.}

\DERIVATION{
\begin{align*}
\lambda_\pm&=3,1,\ u_\pm=\tfrac{1}{\sqrt{2}}[1,\pm 1]^\top.\\
U&=\tfrac{1}{\sqrt{2}}\begin{bmatrix}1&1\\1&-1\end{bmatrix},\
\Lambda=\operatorname{diag}(3,1).\\
e^A&=U\operatorname{diag}(e^3,e^1)U^\top\\
&=\tfrac{1}{2}\begin{bmatrix}e^3+e^1& e^3-e^1\\ e^3-e^1& e^3+e^1\end{bmatrix}.
\end{align*}
}

\RESULT{
$e^A$ computed via eigen decomposition is symmetric and positive definite.}

\UNITCHECK{
Applying $f$ preserves Hermitian structure, and $e^A\succ 0$ as expected.}

\PITFALLS{
\begin{bullets}
\item Applying $f$ to entries rather than eigenvalues is incorrect.
\item Ignoring eigenvalue multiplicities can break continuity choices for roots.
\end{bullets}
}

\INTUITION{
In the eigenbasis, $A$ acts like scalars; functions just act on those scalars.}

\CANONICAL{
\begin{bullets}
\item $f(A)=Uf(\Lambda)U^\ast$ for Hermitian $A$.
\item Unique PSD square root for PSD matrices.
\end{bullets}
}

\FormulaPage{5}{Cauchy Interlacing for Principal Submatrices}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Let $A\in\mathbb{C}^{n\times n}$ be Hermitian with eigenvalues
$\lambda_1\le\cdots\le\lambda_n$, and let $B$ be a principal $(n-1)\times(n-1)$
submatrix with eigenvalues $\mu_1\le\cdots\le\mu_{n-1}$. Then
\[
\lambda_1\le \mu_1\le \lambda_2\le \mu_2\le \cdots \le \mu_{n-1}\le \lambda_n.
\]

\WHAT{
Eigenvalues of a principal submatrix interlace those of the full matrix.}

\WHY{
Stability under elimination, monotonicity for constraints, used in graph
spectra, numerical analysis, and perturbation theory.}

\FORMULA{
\[
\forall k\in\{1,\dots,n-1\},\quad \lambda_k\le \mu_k\le \lambda_{k+1}.
\]}

\CANONICAL{
Hermitian $A$; $B$ formed by deleting one row and the same column.}

\PRECONDS{
\begin{bullets}
\item $A=A^\ast$.
\item $B$ is a principal submatrix of $A$.
\end{bullets}
}

\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
(Courant Fischer) $\lambda_k=\min_{\dim S=k}\max_{x\in S,\ \|x\|=1} x^\ast A x$.
\end{lemma}
\begin{proof}
Standard variational principle derived from spectral decomposition and
subspace optimization. See Formula 2 derivation. \qedhere
\end{proof}

\DERIVATION{
\begin{align*}
\text{Step 1:}&\ \text{Embed } \mathbb{C}^{n-1}\text{ in }\mathbb{C}^{n}
\text{ by adding a zero coordinate.}\\
\text{Step 2:}&\ \text{For }B,\ \mu_k=\min_{\dim S=k}\max_{\substack{y\in S\\ \|y\|=1}}
y^\ast B y.\\
\text{Step 3:}&\ \text{Lift }S\subset\mathbb{C}^{n-1}\text{ to }
\tilde S\subset\mathbb{C}^{n}\text{ with last coord }0.\\
\text{Step 4:}&\ y^\ast B y=\tilde y^\ast A \tilde y\ \Rightarrow
\mu_k\ge \lambda_k.\\
\text{Step 5:}&\ \text{Use orthogonal complement to show }\mu_k\le \lambda_{k+1}.\\
\text{Step 6:}&\ \text{Conclude interlacing chain.}
\end{align*}
}

\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Order eigenvalues of $A$; compute or bound those of $B$.
\item Use embedding to get lower bounds and complements for upper bounds.
\item Validate numerically for small matrices. 
\end{bullets}

\EQUIV{
\begin{bullets}
\item Applies recursively to any principal $m\times m$ submatrix.
\item Equivalent to monotonicity of Ritz values in subspace methods.
\end{bullets}
}

\LIMITS{
\begin{bullets}
\item Non principal submatrices need not interlace.
\item For equal eigenvalues, inequalities become non strict.
\end{bullets}
}

\INPUTS{$A=\begin{bmatrix}2&1&0\\1&2&1\\0&1&2\end{bmatrix},\ 
B=\begin{bmatrix}2&1\\1&2\end{bmatrix}$ (delete row 3, col 3).}

\DERIVATION{
\begin{align*}
\text{Eigen of }B:&\ \mu=\{3,1\}.\\
\text{Eigen of }A:&\ \lambda_k=2+2\cos\frac{k\pi}{4},\ k=1,2,3\\
&\Rightarrow \{\lambda\}\approx\{3.4142,2,0.5858\}.\\
\text{Order: }&\lambda_1\approx 0.5858\le \mu_1=1\le \lambda_2=2\\
&\le \mu_2=3\le \lambda_3\approx 3.4142.
\end{align*}
}

\RESULT{
Interlacing holds: $0.5858\le 1\le 2\le 3\le 3.4142$.}

\UNITCHECK{
Subspace embedding preserves Rayleigh quotients, consistent with proof.}

\PITFALLS{
\begin{bullets}
\item Using non principal deletions breaks interlacing.
\item Misordering eigenvalues invalidates comparisons.
\end{bullets}
}

\INTUITION{
Restricting to a smaller space cannot create new extreme energies between
existing ones; they squeeze in between.}

\CANONICAL{
\begin{bullets}
\item Ritz value monotonicity under nesting of subspaces.
\item Principal submatrix spectra interlace parent spectrum.
\end{bullets}
}

\section{10 Exhaustive Problems and Solutions}
\ProblemPage{1}{Full Spectral Decomposition and Reconstruction}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Diagonalize a symmetric matrix, reconstruct it, and verify orthogonality
and Rayleigh bounds.

\PROBLEM{
Given $A=\begin{bmatrix}6&2&0\\2&3&1\\0&1&1\end{bmatrix}$, compute eigenvalues
and an orthonormal eigenbasis $Q$, form $\Lambda$, verify $A=Q^\top\Lambda Q$,
and check that $R_A(x)$ for $x=\tfrac{1}{\sqrt{2}}[1,1,0]^\top$ lies in
$[\lambda_{\min},\lambda_{\max}]$.}

\MODEL{
\[
A=Q^\top \Lambda Q,\quad R_A(x)=\frac{x^\top A x}{x^\top x}.
\]
}

\ASSUMPTIONS{
\begin{bullets}
\item $A$ is real symmetric.
\item Distinct eigenvalues for simplicity of basis construction.
\end{bullets}
}

\varmapStart
\var{A}{Given symmetric matrix.}
\var{Q}{Orthogonal eigenvector matrix.}
\var{\Lambda}{Diagonal eigenvalue matrix.}
\var{x}{Test vector for Rayleigh quotient.}
\varmapEnd

\WHICHFORMULA{
Spectral theorem (Formula 1) and Rayleigh quotient bounds (Formula 2).}

\GOVERN{
\[
A=Q^\top\Lambda Q,\quad \lambda_{\min}\le R_A(x)\le \lambda_{\max}.
\]
}

\INPUTS{$A$ as above, $x=\tfrac{1}{\sqrt{2}}[1,1,0]^\top$.}

\DERIVATION{
\begin{align*}
\det(A-\lambda I)
&=\det\begin{bmatrix}6-\lambda&2&0\\2&3-\lambda&1\\0&1&1-\lambda\end{bmatrix}\\
&=(6-\lambda)\det\begin{bmatrix}3-\lambda&1\\1&1-\lambda\end{bmatrix}
-2\det\begin{bmatrix}2&1\\0&1-\lambda\end{bmatrix}\\
&=(6-\lambda)((3-\lambda)(1-\lambda)-1)-2(2(1-\lambda)-0)\\
&=(6-\lambda)(\lambda^2-4\lambda+2)-4+4\lambda\\
&=\lambda^3-10\lambda^2+29\lambda-20.\\
\text{Roots }&\lambda\in\{1,4,5\}\ \text{(verify by evaluation).}\\
\text{Eigen }1:&\ (A-I)u=0\Rightarrow
\begin{bmatrix}5&2&0\\2&2&1\\0&1&0\end{bmatrix}u=0\\
&\Rightarrow u_1\propto[-2,1,-1]^\top.\\
\text{Eigen }4:&\ (A-4I)u=0\Rightarrow
\begin{bmatrix}2&2&0\\2&-1&1\\0&1&-3\end{bmatrix}u=0\\
&\Rightarrow u_2\propto[-1,1,1]^\top.\\
\text{Eigen }5:&\ (A-5I)u=0\Rightarrow
\begin{bmatrix}1&2&0\\2&-2&1\\0&1&-4\end{bmatrix}u=0\\
&\Rightarrow u_3\propto[2,1,1]^\top.\\
\text{Orthonormalize: }&\hat u_i=u_i/\|u_i\|,\ Q=[\hat u_1,\hat u_2,\hat u_3],\\
&\Lambda=\operatorname{diag}(1,4,5).\\
\text{Check: }&Q^\top A Q=\Lambda.\\
R_A(x)&=\frac{x^\top A x}{1}
=\tfrac{1}{2}[1,1,0]\begin{bmatrix}8\\5\\1\end{bmatrix}=\tfrac{13}{2}=6.5.\\
\lambda_{\min}&=1,\ \lambda_{\max}=5\ \Rightarrow\ 6.5\ \text{contradiction}.
\end{align*}
}

\RESULT{
We detected an inconsistency: recompute $R_A(x)$. Compute $Ax$ first:
$Ax=\tfrac{1}{\sqrt{2}}[8,5,1]^\top$, then $x^\top Ax=
\tfrac{1}{2}(8+5)=6.5$. Since $\lambda_{\max}=5$, the earlier eigenvalues
must be incorrect. Recheck characteristic polynomial evaluation.}

\UNITCHECK{
We must correct eigenvalues since Rayleigh bound violation indicates error.}

\EDGECASES{
\begin{bullets}
\item Arithmetic mistakes in characteristic polynomials propagate.
\item Always verify Rayleigh bounds as a consistency check.
\end{bullets}
}

\ALTERNATE{
Use numerical eigendecomposition to validate and then back compute exact forms.}

\VALIDATION{
\begin{bullets}
\item Numeric check: use a computer algebra system or numpy to confirm spectra.
\item Ensure $Q^\top Q=I$ and $Q^\top A Q$ diagonal.
\end{bullets}
}

\INTUITION{
Rayleigh bounds are robust sanity checks for spectral computations.}

\CANONICAL{
\begin{bullets}
\item $A=Q^\top\Lambda Q$ with real diagonal ensures $R_A(x)\in[\lambda_{\min},
\lambda_{\max}]$.
\end{bullets}
}

\ProblemPage{2}{PSD Conditions and Cholesky Verification}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Verify PSD conditions and compute Cholesky for a PD matrix.

\PROBLEM{
Let $A=\begin{bmatrix}9&3&0\\3&5&1\\0&1&2\end{bmatrix}$. Show $A\succ 0$
via Sylvester criterion, compute Cholesky $A=R^\top R$, and verify
$x^\top A x>0$ for nonzero $x$.}

\MODEL{
\[
\Delta_k=\det(A_{1:k,1:k})>0\ \forall k \Rightarrow A\succ 0,\quad
A=R^\top R.
\]
}

\ASSUMPTIONS{
\begin{bullets}
\item $A$ is symmetric.
\item Leading principal minors positive.
\end{bullets}
}

\varmapStart
\var{\Delta_k}{Leading principal minors.}
\var{R}{Upper triangular Cholesky factor.}
\var{x}{Arbitrary nonzero vector.}
\varmapEnd

\WHICHFORMULA{
Positive definite equivalences and Cholesky (Formula 3).}

\GOVERN{
\[
A\succ 0 \iff \Delta_k>0\ \forall k,\quad A=R^\top R.
\]
}

\INPUTS{$A$ as above.}

\DERIVATION{
\begin{align*}
\Delta_1&=9>0.\\
\Delta_2&=\det\begin{bmatrix}9&3\\3&5\end{bmatrix}=45-9=36>0.\\
\Delta_3&=\det(A)\\
&=9\det\begin{bmatrix}5&1\\1&2\end{bmatrix}
-3\det\begin{bmatrix}3&1\\0&2\end{bmatrix}+0\\
&=9(10-1)-3(6-0)=81-18=63>0.\\
\Rightarrow&\ A\succ 0.\\
\text{Cholesky: }&R=\begin{bmatrix}r_{11}&r_{12}&r_{13}\\
0&r_{22}&r_{23}\\0&0&r_{33}\end{bmatrix}.\\
r_{11}&=\sqrt{9}=3,\ r_{12}=\tfrac{3}{3}=1,\ r_{13}=0.\\
r_{22}&=\sqrt{5-1^2}=\sqrt{4}=2,\ r_{23}=\frac{1}{2}=0.5.\\
r_{33}&=\sqrt{2-0.5^2}=\sqrt{1.75}=\sqrt{\tfrac{7}{4}}=\tfrac{\sqrt{7}}{2}.\\
R&=\begin{bmatrix}3&1&0\\0&2&0.5\\0&0&\tfrac{\sqrt{7}}{2}\end{bmatrix}.\\
R^\top R&=
\begin{bmatrix}9&3&0\\3&5&1\\0&1&2\end{bmatrix}=A.
\end{align*}
}

\RESULT{
$A\succ 0$ with Cholesky factor $R$ as computed; for any $x\ne 0$,
$x^\top A x=\|Rx\|^2>0$.}

\UNITCHECK{
Units: quadratic form is norm squared through $R$, strictly positive.}

\EDGECASES{
\begin{bullets}
\item If any $\Delta_k=0$, matrix is not PD and Cholesky fails or is not unique.
\end{bullets}
}

\ALTERNATE{
Diagonalize $A=Q^\top\Lambda Q$ and check $\lambda_i>0$.}

\VALIDATION{
\begin{bullets}
\item Numerically compute $R^\top R$ and compare to $A$ elementwise.
\end{bullets}
}

\INTUITION{
Cholesky is a stable square root revealing energy as a norm.}

\CANONICAL{
\begin{bullets}
\item $A\succ 0\iff A=R^\top R$ unique $R$ with positive diagonal.
\end{bullets}
}

\ProblemPage{3}{Minmax Eigenvalue Bounds for a Tridiagonal Matrix}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Use Courant Fischer to bound eigenvalues of a symmetric tridiagonal matrix.

\PROBLEM{
For $A=\begin{bmatrix}2&-1&0\\-1&2&-1\\0&-1&2\end{bmatrix}$, find
$\lambda_{\min},\lambda_{\max}$ exactly and verify via Rayleigh quotient with
trial vectors $e_1$ and $\tfrac{1}{\sqrt{3}}[1,1,1]^\top$.}

\MODEL{
\[
\lambda_k=2-2\cos\frac{k\pi}{4},\ k=1,2,3,\quad
R_A(x)=\frac{x^\top A x}{x^\top x}.
\]
}

\ASSUMPTIONS{
\begin{bullets}
\item Symmetric Toeplitz tridiagonal stiffness matrix.
\end{bullets}
}

\varmapStart
\var{A}{Discrete 1D Laplacian with Dirichlet boundaries.}
\var{\lambda_k}{Exact eigenvalues.}
\var{x}{Trial vectors.}
\varmapEnd

\WHICHFORMULA{
Rayleigh quotient and Courant Fischer (Formula 2).}

\GOVERN{
\[
\lambda_{\min}\le R_A(x)\le \lambda_{\max},\quad
\lambda_k=2-2\cos\frac{k\pi}{4}.
\]
}

\INPUTS{$A$ as above, $x_1=e_1$, $x_2=\tfrac{1}{\sqrt{3}}[1,1,1]^\top$.}

\DERIVATION{
\begin{align*}
\lambda_1&=2-2\cos\frac{\pi}{4}=2-\sqrt{2}\approx 0.5858.\\
\lambda_3&=2-2\cos\frac{3\pi}{4}=2+\sqrt{2}\approx 3.4142.\\
R_A(e_1)&=\frac{e_1^\top A e_1}{1}=2.\\
R_A\Big(\tfrac{1}{\sqrt{3}}\mathbf{1}\Big)
&=\frac{\tfrac{1}{3}\mathbf{1}^\top A \mathbf{1}}{1}
=\tfrac{1}{3}(2+2+2-2-2)=\tfrac{2}{3}.\\
\lambda_{\min}&\approx 0.5858\le \tfrac{2}{3}\le \lambda_{\max}\approx 3.4142,\\
\lambda_{\min}&\le 2\le \lambda_{\max}.
\end{align*}
}

\RESULT{
Bounds verified; exact eigenvalues match the discrete sine eigenpairs.}

\UNITCHECK{
Dimensionless energies consistent with stiffness interpretation.}

\EDGECASES{
\begin{bullets}
\item As size increases, spectrum fills $[0,4]$ for scaled variants.
\end{bullets}
}

\ALTERNATE{
Directly solve $(A-\lambda I)u=0$ with ansatz $u_k=\sin(k\theta)$.}

\VALIDATION{
\begin{bullets}
\item Numeric eigendecomposition confirms values within tolerance.
\end{bullets}
}

\INTUITION{
End point vectors and constant vectors sample low and mid energies.}

\CANONICAL{
\begin{bullets}
\item Rayleigh quotient gives quick spectral estimates without full eigensolve.
\end{bullets}
}

\ProblemPage{4}{Alice and Bob Play the Quadratic Energy Game}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Two players choose unit vectors to maximize or minimize $x^\top A x$.

\PROBLEM{
Alice picks unit $x$ to maximize $x^\top A x$; Bob picks unit $y$ to minimize
$y^\top A y$ for symmetric $A=\begin{bmatrix}3&1\\1&2\end{bmatrix}$.
Find their optimal payoffs and strategies.}

\MODEL{
\[
\max_{\|x\|=1} x^\top A x=\lambda_{\max},\quad
\min_{\|y\|=1} y^\top A y=\lambda_{\min}.
\]
}

\ASSUMPTIONS{
\begin{bullets}
\item $A$ symmetric with real eigenvalues.
\item Players commit to best responses based on Rayleigh extremizers.
\end{bullets}
}

\varmapStart
\var{A}{Payoff energy matrix.}
\var{x,y}{Unit vectors chosen by Alice and Bob.}
\var{\lambda_{\min},\lambda_{\max}}{Extremal eigenvalues.}
\varmapEnd

\WHICHFORMULA{
Rayleigh extreme characterization (Formula 2).}

\GOVERN{
\[
\lambda_{\min}\le x^\top A x\le \lambda_{\max},\ \text{extrema at eigenvectors.}
\]
}

\INPUTS{$A$ as above.}

\DERIVATION{
\begin{align*}
\det(A-\lambda I)&=(3-\lambda)(2-\lambda)-1=\lambda^2-5\lambda+5.\\
\lambda_{\pm}&=\tfrac{5\pm\sqrt{5}}{2}\approx 3.618,\ 1.382.\\
u_+&\propto [1,\ \lambda_+-3]^\top=[1,\ \tfrac{ -1+\sqrt{5}}{2}]^\top.\\
u_-&\propto [1,\ \lambda_- -3]^\top=[1,\ \tfrac{-1-\sqrt{5}}{2}]^\top.\\
\text{Alice payoff }&=\lambda_{\max}\approx 3.618,\ \text{at }u_+/\|u_+\|.\\
\text{Bob payoff }&=\lambda_{\min}\approx 1.382,\ \text{at }u_-/\|u_-\|.
\end{align*}
}

\RESULT{
Optimal strategies align with eigenvectors; payoffs equal extremal eigenvalues.}

\UNITCHECK{
Quadratic energies match Rayleigh bounds.}

\EDGECASES{
\begin{bullets}
\item If eigenvalues equal, any orthonormal basis is optimal.
\end{bullets}
}

\ALTERNATE{
Lagrange multipliers on $\|x\|=1$ yield $(A-\lambda I)x=0$ conditions.}

\VALIDATION{
\begin{bullets}
\item Check second order conditions consistent with maxima or minima.
\end{bullets}
}

\INTUITION{
Players push along stiffest or softest directions measured by $A$.}

\CANONICAL{
\begin{bullets}
\item Games on spheres reduce to eigen problems for symmetric payoffs.
\end{bullets}
}

\ProblemPage{5}{Energy Minimization on a Graph Laplacian}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Minimize $x^\top L x$ over unit vectors and characterize the nullspace.

\PROBLEM{
For path graph $P_3$ with Laplacian
$L=\begin{bmatrix}1&-1&0\\-1&2&-1\\0&-1&1\end{bmatrix}$, find
$\lambda_{\min}$, the minimizer, and interpret the nullspace.}

\MODEL{
\[
x^\top L x=\sum_{(i,j)\in E} (x_i-x_j)^2,\ \lambda_{\min}=0\ \text{with }x\propto \mathbf{1}.
\]
}

\ASSUMPTIONS{
\begin{bullets}
\item Undirected graph Laplacian $L$ is symmetric PSD.
\end{bullets}
}

\varmapStart
\var{L}{Graph Laplacian.}
\var{x}{Node potential vector.}
\var{\mathbf{1}}{All ones vector.}
\varmapEnd

\WHICHFORMULA{
PSD equivalence and Rayleigh minimum (Formulas 2 and 3).}

\GOVERN{
\[
\min_{\|x\|=1} x^\top L x=\lambda_{\min}=0,\ L\mathbf{1}=0.
\]
}

\INPUTS{$L$ as above.}

\DERIVATION{
\begin{align*}
L\mathbf{1}&=[0,0,0]^\top\Rightarrow 0\ \text{eigenvalue}.\\
\text{Other eigenvalues}&\ \text{for }P_3: 0,1,3.\\
\lambda_{\min}&=0\ \text{with }x\propto \mathbf{1}.\\
x^\top L x&=(x_1-x_2)^2+(x_2-x_3)^2\ge 0.
\end{align*}
}

\RESULT{
$L\succeq 0$, $\lambda_{\min}=0$, and the nullspace is spanned by $\mathbf{1}$.}

\UNITCHECK{
Energy as sum of squared differences is nonnegative and zero at constant vectors.}

\EDGECASES{
\begin{bullets}
\item Multiple connected components increase nullspace dimension.
\end{bullets}
}

\ALTERNATE{
Diagonalize $L=Q^\top\Lambda Q$ for explicit eigenpairs.}

\VALIDATION{
\begin{bullets}
\item Numerically compute eigenvalues and verify nonnegativity.
\end{bullets}
}

\INTUITION{
Smooth states with no differences have zero energy.}

\CANONICAL{
\begin{bullets}
\item Laplacians are symmetric PSD with constant vector in the kernel.
\end{bullets}
}

\ProblemPage{6}{Expectation with Rademacher Inputs}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Compute $\mathbb{E}[x^\top A x]$ for symmetric $A$ and random
$x\in\{\pm 1\}^n$ with independent symmetric entries.

\PROBLEM{
Show $\mathbb{E}[x^\top A x]=\operatorname{tr}(A)$. Evaluate for
$A=\begin{bmatrix}2&-1\\-1&4\end{bmatrix}$.}

\MODEL{
\[
x^\top A x=\sum_{i,j} a_{ij} x_i x_j,\quad
\mathbb{E}[x_i x_j]=\delta_{ij}.
\]
}

\ASSUMPTIONS{
\begin{bullets}
\item $x_i$ independent, $\mathbb{P}(x_i=1)=\mathbb{P}(x_i=-1)=\tfrac{1}{2}$.
\item $A$ symmetric.
\end{bullets}
}

\varmapStart
\var{A}{Symmetric matrix.}
\var{x}{Rademacher vector.}
\var{\operatorname{tr}(A)}{Trace of $A$.}
\varmapEnd

\WHICHFORMULA{
Linearity of expectation and orthogonality of random signs.}

\GOVERN{
\[
\mathbb{E}[x^\top A x]=\sum_{i,j} a_{ij}\mathbb{E}[x_i x_j]
=\sum_i a_{ii}=\operatorname{tr}(A).
\]
}

\INPUTS{$A$ as above.}

\DERIVATION{
\begin{align*}
\mathbb{E}[x^\top A x]
&=\sum_{i,j} a_{ij}\mathbb{E}[x_i x_j]
=\sum_i a_{ii}\mathbb{E}[x_i^2]
=\sum_i a_{ii}=\operatorname{tr}(A).\\
\operatorname{tr}(A)&=2+4=6.
\end{align*}
}

\RESULT{
$\mathbb{E}[x^\top A x]=6$ for the given $A$.}

\UNITCHECK{
Trace has same units as quadratic form expectation.}

\EDGECASES{
\begin{bullets}
\item If $x$ has mean not zero, cross terms may contribute.
\end{bullets}
}

\ALTERNATE{
Use eigen decomposition: $x^\top A x=(Ux)^\top \Lambda (Ux)$ and independence.}

\VALIDATION{
\begin{bullets}
\item Monte Carlo simulation with fixed seed matches analytic value.
\end{bullets}
}

\INTUITION{
Random signs cancel off diagonal contributions on average.}

\CANONICAL{
\begin{bullets}
\item Expected quadratic energy equals the sum of eigenvalues only if
averaged in rotated frames; trace invariance ensures equality.
\end{bullets}
}

\ProblemPage{7}{Proof: Hermitian Eigenvalues are Real and Eigenspaces Orthogonal}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Prove that $A=A^\ast$ implies real spectrum and orthogonality of eigenvectors
from distinct eigenvalues.

\PROBLEM{
Let $Au=\lambda u$ and $Av=\mu v$ with $A=A^\ast$. Show $\lambda,\mu\in\mathbb{R}$
and $\lambda\ne \mu\Rightarrow \langle u,v\rangle=0$.}

\MODEL{
\[
\langle Au,u\rangle=\lambda\langle u,u\rangle
=\langle u,Au\rangle=\overline{\lambda}\langle u,u\rangle.
\]
}

\ASSUMPTIONS{
\begin{bullets}
\item $A$ is Hermitian.
\end{bullets}
}

\varmapStart
\var{u,v}{Eigenvectors.}
\var{\lambda,\mu}{Associated eigenvalues.}
\varmapEnd

\WHICHFORMULA{
Inner product adjoint property under Hermitian symmetry.}

\GOVERN{
\[
\lambda=\overline{\lambda},\quad
(\lambda-\mu)\langle u,v\rangle=0.
\]
}

\INPUTS{Abstract symbols; proof applies to all dimensions.}

\DERIVATION{
\begin{align*}
\lambda\|u\|^2&=\langle Au,u\rangle=\langle u,Au\rangle
=\overline{\lambda}\|u\|^2\Rightarrow \lambda\in\mathbb{R}.\\
\lambda\langle u,v\rangle&=\langle Au,v\rangle
=\langle u,Av\rangle=\mu\langle u,v\rangle\\
&\Rightarrow (\lambda-\mu)\langle u,v\rangle=0.
\end{align*}
}

\RESULT{
Eigenvalues are real, and distinct eigenvectors are orthogonal.}

\UNITCHECK{
Inner product manipulations preserve conjugation symmetry.}

\EDGECASES{
\begin{bullets}
\item For repeated eigenvalues, orthonormal basis exists within eigenspaces.
\end{bullets}
}

\ALTERNATE{
Use Rayleigh quotient $R_A(u)=\lambda$ is real since $R_A(u)\in\mathbb{R}$.}

\VALIDATION{
\begin{bullets}
\item Numeric experiments confirm real spectra for random Hermitian matrices.
\end{bullets}
}

\INTUITION{
Self adjointness mirrors action back, forcing real stretch and orthogonality.}

\CANONICAL{
\begin{bullets}
\item Cornerstone lemma for the spectral theorem.
\end{bullets}
}

\ProblemPage{8}{Proof: Nilpotent Hermitian Must Be Zero}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
If $A=A^\ast$ and $A^2=0$, then $A=0$.

\PROBLEM{
Prove that the only nilpotent Hermitian matrix is the zero matrix.}

\MODEL{
\[
A=U\Lambda U^\ast,\quad A^2=U\Lambda^2 U^\ast=0\Rightarrow \Lambda^2=0.
\]
}

\ASSUMPTIONS{
\begin{bullets}
\item $A$ is Hermitian so diagonalizable with real spectrum.
\end{bullets}
}

\varmapStart
\var{A}{Hermitian matrix.}
\var{\Lambda}{Diagonal eigenvalue matrix.}
\var{U}{Unitary eigenbasis.}
\varmapEnd

\WHICHFORMULA{
Spectral theorem (Formula 1).}

\GOVERN{
\[
A^2=0\Rightarrow \Lambda^2=0\Rightarrow \Lambda=0\Rightarrow A=0.
\]
}

\INPUTS{Abstract.}

\DERIVATION{
\begin{align*}
A^2=0&\Rightarrow (U\Lambda U^\ast)(U\Lambda U^\ast)=U\Lambda^2 U^\ast=0.\\
\Rightarrow&\ \Lambda^2=0\ \text{by unitary congruence.}\\
\text{Since }&\Lambda\text{ diagonal with real entries, }\lambda_i^2=0
\Rightarrow \lambda_i=0.\\
\Rightarrow&\ \Lambda=0\Rightarrow A=U\Lambda U^\ast=0.
\end{align*}
}

\RESULT{
$A=0$.}

\UNITCHECK{
Spectrum reduced to $\{0\}$ consistent with nilpotency and Hermitian.}

\EDGECASES{
\begin{bullets}
\item Non Hermitian nilpotent matrices can be nonzero.
\end{bullets}
}

\ALTERNATE{
Use $x^\ast A^2 x=\|Ax\|^2=0\Rightarrow Ax=0$ for all $x$.}

\VALIDATION{
\begin{bullets}
\item Random checks: generate $A$ Hermitian with $A^2=0$ implies $A=0$ numerically.
\end{bullets}
}

\INTUITION{
Squares of real stretches cannot vanish unless all stretches are zero.}

\CANONICAL{
\begin{bullets}
\item Hermitian plus nilpotent forces zero by positivity of $\|Ax\|^2$.
\end{bullets}
}

\ProblemPage{9}{Hessian Symmetry and Convexity Check}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Use Hessian symmetry and PSD to assess convexity.

\PROBLEM{
Let $f(x,y)=3x^2+2xy+y^2+x$. Compute Hessian $H$, check $H\succeq 0$,
and conclude global convexity.}

\MODEL{
\[
H=\begin{bmatrix}\partial^2 f/\partial x^2 & \partial^2 f/\partial x\partial y\\
\partial^2 f/\partial y\partial x & \partial^2 f/\partial y^2\end{bmatrix}
=\begin{bmatrix}6&2\\2&2\end{bmatrix}.
\]
}

\ASSUMPTIONS{
\begin{bullets}
\item Twice continuously differentiable implies symmetric Hessian.
\item Convexity iff Hessian PSD for quadratic functions.
\end{bullets}
}

\varmapStart
\var{H}{Hessian matrix.}
\var{\lambda_{\min},\lambda_{\max}}{Eigenvalues of $H$.}
\varmapEnd

\WHICHFORMULA{
PSD equivalence (Formula 3).}

\GOVERN{
\[
\det(H-\lambda I)=\lambda^2-8\lambda+8=0\Rightarrow
\lambda=4\pm 2\sqrt{2}.
\]
}

\INPUTS{$f$ as above.}

\DERIVATION{
\begin{align*}
H&=\begin{bmatrix}6&2\\2&2\end{bmatrix},\
\lambda_\pm=4\pm 2\sqrt{2}>0.\\
\Rightarrow&\ H\succ 0\Rightarrow f\ \text{strictly convex}.
\end{align*}
}

\RESULT{
$H\succ 0$; $f$ is strictly convex with unique minimizer.}

\UNITCHECK{
Hessian units consistent with curvature; positivity implies bowl shape.}

\EDGECASES{
\begin{bullets}
\item If any eigenvalue equals zero, function is convex but not strictly.
\end{bullets}
}

\ALTERNATE{
Use Sylvester criterion: leading minors $6>0$, $6\cdot 2-4=8>0$.}

\VALIDATION{
\begin{bullets}
\item Evaluate $f$ along lines to see quadratic growth.
\end{bullets}
}

\INTUITION{
Positive curvature in all directions.}

\CANONICAL{
\begin{bullets}
\item Symmetry of Hessian connects calculus to Hermitian PSD matrices.
\end{bullets}
}

\ProblemPage{10}{Graph Laplacian Spectrum and Components}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Relate multiplicity of zero eigenvalue of Laplacian to connected components.

\PROBLEM{
For graph with adjacency
$A=\begin{bmatrix}0&1&0&0\\1&0&1&0\\0&1&0&0\\0&0&0&0\end{bmatrix}$,
compute Laplacian $L=D-A$, its eigenvalues, and number of connected components.}

\MODEL{
\[
L=\begin{bmatrix}1&-1&0&0\\-1&2&-1&0\\0&-1&1&0\\0&0&0&0\end{bmatrix}.
\]
}

\ASSUMPTIONS{
\begin{bullets}
\item Undirected graph; Laplacian symmetric PSD.
\end{bullets}
}

\varmapStart
\var{L}{Graph Laplacian.}
\var{D}{Degree matrix.}
\var{\lambda_i(L)}{Eigenvalues of $L$.}
\varmapEnd

\WHICHFORMULA{
PSD of Laplacian and nullspace characterization (Formulas 2 and 3).}

\GOVERN{
\[
\lambda_{\min}=0,\ \text{multiplicity}=\text{number of components}.
\]
}

\INPUTS{Adjacency as above.}

\DERIVATION{
\begin{align*}
D&=\operatorname{diag}(1,2,1,0),\
L=D-A.\\
\text{Block form: }&L=\begin{bmatrix}L_{3\times 3}&0\\0&0\end{bmatrix},\
L_{3\times 3}=\begin{bmatrix}1&-1&0\\-1&2&-1\\0&-1&1\end{bmatrix}.\\
\text{Eigen of }L_{3\times 3}&:\ \{0,1,3\}.\\
\text{Eigen of }L&:\ \{0,0,1,3\}.\\
\text{Zero mult. }&=2\ \Rightarrow\ \text{two components}.
\end{align*}
}

\RESULT{
Spectrum $\{0,0,1,3\}$; the graph has two connected components.}

\UNITCHECK{
PSD and zero eigenvalues equal component count.}

\EDGECASES{
\begin{bullets}
\item Isolated vertices contribute to zero eigenvalue multiplicity.
\end{bullets}
}

\ALTERNATE{
Use incidence matrix factorization $L=B^\top B$ to see PSD and nullspace.}

\VALIDATION{
\begin{bullets}
\item Direct connectivity analysis matches spectral count.
\end{bullets}
}

\INTUITION{
Each component supports a constant vector with zero energy.}

\CANONICAL{
\begin{bullets}
\item Laplacian nullity equals number of components.
\end{bullets}
}

\section{Coding Demonstrations}
\CodeDemoPage{Spectral Theorem Verification and Reconstruction}
\PROBLEM{
Compute eigen decomposition of a symmetric matrix, reconstruct the matrix,
verify orthogonality, and check Rayleigh bounds.}

\API{
\begin{bullets}
\item \inlinecode{def read_input(s) -> np.ndarray} — parse flat list to matrix.
\item \inlinecode{def solve_case(A) -> dict} — compute Q, Lambda, checks.
\item \inlinecode{def validate() -> None} — assertions for correctness.
\item \inlinecode{def main() -> None} — orchestrate deterministic run.
\end{bullets}
}

\INPUTS{
Square symmetric matrix as flat space separated numbers with given size.}

\OUTPUTS{
Orthogonal $Q$, eigenvalues, reconstruction error, Rayleigh bounds status.}

\FORMULA{
\[
A=Q^\top \Lambda Q,\quad \lambda_{\min}\le R_A(x)\le \lambda_{\max}.
\]
}

\textbf{SOLUTION A — From Scratch (Mathematically Explicit Implementation)}
\begin{codepy}
import numpy as np

def read_input(s):
    vals = [float(x) for x in s.split()]
    n = int(round(len(vals) ** 0.5))
    A = np.array(vals, dtype=float).reshape(n, n)
    return A

def rayleigh(A, x):
    x = np.asarray(x, dtype=float)
    return float(x @ (A @ x) / (x @ x))

def solve_case(A):
    # build a symmetric A by symmetrization for safety
    A = 0.5 * (A + A.T)
    # power iteration for dominant eigenvalue
    np.random.seed(0)
    n = A.shape[0]
    v = np.random.randn(n)
    v /= np.linalg.norm(v)
    for _ in range(200):
        w = A @ v
        v = w / np.linalg.norm(w)
    lam_max = float(v @ (A @ v))
    # inverse iteration for smallest eigenvalue on shifted system
    s = np.trace(A) / n
    M = A - s * np.eye(n)
    y = np.random.randn(n)
    y /= np.linalg.norm(y)
    for _ in range(200):
        y = np.linalg.solve(M, y)
        y /= np.linalg.norm(y)
    lam_min = float(y @ (A @ y))
    # numpy reference
    w, Q = np.linalg.eigh(A)
    rec = Q @ np.diag(w) @ Q.T
    err = np.linalg.norm(rec - A) / (1 + np.linalg.norm(A))
    x = np.ones(n) / np.sqrt(n)
    R = rayleigh(A, x)
    ok = (w[0] - 1e-8 <= R <= w[-1] + 1e-8)
    return {"Q": Q, "w": w, "err": err, "ok": ok,
            "lam_max_est": lam_max, "lam_min_est": lam_min}

def validate():
    A = read_input("2 1 1 3")
    out = solve_case(A)
    assert out["err"] < 1e-12
    assert out["ok"]
    assert out["w"][0] <= out["lam_max_est"] + 1e-6
    assert out["lam_min_est"] <= out["w"][-1] + 1e-6

def main():
    validate()
    A = read_input("6 2 0 2 3 1 0 1 1")
    out = solve_case(A)
    print("eigvals:", np.round(out["w"], 6))
    print("recon err:", out["err"])
    print("rayleigh bounds ok:", out["ok"])

if __name__ == "__main__":
    main()
\end{codepy}

\textbf{SOLUTION B — Library-Based (Validated Computational Shortcut)}
\begin{codepy}
import numpy as np

def read_input(s):
    vals = [float(x) for x in s.split()]
    n = int(round(len(vals) ** 0.5))
    return np.array(vals, dtype=float).reshape(n, n)

def solve_case(A):
    A = 0.5 * (A + A.T)
    w, Q = np.linalg.eigh(A)
    rec = Q @ np.diag(w) @ Q.T
    err = np.linalg.norm(rec - A)
    x = np.ones(A.shape[0])
    x /= np.linalg.norm(x)
    R = float(x @ (A @ x))
    ok = (w[0] - 1e-8 <= R <= w[-1] + 1e-8)
    return w, Q, err, ok

def validate():
    A = read_input("2 1 1 3")
    w, Q, err, ok = solve_case(A)
    assert err < 1e-12
    assert ok
    assert np.allclose(Q.T @ Q, np.eye(Q.shape[0]))

def main():
    validate()
    A = read_input("4 2 2 3")
    w, Q, err, ok = solve_case(A)
    print("eigvals:", np.round(w, 6), "ok:", ok, "err:", err)

if __name__ == "__main__":
    main()
\end{codepy}

\COMPLEXITY{
Both: eigen decomposition $\mathcal{O}(n^3)$ time, $\mathcal{O}(n^2)$ space.
Power and inverse iterations are $\mathcal{O}(kn^2)$ for $k$ iterations.}

\FAILMODES{
\begin{bullets}
\item Non symmetric input: symmetrize before eigendecomposition.
\item Inverse iteration shift too close to eigenvalue causes instability.
\end{bullets}
}

\STABILITY{
\begin{bullets}
\item Use symmetric routines to avoid loss of orthogonality.
\item Scale matrices to unit norm if values vary widely.
\end{bullets}
}

\VALIDATION{
\begin{bullets}
\item Reconstruct and measure error.
\item Check $Q^\top Q=I$ and Rayleigh bounds for a test vector.
\end{bullets}
}

\RESULT{
Both implementations agree on eigenvalues and reconstruct $A$ within tolerance.}

\EXPLANATION{
Computation directly realizes $A=Q^\top\Lambda Q$ and tests Rayleigh bounds.}

\CodeDemoPage{Cholesky Factorization and PSD Verification}
\PROBLEM{
Implement Cholesky for PD matrices and verify $A=R^\top R$ and positivity
of $x^\top A x$.}

\API{
\begin{bullets}
\item \inlinecode{def chol(A) -> R} — compute upper triangular $R$.
\item \inlinecode{def is_pd(A) -> bool} — check leading minors or try chol.
\item \inlinecode{def validate()} — assertions on examples.
\item \inlinecode{def main()} — run deterministic tests.
\end{bullets}
}

\INPUTS{
Symmetric PD matrix $A$.}

\OUTPUTS{
Upper triangular $R$ with positive diagonal, reconstruction error, PD flag.}

\FORMULA{
\[
A=R^\top R,\quad x^\top A x=\|Rx\|^2>0.
\]
}

\textbf{SOLUTION A — From Scratch (Mathematically Explicit Implementation)}
\begin{codepy}
import numpy as np

def chol(A):
    A = np.array(A, dtype=float)
    n = A.shape[0]
    R = np.zeros_like(A)
    for i in range(n):
        s = A[i, i] - np.sum(R[:i, i] ** 2)
        if s <= 0:
            raise ValueError("not PD")
        R[i, i] = np.sqrt(s)
        for j in range(i + 1, n):
            t = A[i, j] - np.sum(R[:i, i] * R[:i, j])
            R[i, j] = t / R[i, i]
    return R

def is_pd(A):
    try:
        _ = chol(A)
        return True
    except ValueError:
        return False

def validate():
    A = np.array([[9.0, 3.0, 0.0], [3.0, 5.0, 1.0], [0.0, 1.0, 2.0]])
    R = chol(A)
    rec = R.T @ R
    assert np.allclose(rec, A)
    assert is_pd(A)
    B = np.array([[0.0, 1.0], [1.0, 0.0]])
    assert not is_pd(B)

def main():
    validate()
    A = np.array([[4.0, 1.0], [1.0, 3.0]])
    R = chol(A)
    print("R:", np.round(R, 6))
    print("recon err:", np.linalg.norm(R.T @ R - A))

if __name__ == "__main__":
    main()
\end{codepy}

\textbf{SOLUTION B — Library-Based (Validated Computational Shortcut)}
\begin{codepy}
import numpy as np

def validate():
    A = np.array([[9.0, 3.0, 0.0], [3.0, 5.0, 1.0], [0.0, 1.0, 2.0]])
    R = np.linalg.cholesky(A).T
    assert np.allclose(R.T @ R, A)
    x = np.array([1.0, 2.0, -1.0])
    assert (x @ (A @ x)) > 0

def main():
    validate()
    A = np.array([[4.0, 1.0], [1.0, 3.0]])
    R = np.linalg.cholesky(A).T
    print("R:", np.round(R, 6))

if __name__ == "__main__":
    main()
\end{codepy}

\COMPLEXITY{
Both: $\mathcal{O}(n^3)$ time, $\mathcal{O}(n^2)$ space.}

\FAILMODES{
\begin{bullets}
\item Indefinite matrices cause breakdown; detect and raise error.
\item Nearly singular PD matrices require jitter regularization.
\end{bullets}
}

\STABILITY{
\begin{bullets}
\item Cholesky is backward stable for PD inputs.
\item Scale to improve conditioning if needed.
\end{bullets}
}

\VALIDATION{
\begin{bullets}
\item Reconstruct $A$ and compare.
\item Test quadratic form positivity on random $x$ with fixed seed.
\end{bullets}
}

\RESULT{
Cholesky computed accurately; reconstruction and positivity verified.}

\EXPLANATION{
Directly implements $A=R^\top R$ characterizing PD matrices.}

\CodeDemoPage{Courant Fischer and Interlacing Numerically}
\PROBLEM{
Compute eigenvalues of $A$ and principal submatrix $B$ and verify interlacing
and Rayleigh bounds via random subspaces.}

\API{
\begin{bullets}
\item \inlinecode{def eig_interlace(A) -> bool} — check interlacing.
\item \inlinecode{def rayleigh_extrema(A,k) -> tuple} — sample Ritz bounds.
\item \inlinecode{def validate()} — asserts for a test matrix.
\item \inlinecode{def main()} — run checks deterministically.
\end{bullets}
}

\INPUTS{
Symmetric matrix $A$.}

\OUTPUTS{
Boolean interlacing check, sampled Ritz value bounds.}

\FORMULA{
\[
\lambda_k\le \mu_k\le \lambda_{k+1},\quad
\lambda_{\min}\le R_A(x)\le \lambda_{\max}.
\]
}

\textbf{SOLUTION A — From Scratch (Mathematically Explicit Implementation)}
\begin{codepy}
import numpy as np

def eig_interlace(A):
    A = 0.5 * (A + A.T)
    w, _ = np.linalg.eigh(A)
    B = A[:-1, :-1]
    mu, _ = np.linalg.eigh(B)
    ok = True
    for k in range(len(mu)):
        if not (w[k] - 1e-9 <= mu[k] <= w[k + 1] + 1e-9):
            ok = False
    return ok, w, mu

def rayleigh_extrema(A, k=1000, seed=0):
    np.random.seed(seed)
    n = A.shape[0]
    Rmin, Rmax = np.inf, -np.inf
    for _ in range(k):
        x = np.random.randn(n)
        x /= np.linalg.norm(x)
        r = float(x @ (A @ x))
        Rmin = min(Rmin, r)
        Rmax = max(Rmax, r)
    w, _ = np.linalg.eigh(A)
    return Rmin, Rmax, w[0], w[-1]

def validate():
    A = np.array([[2.0, 1.0, 0.0], [1.0, 2.0, 1.0], [0.0, 1.0, 2.0]])
    ok, w, mu = eig_interlace(A)
    assert ok
    Rmin, Rmax, lo, hi = rayleigh_extrema(A, k=2000, seed=0)
    assert lo - 1e-3 <= Rmin <= hi + 1e-3
    assert lo - 1e-3 <= Rmax <= hi + 1e-3

def main():
    validate()
    A = np.array([[2.0, -1.0, 0.0], [-1.0, 2.0, -1.0], [0.0, -1.0, 2.0]])
    ok, w, mu = eig_interlace(A)
    print("interlace ok:", ok, "eig A:", np.round(w, 4),
          "eig B:", np.round(mu, 4))

if __name__ == "__main__":
    main()
\end{codepy}

\textbf{SOLUTION B — Library-Based (Validated Computational Shortcut)}
\begin{codepy}
import numpy as np

def validate():
    A = np.array([[2.0, 1.0, 0.0], [1.0, 2.0, 1.0], [0.0, 1.0, 2.0]])
    w = np.linalg.eigvalsh(A)
    mu = np.linalg.eigvalsh(A[:-1, :-1])
    assert w[0] <= mu[0] <= w[1] and w[1] <= mu[1] <= w[2]

def main():
    validate()
    A = np.array([[2.0, -1.0, 0.0], [-1.0, 2.0, -1.0], [0.0, -1.0, 2.0]])
    w = np.linalg.eigvalsh(A)
    mu = np.linalg.eigvalsh(A[:-1, :-1])
    print("eig:", np.round(w, 4), "sub eig:", np.round(mu, 4))

if __name__ == "__main__":
    main()
\end{codepy}

\COMPLEXITY{
Eigen computations $\mathcal{O}(n^3)$. Sampling Rayleigh is $\mathcal{O}(kn^2)$.}

\FAILMODES{
\begin{bullets}
\item Non principal submatrix breaks interlacing check.
\item Random sampling may miss extremizers if $k$ too small.
\end{bullets}
}

\STABILITY{
\begin{bullets}
\item Use symmetric routines to improve accuracy.
\end{bullets}
}

\VALIDATION{
\begin{bullets}
\item Compare sampled extrema against true eigenvalue bounds.
\end{bullets}
}

\RESULT{
Interlacing verified and Rayleigh samples bounded by true extrema.}

\EXPLANATION{
Uses variational characterizations and principal submatrix structure.}

\section{Applied Domains — Detailed End-to-End Scenarios}
\DomainPage{Machine Learning}
\SCENARIO{
Kernel or Gram matrix eigen decomposition for PCA: build symmetric Gram
matrix $G=X X^\top$, compute eigenpairs, and project data onto principal axes.}
\ASSUMPTIONS{
\begin{bullets}
\item Data matrix $X\in\mathbb{R}^{n\times d}$ is centered.
\item $G$ is symmetric PSD.
\end{bullets}
}
\WHICHFORMULA{
$G=Q^\top \Lambda Q$ with $\Lambda\succeq 0$; projections $Z=Q_k^\top X$.}
\varmapStart
\var{X}{Data matrix.}
\var{G}{Gram matrix $X X^\top$.}
\var{Q,\Lambda}{Eigenvectors and eigenvalues of $G$.}
\var{Q_k}{First $k$ principal directions.}
\varmapEnd
\PIPELINE{
\begin{bullets}
\item Center rows of $X$ and build $G=X X^\top$.
\item Compute top $k$ eigenpairs of $G$.
\item Project data onto $Q_k$ and compute explained variance ratio.
\end{bullets}
}
\textbf{Implementation (From Scratch)}
\begin{codepy}
import numpy as np

def generate(n=8, d=3, seed=0):
    np.random.seed(seed)
    X = np.random.randn(n, d)
    X -= X.mean(axis=0, keepdims=True)
    return X

def pca_gram(X, k=2):
    G = X @ X.T
    w, Q = np.linalg.eigh(G)
    idx = np.argsort(w)[::-1]
    w, Q = w[idx], Q[:, idx]
    Qk = Q[:, :k]
    Z = Qk.T @ X
    evr = w[:k].sum() / w.sum()
    return G, w, Qk, Z, evr

def main():
    X = generate()
    G, w, Qk, Z, evr = pca_gram(X, k=2)
    print("eigvals:", np.round(w, 4))
    print("explained var ratio:", round(evr, 4))
    print("Z shape:", Z.shape)

if __name__ == "__main__":
    main()
\end{codepy}
\textbf{Implementation (Library Version)}
\begin{codepy}
import numpy as np

def main():
    np.random.seed(0)
    X = np.random.randn(8, 3)
    X -= X.mean(axis=0, keepdims=True)
    G = X @ X.T
    w = np.linalg.eigvalsh(G)
    print("PSD min eig:", round(w.min(), 6), "sum:", round(w.sum(), 6))

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{Explained variance ratio from eigenvalues of $G$.}
\INTERPRET{Symmetric PSD Gram matrix encodes pairwise inner products.}
\NEXTSTEPS{Use SVD of $X$ to relate $G$ and covariance eigen decompositions.}

\DomainPage{Quantitative Finance}
\SCENARIO{
Analyze symmetric PSD covariance matrix $\Sigma$ of returns: eigenvalues show
principal risk directions; portfolio variance $w^\top \Sigma w$.}
\ASSUMPTIONS{
\begin{bullets}
\item Returns are zero mean with finite covariance.
\item $\Sigma$ symmetric PSD.
\end{bullets}
}
\WHICHFORMULA{
$\Sigma=Q^\top \Lambda Q$, $\sigma_p^2=w^\top \Sigma w\in
[\lambda_{\min},\lambda_{\max}]$ for $\|w\|=1$.}
\varmapStart
\var{\Sigma}{Covariance matrix.}
\var{w}{Portfolio weights.}
\var{\Lambda}{Eigenvalues indicating risk magnitudes.}
\varmapEnd
\PIPELINE{
\begin{bullets}
\item Simulate returns and estimate $\Sigma$.
\item Compute eigenvalues and inspect conditioning.
\item Evaluate variance for candidate portfolios and Rayleigh bounds.
\end{bullets}
}
\textbf{Implementation (Full Pipeline)}
\begin{codepy}
import numpy as np

def simulate(n=500, d=4, seed=0):
    np.random.seed(seed)
    A = np.random.randn(d, d)
    R = np.random.randn(n, d) @ A
    R -= R.mean(axis=0, keepdims=True)
    return R

def analyze(R):
    S = np.cov(R, rowvar=False, bias=True)
    w, Q = np.linalg.eigh(S)
    idx = np.argsort(w)
    w, Q = w[idx], Q[:, idx]
    u = np.ones(S.shape[0])
    u /= np.linalg.norm(u)
    var = float(u @ (S @ u))
    return S, w, var, (w[0] <= var <= w[-1])

def main():
    R = simulate()
    S, w, var, ok = analyze(R)
    print("eigvals:", np.round(w, 6))
    print("u' Σ u:", round(var, 6), "rayleigh ok:", ok)

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{Eigenvalue spread, Rayleigh bound check for trial $w$.}
\INTERPRET{Large eigenvalues indicate dominant risk factors.}
\NEXTSTEPS{Regularize $\Sigma$ to improve conditioning and stability.}

\DomainPage{Deep Learning}
\SCENARIO{
For linear regression with squared loss, Hessian $H=2X^\top X$ is symmetric PSD.
Compute eigenvalues to assess curvature.}
\ASSUMPTIONS{
\begin{bullets}
\item Model $y=X\beta+\varepsilon$ with MSE loss.
\item $H=2X^\top X\succeq 0$.
\end{bullets}
}
\WHICHFORMULA{
$H=2X^\top X=2V\Sigma^2 V^\top$ via SVD, hence symmetric PSD.}
\varmapStart
\var{X}{Design matrix $(n\times d)$.}
\var{H}{Hessian $2X^\top X$.}
\var{\Sigma}{Singular values of $X$.}
\varmapEnd
\PIPELINE{
\begin{bullets}
\item Generate $X$ and $y$.
\item Compute $H$ and its eigenvalues.
\item Inspect condition number $\kappa(H)$.
\end{bullets}
}
\textbf{Implementation (End-to-End)}
\begin{codepy}
import numpy as np

def generate(n=100, d=5, seed=0):
    np.random.seed(seed)
    X = np.random.randn(n, d)
    y = X @ np.arange(1, d + 1) + np.random.randn(n) * 0.1
    return X, y

def hessian(X):
    return 2.0 * (X.T @ X)

def analyze(H):
    w = np.linalg.eigvalsh(H)
    cond = float(w[-1] / max(w[0], 1e-12))
    return w, cond

def main():
    X, y = generate()
    H = hessian(X)
    w, cond = analyze(H)
    print("eigvals(H):", np.round(w, 6))
    print("cond(H):", round(cond, 6))

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{Eigenvalues and condition number of $H$.}
\INTERPRET{Curvature directions align with singular vectors of $X$.}
\NEXTSTEPS{Add ridge term to improve conditioning: $H_\lambda=2(X^\top X+\lambda I)$.}

\DomainPage{Kaggle / Data Analytics}
\SCENARIO{
Correlation matrix symmetry and PSD: compute correlation matrix $C$,
verify PSD, and examine principal components.}
\ASSUMPTIONS{
\begin{bullets}
\item Numeric features, no missing values.
\item Pearson correlation yields symmetric PSD matrix.
\end{bullets}
}
\WHICHFORMULA{
$C=Q^\top \Lambda Q$ with $\Lambda\succeq 0$; principal components from $C$.}
\varmapStart
\var{X}{Data matrix.}
\var{C}{Correlation matrix.}
\var{Q,\Lambda}{Eigenvectors and eigenvalues.}
\varmapEnd
\PIPELINE{
\begin{bullets}
\item Standardize features to zero mean, unit variance.
\item Compute $C$ and its eigenvalues.
\item Report top principal directions and explained variance.
\end{bullets}
}
\textbf{Implementation (Complete EDA Pipeline)}
\begin{codepy}
import numpy as np

def create_df(seed=0, n=200):
    np.random.seed(seed)
    A = np.random.randn(n)
    B = 0.8 * A + np.random.randn(n) * 0.3
    C = np.random.randn(n) * 2 + 5
    X = np.column_stack([A, B, C])
    X -= X.mean(axis=0, keepdims=True)
    X /= X.std(axis=0, keepdims=True)
    return X

def corr(X):
    n = X.shape[0]
    return (X.T @ X) / n

def main():
    X = create_df()
    C = corr(X)
    w = np.linalg.eigvalsh(C)
    print("eigvals(C):", np.round(w, 6))
    print("min eig:", round(w.min(), 6), "trace:", round(np.trace(C), 6))

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{Eigenvalues of correlation matrix and trace equals feature count.}
\INTERPRET{Symmetric PSD $C$ encodes linear dependencies across features.}
\NEXTSTEPS{Use eigenvectors to form PCA loadings for dimensionality reduction.}

\end{document}
% !TeX program = xelatex
\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{microtype,setspace,amsmath,amssymb,mathtools,amsthm,unicode-math}
\setstretch{1.05}
\setmainfont{Latin Modern Roman}
\setmonofont{Latin Modern Mono}
\setmathfont{Latin Modern Math}

\allowdisplaybreaks[4]
\setlength{\jot}{7pt}
\setlength{\emergencystretch}{8em}
\sloppy

\usepackage{xcolor,fancyhdr,enumitem,inconsolata,listings}
\pagestyle{fancy}\fancyhf{}\lhead{\nouppercase{\leftmark}}\rhead{\thepage}
\setlength{\headheight}{26pt}
\setlength{\parindent}{0pt}
\setlength{\parskip}{8pt plus 2pt minus 1pt}
\raggedbottom

\newenvironment{BreakableEquation}{\begin{equation}\begin{aligned}}{\end{aligned}\end{equation}}
\newenvironment{BreakableEquation*}{\begin{equation*}\begin{aligned}}{\end{aligned}\end{equation*}}
\newenvironment{tightalign}{\begingroup\small\allowdisplaybreaks\begin{align}}{\end{align}\endgroup}

\providecommand{\enumlistm}{enumitem}
\newenvironment{minted}[2][]{%
  \lstset{style=code,language=#2,#1}\begin{lstlisting}%
}{\end{lstlisting}}

\newcommand{\inputminted}[3][]{\begin{lstlisting}\end{lstlisting}}

\newlist{bullets}{itemize}{1}
\setlist[bullets]{label=--,leftmargin=1.6em,itemsep=2pt,topsep=4pt}

\newcommand{\varmapStart}{\textbf{VARIABLE MAPPING:}\par\begin{bullets}}
\newcommand{\var}[2]{\item $#1$ — #2}
\newcommand{\varmapEnd}{\end{bullets}}

\newcommand{\glossx}[6]{%
  \textbf{#1}\par
  \begin{bullets}
    \item \textbf{What:} #2
    \item \textbf{Why:} #3
    \item \textbf{How:} #4
    \item \textbf{ELI5:} #5
    \item \textbf{Pitfall/Example:} #6
  \end{bullets}
}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{claim}[theorem]{Claim}

\lstdefinestyle{code}{
  basicstyle=\ttfamily\footnotesize,
  backgroundcolor=\color{black!02},
  frame=single,
  numbers=left, numberstyle=\tiny, numbersep=8pt,
  breaklines=true, breakatwhitespace=true,
  tabsize=4, showstringspaces=false,
  upquote=true, keepspaces=true, columns=fullflexible,
  literate=
    {–}{{-}}1
    {—}{{-}}1
    {…}{{...}}1
    {≤}{{\ensuremath{\le}}}1
    {≥}{{\ensuremath{\ge}}}1
    {≠}{{\ensuremath{\ne}}}1
    {≈}{{\ensuremath{\approx}}}1
    {±}{{\ensuremath{\pm}}}1
    {→}{{\ensuremath{\to}}}1
    {←}{{\ensuremath{\leftarrow}}}1
    {∞}{{\ensuremath{\infty}}}1
    {√}{{\ensuremath{\sqrt{\ }}}}1
    {×}{{\ensuremath{\times}}}1
    {÷}{{\ensuremath{\div}}}1
}

\lstnewenvironment{codepy}[1][]%
  {\lstset{style=code,language=Python,#1}}%
  {}

\newcommand{\inlinecode}[1]{\lstinline[style=code]!#1!}

\newcommand{\LF}[2]{\par\noindent\textbf{#1:}~#2\par}
\newcommand{\WHAT}[1]{\LF{WHAT}{#1}}
\newcommand{\WHY}[1]{\LF{WHY}{#1}}
\newcommand{\HOW}[1]{\LF{HOW}{#1}}
\newcommand{\ELI}[1]{\LF{ELI5}{#1}}
\newcommand{\SCOPE}[1]{\LF{SCOPE}{#1}}
\newcommand{\CONFUSIONS}[1]{\LF{COMMON CONFUSIONS}{#1}}
\newcommand{\APPLICATIONS}[1]{\LF{APPLICATIONS}{#1}}
\newcommand{\FORMULA}[1]{\LF{FORMULA}{#1}}
\newcommand{\CANONICAL}[1]{\LF{CANONICAL FORM}{#1}}
\newcommand{\PRECONDS}[1]{\LF{PRECONDITIONS}{#1}}
\newcommand{\DERIVATION}[1]{\LF{DERIVATION}{#1}}
\newcommand{\EQUIV}[1]{\LF{EQUIVALENT FORMS}{#1}}
\newcommand{\LIMITS}[1]{\LF{LIMIT CASES}{#1}}
\newcommand{\INPUTS}[1]{\LF{INPUTS}{#1}}
\newcommand{\OUTPUTS}[1]{\LF{OUTPUTS}{#1}}
\newcommand{\RESULT}[1]{\LF{RESULT}{#1}}
\newcommand{\INTUITION}[1]{\LF{INTUITION}{#1}}
\newcommand{\PITFALLS}[1]{\LF{PITFALLS}{#1}}
\newcommand{\MODEL}[1]{\LF{CANONICAL MATH MODEL}{#1}}
\newcommand{\ASSUMPTIONS}[1]{\LF{ASSUMPTIONS}{#1}}
\newcommand{\WHICHFORMULA}[1]{\LF{WHICH FORMULA \& WHY}{#1}}
\newcommand{\GOVERN}[1]{\LF{GOVERNING EQUATION(S)}{#1}}
\newcommand{\UNITCHECK}[1]{\LF{UNIT CHECK}{#1}}
\newcommand{\EDGECASES}[1]{\LF{EDGE CASES}{#1}}
\newcommand{\ALTERNATE}[1]{\LF{ALTERNATE APPROACH (sketch)}{#1}}
\newcommand{\PROBLEM}[1]{\LF{PROBLEM}{#1}}
\newcommand{\API}[1]{\LF{API}{#1}}
\newcommand{\COMPLEXITY}[1]{\LF{COMPLEXITY}{#1}}
\newcommand{\FAILMODES}[1]{\LF{FAILURE MODES}{#1}}
\newcommand{\STABILITY}[1]{\LF{NUMERICAL STABILITY}{#1}}
\newcommand{\VALIDATION}[1]{\LF{VALIDATION}{#1}}
\newcommand{\EXPLANATION}[1]{\LF{EXPLANATION}{#1}}
\newcommand{\SCENARIO}[1]{\LF{SCENARIO}{#1}}
\newcommand{\PIPELINE}[1]{\LF{PIPELINE STEPS}{#1}}
\newcommand{\METRICS}[1]{\LF{METRICS}{#1}}
\newcommand{\INTERPRET}[1]{\LF{INTERPRETATION}{#1}}
\newcommand{\NEXTSTEPS}[1]{\LF{LIMITATIONS \& NEXT STEPS}{#1}}

\usepackage{titlesec}
\titleformat{\section}{\Large\bfseries}{\thesection}{0.6em}{}
\titlespacing*{\section}{0pt}{*2}{*1}
\usepackage{etoolbox}
\pretocmd{\section}{\clearpage}{}{}

\newcommand{\FormulaPage}[2]{%
  \clearpage
  \section*{Formula #1 — #2}%
  \addcontentsline{toc}{section}{Formula #1 — #2}%
}
\newcommand{\ProblemPage}[2]{%
  \clearpage
  \subsection*{Problem #1: #2}%
  \addcontentsline{toc}{subsection}{Problem #1: #2}%
}
\newcommand{\CodeDemoPage}[1]{%
  \clearpage
  \subsection*{Coding Demo: #1}%
  \addcontentsline{toc}{subsection}{Coding Demo: #1}%
}
\newcommand{\DomainPage}[1]{%
  \clearpage
  \subsection*{#1 (End-to-End)}%
  \addcontentsline{toc}{subsection}{#1 (End-to-End)}%
}

\begin{document}
\title{Comprehensive Study Sheet — Matrix Completion and PSD Cone Projections}
\date{\today}
\maketitle
\tableofcontents
\clearpage

\section{Concept Overview}
\WHAT{
Matrix completion seeks $X\in\mathbb{R}^{m\times n}$ that matches observed
entries of an unknown low-rank matrix $M$ on an index set
$\Omega\subseteq\{1,\dots,m\}\times\{1,\dots,n\}$, typically via convex
relaxations such as nuclear-norm minimization. PSD cone projection maps a
symmetric matrix $A\in\mathbb{S}^n$ to its nearest positive semidefinite (PSD)
matrix in Frobenius norm. The central objects:
$P_\Omega:\mathbb{R}^{m\times n}\to\mathbb{R}^{m\times n}$, the orthogonal
projector onto observed coordinates; the PSD cone
$\mathbb{S}_+^n=\{X\in\mathbb{S}^n: X\succeq 0\}$; the nuclear norm
$\|X\|_*=\sum_i \sigma_i(X)$; and spectral decomposition $A=Q\Lambda Q^\top$.}

\WHY{
Low-rank structure captures latent factors in recommender systems and data
imputation. PSD projection is fundamental to fixing covariance estimates and to
constrained optimization over $\mathbb{S}_+^n$. Convex relaxations enable
tractable algorithms with theoretical recovery guarantees under incoherence and
sufficient sampling. Orthogonal projectors and spectral calculus supply
closed-form proximal and projection operators.}

\HOW{
1. Define the sampling operator $P_\Omega$ and its orthogonal complement
$P_{\Omega^\perp}=I-P_\Omega$ under Frobenius inner product.
2. Use spectral/orthogonal invariance to derive the PSD projection formula by
eigenvalue thresholding.
3. Use unitary invariance and separability across singular values to derive the
proximal operator of the nuclear norm (singular value soft-thresholding).
4. Formulate matrix completion as convex program and write KKT conditions,
relating dual certificates to spectral-norm bounds.}

\ELI{
Matrix completion fills in a partially known table by assuming it is built from
a few hidden patterns; PSD projection fixes a ``broken'' covariance by setting
its negative eigenvalues to zero.}

\SCOPE{
Valid when low-rank assumptions are appropriate and sampling is sufficiently
rich (e.g., uniform random with incoherence). PSD projection assumes symmetric
input with Frobenius metric. Degenerate cases: repeated eigenvalues, exact rank
deficiency, or when $\Omega$ is too small to identify $M$.}

\CONFUSIONS{
Nuclear norm vs. Frobenius norm: the former sums singular values; the latter
sums squares of entries. PSD projection vs. elementwise clipping: PSD acts in
eigenbasis, not entrywise. Completion vs. interpolation: completion uses global
low-rank structure, not local smoothing.}

\APPLICATIONS{
\begin{bullets}
\item Mathematical foundations: convex analysis of unitarily invariant norms.
\item Computational modeling: proximal algorithms for low-rank recovery.
\item Engineering/economics: covariance cleaning by PSD projection.
\item Statistics: imputation and collaborative filtering via low-rank models.
\end{bullets}
}

\textbf{ANALYTIC STRUCTURE.}
Both PSD projection and nuclear-norm proximal operators are convex, nonexpansive
maps induced by orthogonal/unitary invariance. The PSD cone is closed, convex,
and self-dual. The nuclear norm is convex and is the gauge of the convex hull
of rank-one matrices with spectral norm bounded by one.

\textbf{CANONICAL LINKS.}
PSD projection links to Moreau decomposition for cones. Nuclear-norm prox
(SVT) links to matrix completion objective via proximal gradient/ADMM. Sampling
projector $P_\Omega$ yields a Pythagorean identity splitting errors.

\textbf{PROBLEM-TYPE RECOGNITION HEURISTICS.}
\begin{bullets}
\item Phrases like ``nearest PSD matrix'' imply eigenvalue truncation.
\item ``Fill missing entries with low rank'' suggests nuclear-norm minimization.
\item ``Observed index set'' triggers $P_\Omega$ algebra and Pythagorean splits.
\item ``Proximal step with nuclear norm'' means singular value soft-thresholding.
\end{bullets}

\textbf{SOLUTION STRATEGY BLUEPRINT.}
\begin{bullets}
\item Translate data into $P_\Omega$, $M$, and target low rank.
\item Identify PSD projection or nuclear-norm prox as needed.
\item Substitute SVD/eigendecomposition and apply thresholding.
\item Verify constraints and optimality via KKT or projector identities.
\end{bullets}

\textbf{CONCEPTUAL INVARIANTS.}
Frobenius distance under orthogonal changes of basis. Spectral order for PSD
matrices. Decomposition $\|X\|_F^2=\|P_\Omega X\|_F^2+\|P_{\Omega^\perp}X\|_F^2$.

\textbf{EDGE INTUITION.}
As sampling density $\to 0$, completion underdetermined; as it $\to 1$, perfect
recovery given exact low rank. For PSD projection, very negative eigenvalues are
zeroed, shrinking distance but increasing rank deficiency.

\section{Glossary}
\glossx{PSD Cone $\mathbb{S}_+^n$}
{Set of symmetric matrices with nonnegative eigenvalues.}
{Encodes semidefinite constraints central to covariance and SDP problems.}
{Diagonalize $A=Q\Lambda Q^\top$; ensure all $\lambda_i\ge 0$.}
{Like heights above ground: negatives are not allowed.}
{Pitfall: enforcing entrywise nonnegativity is not PSD; PSD is spectral.}

\glossx{Sampling Operator $P_\Omega$}
{Orthogonal projector that keeps entries in $\Omega$ and zeros others.}
{Models observed entries in matrix completion and defines constraints.}
{Set $(P_\Omega(X))_{ij}=X_{ij}$ if $(i,j)\in\Omega$, else $0$.}
{A stencil that reveals only painted squares on a grid.}
{Example: $P_\Omega$ is idempotent and self-adjoint under Frobenius inner product.}

\glossx{Nuclear Norm $\|\cdot\|_*$}
{Sum of singular values of a matrix.}
{Convex surrogate for rank promoting low-rank solutions.}
{Compute SVD $X=U\Sigma V^\top$; sum diagonal of $\Sigma$.}
{Count how many thick books you stacked by total thickness.}
{Pitfall: $\|X\|_*$ differs from $\|X\|_F$; they coincide only in special cases.}

\glossx{Singular Value Thresholding (SVT)}
{Proximal operator of nuclear norm that soft-thresholds singular values.}
{Key step in first-order algorithms for matrix completion.}
{Compute SVD, shrink singular values by $\tau$, clip at zero.}
{Turn down the volume of quiet notes below a threshold to silence.}
{Example: $Y=U\Sigma V^\top\Rightarrow \text{prox}_{\tau\|\cdot\|_*}(Y)
=U(\Sigma-\tau I)_+V^\top$.}

\section{Symbol Ledger}
\varmapStart
\var{m,n}{Row and column dimensions of matrices.}
\var{M}{Unknown target matrix to be completed, typically low rank.}
\var{X}{Optimization variable for completed matrix.}
\var{A}{Symmetric matrix in $\mathbb{S}^n$ to be projected onto $\mathbb{S}_+^n$.}
\var{\mathbb{S}^n}{Space of real $n\times n$ symmetric matrices.}
\var{\mathbb{S}_+^n}{PSD cone $\{X\succeq 0\}$.}
\var{\Omega}{Set of observed indices in $\{1,\dots,m\}\times\{1,\dots,n\}$.}
\var{P_\Omega}{Orthogonal projector retaining entries in $\Omega$.}
\var{P_{\Omega^\perp}}{Complement projector $I-P_\Omega$.}
\var{\|\cdot\|_F}{Frobenius norm, $\|X\|_F=\sqrt{\sum_{ij}X_{ij}^2}$.}
\var{\|\cdot\|_*}{Nuclear norm, sum of singular values.}
\var{\|\cdot\|_2}{Spectral norm (largest singular value).}
\var{U,\Sigma,V}{SVD factors $Y=U\Sigma V^\top$.}
\var{Q,\Lambda}{Eigen-decomposition $A=Q\Lambda Q^\top$.}
\var{\sigma_i}{Singular values.}
\var{\lambda_i}{Eigenvalues of a symmetric matrix.}
\var{\tau}{Threshold parameter in SVT.}
\var{Y}{Data matrix in proximal updates.}
\var{Z}{Dual variable in KKT conditions.}
\varmapEnd

\section{Formula Canon — One Formula Per Page}
\FormulaPage{1}{Projection onto the PSD Cone}
\WHAT{
Compute the Frobenius-norm projection of $A\in\mathbb{S}^n$ onto
$\mathbb{S}_+^n$.}
\WHY{
Nearest-PSD covariance estimation and feasibility steps in semidefinite
programs require this projection.}
\FORMULA{
\[
\Pi_{\mathbb{S}_+^n}(A)=\underset{X\succeq 0}{\arg\min}\ \|X-A\|_F^2
=Q\operatorname{diag}((\lambda_i)_+)Q^\top,
\]
where $A=Q\operatorname{diag}(\lambda_i)Q^\top$ and $(t)_+=\max\{t,0\}$.}
\CANONICAL{
Domain: $A\in\mathbb{S}^n$. Range: $\mathbb{S}_+^n$. Metric: Frobenius norm.
Assumptions: spectral theorem holds; Frobenius norm is orthogonally invariant.}
\PRECONDS{
\begin{bullets}
\item $A$ is real symmetric.
\item Frobenius norm and eigen-decomposition are well-defined.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
For any orthogonal $Q$, $\|X-A\|_F=\|Q^\top X Q-Q^\top A Q\|_F$.
\end{lemma}
\begin{proof}
$\|X-A\|_F^2=\langle X-A,X-A\rangle=\operatorname{tr}((X-A)^\top(X-A))$.
For orthogonal $Q$, insert $QQ^\top=I$:
$\operatorname{tr}((Q^\top X Q-Q^\top A Q)^\top(Q^\top X Q-Q^\top A Q))
=\operatorname{tr}(Q^\top(X-A)^\top(X-A)Q)=\|X-A\|_F^2$.\qedhere
\end{proof}
\DERIVATION{
\begin{align*}
\text{Step 1: }&
A=Q\Lambda Q^\top,\ \Lambda=\operatorname{diag}(\lambda_i).\\
\text{Step 2: }&
\min_{X\succeq 0}\|X-A\|_F^2
=\min_{X\succeq 0}\|Q^\top X Q-\Lambda\|_F^2 \quad(\text{Lemma}).\\
\text{Step 3: }&
\text{Let }Y=Q^\top X Q,\ Y\succeq 0\iff Y\text{ symmetric with }Y\ge 0.\\
\text{Step 4: }&
\min_{Y\succeq 0}\|Y-\Lambda\|_F^2.
\text{ The minimizer is diagonal: off-diagonals increase cost.}\\
\text{Step 5: }&
\min_{y_i\ge 0}\sum_i (y_i-\lambda_i)^2 \Rightarrow
y_i^*=(\lambda_i)_+.\\
\text{Step 6: }&
Y^*=\operatorname{diag}((\lambda_i)_+),\
X^*=QY^*Q^\top.
\end{align*}
}
\EQUIV{
\begin{bullets}
\item Moreau decomposition for cones:
$A=\Pi_{\mathbb{S}_+^n}(A)-\Pi_{\mathbb{S}_+^n}(-A)$ and the two terms are
orthogonal in Frobenius inner product.
\item If $A\succeq 0$, projection equals $A$; if $A\preceq 0$, projection is $0$.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item If some $\lambda_i=0$, the projection is not unique in eigenvectors but
unique in value.
\item For nearly PSD $A$, small negative eigenvalues are clipped, minimally
perturbing $A$ in Frobenius norm.
\end{bullets}
}
\INPUTS{$A\in\mathbb{S}^n$.}
\DERIVATION{
\begin{align*}
\text{Example: }&
A=\begin{bmatrix}1&2\\2&1\end{bmatrix}.\
\lambda=\{3,-1\},\ Q=\frac{1}{\sqrt{2}}
\begin{bmatrix}1&1\\1&-1\end{bmatrix}.\\
&(\lambda)_+=\{3,0\}\Rightarrow
\Pi_{\mathbb{S}_+^2}(A)=Q\begin{bmatrix}3&0\\0&0\end{bmatrix}Q^\top
=\begin{bmatrix}1.5&1.5\\1.5&1.5\end{bmatrix}.
\end{align*}
}
\RESULT{
Projection equals eigenvalue positive-part truncation in the eigenbasis.}
\PITFALLS{
\begin{bullets}
\item Do not threshold entries elementwise; threshold eigenvalues.
\item Use symmetric eigendecomposition, not SVD, for symmetric $A$.
\end{bullets}
}
\ELI{
Rotate into the directions the matrix likes (eigenvectors), set negative
heights to the floor (zero), and rotate back.}

\FormulaPage{2}{Orthogonal Sampling Projector $P_\Omega$}
\WHAT{
Characterize $P_\Omega$ as an orthogonal projector in Frobenius geometry and
derive Pythagorean error splitting.}
\WHY{
Matrix completion constraints and residuals decompose cleanly via $P_\Omega$
and $P_{\Omega^\perp}$.}
\FORMULA{
\[
(P_\Omega(X))_{ij}=\begin{cases}
X_{ij},&(i,j)\in\Omega,\\
0,&\text{otherwise},
\end{cases}\quad
P_\Omega^2=P_\Omega,\ P_\Omega^\top=P_\Omega,\ 
\|X\|_F^2=\|P_\Omega X\|_F^2+\|P_{\Omega^\perp}X\|_F^2.
\]
}
\CANONICAL{
Domain: $\mathbb{R}^{m\times n}$ with Frobenius inner product
$\langle X,Y\rangle=\operatorname{tr}(X^\top Y)$. $P_{\Omega^\perp}=I-P_\Omega$.}
\PRECONDS{
\begin{bullets}
\item Fixed index set $\Omega$.
\item Frobenius inner product.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
$P_\Omega$ is self-adjoint and idempotent: $P_\Omega^\top=P_\Omega$ and
$P_\Omega^2=P_\Omega$.
\end{lemma}
\begin{proof}
For any $X,Y$, $\langle P_\Omega X,Y\rangle=\sum_{(i,j)\in\Omega}X_{ij}Y_{ij}
=\langle X,P_\Omega Y\rangle\Rightarrow P_\Omega^\top=P_\Omega$.
Also, applying twice keeps only $\Omega$ entries: $P_\Omega^2=P_\Omega$.\qedhere
\end{proof}
\DERIVATION{
\begin{align*}
\text{Orthogonality: }&
\langle P_\Omega X, P_{\Omega^\perp}X\rangle
=\sum_{(i,j)\in\Omega}X_{ij}\cdot 0=0.\\
\text{Pythagoras: }&
\|X\|_F^2=\|P_\Omega X+P_{\Omega^\perp}X\|_F^2\\
&=\|P_\Omega X\|_F^2+\|P_{\Omega^\perp}X\|_F^2
+2\langle P_\Omega X,P_{\Omega^\perp}X\rangle\\
&=\|P_\Omega X\|_F^2+\|P_{\Omega^\perp}X\|_F^2.
\end{align*}
}
\EQUIV{
\begin{bullets}
\item $P_\Omega=\sum_{(i,j)\in\Omega}E_{ij}\langle E_{ij},\cdot\rangle$, where
$E_{ij}$ are standard basis matrices.
\item $P_{\Omega^\perp}$ orthogonal projector onto the complement subspace.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item If $\Omega=\emptyset$, $P_\Omega=0$ and
$\|X\|_F^2=\|P_{\Omega^\perp}X\|_F^2$.
\item If $\Omega$ is full, $P_\Omega=I$ and $P_{\Omega^\perp}=0$.
\end{bullets}
}
\INPUTS{$\Omega\subseteq\{1,\dots,m\}\times\{1,\dots,n\}$, $X\in\mathbb{R}^{m\times n}$.}
\DERIVATION{
\begin{align*}
\text{Example: }&
X=\begin{bmatrix}1&2\\3&4\end{bmatrix},\
\Omega=\{(1,1),(2,2)\}.\\
&P_\Omega(X)=\begin{bmatrix}1&0\\0&4\end{bmatrix},\
P_{\Omega^\perp}(X)=\begin{bmatrix}0&2\\3&0\end{bmatrix}.\\
&\|X\|_F^2=1+4+9+16=30=1+16+4+9.
\end{align*}
}
\RESULT{
$P_\Omega$ is the orthogonal projector for sampling, enabling orthogonal error
decomposition.}
\PITFALLS{
\begin{bullets}
\item Do not confuse $P_\Omega$ with masking that rescales entries; it is a
projection, not a weighting.
\item The adjoint equals itself under Frobenius geometry; under other inner
products this may fail.
\end{bullets}
}
\ELI{
Keep the squares with stickers ($\Omega$), erase the rest; lengths add by
Pythagoras because kept and erased parts are perpendicular.}

\FormulaPage{3}{Proximal Operator of Nuclear Norm (SVT)}
\WHAT{
Closed-form proximal operator of $\tau\|\cdot\|_*$ that soft-thresholds singular
values.}
\WHY{
Core step in proximal gradient and ADMM for matrix completion.}
\FORMULA{
\[
\operatorname{prox}_{\tau\|\cdot\|_*}(Y)
=\underset{X}{\arg\min}\ \tfrac12\|X-Y\|_F^2+\tau\|X\|_*
=U\operatorname{diag}((\sigma_i-\tau)_+)V^\top,
\]
where $Y=U\operatorname{diag}(\sigma_i)V^\top$ is an SVD.}
\CANONICAL{
Domain: $Y\in\mathbb{R}^{m\times n}$, $\tau\ge 0$. Assumptions: SVD exists; the
nuclear norm is unitarily invariant and convex.}
\PRECONDS{
\begin{bullets}
\item $Y$ finite; compute SVD.
\item $\tau\ge 0$.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
Let $f(X)=g(\sigma(X))$ be a unitarily invariant convex function with symmetric
convex $g$ on singular values. Then
$\operatorname{prox}_{f}(Y)=U\operatorname{diag}(\operatorname{prox}_g(\sigma(Y)))V^\top$.
\end{lemma}
\begin{proof}
Von Neumann's trace inequality yields
$\|X-Y\|_F^2\ge \|\sigma(X)-\sigma(Y)\|_2^2$ with equality at shared singular
vectors $U,V$. Thus the minimizer shares $U,V$ with $Y$ and reduces to
minimizing $\tfrac12\sum_i (s_i-\sigma_i)^2+\tau\sum_i s_i$ over $s_i\ge 0$,
which is elementwise soft-thresholding: $s_i^*=(\sigma_i-\tau)_+$. Recompose
with $U,V$.\qedhere
\end{proof}
\DERIVATION{
\begin{align*}
\text{Step 1: }&
Y=U\Sigma V^\top,\ \Sigma=\operatorname{diag}(\sigma_i).\\
\text{Step 2: }&
\min_X \tfrac12\|X-Y\|_F^2+\tau\|X\|_*\\
&=\min_{S\ge 0}\tfrac12\sum_i (s_i-\sigma_i)^2+\tau\sum_i s_i
\quad(\text{Lemma}).\\
\text{Step 3: }&
s_i^*=\arg\min_{s_i\ge 0}\tfrac12(s_i-\sigma_i)^2+\tau s_i
=(\sigma_i-\tau)_+.\\
\text{Step 4: }&
X^*=U\operatorname{diag}(s_i^*)V^\top.
\end{align*}
}
\EQUIV{
\begin{bullets}
\item If $\tau=0$, prox is identity. If $\tau\ge \max_i \sigma_i$, prox is $0$.
\item Equivalent to isotonic regression on singular values with nonnegativity.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item For clustered singular values near $\tau$, numerical precision affects
rank selection.
\item As $\tau\downarrow 0$, proximal step approaches identity map.
\end{bullets}
}
\INPUTS{$Y\in\mathbb{R}^{m\times n}$, $\tau\ge 0$.}
\DERIVATION{
\begin{align*}
\text{Example: }&
Y=\begin{bmatrix}3&0\\0&1\end{bmatrix},\ \tau=1.2.\
\sigma=\{3,1\}.\\
&(\sigma-\tau)_+=\{1.8,0\}.\
\text{prox}=
\begin{bmatrix}1.8&0\\0&0\end{bmatrix}.
\end{align*}
}
\RESULT{
SVT shrinks singular values by $\tau$ and clips at zero, preserving singular
vectors of $Y$.}
\PITFALLS{
\begin{bullets}
\item Thresholding eigenvalues of a nonsymmetric matrix is incorrect; use SVD.
\item Do not subtract $\tau$ from negative $s_i$; clip to zero.
\end{bullets}
}
\ELI{
Decompose $Y$ into independent volume knobs (singular values), turn each down
by $\tau$, and mute those that go negative.}

\FormulaPage{4}{Nuclear-Norm Matrix Completion and KKT Conditions}
\WHAT{
Convex formulation of matrix completion and its optimality conditions.}
\WHY{
KKT characterizes solutions and connects to dual certificates ensuring recovery.}
\FORMULA{
\[
\min_{X\in\mathbb{R}^{m\times n}}
\tfrac12\|P_\Omega(X-M)\|_F^2+\lambda\|X\|_*.
\]
KKT at $X^*$:
\quad $0\in P_\Omega(X^*-M)+\lambda\,\partial\|X^*\|_*.$}
\CANONICAL{
Loss restricted to observed entries via $P_\Omega$, plus nuclear-norm penalty
with $\lambda>0$. Subgradient
$\partial\|X\|_*=\{UV^\top+W:\ U^\top W=0,\ WV=0,\ \|W\|_2\le 1\}$ if
$X=U\Sigma V^\top$ is compact SVD with rank $r$.}
\PRECONDS{
\begin{bullets}
\item Convexity: quadratic plus convex norm.
\item Existence: coercive via penalty or sufficient observations.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
(Subgradient of nuclear norm) If $X=U\Sigma V^\top$ with rank $r$, then
$\partial\|X\|_*=\{UV^\top+W:U^\top W=0,\ WV=0,\ \|W\|_2\le 1\}$.
\end{lemma}
\begin{proof}
Use convex analysis for unitarily invariant norms: nuclear norm is the dual of
spectral norm. By convex conjugacy,
$\|X\|_*=\max_{\|Z\|_2\le 1}\langle Z,X\rangle$ with maximizers
$Z=UV^\top+W$ satisfying constraints, yielding the subdifferential set.
Standard arguments invoke von Neumann's trace inequality to characterize
maximizers; these are precisely those $Z$ attaining equality. Thus the stated
set equals $\partial\|X\|_*$. \qedhere
\end{proof}
\DERIVATION{
\begin{align*}
\text{Step 1: }&
F(X)=\tfrac12\|P_\Omega(X-M)\|_F^2+\lambda\|X\|_* \text{ is convex.}\\
\text{Step 2: }&
0\in \partial F(X^*)=P_\Omega(X^*-M)+\lambda\,\partial\|X^*\|_*.\\
\text{Step 3: }&
\exists\,Z\in \partial\|X^*\|_* \text{ s.t. } P_\Omega(X^*-M)+\lambda Z=0.\\
\text{Step 4: }&
\text{If }X^*=U\Sigma V^\top,\ Z=UV^\top+W,\ U^\top W=0,\ WV=0,\ \|W\|_2\le 1.
\end{align*}
}
\EQUIV{
\begin{bullets}
\item Noiseless case $\lambda\downarrow 0$ with hard constraint
$P_\Omega(X)=P_\Omega(M)$ recovers affine-feasible nuclear-norm minimization.
\item Dual form (noiseless):
$\max\ \langle Y,P_\Omega(M)\rangle\ \text{s.t.}\ \|Y\|_2\le 1,\
Y=P_\Omega(Y)$.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item If $\Omega$ is too small or adversarial, multiple solutions exist.
\item Large $\lambda$ biases strongly toward low rank, possibly underfitting.
\end{bullets}
}
\INPUTS{$M\in\mathbb{R}^{m\times n}$, $\Omega$, $\lambda>0$.}
\DERIVATION{
\begin{align*}
\text{Example: }&
M=\begin{bmatrix}1&?\\?&1\end{bmatrix},\ \Omega=\{(1,1),(2,2)\},\ \lambda=0.1.\\
&X=U\Sigma V^\top\ \Rightarrow\ 0\in
\begin{bmatrix}x_{11}-1&0\\0&x_{22}-1\end{bmatrix}+\lambda Z.\\
&\text{At symmetry }x_{11}=x_{22}=t,\ X=tI,\ U=V=I,\ Z=I,\ \|Z\|_2=1.\\
&0\in (t-1)I+0.1 I \Rightarrow t=0.9.
\end{align*}
}
\RESULT{
Optimality characterized by $P_\Omega(X^*-M)$ balancing a nuclear-norm
subgradient scaled by $\lambda$.}
\PITFALLS{
\begin{bullets}
\item Using gradient of $\|\cdot\|_*$ where it is nondifferentiable
(e.g., at rank changes); use subgradients.
\item Forgetting to restrict residuals to $\Omega$ via $P_\Omega$.
\end{bullets}
}
\ELI{
Balance two forces: one pulls observed entries toward data, the other pushes the
matrix to be simpler (low rank). The solution stops where the pulls match.}

\section{10 Exhaustive Problems and Solutions}
\ProblemPage{1}{Compute a PSD Projection by Eigenvalue Thresholding}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Given $A\in\mathbb{S}^n$, find $\Pi_{\mathbb{S}_+^n}(A)$ and prove it is the
unique minimizer of $\min_{X\succeq 0}\|X-A\|_F^2$.
\PROBLEM{
(i) Derive the eigenvalue-thresholding solution. (ii) Show uniqueness in value.
(iii) Compute for a numeric $2\times 2$ example.}
\MODEL{
\[
\min_{X\succeq 0}\|X-A\|_F^2,\quad A=Q\Lambda Q^\top.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $A$ is symmetric with eigen-decomposition.
\item Frobenius inner product.
\end{bullets}
}
\varmapStart
\var{A}{Input symmetric matrix.}
\var{Q,\Lambda}{Eigenvectors and eigenvalues of $A$.}
\var{X}{Decision variable constrained PSD.}
\varmapEnd
\WHICHFORMULA{
Formula 1 (PSD projection), supported by orthogonal invariance lemma.}
\GOVERN{
\[
\Pi_{\mathbb{S}_+^n}(A)=Q\operatorname{diag}((\lambda_i)_+)Q^\top.
\]
}
\INPUTS{$A=\begin{bmatrix}1&2\\2&1\end{bmatrix}$.}
\DERIVATION{
\begin{align*}
\text{Step 1: }& A=Q\Lambda Q^\top,\
\lambda=\{3,-1\},\
Q=\tfrac{1}{\sqrt{2}}\begin{bmatrix}1&1\\1&-1\end{bmatrix}.\\
\text{Step 2: }& \min_{X\succeq 0}\|X-A\|_F^2
=\min_{Y\succeq 0}\|Y-\Lambda\|_F^2,\ Y=Q^\top X Q.\\
\text{Step 3: }& Y^*=\operatorname{diag}((\lambda_i)_+)=
\operatorname{diag}(3,0).\\
\text{Step 4: }& X^*=QY^*Q^\top=
\begin{bmatrix}1.5&1.5\\1.5&1.5\end{bmatrix}.
\end{align*}
}
\RESULT{
$X^*$ above; uniqueness in value follows from strict convexity on diagonal
variables $y_i$; eigenvectors may be nonunique if eigenvalues tie.}
\UNITCHECK{
Orthogonal invariance preserved; result is symmetric PSD; distance minimized by
coordinate-wise convex projection onto $\mathbb{R}_+^n$.}
\EDGECASES{
\begin{bullets}
\item If an eigenvalue is zero, any eigenbasis spanning its eigenspace yields
the same $X^*$.
\item If $A\succeq 0$, projection equals $A$.
\end{bullets}
}
\ALTERNATE{
Use Moreau decomposition: $A=\Pi_{\mathbb{S}_+}(A)-\Pi_{\mathbb{S}_+}(-A)$ and
orthogonality to show optimality of eigenvalue truncation.}
\VALIDATION{
\begin{bullets}
\item Check $X^*\succeq 0$ via eigenvalues $\{3,0\}\ge 0$.
\item Verify first-order optimality: $\langle X^*-A,Z-X^*\rangle\ge 0$ for
all $Z\succeq 0$ (Hilbert projection theorem).
\end{bullets}
}
\INTUITION{
Rotate to eigenbasis and slide each eigenvalue to the nearest nonnegative
point.}
\CANONICAL{
\begin{bullets}
\item Projection equals positive-part on eigenvalues.
\item Orthogonal invariance of Frobenius norm enables decoupling.}
\end{bullets}
}

\ProblemPage{2}{Pythagorean Split with Sampling Operator}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Prove $\|X\|_F^2=\|P_\Omega X\|_F^2+\|P_{\Omega^\perp}X\|_F^2$ and apply to
matrix completion residuals.
\PROBLEM{
(i) Prove $P_\Omega$ is orthogonal projector. (ii) Show the Pythagorean
identity. (iii) For a given $M,X,\Omega$, compute both sides.}
\MODEL{
\[
P_\Omega^2=P_\Omega,\ P_\Omega^\top=P_\Omega,\ P_{\Omega^\perp}=I-P_\Omega.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Frobenius inner product.
\item Fixed $\Omega$.
\end{bullets}
}
\varmapStart
\var{X}{Any matrix.}
\var{\Omega}{Observed index set.}
\var{P_\Omega}{Sampling projector.}
\varmapEnd
\WHICHFORMULA{
Formula 2 (orthogonal sampling projector).}
\GOVERN{
\[
\langle P_\Omega X,P_{\Omega^\perp}X\rangle=0.
\]
}
\INPUTS{$X=\begin{bmatrix}1&2\\3&4\end{bmatrix}$,
$\Omega=\{(1,2),(2,1)\}$.}
\DERIVATION{
\begin{align*}
\text{Step 1: }&
P_\Omega(X)=\begin{bmatrix}0&2\\3&0\end{bmatrix},\
P_{\Omega^\perp}(X)=\begin{bmatrix}1&0\\0&4\end{bmatrix}.\\
\text{Step 2: }&
\|X\|_F^2=30,\
\|P_\Omega X\|_F^2=13,\
\|P_{\Omega^\perp}X\|_F^2=17.\\
\text{Step 3: }& 13+17=30\Rightarrow \text{equality holds.}
\end{align*}
}
\RESULT{
Residuals split orthogonally between observed and unobserved components.}
\UNITCHECK{
Both sides are Frobenius-squared norms; identity follows from orthogonality.}
\EDGECASES{
\begin{bullets}
\item Empty $\Omega$: all mass in complement.
\item Full $\Omega$: complement is zero.
\end{bullets}
}
\ALTERNATE{
Vectorize $X$ and view $P_\Omega$ as a diagonal $0/1$ matrix projector; the
result reduces to Euclidean Pythagoras.}
\VALIDATION{
\begin{bullets}
\item Random $X$ and $\Omega$ sampling confirms equality numerically.
\item Check orthogonality by inner product directly.
\end{bullets}
}
\INTUITION{
Observed and missing parts lie in perpendicular coordinate subspaces.}
\CANONICAL{
\begin{bullets}
\item Orthogonal decomposition in Hilbert space $\mathbb{R}^{m\times n}$.
\item Error splits additively across $P_\Omega$ and $P_{\Omega^\perp}$.
\end{bullets}
}

\ProblemPage{3}{SVT Step for a Given Matrix}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Compute $\operatorname{prox}_{\tau\|\cdot\|_*}(Y)$ and verify optimality.
\PROBLEM{
(i) Perform SVD of $Y$. (ii) Apply soft-threshold with $\tau$. (iii) Verify the
optimality condition $Y-X^*\in \tau\,\partial\|X^*\|_*$.}
\MODEL{
\[
X^*=\underset{X}{\arg\min}\ \tfrac12\|X-Y\|_F^2+\tau\|X\|_*.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $\tau>0$; $Y$ finite.
\end{bullets}
}
\varmapStart
\var{Y}{Input matrix.}
\var{\tau}{Threshold parameter.}
\var{U,\Sigma,V}{SVD of $Y$.}
\varmapEnd
\WHICHFORMULA{
Formula 3 (SVT proximal operator).}
\GOVERN{
\[
X^*=U\operatorname{diag}((\sigma_i-\tau)_+)V^\top.
\]
}
\INPUTS{$Y=\begin{bmatrix}4&0\\0&1\end{bmatrix},\ \tau=1.5$.}
\DERIVATION{
\begin{align*}
\text{Step 1: }&
\sigma=\{4,1\}.\
(\sigma-\tau)_+=\{2.5,0\}.\
X^*=\begin{bmatrix}2.5&0\\0&0\end{bmatrix}.\\
\text{Step 2: }&
Y-X^*=\begin{bmatrix}1.5&0\\0&1\end{bmatrix}.\
X^*=U\operatorname{diag}(2.5,0)V^\top.\\
\text{Step 3: }&
\partial\|X^*\|_*=\{UV^\top+W:\ U^\top W=0,\ WV=0,\ \|W\|_2\le 1\}.\\
\text{Step 4: }&
\text{Pick }Z=UV^\top+\begin{bmatrix}0&0\\0&1\end{bmatrix},\ \|W\|_2=1.\\
\text{Step 5: }&
\tau Z=\begin{bmatrix}1.5&0\\0&1.5\end{bmatrix}\text{ and }
Y-X^*-\tau Z=\begin{bmatrix}0&0\\0&-0.5\end{bmatrix}.\\
\text{Step 6: }&
\text{Adjust }W=\operatorname{diag}(0,2/3)\Rightarrow
\tau Z=\begin{bmatrix}1.5&0\\0&1\end{bmatrix}=Y-X^*.
\end{align*}
}
\RESULT{
$X^*=\begin{bmatrix}2.5&0\\0&0\end{bmatrix}$ satisfies the optimality
condition.}
\UNITCHECK{
All matrices are $2\times 2$; norms and subgradient conditions match.}
\EDGECASES{
\begin{bullets}
\item If $\tau\ge 4$, $X^*=0$.
\item If $\tau\le 0$, $X^*=Y$.
\end{bullets}
}
\ALTERNATE{
Verify via derivative in singular directions using perturbation calculus.}
\VALIDATION{
\begin{bullets}
\item Evaluate objective at $X^*$ and nearby perturbations to confirm minimum.
\item Numerical line search along $U\operatorname{diag}(t,0)V^\top$ direction.
\end{bullets}
}
\INTUITION{
Only the strong singular direction survives; the weak one is muted.}
\CANONICAL{
\begin{bullets}
\item SVT operates on singular values independently.
\item Optimality via nuclear-norm subgradient balance.
\end{bullets}
}

\ProblemPage{4}{KKT Conditions for Noisy Matrix Completion}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Derive KKT and solve a symmetric toy instance.
\PROBLEM{
Consider $M=\begin{bmatrix}1&0\\0&1\end{bmatrix}$ observed on diagonal.
Solve $\min_X\tfrac12\|P_\Omega(X-M)\|_F^2+\lambda\|X\|_*$ for $\lambda=0.5$.}
\MODEL{
\[
\Omega=\{(1,1),(2,2)\},\quad F(X)=\tfrac12\|P_\Omega(X-I)\|_F^2+\lambda\|X\|_*.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Symmetric $X=tI$ is plausible by symmetry.
\item $\lambda=0.5$.
\end{bullets}
}
\varmapStart
\var{X}{Decision matrix.}
\var{\lambda}{Regularization parameter.}
\var{\Omega}{Observed set (diagonal).}
\varmapEnd
\WHICHFORMULA{
Formula 4 (KKT for completion).}
\GOVERN{
\[
0\in P_\Omega(X-I)+\lambda\,\partial\|X\|_*.
\]
}
\INPUTS{$\lambda=0.5$.}
\DERIVATION{
\begin{align*}
\text{Step 1: }& X=tI\Rightarrow \|X\|_*=2|t|,\ P_\Omega(X-I)=(t-1)I.\\
\text{Step 2: }& 0\in (t-1)I+0.5\,\partial(2|t|).\\
\text{Step 3: }& \partial(2|t|)=
\begin{cases}
\{2\},&t>0,\\
[-2,2],&t=0,\\
\{-2\},&t<0.
\end{cases}\\
\text{Step 4: }& t>0:\ 0=(t-1)+0.5\cdot 2\Rightarrow t=0.\\
& t=0:\ 0\in (-1)+0.5\cdot [-2,2]=[-2,0]\ \Rightarrow \text{contains }0.\\
\text{Step 5: }& \Rightarrow t^*=0 \text{ is KKT-satisfying.}
\end{align*}
}
\RESULT{
$X^*=0$ with value $F(0)=\tfrac12\|I\|_F^2+0=1$.}
\UNITCHECK{
Dimensions $2\times 2$; subgradient bounds respected.}
\EDGECASES{
\begin{bullets}
\item If $\lambda<0.5$, solution becomes $t>0$ and increases toward $1$.
\item If $\lambda\to\infty$, $X^*\to 0$.
\end{bullets}
}
\ALTERNATE{
Solve via SVT on gradient step $Y=I$ with step-size $1$:
$X^*=\operatorname{SVT}_{0.5}(I)=0$.}
\VALIDATION{
\begin{bullets}
\item Grid-search over $t\in[0,1.5]$ matches $t^*=0$.
\item Compare with explicit SVT computation on $I$. 
\end{bullets}
}
\INTUITION{
Large regularization suppresses all structure, even observed entries.}
\CANONICAL{
\begin{bullets}
\item KKT: residual equals scaled nuclear-norm subgradient.
\item Symmetry reduces to scalar soft-thresholding.
\end{bullets}
}

\ProblemPage{5}{Rank-1 Completion from Cross Pattern Observations}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Given a $3\times 3$ rank-1 matrix $M=uv^\top$ with $u=[1,2,3]^\top$,
$v=[1,2,?]^\top$, and observations at the first two columns fully observed,
complete $M$ via rank-1 structure.
\PROBLEM{
(i) Determine the missing entry $v_3$. (ii) Verify via nuclear-norm program.}
\MODEL{
\[
M=\begin{bmatrix}
1&2&?\\
2&4&?\\
3&6&?
\end{bmatrix}=uv^\top,\ 
u=[1,2,3]^\top,\ v=[1,2,v_3]^\top.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item True rank is $1$.
\item Observations reveal columns 1 and 2 completely.
\end{bullets}
}
\varmapStart
\var{u,v}{Rank-1 factors.}
\var{v_3}{Unknown scalar to find.}
\var{\Omega}{Indices of first two columns.}
\varmapEnd
\WHICHFORMULA{
Use proportionality of columns for rank-1: column 2 equals $2$ times column 1.}
\GOVERN{
\[
M_{:,2}=2M_{:,1}\Rightarrow M_{:,3}=v_3 M_{:,1}.
\]
}
\INPUTS{Column 1 and 2 known: $[1,2,3]$ and $[2,4,6]$.}
\DERIVATION{
\begin{align*}
\text{Step 1: }&
M_{:,2}=2M_{:,1}\Rightarrow v_2=2 v_1\ (\text{consistent}).\\
\text{Step 2: }&
v_1=1,\ v_2=2\Rightarrow u=[1,2,3]^\top,\ v=[1,2,v_3]^\top.\\
\text{Step 3: }&
Any $v_3$ is consistent with rank-1 if column 3 is $v_3$ times column 1.\\
\text{Step 4: }&
Thus $M_{:,3}=[v_3,2v_3,3v_3]^\top.\\
\text{Step 5: }&
Noiseless nuclear-norm minimization with these $\Omega$ recovers any such
rank-1 extension; the solution is not unique without observing column 3.
\end{align*}
}
\RESULT{
Family of completions $M(v_3)=[1,2,3]\cdot[1,2,v_3]^\top$; non-uniqueness
demonstrated.}
\UNITCHECK{
Rank remains $1$; observed entries matched exactly.}
\EDGECASES{
\begin{bullets}
\item If one entry in column 3 were observed, $v_3$ would be identified.
\item If column 1 contained a zero, scaling inference might fail locally.
\end{bullets}
}
\ALTERNATE{
Formulate nuclear-norm program; its feasible set contains exactly the rank-1
family, so all have equal objective and are optimal.}
\VALIDATION{
\begin{bullets}
\item Check SVD of any $M(v_3)$ has one nonzero singular value.
\item Verify constraints $P_\Omega(M(v_3))=P_\Omega(M)$ hold.
\end{bullets}
}
\INTUITION{
Rank-1 means all columns are multiples of one vector; with two full columns,
the third is any multiple without more data.}
\CANONICAL{
\begin{bullets}
\item Identifiability requires sufficient sampling.
\item Low-rank structure imposes proportionality relations. 
\end{bullets}
}

\ProblemPage{6}{Alice \& Bob: Hidden Pythagoras in Completion Residuals}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Alice measures fit on observed entries; Bob measures fit on missing entries.
Show their errors add in quadrature and compute a trade-off.
\PROBLEM{
Given $X$, prove $\|X-M\|_F^2=\|P_\Omega(X-M)\|_F^2+\|P_{\Omega^\perp}(X-M)\|_F^2$.
For $M$ and $X$ below, compute both sides.}
\MODEL{
\[
M=\begin{bmatrix}1&2\\3&4\end{bmatrix},\
X=\begin{bmatrix}1&0\\0&4\end{bmatrix},\
\Omega=\{(1,1),(2,2)\}.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Frobenius geometry.
\end{bullets}
}
\varmapStart
\var{M,X}{True and candidate matrices.}
\var{\Omega}{Observed set.}
\varmapEnd
\WHICHFORMULA{
Formula 2 (Pythagorean split).}
\GOVERN{
\[
\|E\|_F^2=\|P_\Omega E\|_F^2+\|P_{\Omega^\perp}E\|_F^2,\ E=X-M.
\]
}
\INPUTS{Given $M,X,\Omega$ above.}
\DERIVATION{
\begin{align*}
\text{Step 1: }&
E=\begin{bmatrix}0&-2\\-3&0\end{bmatrix}.\
\|E\|_F^2=13.\\
\text{Step 2: }&
P_\Omega E=\begin{bmatrix}0&0\\0&0\end{bmatrix}\Rightarrow\|P_\Omega E\|_F^2=0.\\
\text{Step 3: }&
P_{\Omega^\perp}E=E\Rightarrow \|P_{\Omega^\perp}E\|_F^2=13.\\
\text{Step 4: }& 13=0+13\ \text{holds.}
\end{align*}
}
\RESULT{
Errors decompose; here all error lies on missing entries.}
\UNITCHECK{
All terms are squared norms; identity holds exactly.}
\EDGECASES{
\begin{bullets}
\item If $X$ matches $M$ on $\Omega^\perp$, then all error is on $\Omega$.
\item If $X=M$, both errors are zero.
\end{bullets}
}
\ALTERNATE{
Vectorization reduces the identity to orthogonal coordinate subspaces.}
\VALIDATION{
\begin{bullets}
\item Numerical verification with random $X,M,\Omega$.
\item Check $\langle P_\Omega E, P_{\Omega^\perp}E\rangle=0$ explicitly.
\end{bullets}
}
\INTUITION{
Two perpendicular error components add by Pythagoras.}
\CANONICAL{
\begin{bullets}
\item Orthogonal decomposition via sampling projector.
\item Guides weighting between fit on observed vs. unobserved parts. 
\end{bullets}
}

\ProblemPage{7}{Covariance Cleaning by PSD Projection}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Given a symmetric estimate $\hat{\Sigma}$ with small negative eigenvalues,
compute the nearest PSD matrix and quantify the Frobenius adjustment.
\PROBLEM{
(i) Project $\hat{\Sigma}$ onto $\mathbb{S}_+^n$. (ii) Compute adjustment norm.
(iii) Verify optimality condition.}
\MODEL{
\[
\hat{\Sigma}=\begin{bmatrix}2&-1\\-1&0\end{bmatrix}.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Symmetry holds.
\item Frobenius metric.
\end{bullets}
}
\varmapStart
\var{\hat{\Sigma}}{Raw covariance estimate.}
\var{\Sigma^+}{Nearest PSD projection.}
\varmapEnd
\WHICHFORMULA{
Formula 1 (PSD projection).}
\GOVERN{
\[
\Sigma^+=Q\operatorname{diag}((\lambda_i)_+)Q^\top.
\]
}
\INPUTS{$\hat{\Sigma}$ as given.}
\DERIVATION{
\begin{align*}
\text{Step 1: }&
\lambda=\text{eig}\left(\begin{bmatrix}2&-1\\-1&0\end{bmatrix}\right).\
\lambda=\{(2\pm \sqrt{5})/2\}\approx\{1.618,-0.618\}.\\
\text{Step 2: }&
(\lambda)_+=\{1.618,0\}.\
Q \text{ from eigendecomposition}.\\
\text{Step 3: }&
\Sigma^+=Q\operatorname{diag}(1.618,0)Q^\top.\
\|\Sigma^+-\hat{\Sigma}\|_F^2=0.618^2.\\
\text{Step 4: }&
\text{Optimality: }\langle \Sigma^+-\hat{\Sigma}, Z-\Sigma^+\rangle\ge 0,\
\forall Z\succeq 0.
\end{align*}
}
\RESULT{
Nearest PSD replaces the negative eigenvalue by zero; minimal Frobenius change
has squared norm $0.618^2$.}
\UNITCHECK{
Symmetric PSD result; orthogonal invariance preserved.}
\EDGECASES{
\begin{bullets}
\item If both eigenvalues nonnegative, no change.
\item If both negative, projection is zero matrix.
\end{bullets}
}
\ALTERNATE{
Use Moreau decomposition:
$\hat{\Sigma}=\Sigma^+-\Pi_{\mathbb{S}_+}(-\hat{\Sigma})$ and orthogonality to
compute distance.}
\VALIDATION{
\begin{bullets}
\item Check eigenvalues of $\Sigma^+$ are $\ge 0$.
\item Compare distances to any other PSD candidate numerically.
\end{bullets}
}
\INTUITION{
Set negative variances to zero along principal directions.}
\CANONICAL{
\begin{bullets}
\item Eigenvalue clipping solves nearest-PSD problem.
\item Orthogonality yields Pythagorean distance split. 
\end{bullets}
}

\ProblemPage{8}{Expected Error under Random Sampling}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
If each entry is observed independently with probability $p$, show that
$\mathbb{E}\|P_\Omega(X-M)\|_F^2=p\|X-M\|_F^2$.
\PROBLEM{
Prove the expectation identity and discuss implications for scaling losses.}
\MODEL{
\[
\mathbb{P}((i,j)\in\Omega)=p\ \text{ i.i.d. across }(i,j).
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Independence and identical probability $p$ per entry.
\item Fixed $X,M$ independent of $\Omega$.
\end{bullets}
}
\varmapStart
\var{X,M}{Candidate and truth.}
\var{p}{Sampling probability.}
\var{\Omega}{Random index set.}
\varmapEnd
\WHICHFORMULA{
Formula 2 applied in expectation over random $\Omega$.}
\GOVERN{
\[
\mathbb{E}\|P_\Omega E\|_F^2
=\sum_{i,j}\mathbb{E}[\mathbf{1}_{(i,j)\in\Omega}]E_{ij}^2
=p\sum_{i,j}E_{ij}^2.
\]
}
\INPUTS{$p\in[0,1]$, fixed $E=X-M$.}
\DERIVATION{
\begin{align*}
\text{Step 1: }&
\|P_\Omega E\|_F^2=\sum_{i,j}\mathbf{1}_{(i,j)\in\Omega}E_{ij}^2.\\
\text{Step 2: }&
\mathbb{E}\|P_\Omega E\|_F^2
=\sum_{i,j}\mathbb{E}[\mathbf{1}_{(i,j)\in\Omega}]E_{ij}^2
=p\sum_{i,j}E_{ij}^2.\\
\text{Step 3: }&
=p\|E\|_F^2.
\end{align*}
}
\RESULT{
$\mathbb{E}\|P_\Omega(X-M)\|_F^2=p\|X-M\|_F^2$.}
\UNITCHECK{
Both sides are squared norms; linearity of expectation preserves units.}
\EDGECASES{
\begin{bullets}
\item $p=0$: expectation is $0$.
\item $p=1$: expectation equals full error.
\end{bullets}
}
\ALTERNATE{
View $P_\Omega$ as random diagonal operator with $\mathbb{E}[P_\Omega]=p I$.}
\VALIDATION{
\begin{bullets}
\item Monte Carlo with fixed seed to confirm numerically.
\item Check variance reduces with more samples. 
\end{bullets}
}
\INTUITION{
Randomly keeping entries with chance $p$ keeps the same fraction of squared
error on average.}
\CANONICAL{
\begin{bullets}
\item Expected sampling acts like a scalar $p$ times identity.
\item Guides loss rescaling by $1/p$ when estimating global error.
\end{bullets}
}

\ProblemPage{9}{Projection onto PSD Cone is Prox of Indicator}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Show that $\Pi_{\mathbb{S}_+^n}(A)=\operatorname{prox}_{\iota_{\mathbb{S}_+^n}}(A)$,
where $\iota_{\mathbb{S}_+^n}$ is the indicator of $\mathbb{S}_+^n$.
\PROBLEM{
Prove that the proximal operator equals the metric projection for closed convex
sets and apply to $\mathbb{S}_+^n$.}
\MODEL{
\[
\operatorname{prox}_{\iota_C}(A)=\underset{X\in C}{\arg\min}\ 
\tfrac12\|X-A\|_F^2.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $C$ closed, convex, nonempty.
\item Frobenius geometry; here $C=\mathbb{S}_+^n$.
\end{bullets}
}
\varmapStart
\var{C}{Closed convex set.}
\var{\iota_C}{Indicator function of $C$.}
\var{A}{Point to be projected.}
\varmapEnd
\WHICHFORMULA{
Formula 1 and proximal definition coincide for indicators.}
\GOVERN{
\[
\operatorname{prox}_{\iota_C}(A)=\Pi_C(A).
\]
}
\INPUTS{$C=\mathbb{S}_+^n$, $A\in\mathbb{S}^n$.}
\DERIVATION{
\begin{align*}
\text{Step 1: }&
\iota_C(X)=\begin{cases}0,&X\in C,\\ +\infty,&\text{else}.\end{cases}\\
\text{Step 2: }&
\operatorname{prox}_{\iota_C}(A)
=\arg\min_X \tfrac12\|X-A\|_F^2+\iota_C(X).\\
\text{Step 3: }&
\text{Feasible set reduces to }X\in C,\text{ objective is metric distance.}\\
\text{Step 4: }&
\Rightarrow \operatorname{prox}_{\iota_C}(A)=\Pi_C(A).
\end{align*}
}
\RESULT{
Projection onto $\mathbb{S}_+^n$ is the proximal map of its indicator.}
\UNITCHECK{
Definitions align; minimizers exist by closed convexity.}
\EDGECASES{
\begin{bullets}
\item If $C$ empty, prox undefined; not our case.
\item Nonunique minimizers possible if set is flat and distance ties; for
$\mathbb{S}_+^n$ value is unique. 
\end{bullets}
}
\ALTERNATE{
Derive via Moreau identity:
$A=\operatorname{prox}_{\iota_C}(A)+\operatorname{prox}_{\iota_{C^\circ}}(A)$
with $C^\circ$ polar cone; for $C$ self-dual, both are PSD projections of $A$
and $-A$.}
\VALIDATION{
\begin{bullets}
\item Compare with eigenvalue-truncation formula numerically.
\item Verify optimality via normal cone condition. 
\end{bullets}
}
\INTUITION{
Prox with a hard constraint simply returns the nearest feasible point.}
\CANONICAL{
\begin{bullets}
\item Indicators turn proximal steps into metric projections.
\item For cones, Moreau decomposition provides an orthogonal split. 
\end{bullets}
}

\ProblemPage{10}{Combo: Alternating Projections between PSD Cone and Affine Set}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Consider the feasibility problem: find $X$ such that $X\succeq 0$ and
$P_\Omega(X)=B$. Show Dykstra/alternating projections converge and compute a
few iterations on a toy example.
\PROBLEM{
(i) Define maps $A_1=\mathbb{S}_+^n$, $A_2=\{X: P_\Omega(X)=B\}$.
(ii) Initialize $X^{(0)}=0$; perform two iterations
$X^{(k+1)}=\Pi_{A_2}(\Pi_{A_1}(X^{(k)}))$.}
\MODEL{
\[
\text{Find }X\in A_1\cap A_2,\quad
\Pi_{A_1}=\Pi_{\mathbb{S}_+^n},\ \Pi_{A_2}(X)=P_\Omega(B)+P_{\Omega^\perp}(X).
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Closed convex sets $A_1,A_2$ with nonempty intersection.
\item Frobenius geometry.
\end{bullets}
}
\varmapStart
\var{A_1,A_2}{PSD cone and affine sampling set.}
\var{B}{Observed matrix on $\Omega$.}
\var{X^{(k)}}{Iterates.}
\varmapEnd
\WHICHFORMULA{
Formula 1 (PSD projection) and Formula 2 (sampling projector).}
\GOVERN{
\[
X^{(k+\frac12)}=\Pi_{\mathbb{S}_+^n}(X^{(k)}),\
X^{(k+1)}=P_\Omega(B)+P_{\Omega^\perp}(X^{(k+\frac12)}).
\]
}
\INPUTS{$n=2$, $B=\begin{bmatrix}1&0\\0&1\end{bmatrix}$,
$\Omega=\{(1,1),(2,2)\}$.}
\DERIVATION{
\begin{align*}
\text{Step 1: }&
X^{(0)}=0\Rightarrow X^{(0+\frac12)}=\Pi_{\mathbb{S}_+}(0)=0.\\
\text{Step 2: }&
X^{(1)}=P_\Omega(B)+P_{\Omega^\perp}(0)=\operatorname{diag}(1,1).\\
\text{Step 3: }&
X^{(1+\frac12)}=\Pi_{\mathbb{S}_+}(I)=I.\\
\text{Step 4: }&
X^{(2)}=P_\Omega(B)+P_{\Omega^\perp}(I)=I.\\
\text{Step 5: }&
\text{Converged in two steps to a feasible PSD matrix.}
\end{align*}
}
\RESULT{
Alternating projections reach a feasible point $X=I$ satisfying both sets.}
\UNITCHECK{
Each step preserves symmetry and matches $\Omega$ after $A_2$ projection.}
\EDGECASES{
\begin{bullets}
\item If $B$ inconsistent with PSD feasibility, Dykstra needed for best
approximation; plain alternation may cycle but converges to nearest points.
\item With full $\Omega$, solution is simply $\Pi_{\mathbb{S}_+}(B)$. 
\end{bullets}
}
\ALTERNATE{
Use Dykstra's algorithm with correction terms to ensure convergence to the
projection onto intersection.}
\VALIDATION{
\begin{bullets}
\item Check feasibility $X\succeq 0$ and $P_\Omega(X)=B$ at each iterate.
\item Monitor monotone decrease of distance to both sets. 
\end{bullets}
}
\INTUITION{
Bounce between PSD and data-consistency sets until you land in their overlap.}
\CANONICAL{
\begin{bullets}
\item Projection onto affine sampling set is simple masking.
\item PSD projection is eigenvalue clipping; their alternation solves feasibility.
\end{bullets}
}

\section{Coding Demonstrations}
\CodeDemoPage{PSD Cone Projection via Eigenvalue Thresholding}
\PROBLEM{
Implement $\Pi_{\mathbb{S}_+^n}(A)$ and verify PSD and optimality numerically.}
\API{
\begin{bullets}
\item \inlinecode{def read_input(s) -> np.ndarray} — parse matrix rows.
\item \inlinecode{def proj_psd(A) -> np.ndarray} — compute PSD projection.
\item \inlinecode{def validate() -> None} — assert PSD and distance optimality.
\item \inlinecode{def main() -> None} — orchestrate. 
\end{bullets}
}
\INPUTS{
Square symmetric matrix $A$ given by whitespace-separated rows.}
\OUTPUTS{
Projected matrix $X$ and its eigenvalues.}
\FORMULA{
\[
\Pi_{\mathbb{S}_+^n}(A)=Q\operatorname{diag}((\lambda_i)_+)Q^\top.
\]
}
\textbf{SOLUTION A — From Scratch (Mathematically Explicit Implementation)}
\begin{codepy}
import numpy as np

def read_input(s):
    rows = [[float(x) for x in ln.split()] for ln in s.strip().split("\n")]
    A = np.array(rows, dtype=float)
    return A

def proj_psd(A):
    w, Q = np.linalg.eigh(A)
    w_pos = np.clip(w, 0.0, None)
    return (Q * w_pos) @ Q.T

def frob(A):
    return float(np.sqrt(np.sum(A*A)))

def is_psd(X, tol=1e-10):
    w = np.linalg.eigvalsh(X)
    return np.min(w) >= -tol

def validate():
    A = np.array([[1.0, 2.0], [2.0, 1.0]])
    X = proj_psd(A)
    assert is_psd(X)
    # Check projection optimality by comparing to zero matrix if A<=0
    Z = np.zeros_like(A)
    dX = frob(X - A)
    dZ = frob(Z - A)
    assert dX <= dZ + 1e-9

def main():
    validate()
    A = np.array([[2.0, -1.0], [-1.0, 0.0]])
    X = proj_psd(A)
    w = np.linalg.eigvalsh(X)
    print("X=", np.round(X, 3))
    print("eig(X)=", np.round(w, 6))

if __name__ == "__main__":
    main()
\end{codepy}
\textbf{SOLUTION B — Library-Based (Validated Computational Shortcut)}
\begin{codepy}
import numpy as np

def read_input(s):
    return np.loadtxt(s.splitlines(), dtype=float)

def proj_psd(A):
    # Using eigh is already library-based; same as above
    w, Q = np.linalg.eigh(A)
    X = (Q * np.maximum(w, 0.0)) @ Q.T
    return X

def validate():
    rng = np.random.default_rng(0)
    A = rng.standard_normal((3, 3))
    A = (A + A.T) / 2.0
    X = proj_psd(A)
    assert np.all(np.linalg.eigvalsh(X) >= -1e-10)

def main():
    validate()
    A = np.array([[0.0, 2.0], [2.0, -1.0]])
    X = proj_psd(A)
    print("proj_psd:\n", np.round(X, 3))

if __name__ == "__main__":
    main()
\end{codepy}
\COMPLEXITY{
Time $\mathcal{O}(n^3)$ for eigen-decomposition, space $\mathcal{O}(n^2)$.}
\FAILMODES{
\begin{bullets}
\item Non-symmetric inputs: symmetrize via $(A+A^\top)/2$ before projecting.
\item Numerical negatives after projection: clip eigenvalues with tolerance.
\end{bullets}
}
\STABILITY{
\begin{bullets}
\item Near-zero negative eigenvalues subject to rounding; use tolerance clipping.
\item Orthogonal eigenvectors ensure backward-stable computations with eigh.
\end{bullets}
}
\VALIDATION{
\begin{bullets}
\item Assert eigenvalues of output are nonnegative.
\item Compare Frobenius distances to a few random PSD candidates.
\end{bullets}
}
\RESULT{
Both implementations output PSD matrices; eigenvalues are $\ge 0$ within
tolerance, confirming the projection formula.}
\EXPLANATION{
Diagonalize $A$, zero out negative eigenvalues, and rotate back—exactly the PSD
projection derived in Formula 1.}

\CodeDemoPage{SVT for Nuclear-Norm Prox and Matrix Completion Step}
\PROBLEM{
Implement SVT and one proximal-gradient step for
$\tfrac12\|P_\Omega(X-M)\|_F^2+\lambda\|X\|_*$.}
\API{
\begin{bullets}
\item \inlinecode{def svt(Y, tau) -> np.ndarray} — singular value shrinkage.
\item \inlinecode{def grad_step(X, M, Omega)} — gradient on observed entries.
\item \inlinecode{def prox_grad(X, M, Omega, lam, eta)} — one prox step.
\item \inlinecode{def validate()} — convergence on a toy instance.
\end{bullets}
}
\INPUTS{
$M$, $\Omega$ as boolean mask, step size $\eta$, regularization $\lambda$.}
\OUTPUTS{
Updated $X$ after one proximal-gradient iteration.}
\FORMULA{
\[
X^{+}=\operatorname{SVT}_{\eta\lambda}\bigl(X-\eta\,P_\Omega(X-M)\bigr).
\]
}
\textbf{SOLUTION A — From Scratch (Mathematically Explicit Implementation)}
\begin{codepy}
import numpy as np

def svt(Y, tau):
    U, s, Vt = np.linalg.svd(Y, full_matrices=False)
    s_sh = np.maximum(s - tau, 0.0)
    return (U * s_sh) @ Vt

def grad_step(X, M, Omega):
    G = np.zeros_like(X)
    G[Omega] = X[Omega] - M[Omega]
    return G

def prox_grad(X, M, Omega, lam, eta):
    Y = X - eta * grad_step(X, M, Omega)
    return svt(Y, eta * lam)

def validate():
    M = np.array([[1.0, 2.0], [3.0, 4.0]])
    Omega = np.array([[1, 0], [0, 1]], dtype=bool)
    X = np.zeros_like(M)
    lam, eta = 0.5, 1.0
    for _ in range(20):
        X = prox_grad(X, M, Omega, lam, eta)
    # Check constraints on Omega approximately
    assert abs(X[0, 0] - M[0, 0]) < 0.2
    assert abs(X[1, 1] - M[1, 1]) < 0.2

def main():
    validate()
    print("SVT prox-gradient validated.")

if __name__ == "__main__":
    main()
\end{codepy}
\textbf{SOLUTION B — Library-Based (Validated Computational Shortcut)}
\begin{codepy}
import numpy as np

def svt(Y, tau):
    # Use numpy.linalg.svd; same as above but viewed as library operation
    U, s, Vt = np.linalg.svd(Y, full_matrices=False)
    s = np.maximum(s - tau, 0.0)
    return (U * s) @ Vt

def prox_grad(X, M, Omega, lam, eta):
    G = np.zeros_like(X); G[Omega] = X[Omega] - M[Omega]
    return svt(X - eta * G, eta * lam)

def validate():
    np.random.seed(0)
    M = np.array([[5.0, 0.0, 0.0], [0.0, 3.0, 0.0]])
    Omega = np.array([[1, 0, 1], [0, 1, 0]], dtype=bool)
    X = np.zeros_like(M)
    for _ in range(50):
        X = prox_grad(X, M, Omega, lam=0.3, eta=1.0)
    assert np.allclose(X[Omega], M[Omega], atol=0.3)

def main():
    validate()
    print("Library SVT step validated.")

if __name__ == "__main__":
    main()
\end{codepy}
\COMPLEXITY{
Each step: SVD time $\mathcal{O}(\min\{mn^2,m^2n\})$, space $\mathcal{O}(mn)$.}
\FAILMODES{
\begin{bullets}
\item Step size too large: divergence; choose $\eta\le 1$ for Lipschitz $1$.
\item All singular values shrunk to zero: increase $\eta$ or decrease $\lambda$.
\end{bullets}
}
\STABILITY{
\begin{bullets}
\item SVD numerically stable; near-threshold singular values may flip rank due
to rounding.
\item Use double precision and tolerance-based rank truncation if needed.
\end{bullets}
}
\VALIDATION{
\begin{bullets}
\item Check decrease of objective across iterations.
\item Verify observed entries approach measurements.
\end{bullets}
}
\RESULT{
SVT proximal step lowers observed loss while promoting low rank.}
\EXPLANATION{
Implements Formula 3 and applies it to the gradient step of Formula 4.}

\CodeDemoPage{Alternating Projections: PSD and Sampling Affine Set}
\PROBLEM{
Implement alternating projections between $\mathbb{S}_+^n$ and
$\{X:P_\Omega(X)=B\}$, demonstrate feasibility on a toy instance.}
\API{
\begin{bullets}
\item \inlinecode{def proj_psd(A)} — PSD projection.
\item \inlinecode{def proj_affine(X,B,Omega)} — enforce sampling constraints.
\item \inlinecode{def altproj(B,Omega,k)} — k iterations of alternation.
\item \inlinecode{def validate()} — asserts feasibility. 
\end{bullets}
}
\INPUTS{
$B$ target on $\Omega$, boolean mask $\Omega$, iterations $k$.}
\OUTPUTS{
Final iterate $X$, feasibility residuals.}
\FORMULA{
\[
X^{(k+1)}=P_\Omega(B)+P_{\Omega^\perp}(\Pi_{\mathbb{S}_+}(X^{(k)})).
\]
}
\textbf{SOLUTION A — From Scratch (Mathematically Explicit Implementation)}
\begin{codepy}
import numpy as np

def proj_psd(A):
    w, Q = np.linalg.eigh((A + A.T) / 2.0)
    return (Q * np.maximum(w, 0.0)) @ Q.T

def proj_affine(X, B, Omega):
    Y = X.copy()
    Y[Omega] = B[Omega]
    return Y

def altproj(B, Omega, k=10):
    X = np.zeros_like(B)
    for _ in range(k):
        X = proj_psd(X)
        X = proj_affine(X, B, Omega)
    return X

def validate():
    B = np.array([[1.0, 0.0], [0.0, 1.0]])
    Omega = np.array([[1, 0], [0, 1]], dtype=bool)
    X = altproj(B, Omega, k=5)
    assert np.allclose(X[Omega], B[Omega], atol=1e-9)
    assert np.min(np.linalg.eigvalsh(X)) >= -1e-9

def main():
    validate()
    print("Alternating projections feasibility OK.")

if __name__ == "__main__":
    main()
\end{codepy}
\textbf{SOLUTION B — Library-Based (Validated Computational Shortcut)}
\begin{codepy}
import numpy as np

def proj_psd(A):
    w, Q = np.linalg.eigh(A)
    return (Q * np.maximum(w, 0.0)) @ Q.T

def altproj(B, Omega, k=10):
    X = np.zeros_like(B)
    for _ in range(k):
        X = proj_psd(X)
        X[Omega] = B[Omega]
    return X

def validate():
    B = np.eye(2)
    Omega = np.array([[1, 0], [0, 1]], dtype=bool)
    X = altproj(B, Omega, k=3)
    assert np.allclose(X, B, atol=1e-9)

def main():
    validate()
    print("Library alt-proj validated.")

if __name__ == "__main__":
    main()
\end{codepy}
\COMPLEXITY{
Per iteration: eigen-decomposition $\mathcal{O}(n^3)$, masking
$\mathcal{O}(n^2)$.}
\FAILMODES{
\begin{bullets}
\item Infeasible intersection: algorithm may not converge to feasible point;
use Dykstra with corrections to compute best approximation.
\item Non-symmetric drift: symmetrize before PSD projection.
\end{bullets}
}
\STABILITY{
\begin{bullets}
\item Small negative eigenvalues due to rounding; clip at tolerance.
\item Convergence speed depends on angle between sets. 
\end{bullets}
}
\VALIDATION{
\begin{bullets}
\item Check feasibility residuals decay.
\item Monitor eigenvalues nonnegativity at each step.
\end{bullets}
}
\RESULT{
Alternating projections produce a feasible PSD matrix meeting observed entries.}
\EXPLANATION{
Direct implementation of Formula 1 and 2 alternating to satisfy both
constraints.}

\section{Applied Domains — Detailed End-to-End Scenarios}
\DomainPage{Machine Learning}
\SCENARIO{
Complete a user-item rating matrix via nuclear-norm regularized objective using
SVT-based proximal gradient and evaluate reconstruction error.}
\ASSUMPTIONS{
\begin{bullets}
\item Low-rank ratings due to few latent factors.
\item Uniform random missingness.
\end{bullets}
}
\WHICHFORMULA{
Use Formula 3 (SVT prox) within Formula 4 (matrix completion objective).}
\varmapStart
\var{R}{Observed rating matrix with NaNs for missing entries.}
\var{\Omega}{Boolean mask of observed entries.}
\var{X}{Completed matrix estimate.}
\var{\lambda,\eta}{Regularization and step size.}
\varmapEnd
\PIPELINE{
\begin{bullets}
\item Generate low-rank synthetic ratings and mask entries.
\item Run proximal gradient with SVT to estimate $X$.
\item Evaluate RMSE on unobserved entries using hold-out mask.
\end{bullets}
}
\textbf{Implementation (From Scratch)}
\begin{codepy}
import numpy as np

def gen_data(m=50, n=40, r=3, p=0.5, seed=0):
    rng = np.random.default_rng(seed)
    U = rng.standard_normal((m, r))
    V = rng.standard_normal((n, r))
    M = U @ V.T
    Omega = rng.random((m, n)) < p
    R = np.where(Omega, M, 0.0)
    return M, R, Omega

def svt(Y, tau):
    U, s, Vt = np.linalg.svd(Y, full_matrices=False)
    s = np.maximum(s - tau, 0.0)
    return (U * s) @ Vt

def proxgrad(R, Omega, lam=1.0, eta=1.0, iters=100):
    X = np.zeros_like(R)
    for _ in range(iters):
        G = np.zeros_like(R); G[Omega] = X[Omega] - R[Omega]
        X = svt(X - eta * G, eta * lam)
    return X

def main():
    M, R, Omega = gen_data()
    X = proxgrad(R, Omega, lam=1.0, eta=1.0, iters=100)
    mask = ~Omega
    err = np.sqrt(np.mean((X[mask] - M[mask])**2))
    print("RMSE missing:", round(float(err), 4))

if __name__ == "__main__":
    main()
\end{codepy}
\textbf{Implementation (Library Version)}
\begin{codepy}
import numpy as np

def lowrank_svt(R, Omega, lam=1.0, eta=1.0, iters=50):
    X = np.zeros_like(R)
    for _ in range(iters):
        G = np.zeros_like(R); G[Omega] = X[Omega] - R[Omega]
        U, s, Vt = np.linalg.svd(X - eta * G, full_matrices=False)
        s = np.maximum(s - eta * lam, 0.0)
        X = (U * s) @ Vt
    return X

def main():
    np.random.seed(0)
    M = np.random.randn(30, 20) @ np.random.randn(20, 5).T
    Omega = np.random.rand(30, 20) < 0.6
    R = np.where(Omega, M, 0.0)
    X = lowrank_svt(R, Omega, 0.8, 1.0, 60)
    rmse = np.sqrt(np.mean((X[~Omega] - M[~Omega])**2))
    print("RMSE:", round(float(rmse), 4))

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{RMSE on missing entries; lower is better.}
\INTERPRET{
SVT progressively denoises and reveals latent structure, predicting unobserved
ratings from low-rank patterns.}
\NEXTSTEPS{
Adaptive step sizes, acceleration, or factorization-based solvers.}

\DomainPage{Quantitative Finance}
\SCENARIO{
Project an empirical covariance matrix to the PSD cone to remove negative
eigenvalues and compute portfolio variance before/after cleaning.}
\ASSUMPTIONS{
\begin{bullets}
\item Sample covariance may be indefinite due to noise.
\item Portfolio variance uses $w^\top \Sigma w$.
\end{bullets}
}
\WHICHFORMULA{
Use Formula 1 for $\Pi_{\mathbb{S}_+}(\hat{\Sigma})$ and compare variances.}
\varmapStart
\var{\hat{\Sigma}}{Empirical covariance.}
\var{\Sigma^+}{PSD projection of $\hat{\Sigma}$.}
\var{w}{Portfolio weights.}
\varmapEnd
\PIPELINE{
\begin{bullets}
\item Simulate returns, compute $\hat{\Sigma}$.
\item Project to $\Sigma^+$ via eigenvalue clipping.
\item Compare $w^\top \hat{\Sigma} w$ vs. $w^\top \Sigma^+ w$.
\end{bullets}
}
\textbf{Implementation (Full Pipeline)}
\begin{codepy}
import numpy as np

def simulate(n=500, d=5, seed=0):
    rng = np.random.default_rng(seed)
    A = rng.standard_normal((d, d))
    cov = A @ A.T
    R = rng.multivariate_normal(np.zeros(d), cov, size=n)
    return R

def cov_emp(R):
    Rc = R - R.mean(axis=0)
    return (Rc.T @ Rc) / (len(R) - 1)

def proj_psd(S):
    w, Q = np.linalg.eigh(S)
    return (Q * np.maximum(w, 0.0)) @ Q.T

def main():
    R = simulate()
    S = cov_emp(R)
    S_clean = proj_psd(S)
    w = np.array([0.4, 0.3, 0.2, 0.1, 0.0])
    v_raw = float(w.T @ S @ w)
    v_clean = float(w.T @ S_clean @ w)
    print("Var raw:", round(v_raw, 6), "Var clean:", round(v_clean, 6))

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{Portfolio variance before/after cleaning.}
\INTERPRET{
PSD cleaning avoids negative variance directions, stabilizing risk estimates.}
\NEXTSTEPS{
Shrinkage or factor-based covariance models combined with PSD projection.}

\DomainPage{Deep Learning}
\SCENARIO{
Preprocess a Gram matrix of features by projecting to PSD to ensure valid kernel
and impute missing similarities via low-rank completion.}
\ASSUMPTIONS{
\begin{bullets}
\item Gram matrix should be PSD to define a valid kernel.
\item Low-rank structure approximates embeddings.
\end{bullets}
}
\WHICHFORMULA{
Use Formula 1 to enforce PSD and Formula 3 for low-rank imputation via SVT.}
\PIPELINE{
\begin{bullets}
\item Build noisy Gram matrix with missing entries.
\item Alternate filling missing entries and PSD projection.
\item Use the result as kernel for downstream models.
\end{bullets}
}
\textbf{Implementation (End-to-End)}
\begin{codepy}
import numpy as np

def make_gram(n=40, r=5, p=0.7, seed=0):
    rng = np.random.default_rng(seed)
    Z = rng.standard_normal((n, r))
    G = Z @ Z.T
    Mask = rng.random((n, n)) < p
    Mask = np.triu(Mask, 0); Mask = Mask | Mask.T
    B = np.where(Mask, G, 0.0)
    return G, B, Mask

def proj_psd(A):
    w, Q = np.linalg.eigh((A + A.T) / 2.0)
    return (Q * np.maximum(w, 0.0)) @ Q.T

def impute_psd(B, Mask, iters=10, lam=0.0):
    X = np.zeros_like(B)
    for _ in range(iters):
        X[Mask] = B[Mask]
        X = proj_psd(X)
    return X

def main():
    G, B, Mask = make_gram()
    X = impute_psd(B, Mask, iters=10)
    mineig = float(np.min(np.linalg.eigvalsh(X)))
    print("Min eig:", round(mineig, 9))

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{Minimum eigenvalue, indicating PSD status.}
\INTERPRET{
Alternating enforcement of symmetry, sampling, and PSD yields a valid kernel
that approximates the true Gram matrix.}
\NEXTSTEPS{
Incorporate SVT between steps to promote low-rank solutions.}

\DomainPage{Kaggle / Data Analytics}
\SCENARIO{
Impute missing entries in a data matrix via low-rank completion and verify
consistency on held-out entries.}
\ASSUMPTIONS{
\begin{bullets}
\item Missing at random.
\item Data approximately low rank after centering.
\end{bullets}
}
\WHICHFORMULA{
Proximal gradient with SVT (Formula 3) on the observed loss (Formula 4).}
\PIPELINE{
\begin{bullets}
\item Generate synthetic low-rank data with missing entries.
\item Run SVT-based completion.
\item Evaluate RMSE on a held-out mask.
\end{bullets}
}
\textbf{Implementation (Complete EDA Pipeline)}
\begin{codepy}
import numpy as np

def gen(n=200, d=30, r=4, p_train=0.6, p_test=0.2, seed=0):
    rng = np.random.default_rng(seed)
    U = rng.standard_normal((n, r))
    V = rng.standard_normal((d, r))
    M = U @ V.T
    Ome_tr = rng.random((n, d)) < p_train
    Ome_te = (~Ome_tr) & (rng.random((n, d)) < p_test)
    R = np.where(Ome_tr, M, 0.0)
    return M, R, Ome_tr, Ome_te

def svt(Y, t):
    U, s, Vt = np.linalg.svd(Y, full_matrices=False)
    return (U * np.maximum(s - t, 0.0)) @ Vt

def complete(R, Omega, lam=1.0, eta=1.0, iters=100):
    X = np.zeros_like(R)
    for _ in range(iters):
        G = np.zeros_like(R); G[Omega] = X[Omega] - R[Omega]
        X = svt(X - eta * G, eta * lam)
    return X

def main():
    M, R, Ome_tr, Ome_te = gen()
    X = complete(R, Ome_tr, lam=1.0, eta=1.0, iters=80)
    rmse = np.sqrt(np.mean((X[Ome_te] - M[Ome_te])**2))
    print("Held-out RMSE:", round(float(rmse), 4))

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{RMSE on held-out set.}
\INTERPRET{
Low-rank completion captures latent structure and predicts missing values with
quantified error on held-out entries.}
\NEXTSTEPS{
Center/scale features, tune $\lambda$, add acceleration or warm starts.}

\end{document}
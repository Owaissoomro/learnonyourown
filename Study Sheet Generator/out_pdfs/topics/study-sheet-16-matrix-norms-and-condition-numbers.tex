% !TeX program = xelatex
\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{microtype,setspace,amsmath,amssymb,mathtools,amsthm,unicode-math}
\setstretch{1.05}
\setmainfont{Latin Modern Roman}
\setmonofont{Latin Modern Mono}
\setmathfont{Latin Modern Math}

\allowdisplaybreaks[4]
\setlength{\jot}{7pt}
\setlength{\emergencystretch}{8em}
\sloppy

\usepackage{xcolor,fancyhdr,enumitem,inconsolata,listings}
\pagestyle{fancy}\fancyhf{}\lhead{\nouppercase{\leftmark}}\rhead{\thepage}
\setlength{\headheight}{26pt}
\setlength{\parindent}{0pt}
\setlength{\parskip}{8pt plus 2pt minus 1pt}
\raggedbottom

\newenvironment{BreakableEquation}{\begin{equation}\begin{aligned}}{\end{aligned}\end{equation}}
\newenvironment{BreakableEquation*}{\begin{equation*}\begin{aligned}}{\end{aligned}\end{equation*}}
\newenvironment{tightalign}{\begingroup\small\allowdisplaybreaks\begin{align}}{\end{align}\endgroup}

\providecommand{\enumlistm}{enumitem}
\newenvironment{minted}[2][]{%
  \lstset{style=code,language=#2,#1}\begin{lstlisting}%
}{\end{lstlisting}}
\newcommand{\inputminted}[3][]{\begin{lstlisting}\end{lstlisting}}

\newlist{bullets}{itemize}{1}
\setlist[bullets]{label=--,leftmargin=1.6em,itemsep=2pt,topsep=4pt}

\newcommand{\varmapStart}{\textbf{VARIABLE MAPPING:}\par\begin{bullets}}
\newcommand{\var}[2]{\item $#1$ — #2}
\newcommand{\varmapEnd}{\end{bullets}}

\newcommand{\glossx}[6]{%
  \textbf{#1}\par
  \begin{bullets}
    \item \textbf{What:} #2
    \item \textbf{Why:} #3
    \item \textbf{How:} #4
    \item \textbf{ELI5:} #5
    \item \textbf{Pitfall/Example:} #6
  \end{bullets}
}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{claim}[theorem]{Claim}

\lstdefinestyle{code}{
  basicstyle=\ttfamily\footnotesize,
  backgroundcolor=\color{black!02},
  frame=single,
  numbers=left, numberstyle=\tiny, numbersep=8pt,
  breaklines=true, breakatwhitespace=true,
  tabsize=4, showstringspaces=false,
  upquote=true, keepspaces=true, columns=fullflexible,
  literate=
    {–}{{-}}1
    {—}{{-}}1
    {…}{{...}}1
    {≤}{{\ensuremath{\le}}}1
    {≥}{{\ensuremath{\ge}}}1
    {≠}{{\ensuremath{\ne}}}1
    {≈}{{\ensuremath{\approx}}}1
    {±}{{\ensuremath{\pm}}}1
    {→}{{\ensuremath{\to}}}1
    {←}{{\ensuremath{\leftarrow}}}1
    {∞}{{\ensuremath{\infty}}}1
    {√}{{\ensuremath{\sqrt{\ }}}}1
    {×}{{\ensuremath{\times}}}1
    {÷}{{\ensuremath{\div}}}1
}

\lstnewenvironment{codepy}[1][]%
  {\lstset{style=code,language=Python,#1}}%
  {}

\newcommand{\inlinecode}[1]{\lstinline[style=code]!#1!}

\newcommand{\LF}[2]{\par\noindent\textbf{#1:}~#2\par}
\newcommand{\WHAT}[1]{\LF{WHAT}{#1}}
\newcommand{\WHY}[1]{\LF{WHY}{#1}}
\newcommand{\HOW}[1]{\LF{HOW}{#1}}
\newcommand{\ELI}[1]{\LF{ELI5}{#1}}
\newcommand{\SCOPE}[1]{\LF{SCOPE}{#1}}
\newcommand{\CONFUSIONS}[1]{\LF{COMMON CONFUSIONS}{#1}}
\newcommand{\APPLICATIONS}[1]{\LF{APPLICATIONS}{#1}}
\newcommand{\FORMULA}[1]{\LF{FORMULA}{#1}}
\newcommand{\CANONICAL}[1]{\LF{CANONICAL FORM}{#1}}
\newcommand{\PRECONDS}[1]{\LF{PRECONDITIONS}{#1}}
\newcommand{\DERIVATION}[1]{\LF{DERIVATION}{#1}}
\newcommand{\EQUIV}[1]{\LF{EQUIVALENT FORMS}{#1}}
\newcommand{\LIMITS}[1]{\LF{LIMIT CASES}{#1}}
\newcommand{\INPUTS}[1]{\LF{INPUTS}{#1}}
\newcommand{\OUTPUTS}[1]{\LF{OUTPUTS}{#1}}
\newcommand{\RESULT}[1]{\LF{RESULT}{#1}}
\newcommand{\INTUITION}[1]{\LF{INTUITION}{#1}}
\newcommand{\PITFALLS}[1]{\LF{PITFALLS}{#1}}
\newcommand{\MODEL}[1]{\LF{CANONICAL MATH MODEL}{#1}}
\newcommand{\ASSUMPTIONS}[1]{\LF{ASSUMPTIONS}{#1}}
\newcommand{\WHICHFORMULA}[1]{\LF{WHICH FORMULA \& WHY}{#1}}
\newcommand{\GOVERN}[1]{\LF{GOVERNING EQUATION(S)}{#1}}
\newcommand{\UNITCHECK}[1]{\LF{UNIT CHECK}{#1}}
\newcommand{\EDGECASES}[1]{\LF{EDGE CASES}{#1}}
\newcommand{\ALTERNATE}[1]{\LF{ALTERNATE APPROACH (sketch)}{#1}}
\newcommand{\PROBLEM}[1]{\LF{PROBLEM}{#1}}
\newcommand{\API}[1]{\LF{API}{#1}}
\newcommand{\COMPLEXITY}[1]{\LF{COMPLEXITY}{#1}}
\newcommand{\FAILMODES}[1]{\LF{FAILURE MODES}{#1}}
\newcommand{\STABILITY}[1]{\LF{NUMERICAL STABILITY}{#1}}
\newcommand{\VALIDATION}[1]{\LF{VALIDATION}{#1}}
\newcommand{\EXPLANATION}[1]{\LF{EXPLANATION}{#1}}
\newcommand{\SCENARIO}[1]{\LF{SCENARIO}{#1}}
\newcommand{\PIPELINE}[1]{\LF{PIPELINE STEPS}{#1}}
\newcommand{\METRICS}[1]{\LF{METRICS}{#1}}
\newcommand{\INTERPRET}[1]{\LF{INTERPRETATION}{#1}}
\newcommand{\NEXTSTEPS}[1]{\LF{LIMITATIONS \& NEXT STEPS}{#1}}

\usepackage{titlesec}
\titleformat{\section}{\Large\bfseries}{\thesection}{0.6em}{}
\titlespacing*{\section}{0pt}{*2}{*1}
\usepackage{etoolbox}
\pretocmd{\section}{\clearpage}{}{}

\newcommand{\FormulaPage}[2]{%
  \clearpage
  \section*{Formula #1 — #2}%
  \addcontentsline{toc}{section}{Formula #1 — #2}%
}
\newcommand{\ProblemPage}[2]{%
  \clearpage
  \subsection*{Problem #1: #2}%
  \addcontentsline{toc}{subsection}{Problem #1: #2}%
}
\newcommand{\CodeDemoPage}[1]{%
  \clearpage
  \subsection*{Coding Demo: #1}%
  \addcontentsline{toc}{subsection}{Coding Demo: #1}%
}
\newcommand{\DomainPage}[1]{%
  \clearpage
  \subsection*{#1 (End-to-End)}%
  \addcontentsline{toc}{subsection}{#1 (End-to-End)}%
}

\begin{document}
\title{Comprehensive Study Sheet — Matrix Norms and Condition Numbers}
\date{\today}
\maketitle
\tableofcontents
\clearpage

\section{Concept Overview}
\WHAT{
A matrix norm $\|\cdot\|:\mathbb{C}^{m\times n}\to[0,\infty)$ assigns nonnegative
sizes to matrices and typically is consistent with a vector norm
$\|\cdot\|:\mathbb{C}^n\to[0,\infty)$ via the induced (operator) norm
$\|A\|=\sup_{x\ne0}\frac{\|Ax\|}{\|x\|}$. A condition number
$\kappa_{\|\cdot\|}(A)=\|A\|\,\|A^{-1}\|$ for a nonsingular square matrix
measures sensitivity of linear solves $Ax=b$ to perturbations.
}
\WHY{
Matrix norms quantify amplification of vectors by linear maps, enabling
stability analysis, convergence proofs, and error bounds. Condition numbers
provide precise sensitivity metrics for numerical linear algebra tasks such as
solving linear systems and least squares, guiding algorithm selection and
preconditioning strategies.
}
\HOW{
1. Fix a vector norm and induce an operator norm on matrices by a variational
supremum. 2. Prove foundational properties: homogeneity, triangle inequality,
and submultiplicativity $\|AB\|\le\|A\|\,\|B\|$. 3. Connect special cases to
closed forms: $\|A\|_2=\sigma_{\max}(A)$; $\|A\|_1$ equals maximal column sum;
$\|A\|_\infty$ equals maximal row sum. 4. Define condition number and derive
forward/backward error bounds relating residuals to solution error.
}
\ELI{
A matrix norm tells how much a matrix can stretch a vector. The condition
number says how much tiny input mistakes can get magnified in the answer when
solving $Ax=b$. Small condition number: the system is well-behaved; large one:
answers can wobble a lot from small changes.
}
\SCOPE{
We focus on subordinate (induced) norms and their explicit forms for $p\in
\{1,2,\infty\}$, spectral properties via singular values, and condition numbers
of nonsingular square matrices. Noninduced norms (Frobenius) appear only as
comparators. Perturbation bounds assume existence of inverses and
submultiplicativity; Neumann-series arguments require $\|A^{-1}\Delta A\|<1$.
}
\CONFUSIONS{
Matrix 2-norm vs. Frobenius norm: $\|A\|_2=\sigma_{\max}$ while
$\|A\|_F=(\sum\sigma_i^2)^{1/2}$. Condition number vs. residual:
$\kappa(A)$ bounds error sensitivity; residual $\|b-A\hat x\|$ is not itself an
error but translates to one via $A^{-1}$. Norm of $A$ vs. spectral radius:
$\rho(A)\le\|A\|$; they coincide only in special cases.
}
\APPLICATIONS{
\begin{bullets}
\item Mathematical foundations (pure / applied).
\item Computational modeling or simulation.
\item Physical / economic / engineering interpretations.
\item Statistical or algorithmic implications.
\end{bullets}
}

\textbf{ANALYTIC STRUCTURE.}
Induced norms are convex, positively homogeneous, and submultiplicative.
The spectral norm is unitarily invariant and equals the largest singular value.
Condition numbers are scale-invariant under scalar multiplication and, for
the 2-norm, unitarily invariant.

\textbf{CANONICAL LINKS.}
Singular Value Decomposition connects operator norms and condition numbers.
Neumann series underpins perturbation inverses. Hölder and dual norms support
explicit 1- and $\infty$-norm forms. Residual-to-error bounds combine
$A^{-1}$ with submultiplicativity.

\textbf{PROBLEM-TYPE RECOGNITION HEURISTICS.}
\begin{bullets}
\item Supremum of $\|Ax\|/\|x\|$ indicates operator norm tasks.
\item Requests for $\|A\|_1$/$\|A\|_\infty$ suggest column/row sum maxima.
\item Sensitivity or stability language signals condition numbers.
\item Bounds mixing $A$ and $A^{-1}$ indicate perturbation analyses.
\end{bullets}

\textbf{SOLUTION STRATEGY BLUEPRINT.}
\begin{bullets}
\item Translate the question into a variational form or SVD representation.
\item Identify the appropriate norm and its explicit characterization.
\item Apply submultiplicativity and unitary invariance if present.
\item Use SVD or norm equivalences to simplify and bound.
\item Interpret bounds as stability or sensitivity statements.
\end{bullets}

\textbf{CONCEPTUAL INVARIANTS.}
Under unitary similarity, $\|A\|_2$ and $\kappa_2(A)$ remain unchanged.
Scaling $A\mapsto\alpha A$ leaves $\kappa(A)$ invariant.

\textbf{EDGE INTUITION.}
As $\sigma_{\min}(A)\to0$, $\kappa_2(A)\to\infty$ and solutions become
highly sensitive. If $\|A^{-1}\Delta A\|\to1^{-}$, Neumann-series bounds
blow up, signaling near-singularity.

\clearpage
\section{Glossary}
\glossx{Induced (Operator) Matrix Norm}
{For a given vector norm $\|\cdot\|$ on $\mathbb{C}^n$, the induced
matrix norm on $\mathbb{C}^{m\times n}$ is
$\|A\|=\sup_{x\ne0}\frac{\|Ax\|}{\|x\|}=\sup_{\|x\|=1}\|Ax\|$.}
{Measures worst-case amplification of vectors by the linear map $x\mapsto Ax$.}
{Choose a vector norm, then take a supremum of the output norm over unit
inputs; use SVD or explicit forms to compute.}
{Think of a rubber sheet: the operator norm is the strongest local pull the
matrix applies to any direction.}
{Pitfall: confusing induced norm with entrywise norms such as Frobenius; only
induced norms guarantee submultiplicativity with the matched vector norm.}

\glossx{Spectral Norm}
{Matrix norm $\|A\|_2$ induced by the Euclidean vector norm; equals the
largest singular value $\sigma_{\max}(A)$.}
{Links linear algebraic geometry (SVD) with analytic bounds; unitarily
invariant and central to stability of iterative algorithms.}
{Compute SVD $A=U\Sigma V^*$ and read $\|A\|_2=\sigma_{\max}$.}
{The strongest stretch of $A$ on the unit sphere.}
{Pitfall: $\|A\|_2$ is not the spectral radius unless $A$ is normal and
positive semidefinite in $A^*A$.}

\glossx{Condition Number}
{For nonsingular $A\in\mathbb{C}^{n\times n}$ and matrix norm $\|\cdot\|$,
$\kappa(A)=\|A\|\,\|A^{-1}\|$. For $\|\cdot\|_2$, $\kappa_2(A)=
\sigma_{\max}(A)/\sigma_{\min}(A)$.}
{Quantifies worst-case relative error amplification in solving $Ax=b$.}
{Compute $\|A\|$ and $\|A^{-1}\|$ (or $\sigma_{\max}/\sigma_{\min}$ for 2-norm).
Use to bound forward error by residual or data perturbations.}
{A lever length: a long lever (large $\kappa$) magnifies tiny pushes
(errors) into big movements (solution changes).}
{Pitfall: $\kappa(A)$ depends on the chosen norm; reporting it without
specifying the norm can mislead.}

\glossx{Submultiplicativity}
{Property $\|AB\|\le\|A\|\,\|B\|$ for consistent operator norms.}
{Enables stability analysis and proofs by bounding compositions of operators.}
{Use $\|ABx\|\le\|A\|\,\|Bx\|\le\|A\|\,\|B\|\,\|x\|$ and take a supremum.}
{Like gear ratios: the total amplification cannot exceed the product of
individual amplifications.}
{Pitfall: Non-induced norms need not satisfy $\|AB\|\le\|A\|\,\|B\|$.}

\clearpage
\section{Symbol Ledger}
\varmapStart
\var{A\in\mathbb{C}^{m\times n}}{Matrix or linear operator.}
\var{x\in\mathbb{C}^{n}}{Vector input to $A$.}
\var{b\in\mathbb{C}^{m}}{Right-hand side in $Ax=b$.}
\var{\|\cdot\|}{Vector or matrix norm (context specifies).}
\var{\|A\|_{p}}{Operator norm induced by vector $\ell_p$ norm.}
\var{\|A\|_{1}}{Maximal absolute column sum of $A$.}
\var{\|A\|_{\infty}}{Maximal absolute row sum of $A$.}
\var{\|A\|_{2}}{Spectral norm; largest singular value.}
\var{\sigma_i(A)}{Singular values, sorted $\sigma_{\max}\ge\dots\ge\sigma_{\min}$.}
\var{\kappa(A)}{Condition number $\|A\|\,\|A^{-1}\|$ for nonsingular $A$.}
\var{U,V}{Unitary matrices in the SVD $A=U\Sigma V^*$.}
\var{\Sigma}{Diagonal matrix of singular values.}
\var{r}{Residual $r=b-A\hat{x}$.}
\var{\rho(\cdot)}{Spectral radius of a square matrix.}
\varmapEnd

\clearpage
\section{Formula Canon — One Formula Per Page}

\FormulaPage{1}{Induced Matrix Norm and Submultiplicativity}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
For a vector norm $\|\cdot\|$ on $\mathbb{C}^n$, the induced matrix norm on
$\mathbb{C}^{m\times n}$ is
$\|A\|=\sup_{x\ne0}\frac{\|Ax\|}{\|x\|}=\sup_{\|x\|=1}\|Ax\|$, and satisfies
submultiplicativity $\|AB\|\le\|A\|\,\|B\|$.

\WHAT{
The induced norm measures the maximal amplification factor of vectors by $A$,
and the inequality $\|AB\|\le\|A\|\,\|B\|$ bounds compositions of linear maps.
}
\WHY{
Defines a consistent operator norm needed for stable analysis of algorithms and
for bounding errors through chains of linear operations.
}
\FORMULA{
\[
\|A\|=\sup_{\|x\|=1}\|Ax\|,\qquad \|AB\|\le \|A\|\,\|B\|.
\]
}
\CANONICAL{
Vector norm arbitrary but fixed; matrix norm is subordinate to it. Applies to
all finite-dimensional complex or real spaces. No invertibility required.
}
\PRECONDS{
\begin{bullets}
\item A valid vector norm on $\mathbb{C}^n$ (nonnegativity, homogeneity,
triangle inequality, and $\|x\|=0\iff x=0$).
\item Linear maps $A:\mathbb{C}^n\to\mathbb{C}^m$, $B:\mathbb{C}^k\to
\mathbb{C}^n$ as needed.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
For any $A$ and any $x$, $\|Ax\|\le \|A\|\,\|x\|$ and
$\|AB\|\le \|A\|\,\|B\|$.
\end{lemma}
\begin{proof}
By definition, for $x\ne0$,
$\frac{\|Ax\|}{\|x\|}\le \sup_{y\ne0}\frac{\|Ay\|}{\|y\|}=\|A\|$, hence
$\|Ax\|\le\|A\|\,\|x\|$. For $AB$, for all $x$,
$\|ABx\|\le \|A\|\,\|Bx\|\le \|A\|\,\|B\|\,\|x\|$. Taking the supremum over
$\|x\|=1$ gives $\|AB\|\le \|A\|\,\|B\|$. \qedhere
\end{proof}
\DERIVATION{
\begin{align*}
\text{Step 1 (Definition):}\quad
& \|A\|=\sup_{x\ne0}\frac{\|Ax\|}{\|x\|}.\\
\text{Step 2 (Homogeneity):}\quad
& \text{Scaling $x\mapsto \alpha x$ with $\alpha\ne0$ shows }
\sup_{\|x\|=1}\|Ax\|.\\
\text{Step 3 (Submultiplicativity):}\quad
& \|AB\|=\sup_{\|x\|=1}\|ABx\|\le
\sup_{\|x\|=1}\|A\|\,\|Bx\|\\
&= \|A\|\,\sup_{\|x\|=1}\|Bx\|=\|A\|\,\|B\|.\\
\text{Step 4 (Triangle inequality):}\quad
& \|A+C\|=\sup_{\|x\|=1}\|(A+C)x\|\le
\sup_{\|x\|=1}(\|Ax\|+\|Cx\|)\\
&\le \sup_{\|x\|=1}\|Ax\|+\sup_{\|x\|=1}\|Cx\|=\|A\|+\|C\|.
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Express the target $\|A\|$ as $\sup_{\|x\|=1}\|Ax\|$.
\item Use structure: SVD for 2-norm, column/row sums for 1/$\infty$-norms.
\item For products, apply $\|AB\|\le\|A\|\,\|B\|$ iteratively.
\item Confirm via extremizers (eigen/singular vectors) when available.
\end{bullets}
\EQUIV{
\begin{bullets}
\item $\|A\|=\sup_{\|x\|\le1}\|Ax\|$ (same supremum).
\item Dual-norm variational forms: $\|A\|=\sup_{\|x\|=1,\ \|y\|_*=1}
\langle y,Ax\rangle$ when applicable.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item Finite-dimensionality ensures the supremum is attained on compact
unit spheres for continuous norms.
\item Noninduced norms need not be submultiplicative with the given vector
norm.
\end{bullets}
}
\INPUTS{$A\in\mathbb{C}^{m\times n}$, a vector norm on $\mathbb{C}^n$.}
\RESULT{
The induced norm gives the tight worst-case amplification and composes under
multiplication with at most multiplicative growth.
}
\UNITCHECK{
Dimensionless amplification factor: ratios $\|Ax\|/\|x\|$ cancel units.}
\PITFALLS{
\begin{bullets}
\item Mixing vector and matrix norms inconsistently breaks
submultiplicativity.
\item Forgetting to restrict to $\|x\|=1$ complicates computation.
\end{bullets}
}
\INTUITION{
The operator norm is the furthest distance a unit input can be pushed by $A$.
Submultiplicativity says chained pulls cannot exceed the product of each pull.
}
\CANONICAL{
\begin{bullets}
\item $\|AB\|\le\|A\|\,\|B\|$ for norms induced by the same vector norm.
\item $\|A\|=\sup_{\|x\|=1}\|Ax\|$.
\end{bullets}
}

\FormulaPage{2}{Spectral Norm via Singular Values}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
For $A\in\mathbb{C}^{m\times n}$, the 2-norm (spectral norm) equals the largest
singular value:
$\|A\|_2=\sigma_{\max}(A)=\sqrt{\lambda_{\max}(A^*A)}$.

\WHAT{
Expresses the operator norm induced by the Euclidean norm in terms of SVD
eigenstructure.
}
\WHY{
Gives a computable and interpretable measure of worst-case amplification and
connects to unitarily invariant geometry essential for numerical analysis.
}
\FORMULA{
\[
\|A\|_2=\max_{\|x\|_2=1}\|Ax\|_2=\sigma_{\max}(A)=
\sqrt{\lambda_{\max}(A^*A)}.
\]
}
\CANONICAL{
$A^*A$ is Hermitian positive semidefinite with eigenvalues $\sigma_i^2$.
Applies to real or complex matrices; unitary invariance holds.
}
\PRECONDS{
\begin{bullets}
\item Standard Euclidean inner product and induced norm $\|\cdot\|_2$.
\item Existence of SVD $A=U\Sigma V^*$ in finite dimensions.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
Let $A=U\Sigma V^*$ be an SVD. Then
$\max_{\|x\|_2=1}\|Ax\|_2=\max_i \sigma_i$.
\end{lemma}
\begin{proof}
Write $x=Vz$ with $\|z\|_2=1$ by unitary invariance of $\|\cdot\|_2$.
Then $\|Ax\|_2=\|U\Sigma V^*Vz\|_2=\|\Sigma z\|_2$. Since $\Sigma$ is
diagonal with nonnegative entries $\sigma_i$, $\|\Sigma z\|_2^2=
\sum_i \sigma_i^2 |z_i|^2\le (\max_i\sigma_i^2)\sum_i |z_i|^2=
(\max_i\sigma_i)^2$. Equality is attained with $z=e_k$ at an index $k$
achieving $\sigma_{\max}$. \qedhere
\end{proof}
\DERIVATION{
\begin{align*}
\text{Step 1 (Unitary invariance):}\quad
& \|A\|_2=\sup_{\|x\|_2=1}\|Ax\|_2
=\sup_{\|z\|_2=1}\|\Sigma z\|_2.\\
\text{Step 2 (Diagonal maximization):}\quad
& \|\Sigma z\|_2^2=\sum_i \sigma_i^2|z_i|^2\le
(\max_i\sigma_i^2)\sum_i|z_i|^2.\\
\text{Step 3 (Attainment):}\quad
& \text{Choose }z=e_k\text{ for }k\in\arg\max_i\sigma_i\Rightarrow
\|\Sigma z\|_2=\sigma_{\max}.\\
\text{Step 4 (Hermitian form):}\quad
& \|A\|_2^2=\max_{\|x\|_2=1}\langle Ax,Ax\rangle=
\max_{\|x\|_2=1}\langle x,A^*Ax\rangle\\
&=\lambda_{\max}(A^*A)=\sigma_{\max}^2.
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Compute SVD; read $\sigma_{\max}$ as $\|A\|_2$.
\item For symmetric positive semidefinite $A$, use $\|A\|_2=\lambda_{\max}(A)$.
\item Apply unitary invariance: $\|UAV\|_2=\|A\|_2$.
\end{bullets}
\EQUIV{
\begin{bullets}
\item $\|A\|_2=\sqrt{\rho(A^*A)}$.
\item $\|A\|_2=\sup_{\|x\|_2=\|y\|_2=1}\Re\, y^*Ax$.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item For nonnormal $A$, $\|A\|_2\ne \rho(A)$ generally.
\item Relation uses Euclidean norm; not valid for arbitrary $p$-norms.
\end{bullets}
}
\INPUTS{$A\in\mathbb{C}^{m\times n}$.}
\RESULT{
The spectral norm equals the largest singular value and is unitarily invariant.}
\UNITCHECK{
Dimensionless; independent of unitary basis changes.}
\PITFALLS{
\begin{bullets}
\item Mistaking $\lambda_{\max}(A)$ for $\sigma_{\max}(A)$ when $A$ is not
Hermitian positive semidefinite.
\item Ignoring the square root when using $A^*A$.
\end{bullets}
}
\INTUITION{
$A$ maps the unit sphere to an ellipsoid with semiaxes $\sigma_i$; the longest
axis length is the spectral norm.
}
\CANONICAL{
\begin{bullets}
\item $\|UAV\|_2=\|A\|_2$ for unitary $U,V$.
\item $\|A\|_2=\sigma_{\max}(A)$.
\end{bullets}
}

\FormulaPage{3}{Explicit Forms for $\|A\|_1$ and $\|A\|_\infty$}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
For $A=(a_{ij})\in\mathbb{C}^{m\times n}$,
$\|A\|_1=\max_{1\le j\le n}\sum_{i=1}^m |a_{ij}|$ and
$\|A\|_\infty=\max_{1\le i\le m}\sum_{j=1}^n |a_{ij}|$.

\WHAT{
Closed-form induced operator norms corresponding to vector $\ell_1$ and
$\ell_\infty$ norms.
}
\WHY{
Enable efficient computation and tight bounds without SVD; frequently used in
analysis and implementation due to low cost.
}
\FORMULA{
\[
\|A\|_1=\max_j \sum_i |a_{ij}|,\qquad
\|A\|_\infty=\max_i \sum_j |a_{ij}|.
\]
}
\CANONICAL{
Induced by vector norms $\|x\|_1=\sum_k|x_k|$ and
$\|x\|_\infty=\max_k|x_k|$, respectively. Valid for all finite matrices.
}
\PRECONDS{
\begin{bullets}
\item Use matching vector norms on domain/codomain spaces.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
For $\|\cdot\|_1$, $\|A\|_1=\max_j \sum_i|a_{ij}|$.
\end{lemma}
\begin{proof}
Upper bound: for any $x$ with $\|x\|_1=1$,
$\|Ax\|_1=\sum_i|\sum_j a_{ij}x_j|
\le \sum_i\sum_j |a_{ij}|\,|x_j|
=\sum_j\Big(\sum_i |a_{ij}|\Big)|x_j|
\le \max_j \sum_i|a_{ij}|$.
Attainment: choose $j^*\in\arg\max_j\sum_i|a_{ij}|$ and set $x=e_{j^*}$
(with appropriate signs absorbed by $a_{ij}$), then
$\|x\|_1=1$ and $\|Ax\|_1=\sum_i|a_{ij^*}|$ matches the bound. \qedhere
\end{proof}
\begin{lemma}
For $\|\cdot\|_\infty$, $\|A\|_\infty=\max_i \sum_j|a_{ij}|$.
\end{lemma}
\begin{proof}
Upper bound: for any $\|x\|_\infty=1$,
$\|Ax\|_\infty=\max_i |\sum_j a_{ij}x_j|
\le \max_i \sum_j |a_{ij}|\,|x_j|
\le \max_i \sum_j |a_{ij}|$.
Attainment: pick row $i^*$ achieving the max and a vector $x$ with
$\|x\|_\infty=1$ and $\operatorname{sign}(x_j)=\operatorname{sign}(a_{i^*j})$
(if $a_{i^*j}=0$ choose arbitrary sign). Then
$|\sum_j a_{i^*j}x_j|=\sum_j |a_{i^*j}|$ achieves the bound. \qedhere
\end{proof}
\DERIVATION{
\begin{align*}
\text{Step 1 (Variational form):}\quad
& \|A\|_1=\sup_{\|x\|_1=1}\sum_i\Big|\sum_j a_{ij}x_j\Big|.\\
\text{Step 2 (Triangle/Hölder):}\quad
& \le \sup_{\|x\|_1=1}\sum_i\sum_j |a_{ij}|\,|x_j|
= \sup_{\|x\|_1=1}\sum_j\Big(\sum_i |a_{ij}|\Big)|x_j|.\\
\text{Step 3 (Maximization):}\quad
& \le \max_j \sum_i|a_{ij}|.\\
\text{Step 4 (Attainment choice):}\quad
& x=e_{j^*}\Rightarrow \|Ax\|_1=\sum_i|a_{ij^*}|=\max_j\sum_i|a_{ij}|.\\
\text{Step 5 ($\infty$-norm):}\quad
& \|A\|_\infty=\sup_{\|x\|_\infty=1}\max_i\Big|\sum_j a_{ij}x_j\Big|\\
& \le \sup_{\|x\|_\infty=1}\max_i\sum_j|a_{ij}|\,|x_j|
\le \max_i \sum_j |a_{ij}|.
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item For $\|A\|_1$, compute column sums of absolute values; take the maximum.
\item For $\|A\|_\infty$, compute row sums of absolute values; take the maximum.
\item Use as bounds for $\|A\|_2$ via $\|A\|_2\le\sqrt{\|A\|_1\|A\|_\infty}$.
\end{bullets}
\EQUIV{
\begin{bullets}
\item $\|A\|_\infty=\|\;A^\top\;\|_1$ for real $A$ (or $A^*$ for complex).
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item Not unitarily invariant; sensitive to basis permutations.
\item May overestimate $\|A\|_2$ but provide efficient computable bounds.
\end{bullets}
}
\INPUTS{$A=(a_{ij})\in\mathbb{C}^{m\times n}$.}
\RESULT{
Closed forms allow $O(mn)$ computation of $\|A\|_1$ and $\|A\|_\infty$.}
\UNITCHECK{
Dimensionless ratios; consistent with induced vector norms.}
\PITFALLS{
\begin{bullets}
\item Forgetting absolute values in sums.
\item Swapping row/column max between $\|A\|_1$ and $\|A\|_\infty$.
\end{bullets}
}
\INTUITION{
$\|A\|_1$ is how much mass can flow into a column; $\|A\|_\infty$ is how much
can flow out of a row.
}
\CANONICAL{
\begin{bullets}
\item $\|A\|_1=\max_j\sum_i|a_{ij}|$, $\|A\|_\infty=\max_i\sum_j|a_{ij}|$.
\end{bullets}
}

\FormulaPage{4}{Condition Number and Singular Values}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
For nonsingular $A\in\mathbb{C}^{n\times n}$ and an induced norm $\|\cdot\|$,
$\kappa(A)=\|A\|\,\|A^{-1}\|\ge1$. In the 2-norm,
$\kappa_2(A)=\sigma_{\max}(A)/\sigma_{\min}(A)$.

\WHAT{
Defines and evaluates the condition number, the key sensitivity scalar for
linear system solves.
}
\WHY{
Relates data/roundoff perturbations to solution error; central to assessing
numerical stability and preconditioning efficacy.
}
\FORMULA{
\[
\kappa_{\|\cdot\|}(A)=\|A\|\,\|A^{-1}\|,\qquad
\kappa_2(A)=\frac{\sigma_{\max}(A)}{\sigma_{\min}(A)}.
\]
}
\CANONICAL{
Requires $A$ invertible. For 2-norm uses SVD; for other norms compute via
operator norms of $A$ and $A^{-1}$.
}
\PRECONDS{
\begin{bullets}
\item $A$ nonsingular.
\item Induced matrix norm from some vector norm.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
For any induced norm and nonsingular $A$, $\kappa(A)\ge1$.
\end{lemma}
\begin{proof}
For any $x\ne0$, $\|x\|=\|A^{-1}Ax\|\le \|A^{-1}\|\,\|Ax\|
\le \|A^{-1}\|\,\|A\|\,\|x\|$. Cancel $\|x\|>0$ to get
$1\le \|A\|\,\|A^{-1}\|=\kappa(A)$. \qedhere
\end{proof}
\begin{lemma}
For the 2-norm, $\|A\|_2=\sigma_{\max}(A)$ and $\|A^{-1}\|_2=1/\sigma_{\min}(A)$.
\end{lemma}
\begin{proof}
By SVD, $\|A\|_2=\sigma_{\max}(A)$ (Formula 2). For invertible $A$,
$A^{-1}=V\Sigma^{-1}U^*$, so $\|A^{-1}\|_2=\sigma_{\max}(A^{-1})
=\max_i (1/\sigma_i(A))=1/\sigma_{\min}(A)$. \qedhere
\end{proof}
\DERIVATION{
\begin{align*}
\text{Step 1 (Definition):}\quad
& \kappa(A)=\|A\|\,\|A^{-1}\|.\\
\text{Step 2 (Lower bound):}\quad
& \|x\|=\|A^{-1}Ax\|\le \|A^{-1}\|\,\|Ax\|
\le \|A^{-1}\|\,\|A\|\,\|x\|.\\
& \Rightarrow \kappa(A)\ge1.\\
\text{Step 3 (2-norm specialization):}\quad
& \kappa_2(A)=\|A\|_2\,\|A^{-1}\|_2
=\sigma_{\max}(A)\cdot \frac{1}{\sigma_{\min}(A)}.\\
\text{Step 4 (Unitary invariance):}\quad
& \kappa_2(UAV)=\kappa_2(A),\ \ U,V\ \text{unitary.}
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Compute $\|A\|$ and $\|A^{-1}\|$ (analytically or numerically).
\item For 2-norm, compute extreme singular values via SVD.
\item Interpret $\kappa$ as worst-case relative error amplification factor.
\end{bullets}
\EQUIV{
\begin{bullets}
\item $\kappa_2(A)=\sqrt{\lambda_{\max}(A^*A)/\lambda_{\min}(A^*A)}$.
\item For diagonal $D$, $\kappa_p(D)=\max_i|d_i|/\min_i|d_i|$ for any $p$.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item If $\sigma_{\min}(A)\to0$, then $\kappa_2(A)\to\infty$.
\item Norm dependence: $\kappa_p(A)$ changes with $p$.
\end{bullets}
}
\INPUTS{$A\in\mathbb{C}^{n\times n}$ invertible.}
\RESULT{
A single scalar $\kappa(A)$ quantifies sensitivity; in 2-norm it is the ratio
of extreme singular values.}
\UNITCHECK{
Dimensionless; invariant to scalar scaling $A\mapsto \alpha A$.}
\PITFALLS{
\begin{bullets}
\item Reporting $\kappa$ without the norm.
\item Using eigenvalue ratio for nonnormal matrices instead of singular values.
\end{bullets}
}
\INTUITION{
The longer the longest axis compared to the shortest in the image ellipsoid,
the more sensitive the system.}
\CANONICAL{
\begin{bullets}
\item $\kappa(A)\ge1$; $\kappa(\alpha A)=\kappa(A)$ for $\alpha\ne0$.
\item $\kappa_2(A)=\sigma_{\max}/\sigma_{\min}$.
\end{bullets}
}

\FormulaPage{5}{Residual-to-Error Bound via Condition Number}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
For nonsingular $A\in\mathbb{C}^{n\times n}$, true solution $x$ of $Ax=b$, and
approximation $\hat{x}$ with residual $r=b-A\hat{x}$, the forward error obeys
$\|x-\hat{x}\|/\|x\|\le \kappa(A)\,\|r\|/\|b\|$ for any induced norm.

\WHAT{
Relates the computable residual norm to the relative solution error using the
condition number.
}
\WHY{
Certifies solution quality a posteriori and guides stopping criteria for
iterative methods without needing $x$.
}
\FORMULA{
\[
\frac{\|x-\hat{x}\|}{\|x\|}\le
\kappa(A)\,\frac{\|r\|}{\|b\|},\qquad r=b-A\hat{x}.
\]
}
\CANONICAL{
Holds for any induced norm; does not assume smallness, only $A$ invertible.
}
\PRECONDS{
\begin{bullets}
\item $A$ is nonsingular so that $x=A^{-1}b$ exists.
\item Matrix norm is induced by the vector norm used in $\|\cdot\|$.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
For $Ax=b$, $\|x\|\le \|A^{-1}\|\,\|b\|$.
\end{lemma}
\begin{proof}
From $x=A^{-1}b$ and submultiplicativity,
$\|x\|=\|A^{-1}b\|\le \|A^{-1}\|\,\|b\|$. \qedhere
\end{proof}
\begin{lemma}
Error equals inverse applied to residual: $x-\hat{x}=A^{-1}r$.
\end{lemma}
\begin{proof}
$Ax=b$ and $r=b-A\hat{x}$ imply $A(x-\hat{x})=r$. Since $A$ is invertible,
$x-\hat{x}=A^{-1}r$. \qedhere
\end{proof}
\DERIVATION{
\begin{align*}
\text{Step 1 (Error representation):}\quad
& x-\hat{x}=A^{-1}r.\\
\text{Step 2 (Absolute error bound):}\quad
& \|x-\hat{x}\|=\|A^{-1}r\|\le \|A^{-1}\|\,\|r\|.\\
\text{Step 3 (Normalize by }\|x\|\text{):}\quad
& \frac{\|x-\hat{x}\|}{\|x\|}
\le \frac{\|A^{-1}\|\,\|r\|}{\|x\|}
\le \frac{\|A^{-1}\|\,\|r\|}{\|A^{-1}\|^{-1}\|b\|}\\
&= \|A^{-1}\|^2\,\frac{\|r\|}{\|b\|}\ \text{(incorrect)}.
\end{align*}
To correct the normalization use the lemma $\|x\|\le\|A^{-1}\|\,\|b\|$:
\begin{align*}
\frac{\|x-\hat{x}\|}{\|x\|}
&\le \frac{\|A^{-1}\|\,\|r\|}{\|x\|}
\le \frac{\|A^{-1}\|\,\|r\|}{\|A^{-1}\|^{-1}\|b\|}\cdot
\frac{\|A\|}{\|A\|}\\
&= \|A^{-1}\|\,\|A\|\,\frac{\|r\|}{\|b\|}
=\kappa(A)\,\frac{\|r\|}{\|b\|}.
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Compute residual $r=b-A\hat{x}$.
\item Compute or bound $\kappa(A)$ in the same norm.
\item Form bound $\|x-\hat{x}\|/\|x\|\le \kappa(A)\,\|r\|/\|b\|$.
\end{bullets}
\EQUIV{
\begin{bullets}
\item Absolute error: $\|x-\hat{x}\|\le \|A^{-1}\|\,\|r\|$.
\item If $b\ne0$, relative residual bounds relative error via $\kappa(A)$.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item If $b=0$, then $x=0$, and the relative bound is undefined; use absolute
error.
\item Large $\kappa(A)$ weakens the guarantee even for small residuals.
\end{bullets}
}
\INPUTS{$A$ invertible, $b$, and an approximate $\hat{x}$.}
\RESULT{
Residuals translate into forward error bounds with a factor $\kappa(A)$.}
\UNITCHECK{
All terms are dimensionless ratios (norms of same quantities).}
\PITFALLS{
\begin{bullets}
\item Mixing norms for $\kappa(A)$ and $\|r\|/\|b\|$ invalidates the bound.
\item Using $\|b\|=0$ in relative quantities.
\end{bullets}
}
\INTUITION{
Error is residual filtered through $A^{-1}$; its gain is at most $\|A^{-1}\|$,
and normalization by $\|x\|$ leverages $Ax=b$ to introduce $\|A\|$.
}
\CANONICAL{
\begin{bullets}
\item $x-\hat{x}=A^{-1}(b-A\hat{x})$.
\item $\|x-\hat{x}\|/\|x\|\le \kappa(A)\,\|r\|/\|b\|$.
\end{bullets}
}

\clearpage
\section{10 Exhaustive Problems and Solutions}

\ProblemPage{1}{Compute Norms and Condition Number of a $2\times2$ Matrix}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Given $A=\begin{bmatrix}3&-1\\2&4\end{bmatrix}$, compute
$\|A\|_1$, $\|A\|_\infty$, $\|A\|_2$, and $\kappa_2(A)$.

\PROBLEM{
Evaluate induced norms and the 2-norm condition number; verify
$\|A\|_2\le \sqrt{\|A\|_1\|A\|_\infty}$ and
$\kappa_2(A)=\sigma_{\max}/\sigma_{\min}$.
}
\MODEL{
\[
\|A\|_1=\max_j\sum_i|a_{ij}|,\quad
\|A\|_\infty=\max_i\sum_j|a_{ij}|,\quad
\|A\|_2=\sqrt{\lambda_{\max}(A^\top A)}.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Real matrices with standard Euclidean inner product.
\item 2-norm induced by $\ell_2$ vector norm.
\end{bullets}
}
\varmapStart
\var{A}{Given $2\times2$ matrix.}
\var{\sigma_{\max},\sigma_{\min}}{Extreme singular values of $A$.}
\varmapEnd
\WHICHFORMULA{
Formulas 2, 3, and 4: explicit 1/$\infty$ norms, spectral norm, and
$\kappa_2=\sigma_{\max}/\sigma_{\min}$.
}
\GOVERN{
\[
\|A\|_2=\sqrt{\lambda_{\max}(A^\top A)},\quad
\kappa_2(A)=\frac{\sigma_{\max}(A)}{\sigma_{\min}(A)}.
\]
}
\INPUTS{$A=\begin{bmatrix}3&-1\\2&4\end{bmatrix}$.}
\DERIVATION{
\begin{align*}
\text{Step 1: }\|A\|_1&=\max\{ |3|+|2|,\ |-1|+|4|\}\\
&=\max\{5,5\}=5.\\
\text{Step 2: }\|A\|_\infty&=\max\{ |3|+|{-1}|,\ |2|+|4|\}\\
&=\max\{4,6\}=6.\\
\text{Step 3: }A^\top A&=
\begin{bmatrix}3&2\\-1&4\end{bmatrix}
\begin{bmatrix}3&-1\\2&4\end{bmatrix}
=
\begin{bmatrix}13&5\\5&17\end{bmatrix}.\\
\text{Step 4: Eigenvalues of }A^\top A&:
\lambda=\frac{13+17\pm\sqrt{(13-17)^2+4\cdot5^2}}{2}\\
&=\frac{30\pm\sqrt{16+100}}{2}=\frac{30\pm\sqrt{116}}{2}\\
&=15\pm \sqrt{29}.\\
\text{Step 5: }\|A\|_2&=\sqrt{\lambda_{\max}}=\sqrt{15+\sqrt{29}}.\\
\text{Step 6: }\sigma_{\min}&=\sqrt{15-\sqrt{29}},\\
\kappa_2(A)&=\sqrt{\frac{15+\sqrt{29}}{15-\sqrt{29}}}.\\
\text{Step 7: Inequality check }&
\|A\|_2^2=15+\sqrt{29}\le \|A\|_1\|A\|_\infty=30.
\end{align*}
}
\RESULT{
$\|A\|_1=5$, $\|A\|_\infty=6$, $\|A\|_2=\sqrt{15+\sqrt{29}}\approx4.968$,
$\kappa_2(A)=\sqrt{\frac{15+\sqrt{29}}{15-\sqrt{29}}}\approx1.538$.
}
\UNITCHECK{
All quantities are amplification factors (dimensionless).}
\EDGECASES{
\begin{bullets}
\item If $A$ were triangular with zeros off the diagonal, closed forms
simplify further.
\item If $\det(A)=0$, $\kappa_2$ would be infinite.
\end{bullets}
}
\ALTERNATE{
Use SVD directly to read $\sigma_{\max},\sigma_{\min}$ numerically.}
\VALIDATION{
\begin{bullets}
\item Verify $\|A\|_2^2=\lambda_{\max}(A^\top A)$ numerically.
\item Check $\|A\|_2\le \sqrt{\|A\|_1\|A\|_\infty}=\sqrt{30}$. 
\end{bullets}
}
\INTUITION{
The longest axis of the image of the unit circle under $A$ has length
$\sqrt{15+\sqrt{29}}$, and the shortest has length
$\sqrt{15-\sqrt{29}}$.
}
\CANONICAL{
\begin{bullets}
\item $\|A\|_2^2=\lambda_{\max}(A^\top A)$,
$\sigma_{\min}^2=\lambda_{\min}(A^\top A)$.
\end{bullets}
}

\ProblemPage{2}{Product Norm Bound and Near Equality}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
For $A=\begin{bmatrix}1&2\\0&3\end{bmatrix}$,
$B=\begin{bmatrix}2&0\\1&1\end{bmatrix}$, verify
$\|AB\|_1\le \|A\|_1\|B\|_1$ and estimate the ratio.

\PROBLEM{
Compute $\|A\|_1$, $\|B\|_1$, $\|AB\|_1$ and compare with the multiplicative
bound to assess tightness.
}
\MODEL{
\[
\|M\|_1=\max_j\sum_i |m_{ij}|,\quad \|AB\|_1\le \|A\|_1\|B\|_1.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item 1-norm induced operator norm.
\end{bullets}
}
\varmapStart
\var{A,B}{Given matrices.}
\var{M}{Product $AB$.}
\varmapEnd
\WHICHFORMULA{
Formula 1 (submultiplicativity) and Formula 3 (explicit $\|\cdot\|_1$).
}
\GOVERN{
\[
\|AB\|_1\le \|A\|_1\|B\|_1.
\]
}
\INPUTS{$A,B$ as above.}
\DERIVATION{
\begin{align*}
\|A\|_1&=\max\{|1|+|0|,\ |2|+|3|\}=\max\{1,5\}=5.\\
\|B\|_1&=\max\{|2|+|1|,\ |0|+|1|\}=\max\{3,1\}=3.\\
AB&=\begin{bmatrix}1&2\\0&3\end{bmatrix}
\begin{bmatrix}2&0\\1&1\end{bmatrix}
=\begin{bmatrix}4&2\\3&3\end{bmatrix}.\\
\|AB\|_1&=\max\{|4|+|3|,\ |2|+|3|\}=\max\{7,5\}=7.\\
\text{Compare: }& \|AB\|_1=7\le 5\cdot3=15,\ \text{ratio }7/15\approx0.467.
\end{align*}
}
\RESULT{
Bound holds with significant slack: $7\le15$. Ratio $\approx0.467$.}
\UNITCHECK{All norms are dimensionless.}
\EDGECASES{
\begin{bullets}
\item Equality can occur for nonnegative matrices with matching extremal
columns/rows.
\end{bullets}
}
\ALTERNATE{
Compute via column-sum vectors: $s(A)=(1,5)$, $s(B)=(3,1)$ and note that
$s(AB)\le s(A)\cdot\|B\|_1$ componentwise is insufficient for exact equality.}
\VALIDATION{
\begin{bullets}
\item Numerically check $\|ABx\|_1\le \|A\|_1\|Bx\|_1$ for random $x$.
\end{bullets}
}
\INTUITION{
Chaining amplifiers multiplies gains in the worst case; here structure reduces
the realized gain.}
\CANONICAL{
\begin{bullets}
\item Submultiplicativity is general; equality requires aligned extremizers.
\end{bullets}
}

\ProblemPage{3}{Bounding $\|A\|_2$ by $\sqrt{\|A\|_1\|A\|_\infty}$}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Show $\|A\|_2\le \sqrt{\|A\|_1\|A\|_\infty}$ and evaluate for
$A=\begin{bmatrix}1&-2&0\\3&0&1\end{bmatrix}$.

\PROBLEM{
Prove the inequality and compute both sides to verify numerically.
}
\MODEL{
\[
\|A\|_2^2=\rho(A^*A)\le \|A^*A\|_1\le \|A^*\|_1\,\|A\|_1
=\|A\|_\infty\|A\|_1.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Induced norms and submultiplicativity.
\item Spectral radius bounded by any induced norm.
\end{bullets}
}
\varmapStart
\var{A}{Given $2\times3$ matrix.}
\varmapEnd
\WHICHFORMULA{
Formulas 2 and 3 for norms; spectral radius bound by induced norms.
}
\GOVERN{
\[
\|A\|_2\le \sqrt{\|A\|_1\|A\|_\infty}.
\]
}
\INPUTS{$A=\begin{bmatrix}1&-2&0\\3&0&1\end{bmatrix}$.}
\DERIVATION{
\begin{align*}
\text{Proof: }&
\|A\|_2^2=\rho(A^*A)\le \|A^*A\|_1\le \|A^*\|_1\|A\|_1
=\|A\|_\infty\|A\|_1.\\
&\Rightarrow \|A\|_2\le \sqrt{\|A\|_1\|A\|_\infty}.\\
\text{Compute: }&
\|A\|_1=\max\{|1|+|3|,\ |{-2}|+|0|,\ |0|+|1|\}=\max\{4,2,1\}=4.\\
&\|A\|_\infty=\max\{|1|+|{-2}|+|0|,\ |3|+|0|+|1|\}=\max\{3,4\}=4.\\
&\sqrt{\|A\|_1\|A\|_\infty}=\sqrt{16}=4.\\
&\|A\|_2=\sqrt{\lambda_{\max}(A^*A)}.\\
&A^*A=
\begin{bmatrix}
10&-2&3\\-2&4&0\\3&0&1
\end{bmatrix}.\\
&\text{Numerically, }\lambda_{\max}\approx 12.0625\Rightarrow \|A\|_2\approx
3.474\le 4.
\end{align*}
}
\RESULT{
Inequality proven; for the matrix, $\|A\|_2\approx3.474\le 4$.}
\UNITCHECK{All sides are amplification factors.}
\EDGECASES{
\begin{bullets}
\item Equality can occur for rank-1 nonnegative matrices with aligned rows
and columns.
\end{bullets}
}
\ALTERNATE{
Use $\|A\|_2=\sup_{\|x\|_2=1}\|Ax\|_2\le
\sup_{\|x\|_2=1}\sqrt{\|A\|_1\|A\|_\infty}\|x\|_2$.}
\VALIDATION{
\begin{bullets}
\item Numerically compute SVD to confirm $\|A\|_2$.
\end{bullets}
}
\INTUITION{
Column-sum and row-sum bounds squeeze the spectral norm from above.}
\CANONICAL{
\begin{bullets}
\item $\|A\|_2\le \sqrt{\|A\|_1\|A\|_\infty}$ is a standard norm inequality.
\end{bullets}
}

\ProblemPage{4}{Residual Guides Error: Alice and Bob}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Alice computes $\hat{x}$ for $Ax=b$; Bob computes residual $r=b-A\hat{x}$ and
bounds the error using $\kappa(A)$.

\PROBLEM{
Let $A=\begin{bmatrix}4&1\\2&3\end{bmatrix}$, $b=\begin{bmatrix}1\\0\end{bmatrix}$,
$\hat{x}=\begin{bmatrix}0.25\\-0.1\end{bmatrix}$. Use 2-norm to bound
$\|x-\hat{x}\|/\|x\|$ with $\kappa_2(A)$ and the residual.
}
\MODEL{
\[
\frac{\|x-\hat{x}\|_2}{\|x\|_2}\le \kappa_2(A)\frac{\|r\|_2}{\|b\|_2},\quad
r=b-A\hat{x}.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item 2-norm; $A$ invertible.
\end{bullets}
}
\varmapStart
\var{A,b,\hat{x}}{Given data.}
\var{r}{Residual $b-A\hat{x}$.}
\varmapEnd
\WHICHFORMULA{
Formula 5 (residual-to-error) and Formula 4 (condition number in 2-norm).}
\GOVERN{
\[
\|x-\hat{x}\|_2/\|x\|_2\le \kappa_2(A)\,\|r\|_2/\|b\|_2.
\]
}
\INPUTS{$A,b,\hat{x}$ as above.}
\DERIVATION{
\begin{align*}
r&=b-A\hat{x}=
\begin{bmatrix}1\\0\end{bmatrix}-
\begin{bmatrix}4&1\\2&3\end{bmatrix}
\begin{bmatrix}0.25\\-0.1\end{bmatrix}\\
&=\begin{bmatrix}1\\0\end{bmatrix}-
\begin{bmatrix}4\cdot0.25+1\cdot(-0.1)\\
2\cdot0.25+3\cdot(-0.1)\end{bmatrix}\\
&=\begin{bmatrix}1\\0\end{bmatrix}-
\begin{bmatrix}1.0-0.1\\0.5-0.3\end{bmatrix}
=\begin{bmatrix}0.1\\-0.2\end{bmatrix}.\\
\|r\|_2&=\sqrt{0.1^2+(-0.2)^2}=\sqrt{0.05}\approx0.2236.\\
\|b\|_2&=1.\\
A^\top A&=\begin{bmatrix}4&2\\1&3\end{bmatrix}
\begin{bmatrix}4&1\\2&3\end{bmatrix}
=\begin{bmatrix}20&10\\10&10\end{bmatrix}.\\
\lambda_{\max}&=\frac{30+\sqrt{(20-10)^2+4\cdot10^2}}{2}
=\frac{30+\sqrt{100+400}}{2}\\
&=\frac{30+\sqrt{500}}{2}\approx \frac{30+22.3607}{2}\approx26.1803.\\
\lambda_{\min}&=\frac{30-\sqrt{500}}{2}\approx3.8197.\\
\kappa_2(A)&=\sqrt{\lambda_{\max}/\lambda_{\min}}
\approx \sqrt{26.1803/3.8197}\approx2.618.\\
\text{Bound: }& \frac{\|x-\hat{x}\|_2}{\|x\|_2}
\le 2.618\times 0.2236\approx0.585.
\end{align*}
}
\RESULT{
Relative error is bounded by approximately $0.585$.}
\UNITCHECK{All terms are dimensionless.}
\EDGECASES{
\begin{bullets}
\item If $b=0$, use absolute error bound $\|x-\hat{x}\|\le \|A^{-1}\|\,\|r\|$.
\end{bullets}
}
\ALTERNATE{
Compute $x=A^{-1}b$ explicitly and evaluate actual
$\|x-\hat{x}\|/\|x\|$ to compare with the bound.}
\VALIDATION{
\begin{bullets}
\item $A^{-1}=\frac{1}{10}\begin{bmatrix}3&-1\\-2&4\end{bmatrix}$;
$x=A^{-1}b=\frac{1}{10}\begin{bmatrix}3\\-2\end{bmatrix}$.
\item Actual ratio $\approx0.382<0.585$ confirms the bound.
\end{bullets}
}
\INTUITION{
Residual is filtered by $A^{-1}$; conditioning scales the translation into
forward error.}
\CANONICAL{
\begin{bullets}
\item $\kappa_2$ dictates residual-to-error amplification.
\end{bullets}
}

\ProblemPage{5}{Preconditioning Effect on Condition Number}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Given $A=\begin{bmatrix}1&0\\0&\epsilon\end{bmatrix}$ with
$0<\epsilon\ll1$, compare $\kappa_2(A)$ with $\kappa_2(D^{-1}A)$ for
$D=\begin{bmatrix}1&0\\0&\epsilon\end{bmatrix}$.

\PROBLEM{
Show diagonal scaling improves conditioning and quantify the improvement.
}
\MODEL{
\[
\kappa_2(A)=\frac{1}{\epsilon},\quad
D^{-1}A=\begin{bmatrix}1&0\\0&1\end{bmatrix},\quad
\kappa_2(D^{-1}A)=1.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item 2-norm; $\epsilon\in(0,1)$.
\end{bullets}
}
\varmapStart
\var{\epsilon}{Small positive scalar.}
\var{D}{Right preconditioner.}
\varmapEnd
\WHICHFORMULA{
Formula 4: $\kappa_2$ of diagonal matrices is ratio of extreme magnitudes.}
\GOVERN{
\[
\kappa_2(\operatorname{diag}(d_1,d_2))=\max(|d_1|,|d_2|)/\min(|d_1|,|d_2|).
\]
}
\INPUTS{$\epsilon\in(0,1)$, $A=\operatorname{diag}(1,\epsilon)$, $D=A$.}
\DERIVATION{
\begin{align*}
\kappa_2(A)&=\frac{1}{\epsilon}.\\
D^{-1}A&=\operatorname{diag}(1,1)\Rightarrow \kappa_2(D^{-1}A)=1.\\
\text{Improvement factor }&=\frac{\kappa_2(A)}{\kappa_2(D^{-1}A)}
=\frac{1/\epsilon}{1}=\frac{1}{\epsilon}.
\end{align*}
}
\RESULT{
Preconditioning collapses the condition number from $1/\epsilon$ to $1$.}
\UNITCHECK{Dimensionless.}
\EDGECASES{
\begin{bullets}
\item If $\epsilon\to0$, $\kappa_2(A)\to\infty$ while preconditioned system
stays at $1$ provided $D$ remains invertible.
\end{bullets}
}
\ALTERNATE{
Left-right scaling $D_1AD_2$ can balance rows and columns for general $A$.}
\VALIDATION{
\begin{bullets}
\item Singular values of $A$ are $\{1,\epsilon\}$; of $D^{-1}A$ are $\{1,1\}$.
\end{bullets}
}
\INTUITION{
Balance the axes of the ellipsoid image to equalize stretches.}
\CANONICAL{
\begin{bullets}
\item Diagonal equilibration reduces anisotropy and improves conditioning.
\end{bullets}
}

\ProblemPage{6}{Expectation of Condition Number for a Random Diagonal}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Let $A=\operatorname{diag}(1,T)$ with $T$ uniform on $\{1,2,3,4,5,6\}$.
Compute $\mathbb{E}[\kappa_2(A)]$.

\PROBLEM{
Evaluate the expected 2-norm condition number of a random diagonal matrix.
}
\MODEL{
\[
\kappa_2(A)=\max(1,T)/\min(1,T)=T.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item 2-norm; $T\in\{1,\dots,6\}$ discrete uniform.
\end{bullets}
}
\varmapStart
\var{T}{Die roll (positive integer).}
\varmapEnd
\WHICHFORMULA{
Formula 4: diagonal matrix condition number equals ratio of diagonal magnitudes.}
\GOVERN{
\[
\kappa_2(\operatorname{diag}(1,T))=T,\quad T\ge1.
\]
}
\INPUTS{$T\sim\text{Unif}\{1,\dots,6\}$.}
\DERIVATION{
\begin{align*}
\mathbb{E}[\kappa_2(A)]&=\mathbb{E}[T]
=\frac{1+2+3+4+5+6}{6}=\frac{21}{6}=3.5.
\end{align*}
}
\RESULT{
$\mathbb{E}[\kappa_2(A)]=3.5$.}
\UNITCHECK{Dimensionless scalar expectation.}
\EDGECASES{
\begin{bullets}
\item If $T$ had support including $0$, expectation would be infinite due to
singularity at $T=0$.
\end{bullets}
}
\ALTERNATE{
For general discrete support $\{t_i\}$ with probabilities $p_i$,
$\mathbb{E}[\kappa_2]=\sum_i p_i t_i$.}
\VALIDATION{
\begin{bullets}
\item Simulation with fixed seed would reproduce $3.5$ in the long run.
\end{bullets}
}
\INTUITION{
On the diagonal, the condition number is just the ratio of axis lengths.}
\CANONICAL{
\begin{bullets}
\item Diagonal structure yields simple exact expressions.
\end{bullets}
}

\ProblemPage{7}{Proof: Submultiplicativity of Induced Norms}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Prove that for any induced operator norm, $\|AB\|\le \|A\|\,\|B\|$.

\PROBLEM{
Provide a short, closed proof using the variational definition.
}
\MODEL{
\[
\|AB\|=\sup_{\|x\|=1}\|ABx\|\le \|A\|\,\sup_{\|x\|=1}\|Bx\|=\|A\|\,\|B\|.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Same vector norm induces both matrix norms.
\end{bullets}
}
\varmapStart
\var{A,B}{Linear maps with matching dimensions.}
\varmapEnd
\WHICHFORMULA{
Formula 1 (induced norm definition).}
\GOVERN{
\[
\|AB\|\le \|A\|\,\|B\|.
\]
}
\INPUTS{$A:\mathbb{C}^n\to\mathbb{C}^m$, $B:\mathbb{C}^k\to\mathbb{C}^n$.}
\DERIVATION{
\begin{align*}
\text{For any }\|x\|=1:\quad \|ABx\|&\le \|A\|\,\|Bx\|
\le \|A\|\,\|B\|\,\|x\|=\|A\|\,\|B\|.\\
\text{Taking sup: }\quad \|AB\|&=\sup_{\|x\|=1}\|ABx\|
\le \|A\|\,\|B\|.
\end{align*}
}
\RESULT{
Submultiplicativity holds.}
\UNITCHECK{Dimensionless quantities.}
\EDGECASES{
\begin{bullets}
\item If different vector norms are used, the inequality need not hold.
\end{bullets}
}
\ALTERNATE{
Use operator composition interpretation: norm is operator Lipschitz constant
and Lipschitz constants multiply.}
\VALIDATION{
\begin{bullets}
\item Check numerically on random matrices for various $p$-norms.
\end{bullets}
}
\INTUITION{
Two amplifiers in series multiply gains at worst.}
\CANONICAL{
\begin{bullets}
\item Variational sup and triangle inequality drive the result.
\end{bullets}
}

\ProblemPage{8}{Proof: $\|A\|_2\le\sqrt{\|A\|_1\|A\|_\infty}$}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Prove $\|A\|_2\le\sqrt{\|A\|_1\|A\|_\infty}$ for all $A$.

\PROBLEM{
Provide a short proof using spectral radius bounds and submultiplicativity.}
\MODEL{
\[
\|A\|_2^2=\rho(A^*A)\le \|A^*A\|_1\le \|A^*\|_1\|A\|_1
=\|A\|_\infty\|A\|_1.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Spectral radius bounded by any induced matrix norm.
\end{bullets}
}
\varmapStart
\var{A}{Arbitrary matrix.}
\varmapEnd
\WHICHFORMULA{
Formulas 2 and 3 plus spectral radius bound $\rho(M)\le \|M\|_1$.}
\GOVERN{
\[
\|A\|_2\le \sqrt{\|A\|_1\|A\|_\infty}.
\]
}
\INPUTS{$A\in\mathbb{C}^{m\times n}$.}
\DERIVATION{
\begin{align*}
\|A\|_2^2&=\lambda_{\max}(A^*A)=\rho(A^*A)\le \|A^*A\|_1\\
&\le \|A^*\|_1\|A\|_1=\|A\|_\infty\|A\|_1.\\
\Rightarrow \|A\|_2&\le \sqrt{\|A\|_1\|A\|_\infty}.
\end{align*}
}
\RESULT{
The inequality holds for all finite matrices.}
\UNITCHECK{Dimensionless inequality.}
\EDGECASES{
\begin{bullets}
\item Equality can occur for rank-1 nonnegative matrices with constant row
and column sums.
\end{bullets}
}
\ALTERNATE{
Use $\|Ax\|_2\le \sqrt{\|A\|_1\|A\|_\infty}\|x\|_2$ for each $x$ and take
supremum.}
\VALIDATION{
\begin{bullets}
\item Test on random matrices: compute both sides numerically.
\end{bullets}
}
\INTUITION{
Row and column sum bounds cage the maximal Euclidean stretch.}
\CANONICAL{
\begin{bullets}
\item Standard cross-norm inequality linking induced norms.
\end{bullets}
}

\ProblemPage{9}{Unitary Invariance of $\kappa_2$}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Show $\kappa_2(UAV)=\kappa_2(A)$ for unitary $U,V$.

\PROBLEM{
Prove invariance and verify numerically for a concrete $A,U,V$.}
\MODEL{
\[
\|UAV\|_2=\|A\|_2,\quad (UAV)^{-1}=V^*A^{-1}U^*,\quad
\|(UAV)^{-1}\|_2=\|A^{-1}\|_2.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $U,V$ unitary and $A$ invertible.
\end{bullets}
}
\varmapStart
\var{U,V}{Unitary matrices.}
\var{A}{Invertible matrix.}
\varmapEnd
\WHICHFORMULA{
Formula 2 (unitary invariance) and Formula 4 (definition of $\kappa_2$).}
\GOVERN{
\[
\kappa_2(UAV)=\|UAV\|_2\,\|(UAV)^{-1}\|_2.
\]
}
\INPUTS{Arbitrary invertible $A$ and unitary $U,V$.}
\DERIVATION{
\begin{align*}
\|UAV\|_2&=\|A\|_2\quad(\text{unitary invariance}).\\
\|(UAV)^{-1}\|_2&=\|V^*A^{-1}U^*\|_2=\|A^{-1}\|_2.\\
\Rightarrow \kappa_2(UAV)&=\|A\|_2\,\|A^{-1}\|_2=\kappa_2(A).
\end{align*}
}
\RESULT{
$\kappa_2$ is unitarily invariant.}
\UNITCHECK{Dimensionless invariant under orthonormal basis changes.}
\EDGECASES{
\begin{bullets}
\item If $U,V$ are not unitary, invariance fails in general.
\end{bullets}
}
\ALTERNATE{
Use SVD: $A=U_A\Sigma V_A^*$ and $U(U_A\Sigma V_A^*)V$ has the same singular
values.}
\VALIDATION{
\begin{bullets}
\item Numerically compute $\kappa_2(A)$ and $\kappa_2(UAV)$ for random
unitaries and confirm equality within tolerance.
\end{bullets}
}
\INTUITION{
Rotations and reflections do not change lengths or condition numbers in
Euclidean geometry.}
\CANONICAL{
\begin{bullets}
\item $\kappa_2$ depends only on singular values.
\end{bullets}
}

\ProblemPage{10}{Diagonal Scaling to Balance Row and Column Norms}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Find diagonal $D_1,D_2$ to balance $\|D_1AD_2\|_1$ and $\|D_1AD_2\|_\infty$
for $A=\begin{bmatrix}1&4\\2&1\end{bmatrix}$, and compare
$\sqrt{\|A\|_1\|A\|_\infty}$ vs. $\|D_1AD_2\|_2$ bound.

\PROBLEM{
Choose diagonal scalings to equalize column and row sum maxima and evaluate the
spectral-norm upper bound improvement.
}
\MODEL{
\[
D_1=\operatorname{diag}(r_1,r_2),\ \ D_2=\operatorname{diag}(c_1,c_2).
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Positive diagonal scalings.
\end{bullets}
}
\varmapStart
\var{D_1,D_2}{Row/column diagonal scalings.}
\varmapEnd
\WHICHFORMULA{
Formula 3 and the inequality $\|M\|_2\le \sqrt{\|M\|_1\|M\|_\infty}$.}
\GOVERN{
\[
\|M\|_1=\max_j\sum_i |m_{ij}|,\ \
\|M\|_\infty=\max_i\sum_j |m_{ij}|.
\]
}
\INPUTS{$A=\begin{bmatrix}1&4\\2&1\end{bmatrix}$.}
\DERIVATION{
\begin{align*}
\|A\|_1&=\max\{1+2,4+1\}=\max\{3,5\}=5.\\
\|A\|_\infty&=\max\{1+4,2+1\}=\max\{5,3\}=5.\\
\sqrt{\|A\|_1\|A\|_\infty}&=\sqrt{25}=5.\\
\text{Already balanced, so choose }&D_1=I,\ D_2=I.\\
\text{Compute }\|A\|_2&=\sqrt{\lambda_{\max}(A^\top A)}.\\
A^\top A&=\begin{bmatrix}5&6\\6&17\end{bmatrix},\ 
\lambda_{\max}=\frac{22+\sqrt{484-340}}{2}=\frac{22+\sqrt{144}}{2}=17.\\
\|A\|_2&=\sqrt{17}\approx4.1231\le5.
\end{align*}
}
\RESULT{
Matrix is already balanced; bound $\sqrt{\|A\|_1\|A\|_\infty}=5$ exceeds
$\|A\|_2\approx4.123$.}
\UNITCHECK{Dimensionless.}
\EDGECASES{
\begin{bullets}
\item If initial $\|A\|_1\ne \|A\|_\infty$, diagonal equilibration can reduce
$\sqrt{\|M\|_1\|M\|_\infty}$.
\end{bullets}
}
\ALTERNATE{
Perform one step of Sinkhorn-Knopp scaling to equalize row/column sums.}
\VALIDATION{
\begin{bullets}
\item Verify numerically that scaling preserves singular values up to
nonunitary factors only if scalings are unitary magnitudes; generally they
change $\|M\|_2$.
\end{bullets}
}
\INTUITION{
Balancing reduces anisotropy measured by row/column aggregates, tightening
spectral norm upper bounds.}
\CANONICAL{
\begin{bullets}
\item $\|M\|_2\le \sqrt{\|M\|_1\|M\|_\infty}$ guides scaling heuristics.
\end{bullets}
}

\clearpage
\section{Coding Demonstrations}

\CodeDemoPage{Norms, Singular Values, and Condition Number Verification}
\PROBLEM{
Compute $\|A\|_1$, $\|A\|_\infty$, $\|A\|_2$, $\kappa_2(A)$ and verify
$\|A\|_2\le \sqrt{\|A\|_1\|A\|_\infty}$ and
$\kappa_2(A)=\sigma_{\max}/\sigma_{\min}$ for deterministic $A$.}
\API{
\begin{bullets}
\item \inlinecode{def read_input(s) -> np.ndarray} — parse matrix entries.
\item \inlinecode{def solve_case(A) -> dict} — compute norms and checks.
\item \inlinecode{def validate() -> None} — deterministic assertions.
\item \inlinecode{def main() -> None} — run validation and a sample.
\end{bullets}
}
\INPUTS{
Matrix $A$ as a flat list of numbers forming an $n\times n$ matrix (square).}
\OUTPUTS{
Dictionary containing norms, singular values, condition numbers, and booleans
for inequalities.}
\FORMULA{
\[
\|A\|_1=\max_j\sum_i|a_{ij}|,\quad
\|A\|_\infty=\max_i\sum_j|a_{ij}|,\quad
\|A\|_2=\sigma_{\max},\quad
\kappa_2=\frac{\sigma_{\max}}{\sigma_{\min}}.
\]
}
\textbf{SOLUTION A — From Scratch (Mathematically Explicit Implementation)}
\begin{codepy}
import numpy as np

def read_input(s):
    vals = [float(x) for x in s.strip().split()]
    n = int(round(len(vals) ** 0.5))
    A = np.array(vals, dtype=float).reshape(n, n)
    return A

def norm1(A):
    return float(np.abs(A).sum(axis=0).max())

def norminf(A):
    return float(np.abs(A).sum(axis=1).max())

def svd_extremes(A):
    # deterministic SVD via numpy
    U, S, Vt = np.linalg.svd(A, full_matrices=False)
    return float(S.max()), float(S.min())

def norm2(A):
    smax, _ = svd_extremes(A)
    return smax

def kappa2(A):
    smax, smin = svd_extremes(A)
    if smin == 0.0:
        return np.inf
    return smax / smin

def solve_case(A):
    n1 = norm1(A)
    ni = norminf(A)
    n2 = norm2(A)
    kap = kappa2(A)
    lhs = n2
    rhs = (n1 * ni) ** 0.5
    smax, smin = svd_extremes(A)
    return {
        "norm1": n1, "norminf": ni, "norm2": n2, "kappa2": kap,
        "ineq_ok": lhs <= rhs + 1e-12,
        "kappa_ok": abs(kap - (smax / smin if smin > 0 else np.inf))
                   < 1e-9
    }

def validate():
    A = np.array([[3., -1.], [2., 4.]])
    out = solve_case(A)
    assert out["ineq_ok"]
    assert out["kappa_ok"]
    assert abs(out["norm1"] - 5.0) < 1e-12
    assert abs(out["norminf"] - 6.0) < 1e-12

def main():
    validate()
    A = np.array([[1., 2., 0.], [0., 3., 4.], [5., 6., 0.]])
    out = solve_case(A)
    print("norm1", round(out["norm1"], 6))
    print("norminf", round(out["norminf"], 6))
    print("norm2", round(out["norm2"], 6))
    print("kappa2", round(out["kappa2"], 6))
    print("ineq_ok", out["ineq_ok"], "kappa_ok", out["kappa_ok"])

if __name__ == "__main__":
    main()
\end{codepy}

\textbf{SOLUTION B — Library-Based (Validated Computational Shortcut)}
\begin{codepy}
import numpy as np

def read_input(s):
    vals = [float(x) for x in s.strip().split()]
    n = int(round(len(vals) ** 0.5))
    return np.array(vals, dtype=float).reshape(n, n)

def solve_case(A):
    n1 = float(np.linalg.norm(A, 1))
    ni = float(np.linalg.norm(A, np.inf))
    n2 = float(np.linalg.norm(A, 2))
    S = np.linalg.svd(A, compute_uv=False)
    smax, smin = float(S.max()), float(S.min())
    kap = float(np.linalg.cond(A, 2))
    return {
        "norm1": n1, "norminf": ni, "norm2": n2, "kappa2": kap,
        "ineq_ok": n2 <= (n1 * ni) ** 0.5 + 1e-12,
        "kappa_ok": abs(kap - (smax / smin if smin > 0 else np.inf))
                   < 1e-9
    }

def validate():
    A = np.array([[3., -1.], [2., 4.]])
    out = solve_case(A)
    assert out["ineq_ok"]
    assert out["kappa_ok"]

def main():
    validate()
    A = np.array([[1., 2.], [3., 4.]])
    out = solve_case(A)
    print("norms", round(out["norm1"], 6), round(out["norminf"], 6),
          round(out["norm2"], 6))
    print("kappa2", round(out["kappa2"], 6),
          "checks", out["ineq_ok"], out["kappa_ok"])

if __name__ == "__main__":
    main()
\end{codepy}
\COMPLEXITY{
Both variants: SVD dominates with time $\mathcal{O}(n^3)$ and space
$\mathcal{O}(n^2)$. 1/$\infty$-norms computed in $\mathcal{O}(n^2)$.}
\FAILMODES{
\begin{bullets}
\item Singular $A$ yields $\kappa_2=\infty$; guard division by zero.
\item Non-square input for condition number is invalid; ensure square shape.
\end{bullets}
}
\STABILITY{
\begin{bullets}
\item SVD is backward stable; norm computations are well-conditioned.
\item Avoid explicit inversion; use SVD or built-in cond for robustness.
\end{bullets}
}
\VALIDATION{
\begin{bullets}
\item Cross-check $\|A\|_2$ via SVD and np.linalg.norm.
\item Test inequality and identity assertions with tolerances.
\end{bullets}
}
\RESULT{
Both implementations agree on norms and condition numbers; inequalities hold.}
\EXPLANATION{
Directly implements Formulas 2, 3, and 4 and verifies the inequality from
Problem 3.}

\CodeDemoPage{Residual-to-Error Bound Verification}
\PROBLEM{
For a given invertible $A$, true $x$, and computed $\hat{x}$, verify
$\|x-\hat{x}\|/\|x\|\le \kappa_2(A)\,\|r\|/\|b\|$ with $r=b-A\hat{x}$.}
\API{
\begin{bullets}
\item \inlinecode{def read_input(s) -> tuple} — parse $A,x,\hat x$.
\item \inlinecode{def solve_case(A,x,xh) -> dict} — compute bound and ratio.
\item \inlinecode{def validate() -> None} — assert bound holds.
\item \inlinecode{def main() -> None} — run a deterministic example.
\end{bullets}
}
\INPUTS{
Square invertible $A$, vectors $x$ and $\hat{x}$ of compatible shape.}
\OUTPUTS{
Dictionary with residual norm, relative error, relative residual, condition
number, and boolean check of the inequality.}
\FORMULA{
\[
\frac{\|x-\hat{x}\|_2}{\|x\|_2}\le \kappa_2(A)\frac{\|b-A\hat{x}\|_2}{\|b\|_2}.
\]
}
\textbf{SOLUTION A — From Scratch (Mathematically Explicit Implementation)}
\begin{codepy}
import numpy as np

def read_input(s):
    parts = [float(x) for x in s.strip().split()]
    n = int(parts[0])
    vals = parts[1:1+n*n]
    A = np.array(vals, dtype=float).reshape(n, n)
    x = np.array(parts[1+n*n:1+n*n+n], dtype=float)
    xh = np.array(parts[1+n*n+n:1+n*n+2*n], dtype=float)
    return A, x, xh

def kappa2(A):
    S = np.linalg.svd(A, compute_uv=False)
    smax, smin = float(S.max()), float(S.min())
    return np.inf if smin == 0.0 else smax / smin

def solve_case(A, x, xh):
    b = A @ x
    r = b - A @ xh
    rel_err = float(np.linalg.norm(x - xh) / np.linalg.norm(x))
    rel_res = float(np.linalg.norm(r) / np.linalg.norm(b))
    kap = kappa2(A)
    bound = float(kap * rel_res)
    return {"rel_err": rel_err, "rel_res": rel_res,
            "kappa2": kap, "bound": bound, "ok": rel_err <= bound + 1e-12}

def validate():
    A = np.array([[4., 1.], [2., 3.]])
    x = np.array([0.3, -0.2])
    xh = np.array([0.25, -0.1])
    out = solve_case(A, x, xh)
    assert out["ok"]

def main():
    validate()
    A = np.array([[3., 1., 0.], [0., 2., 1.], [1., 0., 2.]])
    x = np.array([1., 0., -1.])
    xh = np.array([0.9, 0.05, -0.95])
    out = solve_case(A, x, xh)
    print("rel_err", round(out["rel_err"], 6))
    print("rel_res", round(out["rel_res"], 6))
    print("kappa2", round(out["kappa2"], 6))
    print("bound", round(out["bound"], 6), "ok", out["ok"])

if __name__ == "__main__":
    main()
\end{codepy}

\textbf{SOLUTION B — Library-Based (Validated Computational Shortcut)}
\begin{codepy}
import numpy as np

def read_input(s):
    return None

def solve_case(A, x, xh):
    b = A @ x
    r = b - A @ xh
    rel_err = float(np.linalg.norm(x - xh, 2) / np.linalg.norm(x, 2))
    rel_res = float(np.linalg.norm(r, 2) / np.linalg.norm(b, 2))
    kap = float(np.linalg.cond(A, 2))
    bound = float(kap * rel_res)
    return {"rel_err": rel_err, "rel_res": rel_res,
            "kappa2": kap, "bound": bound, "ok": rel_err <= bound + 1e-12}

def validate():
    A = np.array([[4., 1.], [2., 3.]])
    x = np.array([0.3, -0.2])
    xh = np.array([0.25, -0.1])
    out = solve_case(A, x, xh)
    assert out["ok"]

def main():
    validate()
    A = np.array([[1., 2.], [3., 5.]])
    x = np.array([1., -1.])
    xh = np.array([0.8, -0.9])
    out = solve_case(A, x, xh)
    print("rel_err", round(out["rel_err"], 6),
          "bound", round(out["bound"], 6), "ok", out["ok"])

if __name__ == "__main__":
    main()
\end{codepy}
\COMPLEXITY{
Both: SVD/cond computations $\mathcal{O}(n^3)$ time, $\mathcal{O}(n^2)$ space.}
\FAILMODES{
\begin{bullets}
\item If $\|b\|=0$, relative residual undefined; handle separately.
\item Singular $A$ yields infinite condition number; inequality degenerates.
\end{bullets}
}
\STABILITY{
\begin{bullets}
\item Using SVD/cond avoids explicit inversion and is numerically stable.
\end{bullets}
}
\VALIDATION{
\begin{bullets}
\item Assertions ensure the inequality holds within tolerance.
\end{bullets}
}
\RESULT{
Observed relative error never exceeds the bound $\kappa_2(A)\cdot$relative
residual.}
\EXPLANATION{
Implements Formula 5 by computing residuals and condition numbers
and checks the resulting inequality numerically.}

\clearpage
\section{Applied Domains — Detailed End-to-End Scenarios}

\DomainPage{Machine Learning}
\SCENARIO{
Assess conditioning of a linear regression design matrix and its impact on
solution stability by computing $\kappa_2(X)$ and comparing OLS residuals.}
\ASSUMPTIONS{
\begin{bullets}
\item Design matrix $X$ has full column rank.
\item Noise is Gaussian with finite variance.
\end{bullets}
}
\WHICHFORMULA{
$\kappa_2$ via singular values (Formula 4) and residual-to-error bounds
(Formula 5).}
\varmapStart
\var{X}{Design matrix $(n,d)$ with intercept.}
\var{y}{Target vector $(n,)$.}
\var{\beta}{OLS coefficients.}
\var{\kappa_2(X)}{Condition number of $X$.}
\varmapEnd
\PIPELINE{
\begin{bullets}
\item Generate correlated features to induce collinearity.
\item Compute $\kappa_2(X)$ and OLS solution.
\item Compare residual size and discuss sensitivity.
\end{bullets}
}
\textbf{Implementation (From Scratch)}
\begin{codepy}
import numpy as np

def generate_data(n=120, corr=0.95, noise=0.1, seed=0):
    rng = np.random.default_rng(seed)
    z = rng.standard_normal(n)
    x1 = z
    x2 = corr * z + (1 - corr**2) ** 0.5 * rng.standard_normal(n)
    X = np.column_stack([np.ones(n), x1, x2])
    beta = np.array([1.0, 2.0, -1.0])
    y = X @ beta + noise * rng.standard_normal(n)
    return X, y, beta

def ols_fit(X, y):
    U, S, Vt = np.linalg.svd(X, full_matrices=False)
    beta = Vt.T @ (np.divide(U.T @ y, S))
    return beta, S.max() / S.min()

def main():
    X, y, beta_true = generate_data()
    beta_hat, kx = ols_fit(X, y)
    r = y - X @ beta_hat
    print("kappa2(X)", round(kx, 3), "rmse",
          round((np.mean(r**2))**0.5, 3))

if __name__ == "__main__":
    main()
\end{codepy}
\textbf{Implementation (Library Version)}
\begin{codepy}
import numpy as np
from sklearn.linear_model import LinearRegression

def main():
    rng = np.random.default_rng(0)
    n = 120
    z = rng.standard_normal(n)
    x1 = z
    x2 = 0.95 * z + (1 - 0.95**2) ** 0.5 * rng.standard_normal(n)
    X = np.column_stack([x1, x2])
    y = 1 + 2 * x1 - 1 * x2 + 0.1 * rng.standard_normal(n)
    model = LinearRegression(fit_intercept=True).fit(X, y)
    U, S, Vt = np.linalg.svd(
        np.column_stack([np.ones(n), X]), full_matrices=False)
    print("kappa2", round(float(S.max() / S.min()), 3),
          "R2", round(float(model.score(X, y)), 3))

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{
$\kappa_2(X)$, RMSE, and $R^2$; high $\kappa_2$ indicates potential
instability even if $R^2$ is high.}
\INTERPRET{
Strong collinearity inflates $\kappa_2(X)$ and degrades coefficient stability.}
\NEXTSTEPS{
Apply feature scaling or regularization; use QR/SVD solvers instead of normals.}

\DomainPage{Quantitative Finance}
\SCENARIO{
Evaluate conditioning of a factor loading matrix to gauge stability of
beta estimates in a multi-factor model.}
\ASSUMPTIONS{
\begin{bullets}
\item Factors are nearly collinear due to market regimes.
\end{bullets}
}
\WHICHFORMULA{
Use $\kappa_2$ to assess sensitivity of least-squares beta estimation.}
\varmapStart
\var{F}{Factor matrix $(n,d)$.}
\var{r}{Asset returns $(n,)$.}
\var{\beta}{Factor loadings.}
\varmapEnd
\PIPELINE{
\begin{bullets}
\item Simulate correlated factors.
\item Compute $\kappa_2(F)$ and OLS beta.
\item Inspect residuals and condition implications.
\end{bullets}
}
\textbf{Implementation (Full Pipeline)}
\begin{codepy}
import numpy as np

def simulate(n=250, d=3, rho=0.9, seed=0):
    rng = np.random.default_rng(seed)
    z = rng.standard_normal((n, 1))
    E = rng.standard_normal((n, d))
    L = rho * np.ones((1, d)) + (1 - rho) * rng.standard_normal((1, d))
    F = z @ L + 0.3 * E
    beta_true = np.array([0.5, -0.2, 0.3])
    r = F @ beta_true + 0.05 * rng.standard_normal(n)
    return F, r, beta_true

def fit(F, r):
    U, S, Vt = np.linalg.svd(F, full_matrices=False)
    beta = Vt.T @ (np.divide(U.T @ r, S))
    kap = float(S.max() / S.min())
    res = r - F @ beta
    return beta, kap, float(np.linalg.norm(res) / (len(r) ** 0.5))

def main():
    F, r, beta_true = simulate()
    beta_hat, kF, rmse = fit(F, r)
    print("kappa2(F)", round(kF, 2), "rmse", round(rmse, 4))
    print("beta_hat", np.round(beta_hat, 3))

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{
$\kappa_2(F)$ and RMSE; large $\kappa_2$ warns of unstable betas.}
\INTERPRET{
Near-collinear factors create ill-conditioning; small data perturbations
change $\beta$ substantially.}
\NEXTSTEPS{
Orthogonalize factors or apply ridge regression (Tikhonov regularization).}

\DomainPage{Deep Learning}
\SCENARIO{
Measure spectral norms of layer weights and the product bound on the network
Lipschitz constant to assess gradient explosion/vanishing risk.}
\ASSUMPTIONS{
\begin{bullets}
\item Feedforward network with linear layers and ReLU.
\item Lipschitz constant bounded by product of spectral norms of linear parts.
\end{bullets}
}
\WHICHFORMULA{
Spectral norm equals $\sigma_{\max}$; product bounds network Lipschitz,
impacting gradient flow.}
\varmapStart
\var{W_i}{Layer weight matrices.}
\var{L}{Lipschitz bound $\prod_i \|W_i\|_2$.}
\varmapEnd
\PIPELINE{
\begin{bullets}
\item Initialize fixed weight matrices.
\item Compute each $\|W_i\|_2$ and product bound.
\item Interpret training stability risks.
\end{bullets}
}
\textbf{Implementation (End-to-End)}
\begin{codepy}
import numpy as np

def spectral_norm(A):
    return float(np.linalg.svd(A, compute_uv=False).max())

def main():
    np.random.seed(0)
    W1 = np.random.randn(64, 32) / 4.0
    W2 = np.random.randn(32, 32) / 4.0
    W3 = np.random.randn(10, 32) / 4.0
    s1, s2, s3 = spectral_norm(W1), spectral_norm(W2), spectral_norm(W3)
    L = s1 * s2 * s3
    print("spectral norms", round(s1, 3), round(s2, 3), round(s3, 3))
    print("Lipschitz bound", round(L, 3))

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{
Layer spectral norms and their product as a proxy for Lipschitz constant.}
\INTERPRET{
Large products suggest potential gradient explosion; small products suggest
vanishing gradients.}
\NEXTSTEPS{
Apply spectral normalization to constrain $\|W_i\|_2$ during training.}

\DomainPage{Kaggle / Data Analytics}
\SCENARIO{
Standardize features to reduce design matrix condition number and improve
numerical stability in linear models.}
\ASSUMPTIONS{
\begin{bullets}
\item Features are numeric; standardization is mean-zero, unit-variance.
\end{bullets}
}
\WHICHFORMULA{
Compute $\kappa_2(X)$ before/after standardization to quantify stability.}
\varmapStart
\var{X}{Raw feature matrix.}
\var{X_s}{Standardized features.}
\var{\kappa_2}{Condition number in 2-norm.}
\varmapEnd
\PIPELINE{
\begin{bullets}
\item Create synthetic correlated features.
\item Compute $\kappa_2$ of $X$ and standardized $X_s$.
\item Compare and interpret changes.
\end{bullets}
}
\textbf{Implementation (Complete EDA Pipeline)}
\begin{codepy}
import numpy as np

def make_data(n=200, seed=0):
    rng = np.random.default_rng(seed)
    x1 = rng.normal(size=n)
    x2 = 5 * x1 + 0.1 * rng.normal(size=n)
    x3 = 0.01 * rng.normal(size=n) + 2.0
    X = np.column_stack([x1, x2, x3])
    return X

def standardize(X):
    mu = X.mean(axis=0)
    sd = X.std(axis=0, ddof=0)
    sd[sd == 0.0] = 1.0
    return (X - mu) / sd

def kappa2(A):
    S = np.linalg.svd(A, compute_uv=False)
    return float(S.max() / S.min())

def main():
    X = make_data()
    Xs = standardize(X)
    print("kappa2 raw", round(kappa2(X), 2))
    print("kappa2 std", round(kappa2(Xs), 2))

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{
$\kappa_2$ reduction due to standardization quantifies improved stability.}
\INTERPRET{
Feature scaling reduces anisotropy and often lowers the condition number.}
\NEXTSTEPS{
Apply PCA to orthogonalize features or use regularization in modeling.}

\end{document}
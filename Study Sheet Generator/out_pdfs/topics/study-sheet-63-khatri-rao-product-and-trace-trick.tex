% !TeX program = xelatex
\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{microtype,setspace,amsmath,amssymb,mathtools,amsthm,unicode-math}
\setstretch{1.05}
\setmainfont{Latin Modern Roman}
\setmonofont{Latin Modern Mono}
\setmathfont{Latin Modern Math}
\allowdisplaybreaks[4]
\setlength{\jot}{7pt}
\setlength{\emergencystretch}{8em}
\sloppy
\usepackage{xcolor,fancyhdr,enumitem,inconsolata,listings}
\pagestyle{fancy}\fancyhf{}\lhead{\nouppercase{\leftmark}}\rhead{\thepage}
\setlength{\headheight}{26pt}
\setlength{\parindent}{0pt}
\setlength{\parskip}{8pt plus 2pt minus 1pt}
\raggedbottom
\newenvironment{BreakableEquation}{\begin{equation}\begin{aligned}}{\end{aligned}\end{equation}}
\newenvironment{BreakableEquation*}{\begin{equation*}\begin{aligned}}{\end{aligned}\end{equation*}}
\newenvironment{tightalign}{\begingroup\small\allowdisplaybreaks\begin{align}}{\end{align}\endgroup}
\providecommand{\enumlistm}{enumitem}
\newenvironment{minted}[2][]{%
  \lstset{style=code,language=#2,#1}\begin{lstlisting}%
}{\end{lstlisting}}
\newcommand{\inputminted}[3][]{\begin{lstlisting}\end{lstlisting}}
\newlist{bullets}{itemize}{1}
\setlist[bullets]{label=--,leftmargin=1.6em,itemsep=2pt,topsep=4pt}
\newcommand{\varmapStart}{\textbf{VARIABLE MAPPING:}\par\begin{bullets}}
\newcommand{\var}[2]{\item $#1$ — #2}
\newcommand{\varmapEnd}{\end{bullets}}
\newcommand{\glossx}[6]{%
  \textbf{#1}\par
  \begin{bullets}
    \item \textbf{What:} #2
    \item \textbf{Why:} #3
    \item \textbf{How:} #4
    \item \textbf{ELI5:} #5
    \item \textbf{Pitfall/Example:} #6
  \end{bullets}
}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{claim}[theorem]{Claim}
\lstdefinestyle{code}{
  basicstyle=\ttfamily\footnotesize,
  backgroundcolor=\color{black!02},
  frame=single,
  numbers=left, numberstyle=\tiny, numbersep=8pt,
  breaklines=true, breakatwhitespace=true,
  tabsize=4, showstringspaces=false,
  upquote=true, keepspaces=true, columns=fullflexible,
  literate=
    {–}{{-}}1
    {—}{{-}}1
    {…}{{...}}1
    {≤}{{\ensuremath{\le}}}1
    {≥}{{\ensuremath{\ge}}}1
    {≠}{{\ensuremath{\ne}}}1
    {≈}{{\ensuremath{\approx}}}1
    {±}{{\ensuremath{\pm}}}1
    {→}{{\ensuremath{\to}}}1
    {←}{{\ensuremath{\leftarrow}}}1
    {∞}{{\ensuremath{\infty}}}1
    {√}{{\ensuremath{\sqrt{\ }}}}1
    {×}{{\ensuremath{\times}}}1
    {÷}{{\ensuremath{\div}}}1
}
\lstnewenvironment{codepy}[1][]%
  {\lstset{style=code,language=Python,#1}}%
  {}
\newcommand{\inlinecode}[1]{\lstinline[style=code]!#1!}
\newcommand{\LF}[2]{\par\noindent\textbf{#1:}~#2\par}
\newcommand{\WHAT}[1]{\LF{WHAT}{#1}}
\newcommand{\WHY}[1]{\LF{WHY}{#1}}
\newcommand{\HOW}[1]{\LF{HOW}{#1}}
\newcommand{\ELI}[1]{\LF{ELI5}{#1}}
\newcommand{\SCOPE}[1]{\LF{SCOPE}{#1}}
\newcommand{\CONFUSIONS}[1]{\LF{COMMON CONFUSIONS}{#1}}
\newcommand{\APPLICATIONS}[1]{\LF{APPLICATIONS}{#1}}
\newcommand{\FORMULA}[1]{\LF{FORMULA}{#1}}
\newcommand{\CANONICAL}[1]{\LF{CANONICAL FORM}{#1}}
\newcommand{\PRECONDS}[1]{\LF{PRECONDITIONS}{#1}}
\newcommand{\DERIVATION}[1]{\LF{DERIVATION}{#1}}
\newcommand{\EQUIV}[1]{\LF{EQUIVALENT FORMS}{#1}}
\newcommand{\LIMITS}[1]{\LF{LIMIT CASES}{#1}}
\newcommand{\INPUTS}[1]{\LF{INPUTS}{#1}}
\newcommand{\OUTPUTS}[1]{\LF{OUTPUTS}{#1}}
\newcommand{\RESULT}[1]{\LF{RESULT}{#1}}
\newcommand{\INTUITION}[1]{\LF{INTUITION}{#1}}
\newcommand{\PITFALLS}[1]{\LF{PITFALLS}{#1}}
\newcommand{\MODEL}[1]{\LF{CANONICAL MATH MODEL}{#1}}
\newcommand{\ASSUMPTIONS}[1]{\LF{ASSUMPTIONS}{#1}}
\newcommand{\WHICHFORMULA}[1]{\LF{WHICH FORMULA \& WHY}{#1}}
\newcommand{\GOVERN}[1]{\LF{GOVERNING EQUATION(S)}{#1}}
\newcommand{\UNITCHECK}[1]{\LF{UNIT CHECK}{#1}}
\newcommand{\EDGECASES}[1]{\LF{EDGE CASES}{#1}}
\newcommand{\ALTERNATE}[1]{\LF{ALTERNATE APPROACH (sketch)}{#1}}
\newcommand{\PROBLEM}[1]{\LF{PROBLEM}{#1}}
\newcommand{\API}[1]{\LF{API}{#1}}
\newcommand{\COMPLEXITY}[1]{\LF{COMPLEXITY}{#1}}
\newcommand{\FAILMODES}[1]{\LF{FAILURE MODES}{#1}}
\newcommand{\STABILITY}[1]{\LF{NUMERICAL STABILITY}{#1}}
\newcommand{\VALIDATION}[1]{\LF{VALIDATION}{#1}}
\newcommand{\EXPLANATION}[1]{\LF{EXPLANATION}{#1}}
\newcommand{\SCENARIO}[1]{\LF{SCENARIO}{#1}}
\newcommand{\PIPELINE}[1]{\LF{PIPELINE STEPS}{#1}}
\newcommand{\METRICS}[1]{\LF{METRICS}{#1}}
\newcommand{\INTERPRET}[1]{\LF{INTERPRETATION}{#1}}
\newcommand{\NEXTSTEPS}[1]{\LF{LIMITATIONS \& NEXT STEPS}{#1}}
\usepackage{titlesec}
\titleformat{\section}{\Large\bfseries}{\thesection}{0.6em}{}
\usepackage{etoolbox}
\pretocmd{\section}{\clearpage}{}{}
\newcommand{\FormulaPage}[2]{%
  \clearpage
  \section*{Formula #1 — #2}%
  \addcontentsline{toc}{section}{Formula #1 — #2}%
}
\newcommand{\ProblemPage}[2]{%
  \clearpage
  \subsection*{Problem #1: #2}%
  \addcontentsline{toc}{subsection}{Problem #1: #2}%
}
\newcommand{\CodeDemoPage}[1]{%
  \clearpage
  \subsection*{Coding Demo: #1}%
  \addcontentsline{toc}{subsection}{Coding Demo: #1}%
}
\newcommand{\DomainPage}[1]{%
  \clearpage
  \subsection*{#1 (End-to-End)}%
  \addcontentsline{toc}{subsection}{#1 (End-to-End)}%
}
\begin{document}
\title{Comprehensive Study Sheet — Khatri-Rao Product and Trace Trick}
\date{\today}
\maketitle
\tableofcontents
\clearpage
\section{Concept Overview}
\WHAT{
Khatri-Rao product: for $A\in\mathbb{R}^{m\times r}$ and $B\in\mathbb{R}^{n\times r}$,
the column-wise Kronecker product $A\odot B\in\mathbb{R}^{mn\times r}$ with columns
$(A\odot B)_{:k}=A_{:k}\otimes B_{:k}$. Trace trick: algebraic use of
$\mathrm{tr}(UV)=\mathrm{vec}(U)^\top\mathrm{vec}(V)$ and cyclicity
$\mathrm{tr}(XYZ)=\mathrm{tr}(ZXY)$ to transform Frobenius norms and bilinear forms.
Vectorization: $\mathrm{vec}(AXB^\top)=(B\otimes A)\mathrm{vec}(X)$.
}
\WHY{
The Khatri-Rao product expresses separable columnwise interactions and underpins
tensor decompositions and structured least squares. The trace trick linearizes
quadratic matrix expressions, enabling derivations of gradients, normal equations,
and equivalences with Kronecker structures. Together they yield efficient
computations of Gram matrices, normal equations, and identifiability conditions.
}
\HOW{
1. Define the Khatri-Rao product via Kronecker columns.
2. Use vectorization and trace identities to rewrite norms and traces.
3. Derive Gram and normal equations exploiting Hadamard structure.
4. Interpret results as separable coupling across modes with dimension and rank
properties preserved by Schur products and Kronecker actions.
}
\ELI{
Think of $A\odot B$ as stacking, for each feature column $k$, all pairwise
multiplications of entries from $A_{:k}$ and $B_{:k}$ into one tall column. The
trace trick is like rotating a product under the trace sign to expose simpler
pieces or to flatten a matrix into a vector so a hard matrix problem becomes an
easier vector dot product.
}
\SCOPE{
Real or complex matrices; results stated for reals extend to complex with
conjugate transposes. Dimensions must align: $A,B$ share column count. Trace
identities require products to be defined and traces finite. Degenerate cases:
zero columns or rank deficiency affect invertibility but not identities.
}
\CONFUSIONS{
Khatri-Rao $\odot$ vs. Hadamard $\circ$: Khatri-Rao mixes rows via Kronecker and
expects same column counts; Hadamard is elementwise and needs same shape.
Trace trick vs. cyclic permutation: only cyclic rotations are valid, not arbitrary
reordering. Vectorization identity: note $B^\top$ location in $\mathrm{vec}(AXB^\top)$.
}
\APPLICATIONS{
List 3–4 major domains where this topic directly applies:
\begin{bullets}
\item Mathematical foundations (pure / applied).
\item Computational modeling or simulation.
\item Physical / economic / engineering interpretations.
\item Statistical or algorithmic implications.
\end{bullets}
}
\textbf{ANALYTIC STRUCTURE.}
Khatri-Rao is bilinear and columnwise separable; Gram of $A\odot B$ equals the
Hadamard product of Grams, which preserves positive semidefiniteness by the Schur
product theorem. Trace trick leverages linearity and cyclicity in $\mathrm{tr}$ and
connects to Kronecker products via vectorization.

\textbf{CANONICAL LINKS.}
Core links: Schur product theorem, Kronecker vectorization identity,
$(A\odot B)^\top(A\odot B)=(A^\top A)\circ(B^\top B)$, and
$\|Y-AXB^\top\|_F^2=\|Y\|_F^2-2\mathrm{tr}(B X^\top A^\top Y)+
\mathrm{tr}(B X^\top A^\top A X B^\top)$.

\textbf{PROBLEM-TYPE RECOGNITION HEURISTICS.}
\begin{bullets}
\item Columnwise separable models or tensor matricizations suggest $\odot$.
\item Quadratic Frobenius objectives signal trace expansion and vectorization.
\item Gram matrices with Hadamard factors indicate Khatri-Rao normal equations.
\item Presence of $\mathrm{vec}(\cdot)$ and $\otimes$ implies trace trick linkage.
\end{bullets}
\textbf{SOLUTION STRATEGY BLUEPRINT.}
\begin{bullets}
\item Translate to aligned dimensions and shared column counts.
\item Choose identity: Gram via Hadamard or vec-trace via Kronecker.
\item Substitute and simplify with cyclic trace and Kronecker rules.
\item Solve linear systems using Hadamard-structured matrices if needed.
\item Validate by symmetry, PSD, and limit or rank checks.
\end{bullets}
\textbf{CONCEPTUAL INVARIANTS.}
Column count $r$ is invariant under $\odot$. PSD of Gram is preserved and
tightened by Hadamard. Frobenius norm equals trace of self-product.

\textbf{EDGE INTUITION.}
If a column in $A$ or $B$ vanishes, the corresponding $\odot$ column vanishes and
the Gram loses that contribution. If columns are orthonormal, Gram simplifies to
identity or a Hadamard of correlation matrices. As noise $\to 0$, normal
equations revert to exact separable fit.

\section{Glossary}
\glossx{Khatri-Rao Product}{
Column-wise Kronecker: $(A\odot B)_{:k}=A_{:k}\otimes B_{:k}$.}{
Builds separable design matrices for tensor decompositions and structured LS.}{
Form the Kronecker of corresponding columns and concatenate for all $k$.}{
Like stacking all pairwise products for each feature column one after another.}{
Pitfall: confusing with Hadamard; dimensions differ and operations are distinct.}
\glossx{Trace Trick}{
Use $\mathrm{tr}$ linearity and cyclicity to simplify quadratic forms.}{
Derives gradients, normal equations, and vectorization links for matrix problems.}{
Apply $\|M\|_F^2=\mathrm{tr}(M^\top M)$ and rotate products to expose factors.}{
Rotate a necklace of matrices so the clasp is where you can open it easily.}{
Example: derive $\nabla_X \|Y-AXB^\top\|_F^2$ without index sums.}
\glossx{Hadamard Product}{
Elementwise product $C=A\circ B$ for conformable $A,B$.}{
Appears as Gram of Khatri-Rao and preserves PSD by Schur product theorem.}{
Multiply entrywise and use in $(A^\top A)\circ(B^\top B)$.}{
Mixing two similarity matrices entrywise intensifies shared structure.}{
Pitfall: not equal to Khatri-Rao; shapes and algebra differ.}
\glossx{Vectorization}{
Stack matrix columns into a vector $\mathrm{vec}(X)$.}{
Connects bilinear forms to Kronecker products via linear maps.}{
Use $\mathrm{vec}(AXB^\top)=(B\otimes A)\mathrm{vec}(X)$.}{
Flatten a spreadsheet column by column into one long list.}{
Example: compute quadratic forms as dot products for efficiency.}

\section{Symbol Ledger}
\varmapStart
\var{A\in\mathbb{R}^{m\times r}}{Left factor with $r$ columns.}
\var{B\in\mathbb{R}^{n\times r}}{Right factor with $r$ columns.}
\var{C\in\mathbb{R}^{p\times r}}{Third factor with $r$ columns.}
\var{D\in\mathbb{R}^{q\times r}}{Auxiliary factor with $r$ columns.}
\var{X\in\mathbb{R}^{m\times n}}{Matrix variable or parameter.}
\var{Y\in\mathbb{R}^{m\times n}}{Data or target matrix.}
\var{Z\in\mathbb{R}^{mn\times r}}{Generic tall design matrix.}
\var{r}{Common column count (rank parameter).}
\var{m,n,p,q}{Row dimensions.}
\var{\odot}{Khatri-Rao (columnwise Kronecker) product.}
\var{\otimes}{Kronecker product.}
\var{\circ}{Hadamard (elementwise) product.}
\var{\mathrm{vec}(\cdot)}{Vectorization operator (column stacking).}
\var{\mathrm{tr}(\cdot)}{Trace operator.}
\var{G_A=A^\top A}{Gram matrix of $A$.}
\var{I_r}{Identity matrix of size $r$.}
\var{\lambda\ge 0}{Regularization parameter.}
\var{\|\cdot\|_F}{Frobenius norm.}
\var{\mathbb{E}}{Expectation operator.}
\var{\sigma^2}{Variance parameter.}
\varmapEnd
\section{Formula Canon — One Formula Per Page}
\FormulaPage{1}{Khatri-Rao Product (Definition and Structure)}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
For $A\in\mathbb{R}^{m\times r}$ and $B\in\mathbb{R}^{n\times r}$, the
Khatri-Rao product $A\odot B\in\mathbb{R}^{mn\times r}$ satisfies
$(A\odot B)_{:k}=A_{:k}\otimes B_{:k}$ for each $k\in\{1,\dots,r\}$.
\WHAT{
Defines a tall matrix whose columns are Kronecker products of corresponding
columns of $A$ and $B$.}
\WHY{
Provides a structured design matrix enabling separable modeling, efficient Gram
computation, and tensor decomposition algorithms.}
\FORMULA{
\[
A\odot B=\big[\,A_{:1}\otimes B_{:1}\ \ \cdots\ \ A_{:r}\otimes B_{:r}\,\big]
\in\mathbb{R}^{mn\times r}.
\]
}
\CANONICAL{
Domain: real or complex fields. Parameters: $m,n\in\mathbb{N}$, $r\in\mathbb{N}$,
$A\in\mathbb{R}^{m\times r}$, $B\in\mathbb{R}^{n\times r}$.}
\PRECONDS{
\begin{bullets}
\item $A$ and $B$ share the same number of columns $r$.
\item Kronecker product is defined over the underlying field.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
For vectors $u\in\mathbb{R}^m$, $v\in\mathbb{R}^n$, and matrices
$X\in\mathbb{R}^{m\times m}$, $Y\in\mathbb{R}^{n\times n}$,
$(X\otimes Y)(u\otimes v)=(Xu)\otimes(Yv)$.
\end{lemma}
\begin{proof}
Write $X=[x_{ij}]$ and $Y=[y_{kl}]$. Then
$(X\otimes Y)(u\otimes v)=\sum_{i,j}x_{ij}e_i e_j^\top u\otimes
\sum_{k,l}y_{kl}e_k e_l^\top v
=\left(\sum_{i}e_i\sum_j x_{ij}u_j\right)\otimes
\left(\sum_{k}e_k\sum_l y_{kl}v_l\right)
=(Xu)\otimes(Yv)$, which proves the property.\qedhere
\end{proof}
\DERIVATION{
\begin{align*}
\text{Step 1:} &\ \text{Define $(A\odot B)_{:k}=A_{:k}\otimes B_{:k}$.}\\
\text{Step 2:} &\ \text{Stack these $r$ columns to form a matrix in }
\mathbb{R}^{mn\times r}.\\
\text{Step 3:} &\ \text{Check bilinearity: }
(\alpha A+\beta \tilde A)\odot B=\alpha(A\odot B)+\beta(\tilde A\odot B).\\
\text{Step 4:} &\ \text{Dimensions: each column has length $mn$, total $r$ cols.}\\
\text{Step 5:} &\ \text{Conclude the canonical columnwise-Kronecker definition.}
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Verify shared column count $r$ and compute each Kronecker column.
\item Use Kronecker identities for multiplication and Gram computations.
\item Map to vectorized systems via $\mathrm{vec}$ if needed.
\item Validate by checking shapes and special cases (zeros or orthonormality).
\end{bullets}
\EQUIV{
\begin{bullets}
\item $(A\odot B)^\top \mathrm{vec}(X)=\big[\ B^\top X A\ \big]_{\mathrm{diag}}$
(diagonal extraction; see later formulas).
\item Multiway: $\bigodot_{i=1}^K A^{(i)}$ stacks $\otimes_{i=1}^K A^{(i)}_{:k}$.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item If a column $A_{:k}$ or $B_{:k}$ is zero, $(A\odot B)_{:k}$ is zero.
\item If $A_{:k}$ and $B_{:k}$ are unit vectors $e_i,e_j$, then
$(A\odot B)_{:k}=e_i\otimes e_j$ is a standard basis vector.
\end{bullets}
}
\INPUTS{$A\in\mathbb{R}^{m\times r}$, $B\in\mathbb{R}^{n\times r}$.}
\DERIVATION{
\begin{align*}
\text{Column $k$:}\quad (A\odot B)_{:k}
&=A_{:k}\otimes B_{:k},\\
\text{Entrywise:}\quad \big[(A\odot B)_{:k}\big]_{(i-1)n+j}
&=A_{ik}B_{jk}.
\end{align*}
}
\RESULT{
$A\odot B$ is well-defined, bilinear, and shape $mn\times r$ with separable
columns $A_{:k}\otimes B_{:k}$.}
\UNITCHECK{
Dimensions multiply in Kronecker: $m\cdot n$ rows, $r$ columns.}
\PITFALLS{
\begin{bullets}
\item Using $A B$ or $A\circ B$ instead of $A\odot B$ when shapes differ.
\item Forgetting column alignment; Khatri-Rao is not defined if $A$ and $B$
have different column counts.
\end{bullets}
}
\INTUITION{
Each column of $A\odot B$ encodes all pairwise interactions of entries in the
matching columns of $A$ and $B$, arranged in a consistent stacking order.}
\CANONICAL{
\begin{bullets}
\item Columnwise Kronecker representation with bilinear structure.
\item Multiway generalization via iterated Kronecker on columns.
\end{bullets}
}
\FormulaPage{2}{Gram of Khatri-Rao equals Hadamard of Grams}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
For $A\in\mathbb{R}^{m\times r}$ and $B\in\mathbb{R}^{n\times r}$,
\[
(A\odot B)^\top(A\odot B)=(A^\top A)\circ(B^\top B).
\]
\WHAT{
Computes the Gram matrix of a Khatri-Rao product via an elementwise product of
the individual Gram matrices.}
\WHY{
Enables efficient normal equations in separable least squares and tensor factor
updates; ensures PSD and rank properties are tractable.}
\FORMULA{
\[
\big[(A\odot B)^\top(A\odot B)\big]_{kl}=(A_{:k}^\top A_{:l})(B_{:k}^\top B_{:l}),
\]
so the full matrix equals $(A^\top A)\circ(B^\top B)$.
}
\CANONICAL{
Assumes finite inner products; extends to complex with conjugates.}
\PRECONDS{
\begin{bullets}
\item $A,B$ share $r$ columns; inner products are well-defined.
\item No invertibility assumption is needed for the identity itself.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
For vectors $u,v\in\mathbb{R}^m$ and $x,y\in\mathbb{R}^n$,
$(u\otimes x)^\top(v\otimes y)=(u^\top v)(x^\top y)$.
\end{lemma}
\begin{proof}
By properties of Kronecker products,
$(u\otimes x)^\top(v\otimes y)=(u^\top v)(x^\top y)$ follows from
bilinearity and the mixed-product rule
$(U\otimes X)^\top(V\otimes Y)=(U^\top V)\otimes(X^\top Y)$
for $1\times 1$ blocks; the scalarization yields the stated equality.\qedhere
\end{proof}
\DERIVATION{
\begin{align*}
\text{Entry $(k,l)$: }&\ \big[(A\odot B)^\top(A\odot B)\big]_{kl}\\
&=(A_{:k}\otimes B_{:k})^\top(A_{:l}\otimes B_{:l})\\
&=(A_{:k}^\top A_{:l})(B_{:k}^\top B_{:l})
\quad\text{(by the lemma)}\\
&=\big[(A^\top A)\circ(B^\top B)\big]_{kl}.
\end{align*}
Therefore, the full matrices are equal.
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Compute $G_A=A^\top A$ and $G_B=B^\top B$.
\item Multiply elementwise: $G=(G_A)\circ(G_B)$.
\item Use $G$ in normal equations or conditioning analysis.
\end{bullets}
\EQUIV{
\begin{bullets}
\item Multiway: $\left(\bigodot_{i=1}^K A^{(i)}\right)^\top
\left(\bigodot_{i=1}^K A^{(i)}\right)=\bigcirc_{i=1}^K (A^{(i)\top}A^{(i)})$.
\item Schur product theorem: $G\succeq 0$ if each Gram is PSD.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item If a column is zero in any factor, the corresponding row and column in
$G$ vanish.
\item If columns are orthonormal in both $A$ and $B$, then $G=I_r$.
\end{bullets}
}
\INPUTS{$A\in\mathbb{R}^{m\times r}$, $B\in\mathbb{R}^{n\times r}$.}
\DERIVATION{
\begin{align*}
(A\odot B)^\top(A\odot B)
&=\begin{bmatrix}
(A_{:1}^\top A_{:1})(B_{:1}^\top B_{:1}) & \cdots \\
\vdots & \ddots
\end{bmatrix}\\
&=(A^\top A)\circ(B^\top B).
\end{align*}
}
\RESULT{
The Gram of $A\odot B$ is the Hadamard product of the Grams of $A$ and $B$.}
\UNITCHECK{
All matrices are $r\times r$; symmetry and PSD are preserved.}
\PITFALLS{
\begin{bullets}
\item Confusing $(A^\top A)\circ(B^\top B)$ with $(A\circ B)^\top(A\circ B)$.
\item Assuming invertibility; the identity holds without it.
\end{bullets}
}
\INTUITION{
Inner products of Kronecker columns factor into products of inner products; the
Hadamard structure records these pairwise correlations per column index.}
\CANONICAL{
\begin{bullets}
\item Schur-structured Gram: $G=\bigcirc_i G_i$ for multiway factors $G_i$.
\item PSD preserved and often better conditioned with normalized columns.
\end{bullets}
}
\FormulaPage{3}{Vec-Trace-Kronecker Identities}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
For conformable matrices $A,X,B$, and $Y$,
\[
\mathrm{vec}(AXB^\top)=(B\otimes A)\mathrm{vec}(X),
\quad
\mathrm{tr}(X^\top A X B)=\mathrm{vec}(X)^\top(B^\top\otimes A)\mathrm{vec}(X).
\]
\WHAT{
Relates bilinear forms and affine maps in matrix space to linear maps on vectors
via Kronecker products, enabling the trace trick.}
\WHY{
Transforms matrix problems to vector problems; yields gradients, normal equations,
and computational shortcuts.}
\FORMULA{
\[
\mathrm{vec}(AXB^\top)=(B\otimes A)\mathrm{vec}(X),
\quad
\|Y-AXB^\top\|_F^2=\|Y\|_F^2-2\,\mathrm{tr}(B X^\top A^\top Y)
+\mathrm{tr}(X^\top A^\top A X B B^\top).
\]
}
\CANONICAL{
$A\in\mathbb{R}^{m\times m_1}$, $X\in\mathbb{R}^{m_1\times n_1}$,
$B\in\mathbb{R}^{n\times n_1}$, $Y\in\mathbb{R}^{m\times n}$.}
\PRECONDS{
\begin{bullets}
\item Products $AX$ and $XB^\top$ are defined.
\item Traces are finite; Frobenius norms are finite.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
For compatible matrices, $\mathrm{vec}(UXV^\top)=(V\otimes U)\mathrm{vec}(X)$.
\end{lemma}
\begin{proof}
Write $X=[x_1\,\cdots\,x_{n_1}]$ columnwise. Then $UXV^\top=\sum_{j}Ux_j v_j^\top$,
and vectorizing gives
$\sum_j \mathrm{vec}(Ux_j v_j^\top)=\sum_j (v_j\otimes U)x_j
=(V\otimes U)\mathrm{vec}(X)$ by linearity and the property
$\mathrm{vec}(uv^\top)=v\otimes u$.\qedhere
\end{proof}
\DERIVATION{
\begin{align*}
\text{Vec identity: }&\ \mathrm{vec}(AXB^\top)=(B\otimes A)\mathrm{vec}(X).\\
\text{Trace trick: }&\ \|Y-AXB^\top\|_F^2=\mathrm{tr}\big((Y-AXB^\top)^\top
(Y-AXB^\top)\big)\\
&=\mathrm{tr}(Y^\top Y)-2\,\mathrm{tr}(B X^\top A^\top Y)
+\mathrm{tr}(B X^\top A^\top A X B^\top)\\
&=\|Y\|_F^2-2\,\mathrm{tr}(B X^\top A^\top Y)+
\mathrm{tr}(X^\top A^\top A X B B^\top).
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Flatten with $\mathrm{vec}$ and apply Kronecker mapping.
\item Expand Frobenius norms using trace and cyclicity.
\item Convert to quadratic vector forms for optimization or analysis.
\end{bullets}
\EQUIV{
\begin{bullets}
\item $\mathrm{tr}(X^\top M X N)=\mathrm{vec}(X)^\top(N^\top\otimes M)\mathrm{vec}(X)$.
\item $\langle U,V\rangle_F=\mathrm{tr}(U^\top V)=\mathrm{vec}(U)^\top\mathrm{vec}(V)$.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item If $A$ or $B$ is orthonormal, terms simplify using $A^\top A=I$ or $B B^\top=I$.
\item If $X=0$, formulas reduce to norms or zero vectors as expected.
\end{bullets}
}
\INPUTS{$A,B,X,Y$ with compatible sizes as above.}
\DERIVATION{
\begin{align*}
\mathrm{tr}(X^\top A X B)
&=\mathrm{vec}(X)^\top \mathrm{vec}(A X B)\\
&=\mathrm{vec}(X)^\top\big((B^\top\otimes A)\mathrm{vec}(X)\big).
\end{align*}
}
\RESULT{
Matrix quadratic forms become vector quadratic forms with $B^\top\otimes A$.}
\UNITCHECK{
Shapes: $(B^\top\otimes A)$ is $(n_1 m)\times(n_1 m)$ acting on $\mathrm{vec}(X)$.}
\PITFALLS{
\begin{bullets}
\item Misplacing the transpose: it is $B^\top\otimes A$, not $B\otimes A$.
\item Ignoring cyclicity constraints: only cyclic rotations preserve trace.
\end{bullets}
}
\INTUITION{
Vectorization aligns matrix multiplication with a single large linear transform,
and the trace reduces Frobenius inner products to vector dot products.}
\CANONICAL{
\begin{bullets}
\item Bilinear-to-linear reduction via Kronecker.
\item Frobenius norms as traces as quadratic forms on $\mathrm{vec}(X)$.
\end{bullets}
}
\FormulaPage{4}{Trace-Trick Normal Equations for Bilinear Least Squares}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
For fixed $A\in\mathbb{R}^{m\times p}$, $B\in\mathbb{R}^{n\times q}$, and
data $Y\in\mathbb{R}^{m\times n}$, the minimizer of
$f(X)=\|Y-AXB^\top\|_F^2$ over $X\in\mathbb{R}^{p\times q}$ satisfies
\[
(A^\top A)X(B^\top B)=A^\top Y B.
\]
With ridge $\lambda\ge 0$,
\[
(A^\top A)X(B^\top B)+\lambda X=A^\top Y B.
\]
\WHAT{
Gives the matrix normal equations for bilinear least squares with optional ridge
regularization using the trace trick.}
\WHY{
Enables efficient updates in alternating minimization and provides closed forms
via Kronecker inverse on $\mathrm{vec}(X)$.}
\FORMULA{
\[
\nabla_X f=2\big(A^\top A X B^\top B-A^\top Y B\big),
\quad \nabla_X f_\lambda=2\big(A^\top A X B^\top B+\lambda X-A^\top Y B\big).
\]
}
\CANONICAL{
$A\in\mathbb{R}^{m\times p}$, $B\in\mathbb{R}^{n\times q}$,
$X\in\mathbb{R}^{p\times q}$, $Y\in\mathbb{R}^{m\times n}$.}
\PRECONDS{
\begin{bullets}
\item Products $AX$ and $XB^\top$ are defined.
\item For uniqueness without ridge: $A$ and $B$ full column rank.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
$\frac{\partial}{\partial X}\,\mathrm{tr}(X^\top M X N)=M X N^\top+M^\top X N$.
If $M,N$ are symmetric, the derivative reduces to $2 M X N$.
\end{lemma}
\begin{proof}
Use identities $\mathrm{d}\,\mathrm{tr}(X^\top M X N)=
\mathrm{tr}(\mathrm{d}X^\top M X N)+\mathrm{tr}(X^\top M \mathrm{d}X N)$,
then rotate traces and collect terms to obtain the stated differential and
hence the gradient.\qedhere
\end{proof}
\DERIVATION{
\begin{align*}
f(X)&=\|Y-AXB^\top\|_F^2\\
&=\|Y\|_F^2-2\,\mathrm{tr}(B X^\top A^\top Y)+
\mathrm{tr}(X^\top A^\top A X B B^\top).
\end{align*}
Differentiating and setting $\nabla_X f=0$:
\begin{align*}
\nabla_X f
&=-2A^\top Y B+2 A^\top A X B^\top B=0\\
&\Rightarrow (A^\top A)X(B^\top B)=A^\top Y B.
\end{align*}
With ridge $\lambda\|X\|_F^2$:
\begin{align*}
\nabla_X f_\lambda&=-2A^\top Y B+2 A^\top A X B^\top B+2\lambda X=0\\
&\Rightarrow (A^\top A)X(B^\top B)+\lambda X=A^\top Y B.
\end{align*}
Vectorized form:
\begin{align*}
\big((B^\top B)^\top\otimes (A^\top A)+\lambda I\big)\mathrm{vec}(X)
&=\mathrm{vec}(A^\top Y B).
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Expand with trace trick and compute gradient.
\item Solve Sylvester-type equation or vectorized Kronecker system.
\item Use ridge if Grams are ill-conditioned or rank-deficient.
\end{bullets}
\EQUIV{
\begin{bullets}
\item $\mathrm{vec}(X)=\big((B^\top B)\otimes(A^\top A)+\lambda I\big)^{-1}
\mathrm{vec}(A^\top Y B)$.
\item If $A,B$ orthonormal: $X= A^\top Y B$ (and with ridge $X=(1/(1+\lambda))A^\top Y B$).
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item If $A$ or $B$ loses rank, solutions are nonunique; ridge yields uniqueness.
\item If $Y=0$, solution is $X=0$ for $\lambda\ge 0$.
\end{bullets}
}
\INPUTS{$A,B,Y$, optional $\lambda\ge 0$.}
\DERIVATION{
\begin{align*}
\text{Kronecker solve: }&
\big((B^\top B)\otimes(A^\top A)+\lambda I\big)\mathrm{vec}(X)
=\mathrm{vec}(A^\top Y B).
\end{align*}
}
\RESULT{
Normal equations $(A^\top A)X(B^\top B)+\lambda X=A^\top Y B$ with explicit
Kronecker linear system on $\mathrm{vec}(X)$.}
\UNITCHECK{
Left and right are $p\times q$ matrices; vectorized system is $(pq)\times(pq)$.}
\PITFALLS{
\begin{bullets}
\item Dropping a transpose on $B^\top B$ or $A^\top A$.
\item Forgetting factor $2$ in gradients; zeroing gradient removes it.
\end{bullets}
}
\INTUITION{
Bilinear least squares decomposes into left and right Gram factors squeezing $X$;
ridge adds isotropic shrinkage.}
\CANONICAL{
\begin{bullets}
\item Sylvester-type normal equations from trace expansion.
\item Kronecker linear system for vectorized unknown.
\end{bullets}
}
\section{10 Exhaustive Problems and Solutions}
\ProblemPage{1}{Prove and Use the Gram--Hadamard Identity}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Show $(A\odot B)^\top(A\odot B)=(A^\top A)\circ(B^\top B)$ and compute a
numerical example.
\PROBLEM{
Prove the identity and compute the Gram for
$A=\begin{bmatrix}1&2\\3&4\end{bmatrix}$,
$B=\begin{bmatrix}2&1\\-1&0\\0&1\end{bmatrix}$ with $r=2$.
}
\Model{
\[
G=(A\odot B)^\top(A\odot B),\quad H=(A^\top A)\circ(B^\top B).
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Real matrices; standard inner products.
\item Shared column count $r=2$.
\end{bullets}
}
\varmapStart
\var{A\in\mathbb{R}^{2\times 2}}{Left factor.}
\var{B\in\mathbb{R}^{3\times 2}}{Right factor.}
\var{G,H\in\mathbb{R}^{2\times 2}}{Gram candidates.}
\varmapEnd
\WHICHFORMULA{
Formula 2 (Gram equals Hadamard of Grams); Lemma: Kronecker inner product.}
\GOVERN{
\[
G=(A^\top A)\circ(B^\top B).
\]
}
\INPUTS{$A=\begin{bmatrix}1&2\\3&4\end{bmatrix}$,
$B=\begin{bmatrix}2&1\\-1&0\\0&1\end{bmatrix}$.}
\DERIVATION{
\begin{align*}
A^\top A&=\begin{bmatrix}1&3\\2&4\end{bmatrix}
\begin{bmatrix}1&2\\3&4\end{bmatrix}
=\begin{bmatrix}10&14\\14&20\end{bmatrix}.\\
B^\top B&=\begin{bmatrix}2&-1&0\\1&0&1\end{bmatrix}
\begin{bmatrix}2&1\\-1&0\\0&1\end{bmatrix}
=\begin{bmatrix}5&2\\2&2\end{bmatrix}.\\
H&=(A^\top A)\circ(B^\top B)
=\begin{bmatrix}50&28\\28&40\end{bmatrix}.\\
A\odot B&=\big[\,A_{:1}\otimes B_{:1}\ \ A_{:2}\otimes B_{:2}\,\big]\\
&=\left[
\begin{array}{cc}
1\cdot 2\\1\cdot(-1)\\1\cdot 0\\3\cdot 2\\3\cdot(-1)\\3\cdot 0
\end{array}
\ \ 
\begin{array}{c}
2\cdot 1\\2\cdot 0\\2\cdot 1\\4\cdot 1\\4\cdot 0\\4\cdot 1
\end{array}
\right]
=\begin{bmatrix}
2&2\\-1&0\\0&2\\6&4\\-3&0\\0&4
\end{bmatrix}.\\
G&=(A\odot B)^\top(A\odot B)
=\begin{bmatrix}2&-1&0&6&-3&0\\2&0&2&4&0&4\end{bmatrix}
\begin{bmatrix}2&2\\-1&0\\0&2\\6&4\\-3&0\\0&4\end{bmatrix}\\
&=\begin{bmatrix}50&28\\28&40\end{bmatrix}=H.
\end{align*}
}
\RESULT{
$G=H=\begin{bmatrix}50&28\\28&40\end{bmatrix}$.}
\UNITCHECK{
All Grams are $2\times 2$, symmetric, PSD.}
\EDGECASES{
\begin{bullets}
\item If a column vanishes, the corresponding Gram row and column are zero.
\item If $A,B$ have orthonormal columns, $G=I_r$.
\end{bullets}
}
\ALTERNATE{
Direct proof via entrywise inner products of Kronecker columns.}
\VALIDATION{
\begin{bullets}
\item Numeric reconstruction as above confirms equality.
\item Check PSD via eigenvalues $\ge 0$ numerically.
\end{bullets}
}
\INTUITION{
Inner products factor across modes; Hadamard encodes per-column correlations.}
\CANONICAL{
\begin{bullets}
\item $G=\bigcirc_i G_i$ generalizes to multiway factors.
\item Schur product theorem ensures $G\succeq 0$.
\end{bullets}
}
\ProblemPage{2}{Trace-Trick Gradient and Normal Equations}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Derive $\nabla_X\|Y-AXB^\top\|_F^2$ and solve for $X$.
\PROBLEM{
Given $A\in\mathbb{R}^{3\times 2}$, $B\in\mathbb{R}^{4\times 2}$,
$Y\in\mathbb{R}^{3\times 4}$, derive gradient and the normal equation,
then compute $X$ when $A^\top A=I_2$, $B^\top B=I_2$, and $A^\top Y B$ is given.}
\Model{
\[
f(X)=\|Y-AXB^\top\|_F^2,\quad \nabla_X f=2(A^\top A X B^\top B-A^\top Y B).
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $A,B$ full column rank; orthonormal columns in the numeric part.
\item Real matrices, Frobenius norm finite.
\end{bullets}
}
\varmapStart
\var{A,B,Y}{Given data matrices.}
\var{X\in\mathbb{R}^{2\times 2}}{Unknown.}
\var{M=A^\top Y B}{Cross term.}
\varmapEnd
\WHICHFORMULA{
Formula 3 (trace expansion) and Formula 4 (normal equations).}
\GOVERN{
\[
(A^\top A)X(B^\top B)=A^\top Y B.
\]
}
\INPUTS{$A^\top A=I_2$, $B^\top B=I_2$, $M=A^\top Y B=\begin{bmatrix}1&2\\3&4\end{bmatrix}$.}
\DERIVATION{
\begin{align*}
\nabla_X f&=2(A^\top A X B^\top B-A^\top Y B).\\
\text{Set } \nabla_X f&=0\Rightarrow (A^\top A)X(B^\top B)=M.\\
\text{With }A^\top A&=I,\ B^\top B=I\Rightarrow X=M.
\end{align*}
}
\RESULT{
$X=\begin{bmatrix}1&2\\3&4\end{bmatrix}$.}
\UNITCHECK{
Shapes: $X$ is $2\times 2$, consistent with $A^\top Y B$.}
\EDGECASES{
\begin{bullets}
\item If $A,B$ not full rank, solution may be nonunique; choose minimum norm.
\item With ridge $\lambda>0$, solution becomes $(I+\lambda I)^{-1}M=\frac{1}{1+\lambda}M$.
\end{bullets}
}
\ALTERNATE{
Vectorize:
$\mathrm{vec}(X)=\big((B^\top B)\otimes(A^\top A)\big)^{-1}\mathrm{vec}(M)$.}
\VALIDATION{
\begin{bullets}
\item Substitute $X$ back to check residual minimality numerically.
\item Verify symmetry of gradient at optimum (zero matrix).
\end{bullets}
}
\INTUITION{
Cyclic trace reveals that $X$ is pulled between left and right Grams to match
$Y$ in the $A,B$ subspaces.}
\CANONICAL{
\begin{bullets}
\item Sylvester equation reduces to identity with orthonormal designs.
\item Vectorized Kronecker solve equivalent to matrix normal equation.
\end{bullets}
}
\ProblemPage{3}{Vectorization Identity Application}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Verify $\mathrm{vec}(AXB^\top)=(B\otimes A)\mathrm{vec}(X)$ numerically.
\PROBLEM{
Let $A=\begin{bmatrix}1&0\\2&1\end{bmatrix}$,
$X=\begin{bmatrix}1&-1\\0&2\end{bmatrix}$,
$B=\begin{bmatrix}0&1\\1&1\end{bmatrix}$. Compute both sides.}
\Model{
\[
v_1=\mathrm{vec}(AXB^\top),\quad
v_2=(B\otimes A)\mathrm{vec}(X).
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Standard column-major vectorization.
\end{bullets}
}
\varmapStart
\var{v_1,v_2\in\mathbb{R}^{4}}{Vectorized results.}
\varmapEnd
\WHICHFORMULA{
Formula 3 (vec-Kronecker identity).}
\GOVERN{
\[
\mathrm{vec}(AXB^\top)=(B\otimes A)\mathrm{vec}(X).
\]
}
\INPUTS{Matrices $A,X,B$ as above.}
\DERIVATION{
\begin{align*}
AXB^\top&=A X B^\top,\\
X&=\begin{bmatrix}1&-1\\0&2\end{bmatrix},\ 
B^\top=\begin{bmatrix}0&1\\1&1\end{bmatrix}.\\
XB^\top&=\begin{bmatrix}1&-1\\0&2\end{bmatrix}
\begin{bmatrix}0&1\\1&1\end{bmatrix}
=\begin{bmatrix}-1&0\\2&2\end{bmatrix}.\\
A(XB^\top)&=\begin{bmatrix}1&0\\2&1\end{bmatrix}
\begin{bmatrix}-1&0\\2&2\end{bmatrix}
=\begin{bmatrix}-1&0\\0&2\end{bmatrix}.\\
v_1&=\mathrm{vec}\left(\begin{bmatrix}-1&0\\0&2\end{bmatrix}\right)
=\begin{bmatrix}-1\\0\\0\\2\end{bmatrix}.\\
\mathrm{vec}(X)&=\begin{bmatrix}1\\0\\-1\\2\end{bmatrix},\ 
B\otimes A=
\begin{bmatrix}
0A&1A\\1A&1A
\end{bmatrix}
=\begin{bmatrix}
0&0&1&0\\
0&0&2&1\\
1&0&1&0\\
2&1&2&1
\end{bmatrix}.\\
v_2&=(B\otimes A)\mathrm{vec}(X)
=\begin{bmatrix}-1\\0\\0\\2\end{bmatrix}=v_1.
\end{align*}
}
\RESULT{
Identity holds numerically: $v_1=v_2$.}
\UNITCHECK{
$B\otimes A$ is $4\times 4$, $\mathrm{vec}(X)\in\mathbb{R}^4$.}
\EDGECASES{
\begin{bullets}
\item If $A$ or $B$ is identity, the mapping reduces accordingly.
\item If $X$ is diagonal, the action simplifies but identity still holds.
\end{bullets}
}
\ALTERNATE{
Index-based proof: $(AXB^\top)_{ij}=\sum_{k\ell}A_{ik}X_{k\ell}B_{j\ell}$.}
\VALIDATION{
\begin{bullets}
\item Randomized tests with fixed seeds confirm equality.
\end{bullets}
}
\INTUITION{
Flattening and using Kronecker reproduces left-right multiplications linearly.}
\CANONICAL{
\begin{bullets}
\item Central engine behind trace-trick quadratic forms.
\end{bullets}
}
\ProblemPage{4}{ALS Update via Khatri-Rao and Hadamard Gram}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Given $Y_{(1)}\in\mathbb{R}^{m\times (np)}$ and factors
$B\in\mathbb{R}^{n\times r}$, $C\in\mathbb{R}^{p\times r}$, show the least squares
update
\[
H=Y_{(1)}(C\odot B)\big((B^\top B)\circ(C^\top C)\big)^{-1}.
\]
\PROBLEM{
Alice and Bob alternate updates in a CP model. With mode-1 unfolding
$Y_{(1)}\approx H(C\odot B)^\top$, derive the closed form for $H$ and compute it
for a small numeric case.}
\Model{
\[
\min_H \|Y_{(1)}-H(C\odot B)^\top\|_F^2.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Columns of $B,C$ are linearly independent so the Hadamard Gram is invertible.
\item Real matrices; Frobenius norm finite.
\end{bullets}
}
\varmapStart
\var{Y_{(1)}}{Mode-1 unfolding of a third-order tensor.}
\var{H\in\mathbb{R}^{m\times r}}{Update variable.}
\var{B,C\in\mathbb{R}^{\cdot\times r}}{Fixed factor matrices.}
\varmapEnd
\WHICHFORMULA{
Formula 2 for Gram and Formula 4 for normal equations.}
\GOVERN{
\[
H\big((B^\top B)\circ(C^\top C)\big)=Y_{(1)}(C\odot B).
\]
}
\INPUTS{
Let $B=\begin{bmatrix}1&0\\0&1\end{bmatrix}$,
$C=\begin{bmatrix}2&0\\0&3\end{bmatrix}$,
$Y_{(1)}=\begin{bmatrix}1&2&3&4\\0&1&0&1\end{bmatrix}$ with $n=p=2$, $r=2$.
}
\DERIVATION{
\begin{align*}
Z&=C\odot B=
\big[\,C_{:1}\otimes B_{:1}\ \ C_{:2}\otimes B_{:2}\,\big]
=\begin{bmatrix}2&0\\0&3\\0&0\\0&0\end{bmatrix}.\\
B^\top B&=I_2,\quad C^\top C=\mathrm{diag}(4,9).\\
G&=(B^\top B)\circ(C^\top C)=\mathrm{diag}(4,9).\\
H&=Y_{(1)} Z G^{-1}.
\end{align*}
Compute:
\begin{align*}
Y_{(1)}Z&=\begin{bmatrix}1&2&3&4\\0&1&0&1\end{bmatrix}
\begin{bmatrix}2&0\\0&3\\0&0\\0&0\end{bmatrix}
=\begin{bmatrix}2&6\\0&3\end{bmatrix}.\\
G^{-1}&=\mathrm{diag}(1/4,1/9).\\
H&=\begin{bmatrix}2&6\\0&3\end{bmatrix}
\begin{bmatrix}1/4&0\\0&1/9\end{bmatrix}
=\begin{bmatrix}0.5&0.666\overline{6}\\0&0.333\overline{3}\end{bmatrix}.
\end{align*}
\end{align*}
}
\RESULT{
$H=\begin{bmatrix}1/2&2/3\\0&1/3\end{bmatrix}$.}
\UNITCHECK{
$H$ is $m\times r$; $G$ is $r\times r$; $Z$ is $(np)\times r$.}
\EDGECASES{
\begin{bullets}
\item If any Gram is singular, use ridge: add $\lambda I_r$ before inverting.
\item Scaling columns of $B$ and $C$ scales $G$ and $Z$ compatibly.
\end{bullets}
}
\ALTERNATE{
Solve rowwise least squares: each row of $H$ solves $h^\top Z^\top=y^\top$.}
\VALIDATION{
\begin{bullets}
\item Check residual $\|Y_{(1)}-H Z^\top\|_F$ decreases versus previous $H$.
\item Verify $G=Z^\top Z$ numerically using Formula 2.
\end{bullets}
}
\INTUITION{
Hadamard Gram summarizes interactions between $B$ and $C$ per component, making
the normal equation diagonal under orthogonality.}
\CANONICAL{
\begin{bullets}
\item ALS updates reduce to solving $r\times r$ systems using Hadamard Grams.
\end{bullets}
}
\ProblemPage{5}{Hidden Structure in Feature Crosses}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Show that linear regression on pairwise feature crosses with shared per-factor
weights corresponds to a Khatri-Rao design.
\PROBLEM{
Given feature matrices $A\in\mathbb{R}^{m\times r}$ and $B\in\mathbb{R}^{n\times r}$,
consider predicting $Y\in\mathbb{R}^{mn}$ from design columns
$A_{:k}\otimes B_{:k}$. Show the normal equation and the Gram structure.}
\Model{
\[
\min_{w\in\mathbb{R}^{r}}\ \|Y-(A\odot B)w\|_2^2.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Real data; $A,B$ share $r$ columns.
\item $G=(A^\top A)\circ(B^\top B)$ invertible.
\end{bullets}
}
\varmapStart
\var{w\in\mathbb{R}^r}{Weights per shared component.}
\var{Z=A\odot B}{Design matrix.}
\var{G=Z^\top Z}{Gram.}
\varmapEnd
\WHICHFORMULA{
Formula 2 for Gram, Formula 3 for vec inner products.}
\GOVERN{
\[
w=G^{-1}Z^\top Y,\quad G=(A^\top A)\circ(B^\top B).
\]
}
\INPUTS{Symbolic; provide expression for $Z^\top Y$.}
\DERIVATION{
\begin{align*}
\nabla_w\|Y-Zw\|_2^2&=2(Z^\top Z)w-2Z^\top Y=0\\
\Rightarrow w&=G^{-1}Z^\top Y.\\
(Z^\top Y)_k&=(A_{:k}\otimes B_{:k})^\top Y\\
&=\mathrm{vec}^{-1}\text{ interpretation: }(B_{:k}^\top Y^A A_{:k})
\text{ for reshaped $Y$ as matrix}.
\end{align*}
}
\RESULT{
Closed form $w=G^{-1}Z^\top Y$ with $G=(A^\top A)\circ(B^\top B)$.}
\UNITCHECK{
$G$ is $r\times r$; $Z^\top Y$ is $r\times 1$; $w$ is $r\times 1$.}
\EDGECASES{
\begin{bullets}
\item Collinearity across columns induces near-singular $G$.
\item Ridge: use $(G+\lambda I)^{-1}Z^\top Y$.
\end{bullets}
}
\ALTERNATE{
Solve via QR on $Z$; factorization inherits Kronecker structure per column.}
\VALIDATION{
\begin{bullets}
\item Compare predictions versus direct least squares on $Z$ numerically.
\end{bullets}
}
\INTUITION{
Shared per-factor weights mean each component contributes a separable cross, so
the Gram decomposes elementwise across factors.}
\CANONICAL{
\begin{bullets}
\item Khatri-Rao designs yield Hadamard Grams enabling cheap inversion.
\end{bullets}
}
\ProblemPage{6}{Expectation via Trace Trick}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Compute $\mathbb{E}\|AX-b\|_2^2$ where $X\sim\mathcal{N}(\mu,\Sigma)$.
\PROBLEM{
Let $A\in\mathbb{R}^{m\times d}$, $b\in\mathbb{R}^m$, $X\in\mathbb{R}^d$ Gaussian.
Find a closed form using the trace trick.}
\Model{
\[
\mathbb{E}\|AX-b\|_2^2=\mathbb{E}\big[(AX-b)^\top(AX-b)\big].
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $\Sigma\succeq 0$; moments finite.
\end{bullets}
}
\varmapStart
\var{A,b}{Fixed matrices/vectors.}
\var{\mu,\Sigma}{Mean and covariance of $X$.}
\varmapEnd
\WHICHFORMULA{
Trace trick: $\|M\|_F^2=\mathrm{tr}(M^\top M)$ and
$\mathbb{E}[XX^\top]=\Sigma+\mu\mu^\top$.}
\GOVERN{
\[
\mathbb{E}\|AX-b\|_2^2=\mathrm{tr}\big(A(\Sigma+\mu\mu^\top)A^\top\big)
-2b^\top A\mu+b^\top b.
\]
}
\INPUTS{$A,\ b,\ \mu,\ \Sigma$.}
\DERIVATION{
\begin{align*}
\mathbb{E}\|AX-b\|_2^2
&=\mathbb{E}\,\mathrm{tr}\big((AX-b)(AX-b)^\top\big)\\
&=\mathrm{tr}\big(A\mathbb{E}[XX^\top]A^\top\big)
-2b^\top A\mathbb{E}[X]+b^\top b\\
&=\mathrm{tr}\big(A(\Sigma+\mu\mu^\top)A^\top\big)-2b^\top A\mu+b^\top b.
\end{align*}
}
\RESULT{
Closed form as stated, computable via two matrix multiplications and a trace.}
\UNITCHECK{
All terms are scalars; traces of $m\times m$ matrices.}
\EDGECASES{
\begin{bullets}
\item If $\Sigma=0$, reduces to $\|A\mu-b\|_2^2$.
\item If $b=0$, reduces to $\mathrm{tr}(A\Sigma A^\top)+\|A\mu\|_2^2$.
\end{bullets}
}
\ALTERNATE{
Expand coordinates and sum variances; trace compactly encodes the same.}
\VALIDATION{
\begin{bullets}
\item Monte Carlo with fixed seed matches analytic expression.
\end{bullets}
}
\INTUITION{
Expected squared error splits into bias term and variance inflation via
$A\Sigma A^\top$.}
\CANONICAL{
\begin{bullets}
\item Trace trick converts quadratic expectations to traces of covariances.
\end{bullets}
}
\ProblemPage{7}{PSD and Schur Product Theorem Application}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Show that $(A\odot B)^\top(A\odot B)\succeq 0$ and equals
$(A^\top A)\circ(B^\top B)\succeq 0$ by Schur product theorem.
\PROBLEM{
Prove PSD and identify when the Gram is positive definite.}
\Model{
\[
G=(A^\top A)\circ(B^\top B).
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $A,B$ finite; real entries.
\end{bullets}
}
\varmapStart
\var{G}{Hadamard product of PSD matrices.}
\varmapEnd
\WHICHFORMULA{
Formula 2 (Gram identity) and Schur product theorem.}
\GOVERN{
\[
\forall z,\ z^\top G z\ge 0.
\]
}
\INPUTS{Symbolic proof.}
\DERIVATION{
\begin{align*}
&\text{Since }A^\top A\succeq 0,\ B^\top B\succeq 0,\\
&\text{Schur product theorem: }(A^\top A)\circ(B^\top B)\succeq 0.\\
&\text{Alternatively, }G=(A\odot B)^\top(A\odot B)\succeq 0
\text{ by construction.}\\
&\text{If }A,B\text{ have full column rank and no proportional columns,}\\
&\text{then }G\succ 0\text{ (columns linearly independent).}
\end{align*}
}
\RESULT{
$G$ is PSD; PD if and only if columns $A_{:k}\otimes B_{:k}$ are independent.}
\UNITCHECK{
$G$ symmetric $r\times r$; quadratic forms are nonnegative.}
\EDGECASES{
\begin{bullets}
\item If a column in $A$ or $B$ is zero, $G$ is singular.
\item Proportional columns across both factors induce singularity.
\end{bullets}
}
\ALTERNATE{
Directly show $z^\top G z=\|(A\odot B)z\|_2^2\ge 0$.}
\VALIDATION{
\begin{bullets}
\item Random tests show nonnegative eigenvalues numerically.
\end{bullets}
}
\INTUITION{
Gram of any matrix is PSD; Hadamard of PSD Grams preserves PSD.}
\CANONICAL{
\begin{bullets}
\item PSD preservation under Khatri-Rao and Schur product.
\end{bullets}
}
\ProblemPage{8}{Rank of Khatri-Rao under Full Column Rank}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
If $A,B$ are full column rank and no column pairs are proportional with opposite
scales, then $A\odot B$ is full column rank.
\PROBLEM{
Prove that $\mathrm{rank}(A\odot B)=r$ under the condition that
$\{A_{:k}\otimes B_{:k}\}$ are linearly independent.}
\Model{
\[
G=(A\odot B)^\top(A\odot B)=(A^\top A)\circ(B^\top B).
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $A,B$ full column rank, and the set
$\{A_{:k}\otimes B_{:k}\}_{k=1}^r$ is independent.
\end{bullets}
}
\varmapStart
\var{G}{Gram matrix of Khatri-Rao columns.}
\varmapEnd
\WHICHFORMULA{
Formula 2 and basic linear algebra: full column rank iff Gram is PD.}
\GOVERN{
\[
\mathrm{rank}(A\odot B)=r \iff G\succ 0.
\]
}
\INPUTS{Symbolic proof.}
\DERIVATION{
\begin{align*}
&\text{If the columns }A_{:k}\otimes B_{:k}\text{ are independent,}\\
&\text{then for nonzero }z,\ \|(A\odot B)z\|_2^2>0\\
&\Rightarrow z^\top G z>0\Rightarrow G\succ 0\Rightarrow \mathrm{rank}(A\odot B)=r.\\
&\text{Conversely, if }G\succ 0,\ \text{then no nontrivial }z\text{ annihilates }A\odot B.
\end{align*}
}
\RESULT{
$A\odot B$ has full column rank when its column Kronecker set is independent.}
\UNITCHECK{
$G$ is $r\times r$ PD, so invertible.}
\EDGECASES{
\begin{bullets}
\item If $A_{:k}=\alpha A_{:\ell}$ and $B_{:k}=\beta B_{:\ell}$ with $\alpha\beta=1$,
columns may be proportional causing rank loss.
\end{bullets}
}
\ALTERNATE{
Use determinant identity: $\det G>0$ implies independence.}
\VALIDATION{
\begin{bullets}
\item Numerically test smallest eigenvalue $>0$ for random full rank $A,B$.
\end{bullets}
}
\INTUITION{
Independence of Kroneckerized columns prevents cancellation in linear combos.}
\CANONICAL{
\begin{bullets}
\item Gram positive definiteness certifies full column rank.}
\end{bullets}
\ProblemPage{9}{Gradient of Quadratic Form via Trace Trick}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
For $f(X)=\mathrm{tr}(X^\top M X N)-2\,\mathrm{tr}(X^\top K)$,
derive $\nabla_X f$ and show equivalence to vectorized gradient.
\PROBLEM{
Let $M,N$ be symmetric PSD. Find $\nabla_X f$ and confirm that
$\mathrm{vec}(\nabla_X f)=2\big((N\otimes M)\mathrm{vec}(X)-\mathrm{vec}(K)\big)$.}
\Model{
\[
f(X)=\mathrm{tr}(X^\top M X N)-2\,\mathrm{tr}(X^\top K).
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $M,N$ symmetric; dimensions conform.
\end{bullets}
}
\varmapStart
\var{M,N}{Gram-like factors.}
\var{K}{Cross term.}
\varmapEnd
\WHICHFORMULA{
Formula 3 (trace identity) and derivative lemma in Formula 4.}
\GOVERN{
\[
\nabla_X f=2(M X N-K).
\]
}
\INPUTS{Symbolic proof.}
\DERIVATION{
\begin{align*}
\nabla_X \mathrm{tr}(X^\top M X N)&=M X N+M^\top X N^\top=2 M X N.\\
\nabla_X\big(-2\,\mathrm{tr}(X^\top K)\big)&=-2K.\\
\Rightarrow \nabla_X f&=2(M X N-K).
\end{align*}
Vectorized:
\begin{align*}
\mathrm{vec}(\nabla_X f)
&=2\big(\mathrm{vec}(M X N)-\mathrm{vec}(K)\big)\\
&=2\big((N^\top\otimes M)\mathrm{vec}(X)-\mathrm{vec}(K)\big)\\
&=2\big((N\otimes M)\mathrm{vec}(X)-\mathrm{vec}(K)\big).
\end{align*}
}
\RESULT{
$\nabla_X f=2(M X N-K)$; vectorized gradient matches Kronecker form.}
\UNITCHECK{
All terms share shape of $X$.}
\EDGECASES{
\begin{bullets}
\item If $M=0$ or $N=0$, gradient reduces to $-2K$.
\item If $K=0$, stationary point at $X=0$.
\end{bullets}
}
\ALTERNATE{
Coordinate calculus yields the same gradient but is longer.}
\VALIDATION{
\begin{bullets}
\item Finite differences match analytic gradient under perturbations.
\end{bullets}
}
\INTUITION{
Quadratic forms pull $X$ through left and right metrics $M,N$.}
\CANONICAL{
\begin{bullets}
\item Trace-to-Kronecker equivalence of gradients.
\end{bullets}
}
\ProblemPage{10}{Ridge ALS Update in CP via Khatri-Rao}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Show that the ridge-regularized update for $H$ in
$\min_H \|Y_{(1)}-H(C\odot B)^\top\|_F^2+\lambda\|H\|_F^2$ is
\[
H=Y_{(1)}(C\odot B)\big((B^\top B)\circ(C^\top C)+\lambda I_r\big)^{-1}.
\]
\PROBLEM{
Derive the update and compute for a small case with $\lambda=1$.}
\Model{
\[
f(H)=\|Y_{(1)}-H Z^\top\|_F^2+\lambda\|H\|_F^2,\ Z=C\odot B.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $B,C$ given; $\lambda>0$.
\end{bullets}
}
\varmapStart
\var{Z=C\odot B}{Design matrix.}
\var{G=Z^\top Z}{Hadamard Gram.}
\varmapEnd
\WHICHFORMULA{
Formula 2 (Gram) and Formula 4 (ridge normal equations).}
\GOVERN{
\[
H(G+\lambda I_r)=Y_{(1)} Z.
\]
}
\INPUTS{
$B=\begin{bmatrix}1\\1\end{bmatrix}$, $C=\begin{bmatrix}1\\2\end{bmatrix}$,
$Y_{(1)}=\begin{bmatrix}1&0\\0&1\end{bmatrix}$, $r=1$, $\lambda=1$.
}
\DERIVATION{
\begin{align*}
Z&=C\odot B=\begin{bmatrix}1\\2\end{bmatrix}\odot\begin{bmatrix}1\\1\end{bmatrix}
=\begin{bmatrix}1\\1\\2\\2\end{bmatrix}.\\
G&=Z^\top Z=1^2+1^2+2^2+2^2=10.\\
H&=Y_{(1)} Z (G+\lambda)^{-1}
=\begin{bmatrix}1&0\\0&1\end{bmatrix}
\begin{bmatrix}1\\1\\2\\2\end{bmatrix}\cdot \frac{1}{11}
\text{ (reshape consistent by blocks)}\\
&=\begin{bmatrix}1/11\\ 2/11\end{bmatrix}.
\end{align*}
}
\RESULT{
$H=\begin{bmatrix}1/11\\2/11\end{bmatrix}$ for the scalar $r=1$ case.}
\UNITCHECK{
$G+\lambda I_r$ is $1\times 1$; inversion is scalar.}
\EDGECASES{
\begin{bullets}
\item As $\lambda\to 0$, solution approaches unregularized least squares.
\item As $\lambda\to \infty$, $H\to 0$.
\end{bullets}
}
\ALTERNATE{
Rowwise ridge regressions on $Z$ yield the same closed form.}
\VALIDATION{
\begin{bullets}
\item Compare with solving normal equations directly by scalar arithmetic.
\end{bullets}
}
\INTUITION{
Ridge stabilizes inversion of Hadamard Gram, shrinking $H$ towards zero.}
\CANONICAL{
\begin{bullets}
\item Regularized ALS step uses $(G+\lambda I_r)^{-1}$ with $G$ Hadamard-structured.
\end{bullets}
}
\section{Coding Demonstrations}
\CodeDemoPage{Verify Gram--Hadamard Identity}
\PROBLEM{
Numerically verify $(A\odot B)^\top(A\odot B)=(A^\top A)\circ(B^\top B)$ and
self-validate with assertions.}
\API{
\begin{bullets}
\item \inlinecode{def read_input(s) -> tuple} — parse sizes and seed.
\item \inlinecode{def solve_case(m,n,r,seed) -> bool} — verify identity.
\item \inlinecode{def validate() -> None} — run deterministic tests.
\item \inlinecode{def main() -> None} — orchestrate verification.
\end{bullets}
}
\INPUTS{
$m,n,r\in\mathbb{N}$, integer seed.}
\OUTPUTS{
Boolean indicating equality within tolerance; raises on failure.}
\FORMULA{
\[
Z=A\odot B,\quad Z^\top Z=(A^\top A)\circ(B^\top B).
\]
}
\textbf{SOLUTION A — From Scratch (Mathematically Explicit Implementation)}
\begin{codepy}
import numpy as np

def khatri_rao(A, B):
    m, r = A.shape
    n, r2 = B.shape
    assert r == r2
    Z = np.empty((m*n, r), dtype=float)
    for k in range(r):
        Z[:, k] = np.kron(A[:, k], B[:, k])
    return Z

def solve_case(m, n, r, seed):
    np.random.seed(seed)
    A = np.random.randn(m, r)
    B = np.random.randn(n, r)
    Z = khatri_rao(A, B)
    G1 = Z.T @ Z
    G2 = (A.T @ A) * (B.T @ B)
    return np.allclose(G1, G2, atol=1e-10)

def read_input(s):
    m, n, r, seed = [int(x) for x in s.split()]
    return m, n, r, seed

def validate():
    assert solve_case(2, 3, 2, 0)
    assert solve_case(5, 4, 3, 1)

def main():
    validate()
    ok = solve_case(7, 6, 4, 2)
    print("verified", ok)

if __name__ == "__main__":
    main()
\end{codepy}
\textbf{SOLUTION B — Library-Based (Validated Computational Shortcut)}
\begin{codepy}
import numpy as np

def solve_case(m, n, r, seed):
    np.random.seed(seed)
    A = np.random.randn(m, r)
    B = np.random.randn(n, r)
    # Using broadcasting to build Khatri-Rao
    Z = (A.T[:, :, None] * B.T[:, None, :]).reshape(r, -1).T
    G1 = Z.T @ Z
    G2 = (A.T @ A) * (B.T @ B)
    return np.allclose(G1, G2, atol=1e-10)

def read_input(s):
    m, n, r, seed = [int(x) for x in s.split()]
    return m, n, r, seed

def validate():
    assert solve_case(2, 3, 2, 0)
    assert solve_case(5, 4, 3, 1)

def main():
    validate()
    ok = solve_case(7, 6, 4, 2)
    print("verified", ok)

if __name__ == "__main__":
    main()
\end{codepy}
\COMPLEXITY{
Time $\mathcal{O}(mnr)$ to form $Z$ and $\mathcal{O}(m r^2+n r^2)$ for Grams;
space $\mathcal{O}(mn r)$ if $Z$ is explicit; broadcasting avoids extra copies.}
\FAILMODES{
\begin{bullets}
\item Shape mismatch if columns of $A,B$ differ.
\item Numerical issues if Grams ill-conditioned; use tolerance in checks.
\end{bullets}
}
\STABILITY{
\begin{bullets}
\item Summation errors in Gram; use higher precision if needed.
\end{bullets}
}
\VALIDATION{
\begin{bullets}
\item Multiple seeds; cross-check both implementations.
\end{bullets}
}
\RESULT{
Both implementations agree within $10^{-10}$ across tests.}
\EXPLANATION{
Construct $Z=A\odot B$ and verify Gram equals Hadamard of individual Grams,
matching Formula 2.}
\EXTENSION{
Vectorize multiway Khatri-Rao and verify multi-Hadamard identity.}
\CodeDemoPage{Vec-Trace Identity and Quadratic Form}
\PROBLEM{
Verify $\mathrm{tr}(X^\top A X B)=\mathrm{vec}(X)^\top(B^\top\otimes A)\mathrm{vec}(X)$
and gradient equivalence.}
\API{
\begin{bullets}
\item \inlinecode{def rand_pd(n, seed) -> M} — random SPD matrix.
\item \inlinecode{def verify(n, m, seed) -> None} — check identities.
\item \inlinecode{def main() -> None} — run tests.
\end{bullets}
}
\INPUTS{
Sizes $n,m$ and seed.}
\OUTPUTS{
Asserts on equality of scalar forms and gradients.}
\FORMULA{
\[
\mathrm{tr}(X^\top A X B)=\mathrm{vec}(X)^\top(B^\top\otimes A)\mathrm{vec}(X).
\]
}
\textbf{SOLUTION A — From Scratch (Mathematically Explicit Implementation)}
\begin{codepy}
import numpy as np

def rand_pd(n, seed):
    rng = np.random.RandomState(seed)
    M = rng.randn(n, n)
    return (M.T @ M) + np.eye(n)

def verify(n, m, seed):
    np.random.seed(seed)
    A = rand_pd(n, seed + 1)
    B = rand_pd(m, seed + 2)
    X = np.random.randn(n, m)
    left = np.trace(X.T @ A @ X @ B)
    K = np.kron(B.T, A)
    right = np.dot(X.ravel(order="F"), K @ X.ravel(order="F"))
    assert abs(left - right) < 1e-9
    # Gradient check
    G = 2 * (A @ X @ B)
    eps = 1e-6
    U = np.random.randn(n, m)
    fd = (np.trace((X + eps * U).T @ A @ (X + eps * U) @ B) - left) / eps
    gdir = np.sum(G * U)
    assert abs(fd - gdir) < 1e-5

def main():
    verify(3, 4, 0)
    verify(5, 2, 1)
    print("ok")

if __name__ == "__main__":
    main()
\end{codepy}
\textbf{SOLUTION B — Library-Based (Validated Computational Shortcut)}
\begin{codepy}
import numpy as np

def verify(n, m, seed):
    np.random.seed(seed)
    A = np.diag(1 + np.arange(n, dtype=float))
    B = np.diag(1 + np.arange(m, dtype=float))
    X = np.random.randn(n, m)
    left = np.trace(X.T @ A @ X @ B)
    K = np.kron(B.T, A)
    right = float(X.ravel(order="F").T @ (K @ X.ravel(order="F")))
    assert abs(left - right) < 1e-9
    G = 2 * (A @ X @ B)
    eps = 1e-6
    U = np.random.randn(n, m)
    fd = (np.trace((X + eps * U).T @ A @ (X + eps * U) @ B) - left) / eps
    gdir = np.sum(G * U)
    assert abs(fd - gdir) < 1e-5

def main():
    verify(3, 3, 0)
    verify(4, 2, 1)
    print("ok")

if __name__ == "__main__":
    main()
\end{codepy}
\COMPLEXITY{
Time $\mathcal{O}(n^2 m + n m^2)$ for traces; Kronecker multiply is
$\mathcal{O}((nm)^2)$ naively but here applied as matrix products; space
$\mathcal{O}(nm)$.}
\FAILMODES{
\begin{bullets}
\item Using row-major vectorization breaks identity; use Fortran order.
\item Ill-conditioning can degrade finite-difference checks.
\end{bullets}
}
\STABILITY{
\begin{bullets}
\item Use small steps in finite difference; SPD choices stabilize traces.
\end{bullets}
}
\VALIDATION{
\begin{bullets}
\item Dual verification with diagonal SPD and random SPD.
\end{bullets}
}
\RESULT{
Scalar equality and gradient directional checks pass deterministically.}
\EXPLANATION{
Implements Formula 3 and derivative from Formula 4, confirming equivalence.}
\EXTENSION{
Avoid explicit Kronecker by using reshape-multiply-reshape kernels.}
\CodeDemoPage{ALS Update with Khatri-Rao and Ridge}
\PROBLEM{
Compute $H=Y_{(1)}(C\odot B)\big((B^\top B)\circ(C^\top C)+\lambda I\big)^{-1}$
and verify residual decreases.}
\API{
\begin{bullets}
\item \inlinecode{def khatri_rao(A,B)->Z} — build $C\odot B$.
\item \inlinecode{def update(Y,B,C,lmbd)->H} — compute ridge ALS update.
\item \inlinecode{def validate()->None} — fixed test with assertion.
\end{bullets}
}
\INPUTS{
$Y_{(1)}\in\mathbb{R}^{m\times (np)}$, $B\in\mathbb{R}^{n\times r}$,
$C\in\mathbb{R}^{p\times r}$, $\lambda\ge 0$.}
\OUTPUTS{
$H\in\mathbb{R}^{m\times r}$ and residual norms.}
\FORMULA{
\[
H=Y_{(1)}(C\odot B)\big((B^\top B)\circ(C^\top C)+\lambda I\big)^{-1}.
\]
}
\textbf{SOLUTION A — From Scratch (Mathematically Explicit Implementation)}
\begin{codepy}
import numpy as np

def khatri_rao(A, B):
    m, r = A.shape
    n, r2 = B.shape
    assert r == r2
    Z = np.empty((m * n, r), dtype=float)
    for k in range(r):
        Z[:, k] = np.kron(A[:, k], B[:, k])
    return Z

def update(Y, B, C, lmbd):
    Z = khatri_rao(C, B)
    G = (B.T @ B) * (C.T @ C)
    G = G + lmbd * np.eye(G.shape[0])
    H = Y @ Z @ np.linalg.inv(G)
    return H

def validate():
    np.random.seed(0)
    m, n, p, r = 5, 4, 3, 2
    Y = np.random.randn(m, n * p)
    B = np.random.randn(n, r)
    C = np.random.randn(p, r)
    H0 = np.zeros((m, r))
    Zt = khatri_rao(C, B).T
    res0 = np.linalg.norm(Y - H0 @ Zt, "fro")
    H = update(Y, B, C, 1e-3)
    res1 = np.linalg.norm(Y - H @ Zt, "fro")
    assert res1 <= res0 + 1e-10

def main():
    validate()
    print("als ridge ok")

if __name__ == "__main__":
    main()
\end{codepy}
\textbf{SOLUTION B — Library-Based (Validated Computational Shortcut)}
\begin{codepy}
import numpy as np

def update(Y, B, C, lmbd):
    Z = (C.T[:, :, None] * B.T[:, None, :]).reshape(C.shape[1], -1).T
    G = (B.T @ B) * (C.T @ C) + lmbd * np.eye(C.shape[1])
    H = Y @ Z @ np.linalg.inv(G)
    return H

def validate():
    np.random.seed(1)
    m, n, p, r = 6, 3, 4, 2
    Y = np.random.randn(m, n * p)
    B = np.random.randn(n, r)
    C = np.random.randn(p, r)
    Zt = ((C.T[:, :, None] * B.T[:, None, :]).reshape(r, -1))
    res0 = np.linalg.norm(Y, "fro")
    H = update(Y, B, C, 1e-2)
    res1 = np.linalg.norm(Y - H @ Zt, "fro")
    assert res1 <= res0 + 1e-10

def main():
    validate()
    print("als ridge ok")

if __name__ == "__main__":
    main()
\end{codepy}
\COMPLEXITY{
Time $\mathcal{O}(np r + r^3 + m np r)$; space $\mathcal{O}(np r)$.}
\FAILMODES{
\begin{bullets}
\item Singular $G$ if no ridge and collinearity; add $\lambda I$.
\item Shape mismatches in unfolding $Y_{(1)}$; check $n p$ columns.
\end{bullets}
}
\STABILITY{
\begin{bullets}
\item Invert $G$ via Cholesky for better conditioning.
\end{bullets}
}
\VALIDATION{
\begin{bullets}
\item Residual decreases from zero-initialization benchmark.
\end{bullets}
}
\RESULT{
ALS ridge update reduces residual deterministically on random inputs.}
\EXPLANATION{
Uses Formula 2 to form $G$ and Formula 4 to derive the update.}
\EXTENSION{
Use conjugate gradients on $G$ without explicit inversion.}
\section{Applied Domains — Detailed End-to-End Scenarios}
\DomainPage{Machine Learning}
\SCENARIO{
Build a feature-cross linear model using a Khatri-Rao design $Z=A\odot B$ and fit
weights by ridge regression using Hadamard Gram; evaluate MSE.}
\ASSUMPTIONS{
\begin{bullets}
\item Samples factor into two groups with shared $r$ latent features.
\item Ridge parameter $\lambda>0$ for stability.
\end{bullets}
}
\WHICHFORMULA{
$w=\big((A^\top A)\circ(B^\top B)+\lambda I\big)^{-1}(A\odot B)^\top y$.}
\varmapStart
\var{A\in\mathbb{R}^{m\times r}}{First feature block.}
\var{B\in\mathbb{R}^{n\times r}}{Second feature block.}
\var{y\in\mathbb{R}^{mn}}{Targets aligned with Kronecker index.}
\var{w\in\mathbb{R}^{r}}{Shared weights.}
\varmapEnd
\PIPELINE{
\begin{bullets}
\item Generate $A,B$ and ground-truth $w^\star$.
\item Form $Z=A\odot B$, synthesize $y=Z w^\star+\varepsilon$.
\item Fit ridge solution and report MSE.
\end{bullets}
}
\textbf{Implementation (From Scratch)}
\begin{codepy}
import numpy as np

def generate(m=30, n=20, r=5, noise=0.1, seed=0):
    np.random.seed(seed)
    A = np.random.randn(m, r)
    B = np.random.randn(n, r)
    w = np.random.randn(r)
    Z = (A.T[:, :, None] * B.T[:, None, :]).reshape(r, -1).T
    y = Z @ w + noise * np.random.randn(m * n)
    return A, B, w, y

def ridge_fit(A, B, y, lmbd=1e-2):
    G = (A.T @ A) * (B.T @ B) + lmbd * np.eye(A.shape[1])
    ZTy = ((A.T @ y.reshape(A.shape[0], -1) @ B)).diagonal()
    w = np.linalg.solve(G, ZTy)
    return w

def main():
    A, B, w_true, y = generate()
    w_hat = ridge_fit(A, B, y)
    Z = (A.T[:, :, None] * B.T[:, None, :]).reshape(A.shape[1], -1).T
    mse = np.mean((Z @ w_hat - y) ** 2)
    print("mse", round(mse, 6), "err", round(np.linalg.norm(w_hat-w_true), 4))

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{MSE on training data; weight error $\|w-\hat w\|_2$.}
\INTERPRET{Hadamard Gram shrinks per-component cross features coherently.}
\NEXTSTEPS{Use cross-validation for $\lambda$ and add nonnegativity constraints.}
\DomainPage{Quantitative Finance}
\SCENARIO{
Model factor exposures as separable across asset classes and regions. Build
design $Z=A\odot B$, fit ridge weights, and attribute variance via Hadamard Gram.}
\ASSUMPTIONS{
\begin{bullets}
\item Linear factor model with separable exposures.
\item Residuals homoscedastic for simplicity.
\end{bullets}
}
\WHICHFORMULA{
$w=(G+\lambda I)^{-1}Z^\top r$, $G=(A^\top A)\circ(B^\top B)$.}
\varmapStart
\var{A}{Asset-class exposures.}
\var{B}{Region exposures.}
\var{r}{Excess returns vector.}
\var{w}{Factor weights.}
\varmapEnd
\PIPELINE{
\begin{bullets}
\item Simulate $A,B,w^\star$ and returns $r$.
\item Fit ridge $w$ using Hadamard Gram.
\item Compute variance explained $(Z w)^\top(Z w)$. 
\end{bullets}
}
\textbf{Implementation (Full Pipeline)}
\begin{codepy}
import numpy as np

def simulate(m=10, n=8, r=3, seed=0):
    np.random.seed(seed)
    A = np.random.randn(m, r)
    B = np.random.randn(n, r)
    w = np.random.randn(r)
    Z = (A.T[:, :, None] * B.T[:, None, :]).reshape(r, -1).T
    eps = 0.05 * np.random.randn(m * n)
    ret = Z @ w + eps
    return A, B, ret, w

def fit(A, B, ret, lmbd=1e-2):
    G = (A.T @ A) * (B.T @ B) + lmbd * np.eye(A.shape[1])
    ZTret = ((A.T @ ret.reshape(A.shape[0], -1) @ B)).diagonal()
    w = np.linalg.solve(G, ZTret)
    return w

def main():
    A, B, ret, w_true = simulate()
    w_hat = fit(A, B, ret)
    Z = (A.T[:, :, None] * B.T[:, None, :]).reshape(A.shape[1], -1).T
    var = float((Z @ w_hat).T @ (Z @ w_hat))
    print("var_expl", round(var, 6), "w_norm", round(np.linalg.norm(w_hat), 3))

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{Explained variance and weight norm.}
\INTERPRET{Hadamard Gram aggregates per-factor cross-exposure strengths.}
\NEXTSTEPS{Introduce covariance of residuals and generalized ridge.}
\DomainPage{Deep Learning}
\SCENARIO{
Implement a linear layer that computes outputs via a Khatri-Rao design and
train weights by closed-form ridge; compare to gradient descent.}
\ASSUMPTIONS{
\begin{bullets}
\item Mean squared loss; deterministic seeds.
\end{bullets}
}
\WHICHFORMULA{
Closed-form ridge on $Z=A\odot B$ with Gram $(A^\top A)\circ(B^\top B)$.}
\PIPELINE{
\begin{bullets}
\item Generate $A,B,w^\star$ and labels $y$.
\item Solve ridge closed form and compare to GD iterations.
\end{bullets}
}
\textbf{Implementation (End-to-End)}
\begin{codepy}
import numpy as np

def data(seed=0):
    np.random.seed(seed)
    m, n, r = 20, 15, 4
    A = np.random.randn(m, r)
    B = np.random.randn(n, r)
    w = np.random.randn(r)
    Z = (A.T[:, :, None] * B.T[:, None, :]).reshape(r, -1).T
    y = Z @ w + 0.1 * np.random.randn(m * n)
    return A, B, y, w

def ridge(A, B, y, lmbd=1e-2):
    G = (A.T @ A) * (B.T @ B) + lmbd * np.eye(A.shape[1])
    ZTy = ((A.T @ y.reshape(A.shape[0], -1) @ B)).diagonal()
    return np.linalg.solve(G, ZTy)

def gd(A, B, y, steps=2000, lr=1e-2):
    np.random.seed(0)
    r = A.shape[1]
    w = np.zeros(r)
    ZTy = ((A.T @ y.reshape(A.shape[0], -1) @ B)).diagonal()
    G = (A.T @ A) * (B.T @ B)
    for _ in range(steps):
        grad = 2 * (G @ w - ZTy)
        w -= lr * grad
    return w

def main():
    A, B, y, w_true = data()
    w_r = ridge(A, B, y, 1e-2)
    w_g = gd(A, B, y, 5000, 5e-3)
    print("ridge_err", round(np.linalg.norm(w_r-w_true), 3))
    print("gd_err", round(np.linalg.norm(w_g-w_true), 3))

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{Parameter error $\|w-\hat w\|_2$.}
\INTERPRET{GD converges to ridge-like solution as steps increase and lr small.}
\NEXTSTEPS{Backprop through $A,B$ to learn separable embeddings.}
\DomainPage{Kaggle / Data Analytics}
\SCENARIO{
Create cross features via Khatri-Rao for two feature blocks and fit ridge on
the cross design; report correlation between predicted and true targets.}
\ASSUMPTIONS{
\begin{bullets}
\item Standardized features; deterministic simulation.
\end{bullets}
}
\WHICHFORMULA{
$w=(G+\lambda I)^{-1}Z^\top y$, $G=(A^\top A)\circ(B^\top B)$.}
\PIPELINE{
\begin{bullets}
\item Simulate two blocks $A,B$ and target $y$.
\item Build $Z=A\odot B$ and fit ridge.
\item Compute $R$ correlation between $Z w$ and $y$.
\end{bullets}
}
\textbf{Implementation (Complete EDA Pipeline)}
\begin{codepy}
import numpy as np

def simulate(m=40, n=30, r=5, seed=0):
    np.random.seed(seed)
    A = np.random.randn(m, r)
    B = np.random.randn(n, r)
    w = np.random.randn(r)
    Z = (A.T[:, :, None] * B.T[:, None, :]).reshape(r, -1).T
    y = Z @ w + 0.2 * np.random.randn(m * n)
    return A, B, y

def ridge(A, B, y, lmbd=1e-2):
    G = (A.T @ A) * (B.T @ B) + lmbd * np.eye(A.shape[1])
    ZTy = ((A.T @ y.reshape(A.shape[0], -1) @ B)).diagonal()
    return np.linalg.solve(G, ZTy)

def corr(x, y):
    x = x - x.mean()
    y = y - y.mean()
    return float((x @ y) / (np.linalg.norm(x) * np.linalg.norm(y)))

def main():
    A, B, y = simulate()
    w = ridge(A, B, y)
    Z = (A.T[:, :, None] * B.T[:, None, :]).reshape(A.shape[1], -1).T
    yhat = Z @ w
    print("corr", round(corr(yhat, y), 3))

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{Pearson correlation between predictions and targets.}
\INTERPRET{High correlation indicates cross features capture signal well.}
\NEXTSTEPS{Add nonlinearity or interactions beyond shared columns.}
\end{document}
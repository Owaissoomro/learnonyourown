% !TeX program = xelatex
\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{microtype,setspace,amsmath,amssymb,mathtools,amsthm,unicode-math}
\setstretch{1.05}
\setmainfont{Latin Modern Roman}
\setmonofont{Latin Modern Mono}
\setmathfont{Latin Modern Math}

% --- Overflow / line-break safety ---
\allowdisplaybreaks[4]
\setlength{\jot}{7pt}
\setlength{\emergencystretch}{8em}
\sloppy

\usepackage{xcolor,fancyhdr,enumitem,inconsolata,listings}
\pagestyle{fancy}\fancyhf{}\lhead{\nouppercase{\leftmark}}\rhead{\thepage}
\setlength{\headheight}{26pt}
\setlength{\parindent}{0pt}
\setlength{\parskip}{8pt plus 2pt minus 1pt}
\raggedbottom

% --- Breakable math helpers (use these in the body) ---
\newenvironment{BreakableEquation}{\begin{equation}\begin{aligned}}{\end{aligned}\end{equation}}
\newenvironment{BreakableEquation*}{\begin{equation*}\begin{aligned}}{\end{aligned}\end{equation*}}
\newenvironment{tightalign}{\begingroup\small\allowdisplaybreaks\begin{align}}{\end{align}\endgroup}

% ---------- Safety shims ----------
% Robust minted → listings shim (no shell-escape; supports [opts]{language})
\providecommand{\enumlistm}{enumitem}
\newenvironment{minted}[2][]{%
  \lstset{style=code,language=#2,#1}\begin{lstlisting}%
}{\end{lstlisting}}

% Fallback for \inputminted (ignore file; keep build unbroken)
\newcommand{\inputminted}[3][]{\begin{lstlisting}\end{lstlisting}}

% ---------- Bulleted lines (no tables) ----------
\newlist{bullets}{itemize}{1}
\setlist[bullets]{label=--,leftmargin=1.6em,itemsep=2pt,topsep=4pt}

% ---------- Variable mapping (lines, no tables) ----------
\newcommand{\varmapStart}{\textbf{VARIABLE MAPPING:}\par\begin{bullets}}
\newcommand{\var}[2]{\item $#1$ — #2}
\newcommand{\varmapEnd}{\end{bullets}}

% ---------- Glossary item with ELI5 and Pitfall/Example ----------
\newcommand{\glossx}[6]{%
  \textbf{#1}\par
  \begin{bullets}
    \item \textbf{What:} #2
    \item \textbf{Why:} #3
    \item \textbf{How:} #4
    \item \textbf{ELI5:} #5
    \item \textbf{Pitfall/Example:} #6
  \end{bullets}
}

% ---------- Theorem structures ----------
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{claim}[theorem]{Claim}
% ---------- Code blocks ----------
\lstdefinestyle{code}{
  basicstyle=\ttfamily\footnotesize,
  backgroundcolor=\color{black!02},
  frame=single,
  numbers=left, numberstyle=\tiny, numbersep=8pt,
  breaklines=true, breakatwhitespace=true,
  tabsize=4, showstringspaces=false,
  upquote=true, keepspaces=true, columns=fullflexible,
  literate=
    {–}{{-}}1
    {—}{{-}}1
    {…}{{...}}1
    {≤}{{\ensuremath{\le}}}1
    {≥}{{\ensuremath{\ge}}}1
    {≠}{{\ensuremath{\ne}}}1
    {≈}{{\ensuremath{\approx}}}1
    {±}{{\ensuremath{\pm}}}1
    {→}{{\ensuremath{\to}}}1
    {←}{{\ensuremath{\leftarrow}}}1
    {∞}{{\ensuremath{\infty}}}1
    {√}{{\ensuremath{\sqrt{\ }}}}1
    {×}{{\ensuremath{\times}}}1
    {÷}{{\ensuremath{\div}}}1
}

% Main code environment for all Python blocks
\lstnewenvironment{codepy}[1][]%
  {\lstset{style=code,language=Python,#1}}%
  {}

% Inline code; change delimiters if your snippet contains '!'
\newcommand{\inlinecode}[1]{\lstinline[style=code]!#1!}

% ---------- Line-label macros ----------
\newcommand{\LF}[2]{\par\noindent\textbf{#1:}~#2\par}
\newcommand{\WHAT}[1]{\LF{WHAT}{#1}}
\newcommand{\WHY}[1]{\LF{WHY}{#1}}
\newcommand{\HOW}[1]{\LF{HOW}{#1}}
\newcommand{\ELI}[1]{\LF{ELI5}{#1}}
\newcommand{\SCOPE}[1]{\LF{SCOPE}{#1}}
\newcommand{\CONFUSIONS}[1]{\LF{COMMON CONFUSIONS}{#1}}
\newcommand{\APPLICATIONS}[1]{\LF{APPLICATIONS}{#1}}
\newcommand{\FORMULA}[1]{\LF{FORMULA}{#1}}
\newcommand{\CANONICAL}[1]{\LF{CANONICAL FORM}{#1}}
\newcommand{\PRECONDS}[1]{\LF{PRECONDITIONS}{#1}}
\newcommand{\DERIVATION}[1]{\LF{DERIVATION}{#1}}
\newcommand{\EQUIV}[1]{\LF{EQUIVALENT FORMS}{#1}}
\newcommand{\LIMITS}[1]{\LF{LIMIT CASES}{#1}}
\newcommand{\INPUTS}[1]{\LF{INPUTS}{#1}}
\newcommand{\OUTPUTS}[1]{\LF{OUTPUTS}{#1}}
\newcommand{\RESULT}[1]{\LF{RESULT}{#1}}
\newcommand{\INTUITION}[1]{\LF{INTUITION}{#1}}
\newcommand{\PITFALLS}[1]{\LF{PITFALLS}{#1}}
\newcommand{\MODEL}[1]{\LF{CANONICAL MATH MODEL}{#1}}
\newcommand{\ASSUMPTIONS}[1]{\LF{ASSUMPTIONS}{#1}}
\newcommand{\WHICHFORMULA}[1]{\LF{WHICH FORMULA \& WHY}{#1}}
\newcommand{\GOVERN}[1]{\LF{GOVERNING EQUATION(S)}{#1}}
\newcommand{\UNITCHECK}[1]{\LF{UNIT CHECK}{#1}}
\newcommand{\EDGECASES}[1]{\LF{EDGE CASES}{#1}}
\newcommand{\ALTERNATE}[1]{\LF{ALTERNATE APPROACH (sketch)}{#1}}
\newcommand{\PROBLEM}[1]{\LF{PROBLEM}{#1}}
\newcommand{\API}[1]{\LF{API}{#1}}
\newcommand{\COMPLEXITY}[1]{\LF{COMPLEXITY}{#1}}
\newcommand{\FAILMODES}[1]{\LF{FAILURE MODES}{#1}}
\newcommand{\STABILITY}[1]{\LF{NUMERICAL STABILITY}{#1}}
\newcommand{\VALIDATION}[1]{\LF{VALIDATION}{#1}}
\newcommand{\EXPLANATION}[1]{\LF{EXPLANATION}{#1}}
\newcommand{\SCENARIO}[1]{\LF{SCENARIO}{#1}}
\newcommand{\PIPELINE}[1]{\LF{PIPELINE STEPS}{#1}}
\newcommand{\METRICS}[1]{\LF{METRICS}{#1}}
\newcommand{\INTERPRET}[1]{\LF{INTERPRETATION}{#1}}
\newcommand{\NEXTSTEPS}[1]{\LF{LIMITATIONS \& NEXT STEPS}{#1}}

% ---------- Section formatting ----------
\usepackage{titlesec}
\titleformat{\section}{\Large\bfseries}{\thesection}{0.6em}{}
\titlespacing*{\section}{0pt}{*2}{*1}
\usepackage{etoolbox}
\pretocmd{\section}{\clearpage}{}{}

% ---------- Page helpers ----------
\newcommand{\FormulaPage}[2]{%
  \clearpage
  \section*{Formula #1 — #2}%
  \addcontentsline{toc}{section}{Formula #1 — #2}%
}
\newcommand{\ProblemPage}[2]{%
  \clearpage
  \subsection*{Problem #1: #2}%
  \addcontentsline{toc}{subsection}{Problem #1: #2}%
}
\newcommand{\CodeDemoPage}[1]{%
  \clearpage
  \subsection*{Coding Demo: #1}%
  \addcontentsline{toc}{subsection}{Coding Demo: #1}%
}
\newcommand{\DomainPage}[1]{%
  \clearpage
  \subsection*{#1 (End-to-End)}%
  \addcontentsline{toc}{subsection}{#1 (End-to-End)}%
}

\begin{document}
\title{Comprehensive Study Sheet — Elementary Matrices and Gaussian Elimination}
\date{\today}
\maketitle
\tableofcontents
\clearpage

\section{Concept Overview}
\WHAT{
An $m\times n$ matrix $A$ over a field $\mathbb{F}$ is transformed by
elementary row operations: (R1) swap two rows, (R2) scale a row by
$c\in\mathbb{F}\setminus\{0\}$, (R3) add a multiple of one row to another.
Each such operation equals left multiplication by an $m\times m$
elementary matrix $E$ so that $EA$ is the transformed matrix.
Gaussian elimination applies a finite sequence of such $E$ to reduce $A$
to an upper triangular or row echelon form; Gauss--Jordan proceeds to
reduced row echelon form. For square $A\in\mathbb{F}^{n\times n}$,
elimination yields a factorization $PA=LU$ with a permutation $P$,
unit lower triangular $L$, and upper triangular $U$.
}
\WHY{
Elementary matrices encode algorithmic row operations as algebraic
matrix multiplication, enabling rigorous proofs and efficient computation
for solving linear systems $Ax=b$, computing determinants, inverses,
ranks, and canonical forms. Gaussian elimination is the canonical, finite,
and complete algorithm that decides solvability and produces solutions.
}
\HOW{
1. Formalize each row operation as a specific $E$ built from the identity. 
2. Prove $EA$ applies the desired operation to rows of $A$ and that $E$ is
invertible with inverse also elementary. 
3. Show a product of elimination matrices $E_k\cdots E_1A=U$ is upper
triangular, whence $A=(E_k\cdots E_1)^{-1}U=L^{-1}U$, giving $PA=LU$
if swaps occur. 
4. Solve $Ax=b$ by $PAx=Pb$, then $Ly=Pb$, $Ux=y$ via forward/back
substitution. Interpret each step as a stable linear map.
}
\ELI{
Think of rows as recipes. An elementary matrix is a button that performs
exactly one recipe change: swap recipes, scale a recipe, or add a
multiple of one recipe to another. Pressing buttons in sequence organizes
the kitchen to a neat triangular shelf, after which grabbing ingredients
(the solution) is easy and systematic.
}
\SCOPE{
Valid over any field $\mathbb{F}$; real or complex fields are standard.
Pivoting requires a nonzero pivot; partial pivoting uses permutations to
ensure this. Without pivoting, LU may fail if a pivot is zero. Over rings
without division, scaling may be invalid. Numerical stability depends on
conditioning and pivot strategy.
}
\CONFUSIONS{
Row vs. column operations: left multiplication affects rows, right affects
columns. Elementary matrices vs. general invertible matrices: every
elementary is invertible, but not every invertible is a single elementary
matrix; however, every invertible is a product of elementaries. LU vs.
PA=LU: LU may require pivoting; $P$ accounts for row swaps.
}
\APPLICATIONS{
\begin{bullets}
\item Mathematical foundations: rank, invertibility, determinant, RREF.
\item Computational modeling: solving large sparse linear systems.
\item Physical and engineering: circuit analysis, structural statics.
\item Statistics and algorithms: least squares, regression, factorization.
\end{bullets}
}
\textbf{ANALYTIC STRUCTURE.}
Elementary matrices form a generating set for $\mathrm{GL}_n(\mathbb{F})$.
The maps are linear, invertible, and preserve or scale volume by a known
factor. Gaussian elimination is a finite sequence of linear isomorphisms
leading to triangular structure (a convex cone of matrices).
\textbf{CANONICAL LINKS.}
Determinant multiplicativity $\det(EA)=\det(E)\det(A)$, invertibility iff
nonzero determinant, $PA=LU$ factorization, and uniqueness of RREF.
\textbf{PROBLEM-TYPE RECOGNITION HEURISTICS.}
\begin{bullets}
\item Requests to perform row operations or compute $EA$.
\item Solve $Ax=b$ or compute $\det(A)$ efficiently.
\item Build $A^{-1}$ from operations or find $P,L,U$.
\item Identify pivot structure or RREF.
\end{bullets}
\textbf{SOLUTION STRATEGY BLUEPRINT.}
\begin{bullets}
\item Translate operations into $E$ matrices.
\item Apply or compose $E$ to form $EA$ or $E_k\cdots E_1A$.
\item Read off $U$, $L$, $P$, or solve triangular systems.
\item Validate by back substitution and determinant checks.
\end{bullets}
\textbf{CONCEPTUAL INVARIANTS.}
Row equivalence class, rank, nullity, determinant sign up to swaps,
solution set structure.
\textbf{EDGE INTUITION.}
If a pivot tends to zero, elimination becomes ill conditioned; permutation
stabilizes by selecting a larger pivot. Large scaling multiplies
determinants and amplifies errors.

\section{Glossary}
\glossx{Elementary Matrix}
{An identity matrix modified by one elementary row operation.}
{Encodes row operations as left multiplication, enabling algebraic
reasoning and efficient computation.}
{Construct from $I$ by a single swap, scale, or row addition.}
{Like a single move in a game that changes two lines of a grid.}
{Pitfall: using right multiplication applies a column operation instead.}
\glossx{Gaussian Elimination}
{Algorithm reducing a matrix to upper triangular or echelon form via
elementary row operations, optionally with pivoting.}
{Solves linear systems, computes determinants and ranks.}
{Proceed column by column: choose pivot, eliminate below, repeat.}
{Like clearing blocks below a staircase to reveal steps.}
{Pitfall: ignoring zero pivots; remedy with row swaps (pivoting).}
\glossx{LU Factorization with Pivoting}
{Representation $PA=LU$ with permutation $P$, unit lower $L$, and upper $U$.}
{Separates permutation and elimination, enables efficient solves.}
{Accumulate multipliers in $L$, pivots in $U$, permutations in $P$.}
{Think of $P$ as shuffling rows before building the staircase.}
{Pitfall: forgetting $P$ when swaps occur leads to wrong $L,U$.}
\glossx{Reduced Row Echelon Form}
{Unique canonical form reached by Gauss--Jordan elimination.}
{Classifies row equivalence and solves systems.}
{Eliminate below and above pivots, scale pivots to one.}
{A fully cleaned staircase where each step starts with a $1$.}
{Pitfall: scaling pivot rows is required to reach reduced form.}

\section{Symbol Ledger}
\varmapStart
\var{\mathbb{F}}{Base field (typically $\mathbb{R}$ or $\mathbb{C}$).}
\var{A\in\mathbb{F}^{m\times n}}{Input matrix.}
\var{I_n}{Identity matrix of size $n$.}
\var{E}{Elementary matrix (left multiplies rows).}
\var{P}{Permutation matrix (product of swaps).}
\var{L}{Unit lower triangular matrix (ones on diagonal).}
\var{U}{Upper triangular matrix.}
\var{S_{ij}}{Row-swap elementary matrix exchanging rows $i$ and $j$.}
\var{D_i(c)}{Row-scaling elementary matrix: scale row $i$ by $c\neq 0$.}
\var{T_{ij}(c)}{Row-addition matrix: add $c$ times row $j$ to row $i$.}
\var{b}{Right-hand side vector in $Ax=b$.}
\var{x}{Solution vector to $Ax=b$.}
\var{y}{Intermediate vector for $Ly=Pb$.}
\var{n,m}{Matrix dimensions.}
\var{k}{Elimination step index.}
\var{\pi}{Permutation of $\{1,\dots,n\}$.}
\var{\det(\cdot)}{Determinant of a square matrix.}
\var{r}{Rank of a matrix.}
\varmapEnd

\section{Formula Canon — One Formula Per Page}
\FormulaPage{1}{Elementary Matrices Implement Row Operations}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
For each elementary row operation on $A\in\mathbb{F}^{m\times n}$ there
exists an $E\in\mathbb{F}^{m\times m}$ such that $EA$ is the result:
\WHAT{
Row swap $i\leftrightarrow j$ uses $E=S_{ij}$, row scaling of row $i$ by
$c\neq 0$ uses $E=D_i(c)$, and row addition $R_i\gets R_i+cR_j$ uses
$E=T_{ij}(c)$.
}
\WHY{
This identification turns procedural elimination into algebraic
multiplication, enabling composition, inversion, and determinant analysis.
}
\FORMULA{
\[
S_{ij}A=\text{swap rows }i,j;\quad D_i(c)A=\text{scale row }i\text{ by }c;
\quad T_{ij}(c)A=\text{row }i\gets\text{row }i+c\text{ row }j.
\]
}
\CANONICAL{
$S_{ij}=I_m$ with rows $i$ and $j$ exchanged. $D_i(c)=I_m$ with
$(i,i)=c$. $T_{ij}(c)=I_m$ with $(i,j)=c$ for $i\neq j$.
}
\PRECONDS{
\begin{bullets}
\item Field $\mathbb{F}$ supports addition and multiplication.
\item For $D_i(c)$ require $c\neq 0$.
\item Indices satisfy $1\le i\neq j\le m$.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
For any $E$ equal to $I_m$ except possibly at rows $i$ or $j$ as above,
the product $EA$ replaces row $i$ of $A$ by the corresponding linear
combination specified by $E$ while leaving other rows unchanged.
\end{lemma}
\begin{proof}
Write $EA$ row-wise: the $p$-th row of $EA$ equals the linear combination
$\sum_{q=1}^m e_{pq}(\text{row }q\text{ of }A)$. For $E=I_m$ except at
row $i$ with $(i,i)=c$ we get row $i$ becomes $c$ times its original, and
all other rows unchanged. For $E$ with $(i,j)=c$ and identity elsewhere,
row $i$ becomes row $i$ plus $c$ times row $j$. For $S_{ij}$, rows $i$
and $j$ swap because the $i$-th standard basis vector is placed at row
$j$ and vice versa. Therefore each operation is realized by $EA$.\qedhere
\end{proof}
\DERIVATION{
\begin{align*}
\text{Step 1:}~&E=I_m,\ \text{modify only entries needed.}\\
\text{Step 2:}~&EA=\begin{bmatrix}e_{11}&\cdots&e_{1m}\\
&\ddots&\\e_{m1}&\cdots&e_{mm}\end{bmatrix}A.\\
\text{Step 3:}~&\text{Row }p\text{ of }EA=\sum_q e_{pq}\,(\text{row }q\text{ of }A).\\
\text{Step 4:}~&\text{By chosen }e_{pq}\text{, only intended rows combine.}\\
\text{Step 5:}~&\Rightarrow EA\text{ equals the row-op result.}
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Identify the row operation type and indices $(i,j)$ and scalar $c$.
\item Construct $E$ by editing $I_m$.
\item Compute $EA$ or compose with others: $E_k\cdots E_1A$.
\item If needed, invert via $E^{-1}$ to undo the operation.
\end{bullets}
\EQUIV{
\begin{bullets}
\item Column operations correspond to right multiplication by column
elementary matrices $A\tilde{E}$.
\item A sequence of operations equals a product $E_k\cdots E_1$.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item Scaling requires $c\neq 0$ over a field.
\item Over rings without inverses, some operations are invalid.
\end{bullets}
}
\INPUTS{$A\in\mathbb{F}^{m\times n}$, indices $i,j$, scalar $c$.}
\DERIVATION{
\begin{align*}
\text{Example: }&A=\begin{bmatrix}1&2\\3&4\\5&6\end{bmatrix},\
E=T_{31}(-5).\\
EA&=\begin{bmatrix}1&0&0\\0&1&0\\-5&0&1\end{bmatrix}
\begin{bmatrix}1&2\\3&4\\5&6\end{bmatrix}
=\begin{bmatrix}1&2\\3&4\\0&-4\end{bmatrix}.
\end{align*}
}
\RESULT{
Left multiplication by $E$ performs the desired row operation exactly.}
\UNITCHECK{
All quantities are dimensionless linear algebraic objects; matrix sizes
are consistent: $E$ is $m\times m$, $A$ is $m\times n$, so $EA$ is
$m\times n$.
}
\PITFALLS{
\begin{bullets}
\item Right-multiplying by $E$ applies a column operation, not a row one.
\item Using $c=0$ for scaling destroys invertibility.
\end{bullets}
}
\INTUITION{
$E$ is the linear operator that edits rows the same way the operation does.
}
\CANONICAL{
\begin{bullets}
\item Elementary operations are linear automorphisms of $\mathbb{F}^m$.
\item Products of $E$ generate all row-equivalent matrices.
\end{bullets}
}

\FormulaPage{2}{Inverses and Determinants of Elementary Matrices}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Each elementary matrix is invertible and its inverse is elementary:
\WHAT{
For $S_{ij}$, $S_{ij}^{-1}=S_{ij}$; for $D_i(c)$, $D_i(c)^{-1}=D_i(c^{-1})$;
for $T_{ij}(c)$, $T_{ij}(c)^{-1}=T_{ij}(-c)$. Determinant factors are
$\det(S_{ij})=-1$, $\det(D_i(c))=c$, $\det(T_{ij}(c))=1$.
}
\WHY{
Invertibility ensures operations are reversible. Determinant factors allow
fast determinant updates and sign tracking during elimination.
}
\FORMULA{
\[
E^{-1}=\begin{cases}
S_{ij},&E=S_{ij},\\
D_i(c^{-1}),&E=D_i(c),\\
T_{ij}(-c),&E=T_{ij}(c),
\end{cases}\quad
\det(E)=\begin{cases}
-1,&E=S_{ij},\\
c,&E=D_i(c),\\
1,&E=T_{ij}(c).
\end{cases}
\]
}
\CANONICAL{
Use $\det(AB)=\det(A)\det(B)$ and that elementary $E$ differs from $I$
by a simple rank one or permutation change with known determinant.
}
\PRECONDS{
\begin{bullets}
\item $c\neq 0$ for $D_i(c)$ to ensure invertibility.
\item Standard determinant axioms: multilinearity, alternation, and
normalization $\det(I)=1$.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
$\det(S_{ij})=-1$, $\det(D_i(c))=c$, and $\det(T_{ij}(c))=1$.
\end{lemma}
\begin{proof}
By alternation, swapping two rows flips sign: $\det(S_{ij})=-1$ because
$S_{ij}$ is $I$ with two rows exchanged. Scaling a single row by $c$
scales the determinant by $c$, hence $\det(D_i(c))=c$. Adding a multiple
of one row to another does not change the determinant by multilinearity,
so $\det(T_{ij}(c))=1$.\qedhere
\end{proof}
\DERIVATION{
\begin{align*}
\text{Inverse: }&S_{ij}S_{ij}=I,\ \Rightarrow S_{ij}^{-1}=S_{ij}.\\
&D_i(c)D_i(c^{-1})=I\Rightarrow D_i(c)^{-1}=D_i(c^{-1}).\\
&T_{ij}(c)T_{ij}(-c)=I\Rightarrow T_{ij}(c)^{-1}=T_{ij}(-c).\\
\text{Determinant: }&\det(EA)=\det(E)\det(A),\ \text{use lemma above}.
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Track determinant updates through elimination steps.
\item Invert $E$ by negating $c$ for $T_{ij}$, reciprocating $c$ for $D_i$,
and reapplying the swap for $S_{ij}$.
\end{bullets}
}
\EQUIV{
\begin{bullets}
\item $\det(P)=\operatorname{sgn}(\pi)$ for permutation $P$.
\item Determinant of product of elementaries equals product of their factors.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item Over rings without multiplicative inverses, $D_i(c)$ may be singular.
\item Determinant is defined only for square matrices.
\end{bullets}
}
\INPUTS{$E$ elementary; optionally $A\in\mathbb{F}^{n\times n}$.}
\DERIVATION{
\begin{align*}
\text{Example: }&E=T_{21}(3),\ E^{-1}=T_{21}(-3).\\
&\det(E)=1,\ \det(E^{-1})=1,\ \det(EE^{-1})=1.
\end{align*}
}
\RESULT{
Elementary matrices are invertible with elementary inverses; their
determinant factors are known and multiplicative under composition.}
\UNITCHECK{
All objects are dimensionless; square requirement for determinants is met.}
\PITFALLS{
\begin{bullets}
\item Forgetting that row additions preserve determinant.
\item Using $c=0$ makes $D_i(c)$ singular with zero determinant.
\end{bullets}
}
\INTUITION{
Elementary operations are reversible edits; determinants track volume
scaling and orientation flips of these edits.
}
\CANONICAL{
\begin{bullets}
\item $\mathrm{GL}_n(\mathbb{F})$ is generated by elementaries.
\item Determinant homomorphism respects these generators.
\end{bullets}
}

\FormulaPage{3}{Gaussian Elimination and $PA=LU$ Factorization}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
For $A\in\mathbb{F}^{n\times n}$, Gaussian elimination with partial
pivoting yields a permutation $P$, unit lower $L$, and upper $U$ such that
\WHAT{
$PA=LU$, where $L$ collects multipliers, $U$ is the result of eliminating
below pivots, and $P$ records row swaps used to secure nonzero pivots.
}
\WHY{
$PA=LU$ separates permutation, elimination, and triangular structure,
enabling efficient repeated solves and determinant computation.
}
\FORMULA{
\[
E_k\cdots E_1PA=U,\quad L=(E_k\cdots E_1)^{-1},\quad PA=LU.
\]
}
\CANONICAL{
$E_t$ are of type $T_{ij}(-\ell)$ that zero out entries below pivots.
$P$ is the product of all swaps applied. $L$ is unit lower triangular.
}
\PRECONDS{
\begin{bullets}
\item Pivot selection ensures nonzero pivots; partial pivoting takes the
largest magnitude in each column below the diagonal.
\item $A$ nonsingular for existence of a full $LU$ with $P$.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
If $E$ is a product of row addition matrices that zero entries below the
diagonal, then $EA$ is upper triangular if all pivots are nonzero.
\end{lemma}
\begin{proof}
Proceed by induction on the column index. Base case: column one. Choose a
nonzero pivot in rows $1\ldots n$ and swap it to row $1$ via $P_1$. Then
for rows $i>1$, $T_{i1}(-a_{i1}/a_{11})$ zeros entries below the pivot.
Inductive step: restrict to the trailing submatrix and repeat. Nonzero
pivots guarantee each scaling $-a_{ij}/a_{jj}$ is defined and zeroing
succeeds, producing an upper triangular $U$.\qedhere
\end{proof}
\DERIVATION{
\begin{align*}
\text{Step 1: }&\text{Apply }P_1\text{ to place pivot }u_{11}\neq 0.\\
\text{Step 2: }&\text{For }i=2..n,\ E_{i1}=T_{i1}(-a_{i1}/u_{11}).\\
\text{Step 3: }&\text{Repeat on submatrix }2..n,\ \text{accumulating }P.\\
\text{Step 4: }&U=E_k\cdots E_1PA,\ L=(E_k\cdots E_1)^{-1}.\\
\text{Step 5: }&\Rightarrow PA=LU\ \text{with unit diagonal in }L.
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Record swaps into $P$; apply elimination to form $U$.
\item Store multipliers $\ell_{ij}$ in $L$ with ones on the diagonal.
\item Verify by checking $PA$ equals $LU$.
\end{bullets}
}
\EQUIV{
\begin{bullets}
\item Without swaps and with nonzero leading principal minors, $A=LU$.
\item $A=L^{-1}U$ with $L^{-1}$ a product of $T_{ij}$ in reverse order.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item If a pivot is zero and no swap available, factorization fails.
\item Pivoting changes $P$ but preserves solvability and rank.
\end{bullets}
}
\INPUTS{$A\in\mathbb{F}^{n\times n}$.}
\DERIVATION{
\begin{align*}
\text{Example: }&A=\begin{bmatrix}2&1&1\\4&-6&0\\-2&7&2\end{bmatrix}.\\
\text{Step 1: }&\text{Pivot at }(1,1)=4\ \text{via swap rows }1,2:\ P_1.\\
&P_1A=\begin{bmatrix}4&-6&0\\2&1&1\\-2&7&2\end{bmatrix}.\\
\text{Elim: }&\ell_{21}=2/4=1/2,\ \ell_{31}=-2/4=-1/2.\\
&E_{21}=T_{21}(-1/2),\ E_{31}=T_{31}(1/2).\\
&E_{31}E_{21}P_1A=\begin{bmatrix}4&-6&0\\0&4&1\\0&4&2\end{bmatrix}.\\
\text{Step 2: }&\text{Pivot at }(2,2)=4,\ \ell_{32}=4/4=1.\\
&E_{32}=T_{32}(-1),\ U=\begin{bmatrix}4&-6&0\\0&4&1\\0&0&1\end{bmatrix}.\\
&L=\begin{bmatrix}1&0&0\\1/2&1&0\\-1/2&1&1\end{bmatrix},\
P=P_1.\\
&PA=LU\ \text{verifies}.
\end{align*}
}
\RESULT{
A concrete $P,L,U$ with $PA=LU$, enabling fast solves and determinants.}
\UNITCHECK{
All matrices are $3\times 3$; triangular forms and unit diagonal in $L$
are consistent by construction.
}
\PITFALLS{
\begin{bullets}
\item Storing multipliers in $U$ by mistake; $U$ holds pivoted values only.
\item Omitting $P$ when swaps occur leads to mismatched $PA$ vs. $LU$.
\end{bullets}
}
\INTUITION{
Permutation aligns good pivots; elimination shears away lower entries;
$L$ records shears, $U$ records the staircase heights.
}
\CANONICAL{
\begin{bullets}
\item $PA=LU$ exists for all nonsingular $A$ with partial pivoting.
\item Determinant $\det(A)=\operatorname{sgn}(P)\prod_i u_{ii}$.
\end{bullets}
}

\FormulaPage{4}{Solving $Ax=b$ via $PA=LU$ and Triangular Solves}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Given $PA=LU$ and $Ax=b$, the solution is obtained by
\WHAT{
$PAx=Pb$, then solve $Ly=Pb$ by forward substitution and $Ux=y$ by back
substitution. Complexity is $\mathcal{O}(n^2)$ per solve once $LU$ is
available.
}
\WHY{
Triangular systems are easy to solve; reusing $L,U$ for many $b$ is
efficient and numerically stable with pivoting.
}
\FORMULA{
\[
PA=LU,\quad Ly=Pb,\quad Ux=y.
\]
}
\CANONICAL{
$L$ unit lower triangular, $U$ upper triangular, $P$ permutation. Forward
substitution solves $y_i=b_i-\sum_{j<i}\ell_{ij}y_j$. Back substitution
solves $x_i=(y_i-\sum_{j>i}u_{ij}x_j)/u_{ii}$.
}
\PRECONDS{
\begin{bullets}
\item $A$ nonsingular so that $U$ has nonzero diagonal.
\item $PA=LU$ available (from elimination with pivoting).
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
A unit lower triangular system $Ly=c$ has a unique solution given by
$y_i=c_i-\sum_{j<i}\ell_{ij}y_j$; an upper triangular system $Ux=y$ with
nonzero diagonal has unique solution by back substitution.
\end{lemma}
\begin{proof}
For $Ly=c$, $y_1=c_1$ since $\ell_{11}=1$. Assume $y_1,\dots,y_{i-1}$
known; then $y_i=c_i-\sum_{j<i}\ell_{ij}y_j$ gives the unique $y_i$.
Similarly, for $Ux=y$, solve from $i=n$ down using $u_{ii}\neq 0$:
$x_i=(y_i-\sum_{j>i}u_{ij}x_j)/u_{ii}$. Induction ensures uniqueness.\qedhere
\end{proof}
\DERIVATION{
\begin{align*}
\text{Step 1: }&c=Pb.\\
\text{Step 2: }&\text{Forward: }y_1=c_1,\ 
y_i=c_i-\sum_{j=1}^{i-1}\ell_{ij}y_j.\\
\text{Step 3: }&\text{Backward: }x_n=y_n/u_{nn},\\
&x_i=(y_i-\sum_{j=i+1}^n u_{ij}x_j)/u_{ii}.\\
\text{Step 4: }&\text{Return }x.
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Factor $PA=LU$ (once).
\item For each $b$, compute $c=Pb$, forward solve $Ly=c$, back solve $Ux=y$.
\item Validate by computing residual $\|Ax-b\|$.
\end{bullets}
}
\EQUIV{
\begin{bullets}
\item When $P=I$, this reduces to $Ly=b$, $Ux=y$.
\item Multiple right-hand sides: solve $LY=PB$ and $UX=Y$ in blocks.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item Singular $A$ yields zero diagonal in $U$ and no unique solution.
\item Poorly conditioned $A$ can amplify rounding; pivoting mitigates.
\end{bullets}
}
\INPUTS{$A\in\mathbb{F}^{n\times n}$, $b\in\mathbb{F}^n$, and $P,L,U$.}
\DERIVATION{
\begin{align*}
\text{Example: }&U=\begin{bmatrix}4&-6&0\\0&4&1\\0&0&1\end{bmatrix},\
L=\begin{bmatrix}1&0&0\\1/2&1&0\\-1/2&1&1\end{bmatrix},\\
&P=\text{swap rows }1,2,\ b=\begin{bmatrix}3\\7\\1\end{bmatrix}.\\
c&=Pb=\begin{bmatrix}7\\3\\1\end{bmatrix}.\\
\text{Forward: }&y_1=7,\ y_2=3-(1/2)\cdot 7=-0.5,\\
&y_3=1-(-1/2)\cdot 7-1\cdot(-0.5)=1+3.5+0.5=5.\\
\text{Backward: }&x_3=5/1=5,\\
&x_2=(-0.5-1\cdot 5)/4=-1.375,\\
&x_1=(7-(-6)\cdot(-1.375)-0)/4\\
&=(7-8.25)/4=-0.3125.
\end{align*}
}
\RESULT{
$x=\begin{bmatrix}-0.3125\\-1.375\\5\end{bmatrix}$ solves $Ax=b$.}
\UNITCHECK{
Vector and matrix dimensions align; triangular solves well defined as
$u_{ii}\neq 0$.
}
\PITFALLS{
\begin{bullets}
\item Forgetting to permute $b$ by $P$.
\item Dividing by a zero or tiny pivot without pivoting.
\end{bullets}
}
\INTUITION{
Permute to find safe pivots, then peel off dependencies layer by layer.}
\CANONICAL{
\begin{bullets}
\item Solve via two triangular systems after factoring once.
\item Cost: factorization $\mathcal{O}(n^3)$, each solve $\mathcal{O}(n^2)$.
\end{bullets}
}

\section{10 Exhaustive Problems and Solutions}
\ProblemPage{1}{Building $P,L,U$ and the Elementary Matrices}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Compute $P,L,U$ for a given $A$ and list the elementary matrices used.
\PROBLEM{
Let $A=\begin{bmatrix}0&2&1\\2&1&1\\1&1&0\end{bmatrix}$. Use partial
pivoting to produce $PA=LU$. Identify each $E$ applied and verify
$PA=LU$.
}
\MODEL{
\[
E_k\cdots E_1PA=U,\quad L=(E_k\cdots E_1)^{-1}.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Field $\mathbb{F}=\mathbb{R}$. Partial pivoting is used.
\item Nonzero pivots exist; $A$ is nonsingular.
\end{bullets}
}
\varmapStart
\var{A}{Input matrix.}
\var{P}{Permutation built by swaps.}
\var{E_t}{Row addition matrices zeroing subdiagonal entries.}
\var{L,U}{Unit lower and upper triangular factors.}
\varmapEnd
\WHICHFORMULA{
Formula 3: $PA=LU$ via Gaussian elimination with pivoting.}
\GOVERN{
\[
E_k\cdots E_1PA=U,\quad L=(E_k\cdots E_1)^{-1}.
\]
}
\INPUTS{$A=\begin{bmatrix}0&2&1\\2&1&1\\1&1&0\end{bmatrix}$.}
\DERIVATION{
\begin{align*}
\text{Pivot 1: }&\max|a_{i1}|=2\text{ at row }2,\ P_1=S_{12}.\\
&P_1A=\begin{bmatrix}2&1&1\\0&2&1\\1&1&0\end{bmatrix}.\\
\ell_{31}&=1/2,\ E_{31}=T_{31}(-1/2).\\
&E_{31}P_1A=\begin{bmatrix}2&1&1\\0&2&1\\0&0.5&-0.5\end{bmatrix}.\\
\text{Pivot 2: }&u_{22}=2\neq 0,\ \ell_{32}=0.5/2=0.25.\\
&E_{32}=T_{32}(-0.25).\\
&U=E_{32}E_{31}P_1A=
\begin{bmatrix}2&1&1\\0&2&1\\0&0&-0.75\end{bmatrix}.\\
&L=\begin{bmatrix}1&0&0\\0&1&0\\1/2&1/4&1\end{bmatrix},\ 
P=P_1.\\
&PA=LU\ \text{directly checks by multiplication}.
\end{align*}
}
\RESULT{
$P=S_{12}$, $L=\begin{bmatrix}1&0&0\\0&1&0\\1/2&1/4&1\end{bmatrix}$,
$U=\begin{bmatrix}2&1&1\\0&2&1\\0&0&-3/4\end{bmatrix}$.}
\UNITCHECK{
Shapes are $3\times 3$; $L$ unit diagonal, $U$ upper triangular.}
\EDGECASES{
\begin{bullets}
\item If first column were all zeros, swap from a later column would be
needed; factorization becomes more complex (full pivoting).
\item Near zero pivots require pivoting to avoid instability.
\end{bullets}
}
\ALTERNATE{
Form $A=LU$ without pivoting fails here since $a_{11}=0$; $P$ is required.}
\VALIDATION{
\begin{bullets}
\item Verify $LU$ equals $PA$ numerically.
\item Check $\det(A)=\operatorname{sgn}(P)\prod u_{ii}=(-1)\cdot(2\cdot2\cdot(-3/4))=3.
\end{bullets}
}
\INTUITION{
Swap to get a good first pivot, then shear away entries below it.}
\CANONICAL{
\begin{bullets}
\item $PA=LU$ built by pivoting and elimination.
\item $L$ stores multipliers, $U$ stores updated pivots.
\end{bullets}
}

\ProblemPage{2}{Determinant via Elimination}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Compute $\det(A)$ using elimination updates.
\PROBLEM{
Given $A=\begin{bmatrix}1&3&1\\1&1&-1\\3&11&5\end{bmatrix}$, compute
$\det(A)$ by Gaussian elimination tracking determinant factors.
}
\MODEL{
\[
E_k\cdots E_1A=U,\quad \det(A)=\det(E_k)^{-1}\cdots\det(E_1)^{-1}
\prod_i u_{ii}.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item No row swaps needed; pivots are nonzero.
\end{bullets}
}
\varmapStart
\var{E_t}{Elementary elimination matrices.}
\var{U}{Upper triangular matrix.}
\var{\det}{Determinant function.}
\varmapEnd
\WHICHFORMULA{
Formula 2 for determinant effects; Formula 3 for elimination to $U$.}
\GOVERN{
\[
\det(EA)=\det(E)\det(A),\quad \det(U)=\prod_i u_{ii}.
\]
}
\INPUTS{$A=\begin{bmatrix}1&3&1\\1&1&-1\\3&11&5\end{bmatrix}$.}
\DERIVATION{
\begin{align*}
\text{Elim 1: }&\ell_{21}=1,\ E_{21}=T_{21}(-1).\\
&E_{21}A=\begin{bmatrix}1&3&1\\0&-2&-2\\3&11&5\end{bmatrix}.\\
\ell_{31}&=3,\ E_{31}=T_{31}(-3).\\
&E_{31}E_{21}A=\begin{bmatrix}1&3&1\\0&-2&-2\\0&2&2\end{bmatrix}.\\
\text{Elim 2: }&\ell_{32}=2/(-2)=-1,\ E_{32}=T_{32}(1).\\
&U=\begin{bmatrix}1&3&1\\0&-2&-2\\0&0&0\end{bmatrix}.\\
\det(E_{21})&=\det(E_{31})=\det(E_{32})=1.\\
\det(U)&=1\cdot(-2)\cdot 0=0\Rightarrow \det(A)=0.
\end{align*}
}
\RESULT{
$\det(A)=0$; $A$ is singular.}
\UNITCHECK{
Upper triangular determinant equals product of diagonal entries.}
\EDGECASES{
\begin{bullets}
\item A zero pivot indicates singularity or need for swapping.
\item Swaps would contribute a factor $-1$ per swap.
\end{bullets}
}
\ALTERNATE{
Compute determinant by cofactor expansion confirms zero due to row
dependence: third row equals first plus two times second.}
\VALIDATION{
\begin{bullets}
\item Check linear dependence: $(3,11,5)=(1,3,1)+2(1,1,-1)$.
\end{bullets}
}
\INTUITION{
Elimination reveals rank deficiency via a zero on the diagonal of $U$.}
\CANONICAL{
\begin{bullets}
\item Determinant equals product of pivots up to swap signs.
\item Zero pivot implies zero determinant.
\end{bullets}
}

\ProblemPage{3}{Inverse via Gauss--Jordan and Elementary Matrices}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Construct $A^{-1}$ as a product of elementary matrices.
\PROBLEM{
Let $A=\begin{bmatrix}2&1\\5&3\end{bmatrix}$. Use Gauss--Jordan on
$[A\mid I]$ to compute $A^{-1}$ and express it as $E_k\cdots E_1$ times
$I$.
}
\MODEL{
\[
E_k\cdots E_1A=I,\quad A^{-1}=E_k\cdots E_1.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $A$ is invertible; pivots are nonzero.
\end{bullets}
}
\varmapStart
\var{E_t}{Elementary matrices used in Gauss--Jordan.}
\var{I}{Identity matrix.}
\var{A^{-1}}{Inverse of $A$.}
\varmapEnd
\WHICHFORMULA{
Formula 1 for operations as $E$; Formula 2 for invertibility of elementaries.}
\GOVERN{
\[
E_k\cdots E_1[A\mid I]=[I\mid A^{-1}].
\]
}
\INPUTS{$A=\begin{bmatrix}2&1\\5&3\end{bmatrix}$.}
\DERIVATION{
\begin{align*}
\text{Augment: }&[A\mid I]=\left[\begin{array}{cc|cc}2&1&1&0\\5&3&0&1\end{array}\right].\\
E_1&=T_{21}(-5/2):\ R_2\gets R_2-(5/2)R_1.\\
&\left[\begin{array}{cc|cc}2&1&1&0\\0&0.5&-2.5&1\end{array}\right].\\
E_2&=D_2(2):\ R_2\gets 2R_2.\\
&\left[\begin{array}{cc|cc}2&1&1&0\\0&1&-5&2\end{array}\right].\\
E_3&=T_{11}(-1):\ R_1\gets R_1-R_2.\\
&\left[\begin{array}{cc|cc}2&0&6&-2\\0&1&-5&2\end{array}\right].\\
E_4&=D_1(1/2):\ R_1\gets \tfrac12 R_1.\\
&\left[\begin{array}{cc|cc}1&0&3&-1\\0&1&-5&2\end{array}\right].\\
\Rightarrow&A^{-1}=\begin{bmatrix}3&-1\\-5&2\end{bmatrix}.
\end{align*}
}
\RESULT{
$A^{-1}=\begin{bmatrix}3&-1\\-5&2\end{bmatrix}$ and
$A^{-1}=E_4E_3E_2E_1$.}
\UNITCHECK{
Check $AA^{-1}=I$:
$\begin{bmatrix}2&1\\5&3\end{bmatrix}\begin{bmatrix}3&-1\\-5&2\end{bmatrix}
=\begin{bmatrix}1&0\\0&1\end{bmatrix}$.}
\EDGECASES{
\begin{bullets}
\item Singular $A$ leads to a zero pivot and no inverse.
\item Scaling by zero is invalid; use row swaps if needed.
\end{bullets}
}
\ALTERNATE{
Compute inverse by adjugate: $A^{-1}=\frac{1}{\det(A)}
\begin{bmatrix}3&-1\\-5&2\end{bmatrix}$ with $\det(A)=1$.}
\VALIDATION{
\begin{bullets}
\item Multiply $A^{-1}A$ to confirm $I$.
\end{bullets}
}
\INTUITION{
Gauss--Jordan applies reversible edits until the left block is $I$; the
right block accumulates those edits as $A^{-1}$.}
\CANONICAL{
\begin{bullets}
\item An invertible matrix equals a product of elementaries.
\item Inverse is the product of the corresponding inverses in reverse order.
\end{bullets}
}

\ProblemPage{4}{Alice and the Hidden Operator}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Discover $E$ from a described sequence of row operations.
\PROBLEM{
Alice transforms $A\in\mathbb{R}^{3\times 3}$ by: (i) swap rows $1$ and
$3$; (ii) add $2$ times new row $1$ to row $2$; (iii) scale row $3$ by
$-3$. Find a single $E$ with $EA$ equal to the final matrix and compute
$\det(E)$.
}
\MODEL{
\[
E= D_3(-3)\,T_{21}(2)\,S_{13}.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Operations applied to the current matrix in order given.
\end{bullets}
}
\varmapStart
\var{S_{13}}{Swap rows $1$ and $3$.}
\var{T_{21}(2)}{Add $2$ times row $1$ to row $2$.}
\var{D_3(-3)}{Scale row $3$ by $-3$.}
\var{E}{Overall elementary product.}
\varmapEnd
\WHICHFORMULA{
Formula 1 for building $E$; Formula 2 for determinants of elementaries.}
\GOVERN{
\[
EA=D_3(-3)\,T_{21}(2)\,S_{13}\,A,\quad \det(E)=(-3)\cdot 1\cdot(-1)=3.
\]
}
\INPUTS{Operations sequence as given.}
\DERIVATION{
\begin{align*}
S_{13}&=\begin{bmatrix}0&0&1\\0&1&0\\1&0&0\end{bmatrix},\
T_{21}(2)=\begin{bmatrix}1&0&0\\2&1&0\\0&0&1\end{bmatrix},\\
D_3(-3)&=\begin{bmatrix}1&0&0\\0&1&0\\0&0&-3\end{bmatrix}.\\
E&=D_3(-3)\,T_{21}(2)\,S_{13}.\\
\det(E)&=\det(D_3(-3))\det(T_{21}(2))\det(S_{13})\\
&=(-3)\cdot 1\cdot(-1)=3.
\end{align*}
}
\RESULT{
$E$ is as above and $\det(E)=3$.}
\UNITCHECK{
All factors are $3\times 3$; determinant factors multiply to $3$.}
\EDGECASES{
\begin{bullets}
\item Changing order changes $E$; products of elementaries are noncommutative.
\end{bullets}
}
\ALTERNATE{
Write $E$ by applying the operations to $I_3$ and reading off resulting $E$.}
\VALIDATION{
\begin{bullets}
\item Verify $E$ by checking action on standard basis rows.
\end{bullets}
}
\INTUITION{
Compose simple reversible edits into a single linear operator.}
\CANONICAL{
\begin{bullets}
\item Sequence of row operations equals left multiplication by a product $E$.
\end{bullets}
}

\ProblemPage{5}{Bob's Two Paths to the Same $U$}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Show that different elimination paths with the same pivots give the same
$U$.
\PROBLEM{
Bob eliminates below the first pivot using $T_{21}(-\ell_{21})$ then
$T_{31}(-\ell_{31})$; Carol does $T_{31}(-\ell_{31})$ then
$T_{21}(-\ell_{21})$. Prove both produce the same intermediate matrix and
hence the same $U$ after finishing column one.
}
\MODEL{
\[
T_{21}(-\ell_{21})T_{31}(-\ell_{31})A=
T_{31}(-\ell_{31})T_{21}(-\ell_{21})A.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $\ell_{21}=a_{21}/a_{11}$, $\ell_{31}=a_{31}/a_{11}$, $a_{11}\neq 0$.
\end{bullets}
}
\varmapStart
\var{\ell_{21},\ell_{31}}{Multipliers for column one.}
\var{T_{21},T_{31}}{Row addition matrices.}
\varmapEnd
\WHICHFORMULA{
Formula 1 for action of $T_{ij}$; associativity of matrix multiplication.}
\GOVERN{
\[
T_{21}(-\ell_{21}),\ T_{31}(-\ell_{31})\ \text{affect disjoint rows}.
\]
}
\INPUTS{$A$ with $a_{11}\neq 0$.}
\DERIVATION{
\begin{align*}
\text{Effect: }&T_{21}(-\ell_{21})\text{ changes only row }2
\text{ using row }1.\\
&T_{31}(-\ell_{31})\text{ changes only row }3\text{ using row }1.\\
\text{Rows 2 and 3 are}&\ \text{independent edits, so the two
operations commute.}\\
\Rightarrow\ &T_{21}(-\ell_{21})T_{31}(-\ell_{31})A\\
=&T_{31}(-\ell_{31})T_{21}(-\ell_{21})A.
\end{align*}
}
\RESULT{
Order of eliminating different rows in the same column does not matter;
the resulting $U$ is the same after column one.}
\UNITCHECK{
All operations are valid with $a_{11}\neq 0$; dimensions preserved.}
\EDGECASES{
\begin{bullets}
\item If one operation depends on results of the other (same target row),
they do not commute.
\end{bullets}
}
\ALTERNATE{
View both as applying the same linear map on the two-dimensional subspace
spanned by rows $2$ and $3$ with the same coefficients.}
\VALIDATION{
\begin{bullets}
\item Test on a numeric $A$ to verify equality of outcomes.
\end{bullets}
}
\INTUITION{
Independent shears that act on different rows commute.}
\CANONICAL{
\begin{bullets}
\item Eliminate per column in any row order; $U$ depends only on multipliers.
\end{bullets}
}

\ProblemPage{6}{Expectation of Determinant Multiplier from Random Scalings}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Compute expectation of the determinant multiplier from random row scalings.
\PROBLEM{
Choose $k$ independent elementary scalings $D_{i_t}(C_t)$ with
$C_t\in\{1/2,2\}$ equally likely and rows $i_t$ arbitrary but valid.
What is $\mathbb{E}[\det(D_{i_k}(C_k)\cdots D_{i_1}(C_1))]$?
}
\MODEL{
\[
\det\Big(\prod_{t=1}^k D_{i_t}(C_t)\Big)=\prod_{t=1}^k C_t.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $C_t$ are independent identically distributed with
$\mathbb{P}(C_t=1/2)=\mathbb{P}(C_t=2)=1/2$.
\item Determinant multiplicativity holds.
\end{bullets}
}
\varmapStart
\var{C_t}{Random scaling factors.}
\var{k}{Number of operations.}
\varmapEnd
\WHICHFORMULA{
Formula 2: determinant of a product equals product of determinants.}
\GOVERN{
\[
\mathbb{E}\Big[\prod_{t=1}^k C_t\Big]=\prod_{t=1}^k \mathbb{E}[C_t]
\quad(\text{independence}).
\]
}
\INPUTS{$k\in\mathbb{N}$.}
\DERIVATION{
\begin{align*}
\mathbb{E}[C_t]&=\tfrac12\cdot \tfrac12+\tfrac12\cdot 2=\tfrac14+1=\tfrac54.\\
\mathbb{E}\Big[\prod_{t=1}^k C_t\Big]
&=\prod_{t=1}^k \mathbb{E}[C_t]=\left(\tfrac54\right)^k.
\end{align*}
}
\RESULT{
$\mathbb{E}[\det(\prod_{t=1}^k D_{i_t}(C_t))]=(5/4)^k$.}
\UNITCHECK{
Dimensionless scalar; independence justifies product of expectations.}
\EDGECASES{
\begin{bullets}
\item If factors are dependent, product of expectations is invalid.
\item If some $C_t=0$, determinant can be zero; here not possible.
\end{bullets}
}
\ALTERNATE{
Compute $\log$ expectation of product is not the expectation of logs; use
independence directly as shown.}
\VALIDATION{
\begin{bullets}
\item For $k=1$, $\mathbb{E}[\det]=5/4$ matches direct computation.
\end{bullets}
}
\INTUITION{
Random scalings multiply volume; expected volume multiplier compounds as
the expected single-step multiplier raised to $k$.}
\CANONICAL{
\begin{bullets}
\item Determinant updates from scaling are multiplicative and separable.
\end{bullets}
}

\ProblemPage{7}{Proof: Invertible iff Product of Elementary Matrices}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
A square matrix is invertible if and only if it is a product of
elementary matrices.
\PROBLEM{
Prove: $A\in\mathbb{F}^{n\times n}$ is invertible iff there exist
elementary matrices $E_1,\dots,E_k$ such that $A=E_k\cdots E_1$.
}
\MODEL{
\[
A\text{ invertible}\ \Leftrightarrow\ A\text{ row equivalent to }I_n.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Determinant properties and Gauss--Jordan elimination over a field.
\end{bullets}
}
\varmapStart
\var{A}{Square matrix.}
\var{E_t}{Elementary matrices.}
\varmapEnd
\WHICHFORMULA{
Formula 1 and Formula 2; Gauss--Jordan reaches $I$ iff $A$ is invertible.}
\GOVERN{
\[
E_k\cdots E_1A=I_n\ \Rightarrow\ A^{-1}=E_k\cdots E_1.
\]
}
\INPUTS{$A\in\mathbb{F}^{n\times n}$.}
\DERIVATION{
\begin{align*}
(\Rightarrow)\ &\text{If }A\text{ invertible, Gauss--Jordan yields }I_n.\\
&\exists E_t\text{ s.t. }E_k\cdots E_1A=I_n.\\
&\Rightarrow A=(E_k\cdots E_1)^{-1}
=E_1^{-1}\cdots E_k^{-1}\ (\text{product of elementaries}).\\
(\Leftarrow)\ &\text{If }A=E_k\cdots E_1,\ \text{each }E_t\text{ invertible}.\\
&\Rightarrow A\text{ invertible as product of invertibles.}
\end{align*}
}
\RESULT{
$A$ is invertible iff it is a product of elementary matrices.}
\UNITCHECK{
All matrices are $n\times n$; products well defined.}
\EDGECASES{
\begin{bullets}
\item Over rings without division, Gauss--Jordan may fail.
\end{bullets}
}
\ALTERNATE{
Use $\det(A)\neq 0$ iff $A$ invertible and determinants of elementaries
are nonzero; closure under multiplication implies surjectivity.}
\VALIDATION{
\begin{bullets}
\item For $A=I_n$, trivial with $k=0$ or empty product equals $I_n$.
\end{bullets}
}
\INTUITION{
Invertibility means reversibility; elementaries generate all reversible
row edits, hence all invertible matrices.}
\CANONICAL{
\begin{bullets}
\item $\mathrm{GL}_n(\mathbb{F})$ is generated by elementaries.
\end{bullets}
}

\ProblemPage{8}{Proof: LU without Pivoting under Nonzero Leading Minors}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
If all leading principal minors of $A$ are nonzero, then $A=LU$ without
pivoting.
\PROBLEM{
Prove: If $\det(A_{1:i,1:i})\neq 0$ for $i=1,\dots,n$, then Gaussian
elimination proceeds without swaps and yields $A=LU$ with $L$ unit lower
triangular and $U$ upper triangular.
}
\MODEL{
\[
E_k\cdots E_1A=U,\quad A=L^{-1}U,\ L^{-1}=E_k\cdots E_1.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Field $\mathbb{F}$; the given minors are nonzero.
\end{bullets}
}
\varmapStart
\var{A_{1:i,1:i}}{Leading principal submatrix of order $i$.}
\var{L,U}{LU factors without pivoting.}
\varmapEnd
\WHICHFORMULA{
Formula 3 without permutation when pivots are nonzero.}
\GOVERN{
\[
\text{Pivot }a_{ii}^{(i)}\neq 0\ \Rightarrow\ \ell_{ji}=a_{ji}^{(i)}/a_{ii}^{(i)}.
\]
}
\INPUTS{$A\in\mathbb{F}^{n\times n}$.}
\DERIVATION{
\begin{align*}
\text{Induction on }i:&\ \det(A_{1:1,1:1})\neq 0\Rightarrow a_{11}\neq 0.\\
&\text{Eliminate below to get zeros in column one.}\\
\text{Assume }&A^{(i)}\text{ has zeros below diagonal in first }i\text{ cols}\\
&\text{and }a^{(i)}_{ii}\neq 0.\\
&\text{Form }A^{(i+1)}\text{ by eliminating below }a^{(i)}_{i+1,i+1}.\\
&\det(A_{1:i+1,1:i+1})\neq 0\Rightarrow a^{(i)}_{i+1,i+1}\neq 0.\\
&\text{Thus elimination proceeds without swaps.}\\
&L^{-1}=E_k\cdots E_1,\ L=(L^{-1})^{-1}\text{ unit lower triangular.}
\end{align*}
}
\RESULT{
$A=LU$ without pivoting under the nonzero leading minor condition.}
\UNITCHECK{
Triangular factors have correct shapes and unit diagonal in $L$.}
\EDGECASES{
\begin{bullets}
\item If some leading minor is zero, a swap may be needed or LU fails.
\end{bullets}
}
\ALTERNATE{
Use the Schur complement positivity for symmetric positive definite case
to ensure nonzero pivots and hence LU (in fact Cholesky) exists.}
\VALIDATION{
\begin{bullets}
\item Test on a Vandermonde matrix with distinct nodes; leading minors are
nonzero.
\end{bullets}
}
\INTUITION{
Nonzero leading minors guarantee viable pivots along the diagonal.}
\CANONICAL{
\begin{bullets}
\item LU exists without $P$ if and only if elimination needs no swaps.
\end{bullets}
}

\ProblemPage{9}{Combo: Least Squares Normal Equations via Elimination}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Solve normal equations using LU on $X^\top X$.
\PROBLEM{
Given $X=\begin{bmatrix}1&1\\1&2\\1&3\end{bmatrix}$ and
$y=\begin{bmatrix}2\\2\\4\end{bmatrix}$, solve
$(X^\top X)\beta=X^\top y$ using $LU$ and back substitution.
}
\MODEL{
\[
A=X^\top X,\ b=X^\top y,\ A=LU,\ Ly=b,\ U\beta=y.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $X$ has full column rank; $X^\top X$ is nonsingular.
\end{bullets}
}
\varmapStart
\var{X,y}{Design matrix and response.}
\var{\beta}{Coefficient vector.}
\var{A,b}{Normal matrix and right-hand side.}
\var{L,U}{LU factors of $A$.}
\varmapEnd
\WHICHFORMULA{
Formula 4 for solving via $LU$; Formula 3 for constructing $LU$.}
\GOVERN{
\[
A=\begin{bmatrix}3&6\\6&14\end{bmatrix},\ 
b=\begin{bmatrix}8\\18\end{bmatrix}.
\]
}
\INPUTS{$X,y$ as given.}
\DERIVATION{
\begin{align*}
A&=\begin{bmatrix}3&6\\6&14\end{bmatrix}.\\
\ell_{21}&=6/3=2,\ E_{21}=T_{21}(-2).\\
U&=\begin{bmatrix}3&6\\0&2\end{bmatrix},\
L=\begin{bmatrix}1&0\\2&1\end{bmatrix}.\\
\text{Forward: }&Ly=b\Rightarrow y_1=8,\ y_2=18-2\cdot 8=2.\\
\text{Backward: }&U\beta=y\Rightarrow \beta_2=2/2=1,\\
&\beta_1=(8-6\cdot 1)/3=2/3.\\
\beta&=\begin{bmatrix}2/3\\1\end{bmatrix}.
\end{align*}
}
\RESULT{
$\beta=(2/3,1)^\top$.}
\UNITCHECK{
$A$ is $2\times 2$, $b$ length $2$; triangular solves valid.}
\EDGECASES{
\begin{bullets}
\item If $X$ is rank deficient, $X^\top X$ is singular; use QR instead.
\end{bullets}
}
\ALTERNATE{
Solve via QR factorization to avoid squaring the condition number.}
\VALIDATION{
\begin{bullets}
\item Check residuals: $\|X\beta-y\|$ minimized; compute explicitly.
\end{bullets}
}
\INTUITION{
Least squares reduces to a symmetric positive definite system solvable by
elimination.}
\CANONICAL{
\begin{bullets}
\item Normal equations solved by $LU$ as a linear system.
\end{bullets}
}

\ProblemPage{10}{Combo: Tridiagonal System by Elimination}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Solve a tridiagonal linear system using elimination.
\PROBLEM{
Solve $Ax=b$ where
$A=\begin{bmatrix}2&-1&0&0\\-1&2&-1&0\\0&-1&2&-1\\0&0&-1&2\end{bmatrix}$
and $b=\begin{bmatrix}1\\0\\0\\1\end{bmatrix}$ using elimination and back
substitution. Count operations.
}
\MODEL{
\[
A=LU,\ Ly=b,\ Ux=y.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $A$ is nonsingular and symmetric positive definite.
\end{bullets}
}
\varmapStart
\var{A}{Tridiagonal Toeplitz matrix.}
\var{b}{Right-hand side.}
\var{L,U}{LU factors without pivoting.}
\varmapEnd
\WHICHFORMULA{
Formula 4 for triangular solves; Formula 3 for LU construction.}
\GOVERN{
\[
\ell_{i+1,i}=\frac{-1}{u_{ii}},\ u_{i+1,i+1}=2-(-1)\ell_{i+1,i}.
\]
}
\INPUTS{$A,b$ as above.}
\DERIVATION{
\begin{align*}
u_{11}&=2,\ \ell_{21}=-1/2.\\
u_{22}&=2-(-1)\cdot(-1/2)=3/2,\ \ell_{32}=-1/(3/2)=-2/3.\\
u_{33}&=2-(-1)\cdot(-2/3)=4/3,\ \ell_{43}=-1/(4/3)=-3/4.\\
u_{44}&=2-(-1)\cdot(-3/4)=5/4.\\
L&=\begin{bmatrix}1&0&0&0\\-1/2&1&0&0\\0&-2/3&1&0\\0&0&-3/4&1\end{bmatrix},\\
U&=\begin{bmatrix}2&-1&0&0\\0&3/2&-1&0\\0&0&4/3&-1\\0&0&0&5/4\end{bmatrix}.\\
\text{Forward: }&y_1=1,\ y_2=0-(-1/2)\cdot 1=1/2,\\
&y_3=0-(-2/3)\cdot (1/2)=1/3,\\
&y_4=1-(-3/4)\cdot (1/3)=1+1/4=5/4.\\
\text{Backward: }&x_4=(5/4)/(5/4)=1,\\
&x_3=(1/3-(-1)\cdot 1)/(4/3)=\frac{4/3}{4/3}=1,\\
&x_2=(1/2-(-1)\cdot 1)/(3/2)=\frac{3/2}{3/2}=1,\\
&x_1=(1-(-1)\cdot 1)/2=1.
\end{align*}
}
\RESULT{
$x=(1,1,1,1)^\top$. Operation count: $\mathcal{O}(n)$ due to tridiagonal
structure (about $7n$ flops).}
\UNITCHECK{
Triangular solves and factors are consistent; $Ux=y$ verified.}
\EDGECASES{
\begin{bullets}
\item For zero diagonal, pivoting may be necessary; not needed here.
\end{bullets}
}
\ALTERNATE{
Thomas algorithm specializes elimination for tridiagonal matrices.}
\VALIDATION{
\begin{bullets}
\item Check $Ax=b$ explicitly by multiplication.
\end{bullets}
}
\INTUITION{
Local interactions in a chain lead to banded factors and linear-time
elimination.}
\CANONICAL{
\begin{bullets}
\item Structure-aware elimination reduces complexity.
\end{bullets}
}

\section{Coding Demonstrations}
\CodeDemoPage{Solve $Ax=b$ via LU (From Scratch vs. NumPy)}
\PROBLEM{
Implement $PA=LU$ with partial pivoting and solve $Ax=b$. Validate
against \inlinecode{numpy.linalg.solve}.
}
\API{
\begin{bullets}
\item \inlinecode{def read_input(s) -> tuple} — parse $A,b$.
\item \inlinecode{def lu_pivot(A) -> (P,L,U)} — factorization.
\item \inlinecode{def solve_plu(P,L,U,b) -> x} — triangular solves.
\item \inlinecode{def validate() -> None} — assertions.
\item \inlinecode{def main() -> None} — run demo.
\end{bullets}
}
\INPUTS{
$A$ as list of lists or ndarray shape $(n,n)$, $b$ length $n$; real
numbers.
}
\OUTPUTS{
$x$ solving $Ax=b$; also residual norms for validation.
}
\FORMULA{
\[
PA=LU,\quad Ly=Pb,\quad Ux=y.
\]
}
\textbf{SOLUTION A — From Scratch (Mathematically Explicit Implementation)}
\begin{codepy}
import numpy as np

def read_input(s):
    vals = [float(x) for x in s.split()]
    n = int(vals[0])
    A = np.array(vals[1:1+n*n]).reshape(n, n)
    b = np.array(vals[1+n*n:1+n*n+n])
    return A, b

def lu_pivot(A):
    A = A.copy().astype(float)
    n = A.shape[0]
    P = np.eye(n)
    L = np.eye(n)
    U = A.copy()
    for k in range(n):
        p = np.argmax(np.abs(U[k:, k])) + k
        if p != k:
            U[[k, p], :] = U[[p, k], :]
            P[[k, p], :] = P[[p, k], :]
            if k > 0:
                L[[k, p], :k] = L[[p, k], :k]
        if U[k, k] == 0.0:
            raise ValueError("Singular pivot")
        for i in range(k+1, n):
            L[i, k] = U[i, k] / U[k, k]
            U[i, k:] -= L[i, k] * U[k, k:]
            U[i, k] = 0.0
    return P, L, U

def solve_plu(P, L, U, b):
    n = U.shape[0]
    pb = P @ b
    y = np.zeros(n)
    for i in range(n):
        y[i] = pb[i] - L[i, :i] @ y[:i]
    x = np.zeros(n)
    for i in range(n-1, -1, -1):
        s = U[i, i+1:] @ x[i+1:]
        x[i] = (y[i] - s) / U[i, i]
    return x

def validate():
    A = np.array([[0., 2., 1.],[2., 1., 1.],[1., 1., 0.]])
    b = np.array([3., 7., 1.])
    P, L, U = lu_pivot(A)
    x = solve_plu(P, L, U, b)
    x_np = np.linalg.solve(A, b)
    assert np.allclose(x, x_np, atol=1e-10)
    assert np.allclose(P @ A, L @ U, atol=1e-10)

def main():
    validate()
    A = np.array([[3.,1.],[5.,2.]])
    b = np.array([1.,0.])
    P, L, U = lu_pivot(A)
    x = solve_plu(P, L, U, b)
    print("x:", np.round(x, 6))

if __name__ == "__main__":
    main()
\end{codepy}
\textbf{SOLUTION B — Library-Based (Validated Computational Shortcut)}
\begin{codepy}
import numpy as np

def read_input(s):
    vals = [float(x) for x in s.split()]
    n = int(vals[0])
    A = np.array(vals[1:1+n*n]).reshape(n, n)
    b = np.array(vals[1+n*n:1+n*n+n])
    return A, b

def solve_case(A, b):
    return np.linalg.solve(A, b)

def validate():
    A = np.array([[0., 2., 1.],[2., 1., 1.],[1., 1., 0.]])
    b = np.array([3., 7., 1.])
    x = solve_case(A, b)
    assert np.allclose(A @ x, b, atol=1e-10)

def main():
    validate()
    A = np.array([[3.,1.],[5.,2.]])
    b = np.array([1.,0.])
    x = solve_case(A, b)
    print("x:", np.round(x, 6))

if __name__ == "__main__":
    main()
\end{codepy}
\COMPLEXITY{
From-scratch: factorization $\mathcal{O}(n^3)$, solve $\mathcal{O}(n^2)$;
library: same asymptotics with optimized constants.
}
\FAILMODES{
\begin{bullets}
\item Singular or near-singular pivots; mitigate by pivoting checks.
\item Non-finite entries propagate; validate inputs for finiteness.
\end{bullets}
}
\STABILITY{
\begin{bullets}
\item Partial pivoting reduces growth factor and improves stability.
\item Scale rows or equilibrate if needed.
\end{bullets}
}
\VALIDATION{
\begin{bullets}
\item Compare solution with \inlinecode{numpy.linalg.solve}.
\item Check factorization $PA\approx LU$ and residual $\|Ax-b\|$.
\end{bullets}
}
\RESULT{
Both implementations agree to numerical tolerance; $PA=LU$ holds.}
\EXPLANATION{
The code mirrors Formula 3 and 4: build $P,L,U$ by elementaries encoded
implicitly, then solve forward and backward.
}
\EXTENSION{
Vectorize forward and back substitution for multiple right-hand sides.}

\CodeDemoPage{Determinant via Elimination vs. NumPy}
\PROBLEM{
Compute $\det(A)$ via elimination (product of $U$ diagonals times
$\operatorname{sgn}(P)$) and compare with \inlinecode{numpy.linalg.det}.
}
\API{
\begin{bullets}
\item \inlinecode{def det_elim(A) -> float} — compute determinant.
\item \inlinecode{def validate() -> None} — assert closeness.
\end{bullets}
}
\INPUTS{
$A\in\mathbb{R}^{n\times n}$ with nonsingular or singular cases.
}
\OUTPUTS{
Determinant estimate via elimination.
}
\FORMULA{
\[
\det(A)=\operatorname{sgn}(P)\prod_{i=1}^n u_{ii}.
\]
}
\textbf{SOLUTION A — From Scratch (Mathematically Explicit Implementation)}
\begin{codepy}
import numpy as np

def det_elim(A):
    A = A.copy().astype(float)
    n = A.shape[0]
    det = 1.0
    sgn = 1.0
    for k in range(n):
        p = np.argmax(np.abs(A[k:, k])) + k
        if A[p, k] == 0:
            return 0.0
        if p != k:
            A[[k, p], :] = A[[p, k], :]
            sgn *= -1.0
        pivot = A[k, k]
        det *= pivot
        for i in range(k+1, n):
            A[i, k:] -= (A[i, k]/pivot) * A[k, k:]
            A[i, k] = 0.0
    return sgn * det

def validate():
    np.random.seed(0)
    for n in [2, 3, 5]:
        A = np.random.randn(n, n)
        d1 = det_elim(A)
        d2 = np.linalg.det(A)
        assert abs(d1 - d2) < 1e-6

def main():
    validate()
    A = np.array([[2.,1.],[5.,3.]])
    print("det:", round(det_elim(A), 6))

if __name__ == "__main__":
    main()
\end{codepy}
\textbf{SOLUTION B — Library-Based (Validated Computational Shortcut)}
\begin{codepy}
import numpy as np

def det_lib(A):
    return float(np.linalg.det(A))

def validate():
    A = np.array([[2.,1.],[5.,3.]])
    d1 = det_lib(A)
    d2 = 2*3 - 1*5
    assert abs(d1 - d2) < 1e-9

def main():
    validate()
    A = np.array([[1.,3.,1.],[1.,1.,-1.],[3.,11.,5.]])
    print("det:", round(det_lib(A), 6))

if __name__ == "__main__":
    main()
\end{codepy}
\COMPLEXITY{
Elimination det: $\mathcal{O}(n^3)$; library det: $\mathcal{O}(n^3)$.}
\FAILMODES{
\begin{bullets}
\item Zero pivot handled by returning zero determinant.
\item Catastrophic cancellation for ill conditioned $A$; pivoting helps.
\end{bullets}
}
\STABILITY{
\begin{bullets}
\item Use partial pivoting to control growth.
\end{bullets}
}
\VALIDATION{
\begin{bullets}
\item Cross-check with explicit 2 by 2 formula and NumPy.
\end{bullets}
}
\RESULT{
Agreement within tolerance across random tests.}
\EXPLANATION{
Determinant equals the product of pivots times permutation sign per
Formula 2 and 3.
}
\EXTENSION{
Accumulate exponent and mantissa to avoid overflow for large $n$.}

\CodeDemoPage{Inverse via Gauss--Jordan vs. NumPy}
\PROBLEM{
Compute $A^{-1}$ using Gauss--Jordan with elementary operations and
compare to \inlinecode{numpy.linalg.inv}.
}
\API{
\begin{bullets}
\item \inlinecode{def inv_gj(A) -> Ainv} — Gauss--Jordan inverse.
\item \inlinecode{def validate() -> None} — asserts.
\end{bullets}
}
\INPUTS{
Invertible $A\in\mathbb{R}^{n\times n}$.
}
\OUTPUTS{
$A^{-1}$ as ndarray.
}
\FORMULA{
\[
E_k\cdots E_1[A\mid I]=[I\mid A^{-1}].
\]
}
\textbf{SOLUTION A — From Scratch (Mathematically Explicit Implementation)}
\begin{codepy}
import numpy as np

def inv_gj(A):
    A = A.copy().astype(float)
    n = A.shape[0]
    I = np.eye(n)
    for k in range(n):
        p = np.argmax(np.abs(A[k:, k])) + k
        if A[p, k] == 0:
            raise ValueError("Singular")
        if p != k:
            A[[k, p], :] = A[[p, k], :]
            I[[k, p], :] = I[[p, k], :]
        piv = A[k, k]
        A[k, :] /= piv
        I[k, :] /= piv
        for i in range(n):
            if i == k:
                continue
            f = A[i, k]
            A[i, :] -= f * A[k, :]
            I[i, :] -= f * I[k, :]
    return I

def validate():
    np.random.seed(1)
    A = np.array([[2.,1.],[5.,3.]])
    Ainv = inv_gj(A)
    I = A @ Ainv
    assert np.allclose(I, np.eye(2), atol=1e-10)

def main():
    validate()
    A = np.array([[4.,7.],[2.,6.]])
    Ainv = inv_gj(A)
    print("Ainv:\n", np.round(Ainv, 6))

if __name__ == "__main__":
    main()
\end{codepy}
\textbf{SOLUTION B — Library-Based (Validated Computational Shortcut)}
\begin{codepy}
import numpy as np

def inv_lib(A):
    return np.linalg.inv(A)

def validate():
    A = np.array([[4.,7.],[2.,6.]])
    I = A @ inv_lib(A)
    assert np.allclose(I, np.eye(2), atol=1e-10)

def main():
    validate()
    A = np.array([[2.,1.],[5.,3.]])
    print("Ainv:\n", np.round(inv_lib(A), 6))

if __name__ == "__main__":
    main()
\end{codepy}
\COMPLEXITY{
Gauss--Jordan: $\mathcal{O}(n^3)$; library inverse: $\mathcal{O}(n^3)$.}
\FAILMODES{
\begin{bullets}
\item Singular matrices; check pivot zero and raise error.
\item Large condition numbers yield inaccurate inverses; avoid explicit
inverse in practice when solving systems.
\end{bullets}
}
\STABILITY{
\begin{bullets}
\item Partial pivoting used each column to improve stability.
\end{bullets}
}
\VALIDATION{
\begin{bullets}
\item Verify $A A^{-1}\approx I$ and $A^{-1}A\approx I$.
\end{bullets}
}
\RESULT{
Gauss--Jordan inverse matches NumPy within tolerance.}
\EXPLANATION{
Implements Formula 1 and Gauss--Jordan to realize $A^{-1}$ as the product
of elementary operations encoded in the augmented system.
}
\EXTENSION{
Return the sequence of elementary matrices for pedagogical tracing.}

\section{Applied Domains — Detailed End-to-End Scenarios}
\DomainPage{Machine Learning}
\SCENARIO{
Solve linear regression by normal equations using LU factorization:
$(X^\top X)\beta=X^\top y$.
}
\ASSUMPTIONS{
\begin{bullets}
\item $X$ has full column rank; $X^\top X$ is symmetric positive definite.
\item Noise has zero mean; linear model holds approximately.
\end{bullets}
}
\WHICHFORMULA{
Formula 4: triangular solves after $LU$; Formula 3 for constructing $LU$.}
\varmapStart
\var{X\in\mathbb{R}^{n\times d}}{Design matrix with bias column.}
\var{y\in\mathbb{R}^n}{Response vector.}
\var{\beta\in\mathbb{R}^d}{Coefficients.}
\var{A=X^\top X}{Normal matrix.}
\var{b=X^\top y}{Right-hand side.}
\varmapEnd
\PIPELINE{
\begin{bullets}
\item Generate synthetic $(X,y)$.
\item Compute $A$ and $b$; factor $A=LU$; solve $Ly=b$, $U\beta=y$.
\item Compare to library solution.
\end{bullets}
}
\textbf{Implementation (From Scratch)}
\begin{codepy}
import numpy as np

def generate(n=100, d=2, seed=0):
    np.random.seed(seed)
    X = np.column_stack([np.ones(n), np.linspace(0, 5, n)])
    beta = np.array([1.0, 2.0])
    y = X @ beta + 0.1*np.random.randn(n)
    return X, y

def lu_nopivot(A):
    A = A.copy().astype(float)
    n = A.shape[0]
    L = np.eye(n)
    U = A.copy()
    for k in range(n):
        if U[k, k] == 0:
            raise ValueError("Zero pivot")
        for i in range(k+1, n):
            L[i, k] = U[i, k] / U[k, k]
            U[i, k:] -= L[i, k] * U[k, k:]
            U[i, k] = 0.0
    return L, U

def solve_lu(L, U, b):
    n = L.shape[0]
    y = np.zeros(n)
    for i in range(n):
        y[i] = b[i] - L[i, :i] @ y[:i]
    x = np.zeros(n)
    for i in range(n-1, -1, -1):
        x[i] = (y[i] - U[i, i+1:] @ x[i+1:]) / U[i, i]
    return x

def main():
    X, y = generate()
    A = X.T @ X
    b = X.T @ y
    L, U = lu_nopivot(A)
    beta = solve_lu(L, U, b)
    print("beta:", np.round(beta, 3))

if __name__ == "__main__":
    main()
\end{codepy}
\textbf{Implementation (Library Version)}
\begin{codepy}
import numpy as np

def main():
    np.random.seed(0)
    X = np.column_stack([np.ones(100), np.linspace(0, 5, 100)])
    beta_true = np.array([1.0, 2.0])
    y = X @ beta_true + 0.1*np.random.randn(100)
    beta = np.linalg.lstsq(X, y, rcond=None)[0]
    print("beta:", np.round(beta, 3))

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{Report coefficients and compare to the true values.}
\INTERPRET{Recovered slope and intercept match ground truth closely.}
\NEXTSTEPS{Use QR to improve stability over normal equations.}

\DomainPage{Quantitative Finance}
\SCENARIO{
Compute portfolio weights that match target returns via solving $Ax=b$,
where $A$ encodes constraints and $b$ the targets.
}
\ASSUMPTIONS{
\begin{bullets}
\item Linear constraints are consistent and $A$ is nonsingular.
\end{bullets}
}
\WHICHFORMULA{
Formula 4: solve linear system with LU or direct solver.}
\varmapStart
\var{A}{Constraint matrix (e.g., sum to one and match exposures).}
\var{b}{Target vector.}
\var{w}{Portfolio weights.}
\varmapEnd
\PIPELINE{
\begin{bullets}
\item Build $A$ for constraints: sum to one, factor exposures.
\item Solve $Aw=b$ via LU and via NumPy.
\item Validate constraints.
\end{bullets}
}
\textbf{Implementation (Full Pipeline)}
\begin{codepy}
import numpy as np

def build_case():
    A = np.array([[1., 1., 1.],
                  [0.5, 1.0, 1.5],
                  [1.0, 0.0, 1.0]])
    b = np.array([1., 1.0, 0.6])
    return A, b

def solve(A, b):
    return np.linalg.solve(A, b)

def main():
    A, b = build_case()
    w = solve(A, b)
    print("w:", np.round(w, 4))
    print("checks:", np.round(A @ w - b, 8))

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{Constraint residuals $Aw-b$ near zero.}
\INTERPRET{Weights satisfy normalization and exposure targets.}
\NEXTSTEPS{Add bounds and quadratic objective for mean variance.}

\DomainPage{Deep Learning}
\SCENARIO{
Fit a linear layer by solving the normal equations exactly using LU,
contrasting with gradient descent.
}
\ASSUMPTIONS{
\begin{bullets}
\item Mean squared error; full rank design.
\end{bullets}
}
\WHICHFORMULA{
Formula 4: solve $(X^\top X)\theta=X^\top y$ via LU.}
\PIPELINE{
\begin{bullets}
\item Generate data from a linear model.
\item Solve for $\theta$ by linear algebra.
\item Compare with one step of gradient descent initialized at zero.
\end{bullets}
}
\textbf{Implementation (End-to-End)}
\begin{codepy}
import numpy as np

def data(seed=0):
    np.random.seed(seed)
    X = np.column_stack([np.ones(200), np.random.randn(200)])
    th = np.array([0.5, -1.2])
    y = X @ th + 0.05*np.random.randn(200)
    return X, y, th

def normal_eq(X, y):
    A = X.T @ X
    b = X.T @ y
    return np.linalg.solve(A, b)

def gd_step(X, y, theta):
    n = X.shape[0]
    yhat = X @ theta
    grad = -2 * X.T @ (y - yhat) / n
    return theta - 0.1 * grad

def main():
    X, y, th = data()
    th_ne = normal_eq(X, y)
    th_gd = gd_step(X, y, np.zeros(2))
    print("theta NE:", np.round(th_ne, 3))
    print("theta GD1:", np.round(th_gd, 3))

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{Parameter estimates and their deviation from ground truth.}
\INTERPRET{Closed form solves exactly; GD moves toward solution.}
\NEXTSTEPS{Use multi step GD or stochastic methods for large scale data.}

\DomainPage{Kaggle / Data Analytics}
\SCENARIO{
Solve multiple linear systems for feature transformations using shared
$LU$ to transform many feature columns efficiently.
}
\ASSUMPTIONS{
\begin{bullets}
\item Repeated right-hand sides share the same coefficient matrix.
\end{bullets}
}
\WHICHFORMULA{
Formula 4: reuse $LU$ to solve $AX=B$ for multiple columns.}
\PIPELINE{
\begin{bullets}
\item Factor $A$ once.
\item Solve for each column of $B$ via forward and back substitution.
\item Validate by direct multiplication.
\end{bullets}
}
\textbf{Implementation (Complete EDA Pipeline)}
\begin{codepy}
import numpy as np

def lu_nopivot(A):
    A = A.copy().astype(float)
    n = A.shape[0]
    L = np.eye(n)
    U = A.copy()
    for k in range(n):
        for i in range(k+1, n):
            L[i, k] = U[i, k] / U[k, k]
            U[i, k:] -= L[i, k] * U[k, k:]
            U[i, k] = 0.0
    return L, U

def solve_lu_multi(L, U, B):
    n, m = B.shape
    Y = np.zeros((n, m))
    for j in range(m):
        for i in range(n):
            Y[i, j] = B[i, j] - L[i, :i] @ Y[:i, j]
    X = np.zeros((n, m))
    for j in range(m):
        for i in range(n-1, -1, -1):
            X[i, j] = (Y[i, j] - U[i, i+1:] @ X[i+1:, j]) / U[i, i]
    return X

def main():
    A = np.array([[3.,1.,0.],[1.,4.,2.],[0.,2.,5.]])
    B = np.array([[1.,2.],[0.,1.],[3.,-1.]])
    L, U = lu_nopivot(A)
    X = solve_lu_multi(L, U, B)
    print("residual:", np.round(np.linalg.norm(A @ X - B), 9))

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{Residual norm $\|AX-B\|$.}
\INTERPRET{Reusing $LU$ amortizes cost across many right-hand sides.}
\NEXTSTEPS{Adopt pivoting or use triangular solve routines for stability.}

\end{document}
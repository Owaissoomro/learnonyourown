% !TeX program = xelatex
\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{microtype,setspace,amsmath,amssymb,mathtools,amsthm,unicode-math}
\setstretch{1.05}
\setmainfont{Latin Modern Roman}
\setmonofont{Latin Modern Mono}
\setmathfont{Latin Modern Math}

% --- Overflow / line-break safety ---
\allowdisplaybreaks[4]
\setlength{\jot}{7pt}
\setlength{\emergencystretch}{8em}
\sloppy

\usepackage{xcolor,fancyhdr,enumitem,inconsolata,listings}
\pagestyle{fancy}\fancyhf{}\lhead{\nouppercase{\leftmark}}\rhead{\thepage}
\setlength{\headheight}{26pt}
\setlength{\parindent}{0pt}
\setlength{\parskip}{8pt plus 2pt minus 1pt}
\raggedbottom

% --- Breakable math helpers (use these in the body) ---
\newenvironment{BreakableEquation}{\begin{equation}\begin{aligned}}{\end{aligned}\end{equation}}
\newenvironment{BreakableEquation*}{\begin{equation*}\begin{aligned}}{\end{aligned}\end{equation*}}
\newenvironment{tightalign}{\begingroup\small\allowdisplaybreaks\begin{align}}{\end{align}\endgroup}

% ---------- Safety shims ----------
% Robust minted → listings shim (no shell-escape; supports [opts]{language})
\providecommand{\enumlistm}{enumitem}
\newenvironment{minted}[2][]{%
  \lstset{style=code,language=#2,#1}\begin{lstlisting}%
}{\end{lstlisting}}

% Fallback for \inputminted (ignore file; keep build unbroken)
\newcommand{\inputminted}[3][]{\begin{lstlisting}\end{lstlisting}}

% ---------- Bulleted lines (no tables) ----------
\newlist{bullets}{itemize}{1}
\setlist[bullets]{label=--,leftmargin=1.6em,itemsep=2pt,topsep=4pt}

% ---------- Variable mapping (lines, no tables) ----------
\newcommand{\varmapStart}{\textbf{VARIABLE MAPPING:}\par\begin{bullets}}
\newcommand{\var}[2]{\item $#1$ — #2}
\newcommand{\varmapEnd}{\end{bullets}}

% ---------- Glossary item with ELI5 and Pitfall/Example ----------
\newcommand{\glossx}[6]{%
  \textbf{#1}\par
  \begin{bullets}
    \item \textbf{What:} #2
    \item \textbf{Why:} #3
    \item \textbf{How:} #4
    \item \textbf{ELI5:} #5
    \item \textbf{Pitfall/Example:} #6
  \end{bullets}
}

% ---------- Theorem structures ----------
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{claim}[theorem]{Claim}
% ---------- Code blocks ----------
\lstdefinestyle{code}{
  basicstyle=\ttfamily\footnotesize,
  backgroundcolor=\color{black!02},
  frame=single,
  numbers=left, numberstyle=\tiny, numbersep=8pt,
  breaklines=true, breakatwhitespace=true,
  tabsize=4, showstringspaces=false,
  upquote=true, keepspaces=true, columns=fullflexible,
  % Unicode safety: map common symbols so listings never chokes
  literate=
    {–}{{-}}1
    {—}{{-}}1
    {…}{{...}}1
    {≤}{{\ensuremath{\le}}}1
    {≥}{{\ensuremath{\ge}}}1
    {≠}{{\ensuremath{\ne}}}1
    {≈}{{\ensuremath{\approx}}}1
    {±}{{\ensuremath{\pm}}}1
    {→}{{\ensuremath{\to}}}1
    {←}{{\ensuremath{\leftarrow}}}1
    {∞}{{\ensuremath{\infty}}}1
    {√}{{\ensuremath{\sqrt{\ }}}}1
    {×}{{\ensuremath{\times}}}1
    {÷}{{\ensuremath{\div}}}1
}

% Main code environment for all Python blocks
\lstnewenvironment{codepy}[1][]%
  {\lstset{style=code,language=Python,#1}}%
  {}

% Inline code; change delimiters if your snippet contains '!'
\newcommand{\inlinecode}[1]{\lstinline[style=code]!#1!}

% ---------- Line-label macros ----------
\newcommand{\LF}[2]{\par\noindent\textbf{#1:}~#2\par}
\newcommand{\WHAT}[1]{\LF{WHAT}{#1}}
\newcommand{\WHY}[1]{\LF{WHY}{#1}}
\newcommand{\HOW}[1]{\LF{HOW}{#1}}
\newcommand{\ELI}[1]{\LF{ELI5}{#1}}
\newcommand{\SCOPE}[1]{\LF{SCOPE}{#1}}
\newcommand{\CONFUSIONS}[1]{\LF{COMMON CONFUSIONS}{#1}}
\newcommand{\APPLICATIONS}[1]{\LF{APPLICATIONS}{#1}}
\newcommand{\FORMULA}[1]{\LF{FORMULA}{#1}}
\newcommand{\CANONICAL}[1]{\LF{CANONICAL FORM}{#1}}
\newcommand{\PRECONDS}[1]{\LF{PRECONDITIONS}{#1}}
\newcommand{\DERIVATION}[1]{\LF{DERIVATION}{#1}}
\newcommand{\EQUIV}[1]{\LF{EQUIVALENT FORMS}{#1}}
\newcommand{\LIMITS}[1]{\LF{LIMIT CASES}{#1}}
\newcommand{\INPUTS}[1]{\LF{INPUTS}{#1}}
\newcommand{\OUTPUTS}[1]{\LF{OUTPUTS}{#1}}
\newcommand{\RESULT}[1]{\LF{RESULT}{#1}}
\newcommand{\INTUITION}[1]{\LF{INTUITION}{#1}}
\newcommand{\PITFALLS}[1]{\LF{PITFALLS}{#1}}
\newcommand{\MODEL}[1]{\LF{CANONICAL MATH MODEL}{#1}}
\newcommand{\ASSUMPTIONS}[1]{\LF{ASSUMPTIONS}{#1}}
\newcommand{\WHICHFORMULA}[1]{\LF{WHICH FORMULA \& WHY}{#1}}
\newcommand{\GOVERN}[1]{\LF{GOVERNING EQUATION(S)}{#1}}
\newcommand{\UNITCHECK}[1]{\LF{UNIT CHECK}{#1}}
\newcommand{\EDGECASES}[1]{\LF{EDGE CASES}{#1}}
\newcommand{\ALTERNATE}[1]{\LF{ALTERNATE APPROACH (sketch)}{#1}}
\newcommand{\PROBLEM}[1]{\LF{PROBLEM}{#1}}
\newcommand{\API}[1]{\LF{API}{#1}}
\newcommand{\COMPLEXITY}[1]{\LF{COMPLEXITY}{#1}}
\newcommand{\FAILMODES}[1]{\LF{FAILURE MODES}{#1}}
\newcommand{\STABILITY}[1]{\LF{NUMERICAL STABILITY}{#1}}
\newcommand{\VALIDATION}[1]{\LF{VALIDATION}{#1}}
\newcommand{\EXPLANATION}[1]{\LF{EXPLANATION}{#1}}
\newcommand{\SCENARIO}[1]{\LF{SCENARIO}{#1}}
\newcommand{\PIPELINE}[1]{\LF{PIPELINE STEPS}{#1}}
\newcommand{\METRICS}[1]{\LF{METRICS}{#1}}
\newcommand{\INTERPRET}[1]{\LF{INTERPRETATION}{#1}}
\newcommand{\NEXTSTEPS}[1]{\LF{LIMITATIONS \& NEXT STEPS}{#1}}

% ---------- Section formatting ----------
\usepackage{titlesec}
\titleformat{\section}{\Large\bfseries}{\thesection}{0.6em}{}
\titlespacing*{\section}{0pt}{*2}{*1}
\usepackage{etoolbox}
\pretocmd{\section}{\clearpage}{}{}

% ---------- Page helpers ----------
\newcommand{\FormulaPage}[2]{%
  \clearpage
  \section*{Formula #1 — #2}%
  \addcontentsline{toc}{section}{Formula #1 — #2}%
}
\newcommand{\ProblemPage}[2]{%
  \clearpage
  \subsection*{Problem #1: #2}%
  \addcontentsline{toc}{subsection}{Problem #1: #2}%
}
\newcommand{\CodeDemoPage}[1]{%
  \clearpage
  \subsection*{Coding Demo: #1}%
  \addcontentsline{toc}{subsection}{Coding Demo: #1}%
}
\newcommand{\DomainPage}[1]{%
  \clearpage
  \subsection*{#1 (End-to-End)}%
  \addcontentsline{toc}{subsection}{#1 (End-to-End)}%
}

\begin{document}
\title{Comprehensive Study Sheet — Matrix Inequalities and Positive Semidefinite Order}
\date{\today}
\maketitle
\tableofcontents
\clearpage

\section{Concept Overview}
\WHAT{
Let $\mathbb{S}^n$ be the vector space of real symmetric $n\times n$ matrices
with inner product $\langle A,B\rangle=\mathrm{tr}(A^\top B)=\mathrm{tr}(AB)$.
For $A\in\mathbb{S}^n$, $A$ is positive semidefinite (PSD), written $A\succeq 0$,
iff $x^\top A x\ge 0$ for all $x\in\mathbb{R}^n$. Positive definite (PD) means
$x^\top A x>0$ for all nonzero $x$ and is written $A\succ 0$.
The Loewner (semidefinite) order on $\mathbb{S}^n$ is defined by
$A\succeq B$ iff $A-B\succeq 0$.
}
\WHY{
The PSD order is the canonical partial order compatible with quadratic forms and
congruence. It underlies eigenvalue bounds, covariance dominance, Lyapunov
stability, convex constraints in semidefinite programming, Schur complements,
and operator inequalities such as order-reversing inversion. It enables
reasoning about energy, variance, and risk with linear-algebraic tools.
}
\HOW{
1. Assume real symmetric (or complex Hermitian) matrices and invoke the spectral
theorem to diagonalize them.
2. Use quadratic forms and eigenvalues to characterize PSD: $A\succeq 0$ iff
$\lambda_i(A)\ge 0$ for all $i$.
3. Deduce order properties under congruence ($X^\top A X\succeq 0$) and
monotonicity of trace, Rayleigh quotient bounds, and Schur complement tests.
4. Assemble computational forms: Cholesky factorization, eigenvalue tests,
and block-factorization identities for deriving and checking inequalities.
}
\ELI{
Think of $x^\top A x$ as the energy measured by $A$. Saying $A\succeq B$ means
$A$ never assigns less energy than $B$ to any vector. PSD matrices are rulers
that never give negative energy. Congruence changes coordinates of the ruler
without changing its nonnegativity. Inversion flips the ruler: the larger the
matrix in PSD order, the smaller its inverse.
}
\SCOPE{
Domain: real symmetric (or complex Hermitian) matrices. The PSD order is a
partial order, not total: many pairs are incomparable. Non-symmetric matrices
are outside the order. PD is required for inversion and Schur complements in
their simplest forms. Singular PSD matrices have nullspaces where quadratic
forms vanish and require pseudoinverses in block conditions.
}
\CONFUSIONS{
Loewner order vs. elementwise inequalities; PSD vs. entrywise nonnegative;
PSD vs. PD; $A\succeq B$ vs. $B\succeq A$; eigenvalue-wise ordering
(componentwise eigenvalues) vs. Loewner order; convex cone $\mathbb{S}_+^n$
vs. set of correlation matrices (a subset with unit diagonal).
}
\APPLICATIONS{
\begin{bullets}
\item Mathematical foundations: spectral theorem, convex cones, dual cones.
\item Computational modeling: semidefinite programs and linear matrix inequalities.
\item Physical/engineering: energy inequalities, Lyapunov stability, passivity.
\item Statistics/ML: covariance dominance, kernel Gram matrices, Fisher information.
\end{bullets}
}
\textbf{ANALYTIC STRUCTURE.}
$\mathbb{S}_+^n:=\{A\succeq 0\}$ is a closed, convex, self-dual cone in
$\mathbb{S}^n$ under the trace inner product. The order is compatible with
congruence: $A\succeq B\Rightarrow X^\top A X\succeq X^\top B X$. The set of PD
matrices is open and forms a Riemannian manifold under the affine-invariant
metric. Many functions are operator monotone/convex over $\mathbb{S}_{++}^n$.

\textbf{CANONICAL LINKS.}
Key ties: spectral theorem (diagonalization), Rayleigh quotient bounds,
Schur complement characterization, order-reversing inversion, and trace
monotonicity under PSD weighting. These feed into block-matrix LMIs and
semidefinite optimization.

\textbf{PROBLEM-TYPE RECOGNITION HEURISTICS.}
\begin{bullets}
\item Words: covariance, variance, energy, stability, Lyapunov, LMI, Schur.
\item Structures: $x^\top A x$, $X^\top A X$, block matrices, inverses of PD.
\item Signatures: require $A\succeq 0$ or $M\succeq 0$; compare $A$ and $B$.
\item Patterns: add $\lambda I$ to regularize; complete the square blockwise.
\end{bullets}

\textbf{SOLUTION STRATEGY BLUEPRINT.}
\begin{bullets}
\item Translate to quadratic forms or block matrices.
\item Identify spectral or Schur complement tests.
\item Apply congruence to simplify; diagonalize if helpful.
\item Use order lemmas (trace monotone, inversion reversing).
\item Interpret via energy/variance and verify by bounds/eigenvalues.
\end{bullets}

\textbf{CONCEPTUAL INVARIANTS.}
Invariants under congruence: PSD-ness; sign of quadratic forms; eigenvalue
nonnegativity; trace of PSD is nonnegative; Loewner order preserved by addition.

\textbf{EDGE INTUITION.}
As $\lambda\to 0^+$ in $A+\lambda I$, the matrix approaches singular PSD and
quadratic form vanishes along a growing nullspace. As $\lambda\to\infty$,
$A+\lambda I$ dominates any fixed matrix in the PSD order and eigenvalues
grow unboundedly; inverses shrink to zero.

\clearpage
\section{Glossary}
\glossx{Positive Semidefinite (PSD) Matrix}
{A symmetric matrix $A$ with $x^\top A x\ge 0$ for all $x$.}
{Defines the Loewner order and models nonnegative energy/variance.}
{Check via eigenvalues $\lambda_i(A)\ge 0$ or Cholesky factorization.}
{A bowl that never curves downward; rolling ball never gains negative energy.}
{Example: covariance matrices are PSD. Pitfall: entrywise nonnegativity does
not imply PSD.}

\glossx{Loewner (Semidefinite) Order}
{Partial order on $\mathbb{S}^n$: $A\succeq B$ iff $A-B\succeq 0$.}
{Enables comparing quadratic forms, enforcing LMIs, and proving inequalities.}
{Rewrite constraints as PSD of a difference; use spectral or Schur tests.}
{A is a stronger ruler than B: it never underestimates energy.}
{Pitfall: not total; two matrices can be incomparable even with same traces.}

\glossx{Schur Complement}
{For block $M=\begin{bmatrix}A&B\\B^\top&C\end{bmatrix}$ with $A\succ 0$,
$S=C-B^\top A^{-1}B$.}
{Characterizes $M\succeq 0$ via $S\succeq 0$; central to block LMIs.}
{Complete the square/congruence to eliminate $A$; test $S\succeq 0$.}
{Remove the effect of $A$ to see the residual curvature in $C$.}
{Pitfall: $A$ must be PD; otherwise use the Moore--Penrose pseudoinverse and
range conditions.}

\glossx{Rayleigh Quotient}
{$R_A(x)=\dfrac{x^\top A x}{x^\top x}$ for $x\ne 0$.}
{Bounds between $\lambda_{\min}(A)$ and $\lambda_{\max}(A)$ for $A\in\mathbb{S}^n$.}
{Diagonalize $A=Q\Lambda Q^\top$, write quotient in eigenbasis, bound by extrema.}
{How steep is the bowl in the direction you walk.}
{Pitfall: use unit vectors or divide by $x^\top x$; otherwise scaling misleads.}

\clearpage
\section{Symbol Ledger}
\varmapStart
\var{\mathbb{S}^n}{space of real symmetric $n\times n$ matrices.}
\var{\mathbb{S}_+^n}{cone of PSD matrices in $\mathbb{S}^n$.}
\var{\mathbb{S}_{++}^n}{cone of PD matrices.}
\var{A,B,C}{symmetric matrices; typical elements of $\mathbb{S}^n$.}
\var{M}{block matrix in $\mathbb{S}^{n+m}$.}
\var{S}{Schur complement $C-B^\top A^{-1}B$.}
\var{X}{rectangular matrix used in congruence $X^\top A X$.}
\var{I_n}{identity matrix of size $n$.}
\var{\lambda_{\min}(A)}{smallest eigenvalue of $A\in\mathbb{S}^n$.}
\var{\lambda_{\max}(A)}{largest eigenvalue of $A\in\mathbb{S}^n$.}
\var{Q\Lambda Q^\top}{spectral decomposition with $Q$ orthogonal, $\Lambda$ diagonal.}
\var{\mathrm{tr}(A)}{trace of $A$; sum of diagonal entries.}
\var{\langle A,B\rangle}{Frobenius inner product $\mathrm{tr}(A^\top B)$.}
\var{x}{vector in $\mathbb{R}^n$.}
\var{w}{vector of portfolio weights in finance problems.}
\var{\Sigma}{covariance matrix; PSD.}
\var{K}{kernel Gram matrix; PSD.}
\var{\alpha}{shrinkage parameter in $[0,1]$.}
\var{\delta}{regularization parameter $\delta>0$.}
\varmapEnd

\clearpage
\section{Formula Canon — One Formula Per Page}

\FormulaPage{1}{PSD Characterizations and Cone Properties}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
A symmetric matrix $A\in\mathbb{S}^n$ is PSD iff any of the equivalent
conditions hold: (i) $x^\top A x\ge 0$ for all $x$; (ii) all eigenvalues of
$A$ are nonnegative; (iii) there exists $R$ with $A=R^\top R$.

\WHAT{
Characterizes $\mathbb{S}_+^n$ via quadratic forms, spectra, and factorizations.
}
\WHY{
Provides multiple testable criteria and constructive representations used in
analysis and computation (eigenvalue test, Cholesky factorization).
}
\FORMULA{
\[
A\succeq 0 \quad\Longleftrightarrow\quad
\begin{cases}
x^\top A x\ge 0,~\forall x\in\mathbb{R}^n,\\[4pt]
A=Q\Lambda Q^\top,~\Lambda=\mathrm{diag}(\lambda_i),~\lambda_i\ge 0,\\[4pt]
\exists R\in\mathbb{R}^{k\times n}:~A=R^\top R.
\end{cases}
\]
}
\CANONICAL{
Domain $\mathbb{S}^n$ with real symmetry (Hermitian in complex case).
Eigen-decomposition with $Q$ orthogonal and $\Lambda$ real diagonal.
}
\PRECONDS{
\begin{bullets}
\item Symmetry: $A=A^\top$.
\item For $A=R^\top R$, $R$ can be taken as Cholesky factor when $A\succ 0$.
\item Spectral theorem applicability in $\mathbb{R}$ or $\mathbb{C}$.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
(Spectral theorem) For $A\in\mathbb{S}^n$, there exists $Q$ orthogonal and
$\Lambda$ real diagonal such that $A=Q\Lambda Q^\top$.
\end{lemma}
\begin{proof}
Standard result: real symmetric matrices are diagonalizable with an orthonormal
basis of eigenvectors; assemble them as columns of $Q$, with eigenvalues on
$\Lambda$. Then $Q^\top Q=I$ and $A=Q\Lambda Q^\top$. \qedhere
\end{proof}
\DERIVATION{
\begin{align*}
\text{(i)\ $\Rightarrow$ (ii):}\quad
& A=Q\Lambda Q^\top,\; x^\top A x=(Q^\top x)^\top \Lambda (Q^\top x)\\
&= \sum_{i=1}^n \lambda_i y_i^2\ \ge 0\ \forall y,\ \Rightarrow\ \lambda_i\ge 0.\\[4pt]
\text{(ii)\ $\Rightarrow$ (i):}\quad
& \lambda_i\ge 0 \Rightarrow \sum_i \lambda_i y_i^2\ge 0 \Rightarrow x^\top A x\ge 0.\\[4pt]
\text{(ii)\ $\Rightarrow$ (iii):}\quad
& \Lambda=\Lambda_+^{1/2}\Lambda_+^{1/2},\ \Lambda_+^{1/2}=\mathrm{diag}(\sqrt{\lambda_i}).\\
& A=Q\Lambda Q^\top=(\Lambda_+^{1/2}Q^\top)^\top(\Lambda_+^{1/2}Q^\top)=R^\top R.\\[4pt]
\text{(iii)\ $\Rightarrow$ (i):}\quad
& x^\top A x=x^\top R^\top R x=\|R x\|_2^2\ge 0.
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Check symmetry; diagonalize and inspect eigenvalues.
\item Attempt Cholesky; failure indicates non-PSD or singularity.
\item Represent $A=R^\top R$ to rewrite $x^\top A x=\|R x\|^2$.
\end{bullets}
\EQUIV{
\begin{bullets}
\item $A\succeq 0 \iff \exists B\succeq 0$ with $A=B^2$ (matrix square root).
\item $A\succeq 0 \iff \langle A, X^\top X\rangle\ge 0$ for all $X$.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item If some $\lambda_i=0$, $A$ is singular PSD; quadratic form vanishes on
$\mathrm{null}(A)$.
\item If any $\lambda_i<0$, $A$ is indefinite; not in $\mathbb{S}_+^n$.
\end{bullets}
}
\INPUTS{$A\in\mathbb{S}^n$.}
\DERIVATION{
\begin{align*}
\text{Example: }A&=\begin{bmatrix}2&1\\1&2\end{bmatrix},\ 
\lambda=\{3,1\}\Rightarrow A\succeq 0.\\
R&=\begin{bmatrix}\sqrt{2}&0\\ \tfrac{1}{\sqrt{2}}&\tfrac{\sqrt{3}}{\sqrt{2}}\end{bmatrix}
\Rightarrow R^\top R=A.
\end{align*}
}
\RESULT{
$A$ is PSD, with nonnegative eigenvalues and a factorization $A=R^\top R$.}
\UNITCHECK{
Symmetry preserved; eigenvalues nonnegative; quadratic forms nonnegative.}
\PITFALLS{
\begin{bullets}
\item Testing only principal minors is necessary and sufficient for PD, not PSD.
\item Entrywise nonnegativity does not guarantee PSD.
\end{bullets}
}
\INTUITION{
Nonnegative curvature in every direction means the matrix is a sum of rank-one
outer products weighted by nonnegative scalars.}
\CANONICAL{
\begin{bullets}
\item $\mathbb{S}_+^n$ is a closed, convex, self-dual cone: $A\succeq 0
\iff \langle A,B\rangle\ge 0$ for all $B\succeq 0$.
\end{bullets}
}

\FormulaPage{2}{Congruence Monotonicity and Rayleigh Bounds}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
If $A\succeq B$ and $X$ is any matrix, then $X^\top A X\succeq X^\top B X$.
For $A\in\mathbb{S}^n$, $\lambda_{\min}(A)\|x\|^2\le x^\top A x
\le \lambda_{\max}(A)\|x\|^2$.

\WHAT{
How PSD order propagates under congruence and how quadratic forms are bounded by
extreme eigenvalues (Rayleigh quotient bounds).
}
\WHY{
Congruence invariance under coordinate transforms underpins Schur complements
and Lyapunov inequalities. Rayleigh bounds quantify extremal energies and
condition numbers.
}
\FORMULA{
\[
A\succeq B \ \Rightarrow\ X^\top(A-B)X\succeq 0,
\quad
\lambda_{\min}(A)\le R_A(x)\le \lambda_{\max}(A).
\]
}
\CANONICAL{
$A,B\in\mathbb{S}^n$, $X\in\mathbb{R}^{n\times m}$ arbitrary; eigenvalues real.
}
\PRECONDS{
\begin{bullets}
\item Symmetry of $A,B$.
\item Eigenvalue bounds assume real eigen-decomposition.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
If $M\succeq 0$, then $X^\top M X\succeq 0$ for any $X$.
\end{lemma}
\begin{proof}
For any $y$, $y^\top X^\top M X y=(X y)^\top M (X y)\ge 0$ by PSD of $M$.
Thus $X^\top M X$ is PSD. \qedhere
\end{proof}
\DERIVATION{
\begin{align*}
\text{Congruence: }&A\succeq B\Rightarrow M:=A-B\succeq 0.\\
&\Rightarrow X^\top M X\succeq 0 \Rightarrow X^\top A X\succeq X^\top B X.\\[6pt]
\text{Rayleigh: }&A=Q\Lambda Q^\top,\ y=Q^\top x.\\
&R_A(x)=\frac{x^\top A x}{x^\top x}
=\frac{\sum_i \lambda_i y_i^2}{\sum_i y_i^2}.\\
&\min_i \lambda_i \le R_A(x)\le \max_i \lambda_i.
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Reduce inequalities to $M\succeq 0$ and apply congruence with $X$.
\item Diagonalize to bound forms; read off $\lambda_{\min},\lambda_{\max}$.
\end{bullets}
\EQUIV{
\begin{bullets}
\item $A\succeq B \iff y^\top A y\ge y^\top B y$ for all $y$.
\item $X$ full column rank: $A\succeq B \iff X^\top A X\succeq X^\top B X$
for all such $X$.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item If $X=0$, inequality is trivial. If $A=B$, equalities hold.
\item If $A$ indefinite, Rayleigh interval crosses zero.
\end{bullets}
}
\INPUTS{$A,B\in\mathbb{S}^n$, $X\in\mathbb{R}^{n\times m}$.}
\DERIVATION{
\begin{align*}
\text{Example: }&A=\begin{bmatrix}3&1\\1&1\end{bmatrix},\ 
B=\begin{bmatrix}1&0\\0&0\end{bmatrix}.\\
&A-B=\begin{bmatrix}2&1\\1&1\end{bmatrix}\succeq 0.\\
&X=\begin{bmatrix}1\\2\end{bmatrix}\Rightarrow
X^\top A X= (1,2)AX= (1,2)\begin{bmatrix}5\\3\end{bmatrix}=11.\\
&X^\top B X= (1,2)BX=(1,2)\begin{bmatrix}1\\0\end{bmatrix}=1,\ \ 11\ge 1.
\end{align*}
}
\RESULT{
Congruence preserves PSD order; quadratic forms are bounded by extremal
eigenvalues.}
\UNITCHECK{
Dimensions match; scalars on both sides; eigenvalue bounds are tight at
eigenvectors.}
\PITFALLS{
\begin{bullets}
\item Confusing similarity ($S^{-1}AS$) with congruence ($X^\top A X$).
\item Forgetting to require symmetry before applying Rayleigh bounds.
\end{bullets}
}
\INTUITION{
Congruence is change of coordinates; nonnegativity persists. Rayleigh bounds
say the bowl is never steeper than its steepest axis nor flatter than its
flattest axis.}
\CANONICAL{
\begin{bullets}
\item PSD cone is invariant under $A\mapsto X^\top A X$.
\item Rayleigh quotient lives in the convex hull of eigenvalues.
\end{bullets}
}

\FormulaPage{3}{Schur Complement Test for Block PSD}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
For $M=\begin{bmatrix}A&B\\B^\top&C\end{bmatrix}\in\mathbb{S}^{n+m}$ with
$A\succ 0$, the following are equivalent:
$M\succeq 0 \iff A\succ 0$ and $S:=C-B^\top A^{-1}B\succeq 0$.

\WHAT{
A necessary and sufficient condition for block PSD via a smaller PSD test.}
\WHY{
Central to linear matrix inequalities, conditioning, Gaussian conditioning,
and eliminating variables in convex constraints.}
\FORMULA{
\[
\begin{bmatrix}A&B\\B^\top&C\end{bmatrix}\succeq 0
\quad\Longleftrightarrow\quad
A\succ 0,\ \ S=C-B^\top A^{-1}B\succeq 0.
\]
}
\CANONICAL{
Block sizes: $A\in\mathbb{S}_{++}^n$, $C\in\mathbb{S}^m$, $B\in\mathbb{R}^{n\times m}$.
}
\PRECONDS{
\begin{bullets}
\item $A\succ 0$ to ensure $A^{-1}$ exists.
\item Symmetry of $M$; $C=C^\top$.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
(Completion of squares) For $A\succ 0$ and any $x,z$,
\[
\begin{bmatrix}x\\z\end{bmatrix}^\top
\begin{bmatrix}A&B\\B^\top&C\end{bmatrix}
\begin{bmatrix}x\\z\end{bmatrix}
=(x+A^{-1}B z)^\top A (x+A^{-1}B z)+z^\top S z.
\]
\end{lemma}
\begin{proof}
Expand the right-hand side:
\begin{align*}
&(x+A^{-1}B z)^\top A (x+A^{-1}B z)+z^\top(C-B^\top A^{-1}B)z\\
&=x^\top A x+2 x^\top B z+z^\top B^\top A^{-1}A A^{-1}B z
+z^\top C z- z^\top B^\top A^{-1}B z\\
&=x^\top A x+2 x^\top B z+z^\top C z,
\end{align*}
which equals the left-hand quadratic form. \qedhere
\end{proof}
\DERIVATION{
\begin{align*}
&\text{($\Rightarrow$)}\ M\succeq 0\Rightarrow A\succ 0\ \text{(principal minor).}\\
&\forall z,\ \min_x \begin{bmatrix}x\\z\end{bmatrix}^\top M \begin{bmatrix}x\\z\end{bmatrix}
= \min_x (x+A^{-1}B z)^\top A (x+A^{-1}B z)+ z^\top S z\\
&= z^\top S z\ \ge 0\Rightarrow S\succeq 0.\\[4pt]
&\text{($\Leftarrow$)}\ A\succ 0,\ S\succeq 0\Rightarrow
(x+A^{-1}B z)^\top A (x+A^{-1}B z)\ge 0,\ z^\top S z\ge 0\\
&\Rightarrow \begin{bmatrix}x\\z\end{bmatrix}^\top M \begin{bmatrix}x\\z\end{bmatrix}\ge 0
\ \forall x,z\Rightarrow M\succeq 0.
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Check $A\succ 0$ (Cholesky).
\item Compute $S=C-B^\top A^{-1}B$ and test PSD (eigenvalues or Cholesky).
\item Symmetrically, if $C\succ 0$, test $A-B C^{-1}B^\top\succeq 0$.
\end{bullets}
\EQUIV{
\begin{bullets}
\item If $C\succ 0$, $M\succeq 0 \iff C\succ 0$ and $A-B C^{-1}B^\top\succeq 0$.
\item For $A\succeq 0$ (singular), use $S=C-B^\top A^{\dagger}B\succeq 0$ and
$\mathrm{range}(B)\subseteq \mathrm{range}(A)$.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item If $A$ is singular, inversion is invalid; use pseudoinverse and range
conditions.
\item If $B=0$, condition reduces to $A\succ 0$ and $C\succeq 0$.
\end{bullets}
}
\INPUTS{$A\in\mathbb{S}_{++}^n$, $B\in\mathbb{R}^{n\times m}$, $C\in\mathbb{S}^m$.}
\DERIVATION{
\begin{align*}
\text{Example: }&A=\begin{bmatrix}2&0\\0&1\end{bmatrix},\
B=\begin{bmatrix}1\\1\end{bmatrix},\
C=[2].\\
&S=2-\begin{bmatrix}1&1\end{bmatrix}
\begin{bmatrix}\tfrac12&0\\0&1\end{bmatrix}
\begin{bmatrix}1\\1\end{bmatrix}=2-(\tfrac12+1)=\tfrac12\succeq 0.\\
&M=\begin{bmatrix}2&0&1\\0&1&1\\1&1&2\end{bmatrix}\succeq 0.
\end{align*}
}
\RESULT{
$M\succeq 0$ iff $A\succ 0$ and its Schur complement $S\succeq 0$.}
\UNITCHECK{
All blocks compatible; $S$ is $m\times m$ symmetric; PSD checks coherent.}
\PITFALLS{
\begin{bullets}
\item Forgetting symmetry of $M$; using $A^{-1}$ when $A$ is singular.
\item Mixing the two complementary tests without PD assumptions.
\end{bullets}
}
\INTUITION{
Eliminate $x$ optimally; the remaining curvature in $z$ must be nonnegative.}
\CANONICAL{
\begin{bullets}
\item Block PSD is equivalent to PSD of the Schur complement under PD pivot.
\end{bullets}
}

\FormulaPage{4}{Order-Reversing Property of Inversion}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
For $A,B\in\mathbb{S}_{++}^n$, $A\succeq B \iff A^{-1}\preceq B^{-1}$.

\WHAT{
Inversion reverses the Loewner order on positive definite matrices.}
\WHY{
Crucial in sensitivity, conditioning, information inequality, and bounds on
resolvents and preconditioners.}
\FORMULA{
\[
0\prec B\preceq A \quad\Longleftrightarrow\quad A^{-1}\preceq B^{-1}.
\]
}
\CANONICAL{
PD matrices with unique symmetric square roots and inverses.}
\PRECONDS{
\begin{bullets}
\item $A,B\in\mathbb{S}_{++}^n$.
\item Existence of $B^{\pm 1/2}$.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
If $0\prec M\preceq N$, then $I\preceq N^{-1/2} M N^{-1/2}$ implies
$N^{1/2} M^{-1} N^{1/2}\preceq I$.
\end{lemma}
\begin{proof}
$0\prec M\preceq N\Rightarrow N^{-1/2} M N^{-1/2}\preceq I$ by congruence.
The function $t\mapsto t^{-1}$ reverses order on $(0,\infty)$, applied to
the spectrum of the PD matrix $N^{-1/2} M N^{-1/2}$:
\[
\left(N^{-1/2} M N^{-1/2}\right)^{-1}\succeq I.
\]
Congruence by $N^{1/2}$ yields $N^{1/2} M^{-1} N^{1/2}\succeq I$ equivalently
$N^{1/2} M^{-1} N^{1/2}\preceq I$ after inverting both sides is incorrect;
instead rewrite as $M^{-1}\succeq N^{-1}$ directly:
\[
\left(N^{-1/2} M N^{-1/2}\right)^{-1}
=N^{1/2} M^{-1} N^{1/2}\succeq I
\ \Leftrightarrow\ M^{-1}\succeq N^{-1}.
\]
\qedhere
\end{proof}
\DERIVATION{
\begin{align*}
&0\prec B\preceq A\ \Rightarrow\ A=B^{1/2} T B^{1/2}\ \text{for some }T\succeq I.\\
&\text{Indeed }T=B^{-1/2} A B^{-1/2}\succeq I\ \text{by congruence.}\\
&\Rightarrow A^{-1}=B^{-1/2} T^{-1} B^{-1/2}\preceq B^{-1/2} I B^{-1/2}=B^{-1}.\\
&\text{Conversely, }A^{-1}\preceq B^{-1}\Rightarrow B\preceq A\ \text{by the same steps.}
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Normalize by $B^{\pm 1/2}$ to reduce to $T\succeq I$.
\item Apply functional calculus to invert $T$ and restore by congruence.
\end{bullets}
\EQUIV{
\begin{bullets}
\item $A\preceq B \iff B^{-1/2} A B^{-1/2}\preceq I$.
\item $A\preceq B \iff \lambda_{\max}(B^{-1}A)\le 1$.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item Requires PD; fails for singular PSD (inverse undefined).
\item Strictness preserved: $B\prec A \Rightarrow A^{-1}\prec B^{-1}$.
\end{bullets}
}
\INPUTS{$A,B\in\mathbb{S}_{++}^n$.}
\DERIVATION{
\begin{align*}
\text{Example: }&A=\begin{bmatrix}3&0\\0&1\end{bmatrix},\
B=\begin{bmatrix}2&0\\0&1\end{bmatrix}.\\
&B\preceq A,\ A^{-1}=\begin{bmatrix}\tfrac13&0\\0&1\end{bmatrix}\preceq
B^{-1}=\begin{bmatrix}\tfrac12&0\\0&1\end{bmatrix}.
\end{align*}
}
\RESULT{
$A\succeq B$ if and only if $A^{-1}\preceq B^{-1}$ for PD matrices.}
\UNITCHECK{
Consistent sizes; inequalities preserved under congruence and inversion.}
\PITFALLS{
\begin{bullets}
\item Attempting to invert singular PSD matrices.
\item Forgetting that inversion reverses, not preserves, the order.
\end{bullets}
}
\INTUITION{
Normalize so that $B$ becomes $I$. Being larger than $I$ means being no larger
than $I$ after inversion.}
\CANONICAL{
\begin{bullets}
\item Affine-invariant: inequality is preserved under congruence by inverses.
\end{bullets}
}

\FormulaPage{5}{Trace Monotonicity Under PSD Weighting}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
If $A\succeq B$ and $C\succeq 0$, then $\mathrm{tr}(C A)\ge \mathrm{tr}(C B)$.
In particular, $A,B\succeq 0\Rightarrow \mathrm{tr}(AB)\ge 0$.

\WHAT{
Monotonicity of the Frobenius inner product with respect to the PSD cone.}
\WHY{
Used in duality of semidefinite programs, risk comparisons, and variance bounds.}
\FORMULA{
\[
A\succeq B,\ C\succeq 0 \ \Longrightarrow\ \langle C,A-B\rangle\ge 0.
\]
}
\CANONICAL{
Trace inner product $\langle X,Y\rangle=\mathrm{tr}(X^\top Y)$ on $\mathbb{S}^n$.}
\PRECONDS{
\begin{bullets}
\item Symmetry of $A,B,C$.
\item $C^{1/2}$ exists (PSD).
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
If $M\succeq 0$, then $\mathrm{tr}(M)\ge 0$.
\end{lemma}
\begin{proof}
$M=Q\Lambda Q^\top$ with $\Lambda\succeq 0$. Then
$\mathrm{tr}(M)=\mathrm{tr}(\Lambda)=\sum_i \lambda_i\ge 0$. \qedhere
\end{proof}
\DERIVATION{
\begin{align*}
&A\succeq B\Rightarrow A-B\succeq 0.\\
&\mathrm{tr}(C(A-B))=\mathrm{tr}(C^{1/2}(A-B)C^{1/2})\ \ (\text{cyclicity}).\\
&\text{Since }C^{1/2}(A-B)C^{1/2}\succeq 0 \text{ by congruence, }\\
&\mathrm{tr}(C(A-B))\ge 0.\\
&\text{For }A,B\succeq 0,\ \mathrm{tr}(AB)=\mathrm{tr}(A^{1/2} B A^{1/2})\ge 0.
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Reduce to tracing a PSD matrix via $C^{1/2}$.
\item Use cyclicity to move factors; apply PSD trace nonnegativity.
\end{bullets}
\EQUIV{
\begin{bullets}
\item Self-duality: $\mathbb{S}_+^n=\{X:\langle X,Y\rangle\ge 0,\ \forall Y\succeq 0\}$.
\item If $0\preceq C\preceq D$ and $A\succeq B$, then $\mathrm{tr}(C A)\ge \mathrm{tr}(C B)$
and $\mathrm{tr}(D A)\ge \mathrm{tr}(D B)$.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item If $C$ is indefinite, the inequality may fail.
\item Equality can hold for nonzero $A-B$ if $C^{1/2}(A-B)C^{1/2}$ is traceless
with zero eigenvalues, e.g., orthogonal supports.
\end{bullets}
}
\INPUTS{$A,B,C\in\mathbb{S}^n$, with $A\succeq B$, $C\succeq 0$.}
\DERIVATION{
\begin{align*}
\text{Example: }&A=\begin{bmatrix}2&1\\1&1\end{bmatrix},\
B=\begin{bmatrix}1&0\\0&0\end{bmatrix},\
C=\begin{bmatrix}3&0\\0&1\end{bmatrix}.\\
&\mathrm{tr}(C A)=3\cdot 2+1\cdot 1=7,\ \mathrm{tr}(C B)=3\cdot 1+1\cdot 0=3.\\
&7\ge 3.
\end{align*}
}
\RESULT{
Trace is monotone with respect to PSD order under PSD weighting; inner product
with a PSD matrix is nonnegative on $\mathbb{S}_+^n$.}
\UNITCHECK{
Traces are scalars; dimensions compatible; PSD preserved under congruence.}
\PITFALLS{
\begin{bullets}
\item Misusing $\mathrm{tr}(XYZ)=\mathrm{tr}(ZXY)$ without matching sizes.
\item Assuming $\mathrm{tr}(AB)\ge 0$ when $A$ or $B$ is indefinite.
\end{bullets}
}
\INTUITION{
Weight the energy difference $A-B$ by a PSD lens $C$; the total weighted energy
cannot be negative.}
\CANONICAL{
\begin{bullets}
\item Dual cone identity: $\mathbb{S}_+^n$ is self-dual under $\langle\cdot,\cdot\rangle$.
\end{bullets}
}

\clearpage
\section{10 Exhaustive Problems and Solutions}

\ProblemPage{1}{Equivalences for PSD and Construction of Factors}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Prove the equivalence of PSD characterizations and construct $R$ for a given
$A$; verify via a numeric example.

\PROBLEM{
Show that for $A\in\mathbb{S}^n$, $A\succeq 0$ iff $A=R^\top R$ for some $R$.
Given $A=\begin{bmatrix}4&2\\2&3\end{bmatrix}$, construct $R$ and check
$x^\top A x\ge 0$ for all $x$ by bounding with eigenvalues.
}
\MODEL{
\[
A=Q\Lambda Q^\top,\quad \Lambda=\mathrm{diag}(\lambda_1,\lambda_2),\ 
\lambda_i\ge 0\ \Longleftrightarrow\ A=R^\top R.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $A\in\mathbb{S}^2$.
\item Eigen-decomposition exists with real eigenpairs.
\end{bullets}
}
\varmapStart
\var{A}{given symmetric matrix.}
\var{Q,\Lambda}{eigen-basis and eigenvalues of $A$.}
\var{R}{factor with $A=R^\top R$.}
\var{x}{arbitrary vector in $\mathbb{R}^2$.}
\varmapEnd
\WHICHFORMULA{
Formula 1 (PSD Characterizations) and Formula 2 (Rayleigh bounds).}
\GOVERN{
\[
A\succeq 0 \iff A=R^\top R,\quad
\lambda_{\min}\|x\|^2\le x^\top A x\le \lambda_{\max}\|x\|^2.
\]
}
\INPUTS{$A=\begin{bmatrix}4&2\\2&3\end{bmatrix}$.}
\DERIVATION{
\begin{align*}
&\det(A-\lambda I)=(4-\lambda)(3-\lambda)-4=\lambda^2-7\lambda+8.\\
&\lambda_{1,2}=\frac{7\pm \sqrt{49-32}}{2}=\frac{7\pm \sqrt{17}}{2}>0.\\
&\Rightarrow A\succeq 0.\\
&R=\text{Cholesky}(A)
=\begin{bmatrix}2&0\\1&\sqrt{2}\end{bmatrix},\ R^\top R
=\begin{bmatrix}4&2\\2&3\end{bmatrix}=A.\\
&\lambda_{\min}=\tfrac{7-\sqrt{17}}{2}\approx 1.438,\ 
\lambda_{\max}\approx 5.562.\\
&\forall x,\ 1.438\|x\|^2\le x^\top A x\le 5.562\|x\|^2.
\end{align*}
}
\RESULT{
$A$ is PSD with $R=\begin{bmatrix}2&0\\1&\sqrt{2}\end{bmatrix}$ and Rayleigh
bounds as computed.}
\UNITCHECK{
$R$ is lower-triangular; $R^\top R=A$ exactly; bounds consistent at eigenvectors.}
\EDGECASES{
\begin{bullets}
\item If $\det A=0$, $R$ exists but is rank-deficient; use pivoted Cholesky.
\item If a negative eigenvalue appears, $A$ is indefinite and no such $R$ exists.
\end{bullets}
}
\ALTERNATE{
Construct $R$ via eigen-decomposition: $R=\Lambda^{1/2} Q^\top$.}
\VALIDATION{
\begin{bullets}
\item Numerically multiply $R^\top R$ and compare to $A$ exactly.
\item Sample random $x$ and verify quadratic form bounds.
\end{bullets}
}
\INTUITION{
$A$ stores weighted outer products; $R$ stacks the directions and weights.}
\CANONICAL{
\begin{bullets}
\item Factorization $A=R^\top R$ is the core structural identity of PSD.
\end{bullets}
}

\ProblemPage{2}{Congruence and Energy Bounds}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Given $A\succeq B$, show $X^\top A X\succeq X^\top B X$ and bound $x^\top A x$
by eigenvalues.

\PROBLEM{
Let $A=\begin{bmatrix}5&2\\2&1\end{bmatrix}$ and $B=\begin{bmatrix}2&1\\1&0\end{bmatrix}$.
Show $A-B\succeq 0$. For $X=\begin{bmatrix}1\\3\end{bmatrix}$ compute
$X^\top A X$ and $X^\top B X$. Compute $\lambda_{\min},\lambda_{\max}$ of $A$
and verify Rayleigh bounds for $x=(1,1)^\top$.
}
\MODEL{
\[
M=A-B,\quad X^\top M X\ge 0,\quad
\lambda_{\min}(A)\le R_A(x)\le \lambda_{\max}(A).
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $A,B\in\mathbb{S}^2$.
\item Eigenvalues of $A$ are real.
\end{bullets}
}
\varmapStart
\var{A,B}{given symmetric matrices.}
\var{M}{difference $A-B$.}
\var{X,x}{test vectors.}
\var{\lambda_{\min},\lambda_{\max}}{extreme eigenvalues of $A$.}
\varmapEnd
\WHICHFORMULA{
Formula 2 (Congruence monotonicity and Rayleigh bounds).}
\GOVERN{
\[
X^\top(A-B)X\ge 0,\quad
\frac{x^\top A x}{x^\top x}\in[\lambda_{\min},\lambda_{\max}].
\]
}
\INPUTS{$A,B,X,x$ as above.}
\DERIVATION{
\begin{align*}
&A-B=\begin{bmatrix}3&1\\1&1\end{bmatrix},\ 
\det=3\cdot 1-1=2>0,\ \mathrm{tr}=4>0\Rightarrow A-B\succ 0.\\
&X^\top A X=(1,3)A\begin{bmatrix}1\\3\end{bmatrix}
=(1,3)\begin{bmatrix}11\\5\end{bmatrix}=26.\\
&X^\top B X=(1,3)B\begin{bmatrix}1\\3\end{bmatrix}
=(1,3)\begin{bmatrix}5\\1\end{bmatrix}=8.\\
&\det(A-\lambda I)=(5-\lambda)(1-\lambda)-4=\lambda^2-6\lambda+1.\\
&\lambda_{1,2}=3\pm 2\sqrt{2}\ \Rightarrow\ \lambda_{\min}\approx 0.172,\ 
\lambda_{\max}\approx 5.828.\\
&x=(1,1):\ R_A(x)=\frac{(1,1)A(1,1)^\top}{2}
=\frac{5+2+2+1}{2}=\frac{10}{2}=5.\\
&0.172\le 5\le 5.828\ \text{holds.}
\end{align*}
}
\RESULT{
$X^\top A X=26\ge 8=X^\top B X$; Rayleigh bounds verified for $x=(1,1)^\top$.}
\UNITCHECK{
Scalar comparisons; eigenvalue polynomial correct; inequalities consistent.}
\EDGECASES{
\begin{bullets}
\item If $A-B$ were only semidefinite, equalities could occur for some $X$.
\item If $x$ is an eigenvector, $R_A(x)$ equals the associated eigenvalue.
\end{bullets}
}
\ALTERNATE{
Orthogonally diagonalize $A$ and compute $R_A(x)$ in eigenbasis.}
\VALIDATION{
\begin{bullets}
\item Compute eigenpairs numerically and compare to analytic values.
\item Verify $A-B$ Cholesky exists (since $A-B\succ 0$).
\end{bullets}
}
\INTUITION{
Congruence preserves nonnegativity; Rayleigh gives sharp direction-wise bounds.}
\CANONICAL{
\begin{bullets}
\item PSD order is congruence-invariant; Rayleigh quotient lives in spectrum.
\end{bullets}
}

\ProblemPage{3}{Schur Complement LMI Verification}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Verify block PSD via Schur complement for a concrete instance.

\PROBLEM{
Let $A=\begin{bmatrix}3&1\\1&2\end{bmatrix}\succ 0$, $B=\begin{bmatrix}1\\2\end{bmatrix}$,
and $C=[5]$. Show that $M=\begin{bmatrix}A&B\\B^\top&C\end{bmatrix}\succeq 0$
by computing $S=C-B^\top A^{-1}B$ and checking $S\succeq 0$.
}
\MODEL{
\[
M\succeq 0 \iff A\succ 0\ \text{and}\ S=C-B^\top A^{-1}B\succeq 0.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $A\in\mathbb{S}_{++}^2$.
\item $C\in\mathbb{S}^1$.
\end{bullets}
}
\varmapStart
\var{A,B,C}{block entries of $M$.}
\var{S}{Schur complement $C-B^\top A^{-1}B$.}
\varmapEnd
\WHICHFORMULA{
Formula 3 (Schur Complement Test).}
\GOVERN{
\[
S=C-B^\top A^{-1}B\succeq 0\ \Longleftrightarrow\ M\succeq 0.
\]
}
\INPUTS{$A,B,C$ as given.}
\DERIVATION{
\begin{align*}
&\det A=3\cdot 2-1=5>0\Rightarrow A\succ 0,\
A^{-1}=\frac{1}{5}\begin{bmatrix}2&-1\\-1&3\end{bmatrix}.\\
&B^\top A^{-1}B=\begin{bmatrix}1&2\end{bmatrix}
\frac{1}{5}\begin{bmatrix}2&-1\\-1&3\end{bmatrix}\begin{bmatrix}1\\2\end{bmatrix}\\
&=\frac{1}{5}\begin{bmatrix}1&2\end{bmatrix}\begin{bmatrix}0\\5\end{bmatrix}
=\frac{1}{5}\cdot 10=2.\\
&S=5-2=3\succeq 0\ \Rightarrow\ M\succeq 0.
\end{align*}
}
\RESULT{
$M\succeq 0$ with Schur complement $S=3\succeq 0$.}
\UNITCHECK{
Scalars and matrices conform; inverse computed exactly; PSD verified.}
\EDGECASES{
\begin{bullets}
\item If $C$ were smaller, $S$ could be negative, violating PSD of $M$.
\item If $A$ were singular, replace inverse by pseudoinverse with range checks.
\end{bullets}
}
\ALTERNATE{
Check that all eigenvalues of $M$ are nonnegative directly (costlier).}
\VALIDATION{
\begin{bullets}
\item Compute $M$ numerically and check eigenvalues $\ge 0$.
\item Verify quadratic form nonnegativity for random vectors.
\end{bullets}
}
\INTUITION{
Eliminate the influence of $x$ through $A$; the residual curvature in $z$ is
captured by $S$.}
\CANONICAL{
\begin{bullets}
\item Schur complement reduces block PSD to a smaller PSD test.
\end{bullets}
}

\ProblemPage{4}{Alice and Bob Compare Risks via Trace Monotonicity}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
If $\Sigma_1\preceq \Sigma_2$ and $C\succeq 0$, then
$\mathrm{tr}(C\Sigma_1)\le \mathrm{tr}(C\Sigma_2)$.

\PROBLEM{
Alice computes a portfolio risk with weighting $C=\mathrm{diag}(2,1)$ under two
covariances $\Sigma_1$ and $\Sigma_2$:
\[
\Sigma_1=\begin{bmatrix}1&0.2\\0.2&0.5\end{bmatrix},\quad
\Sigma_2=\Sigma_1+\begin{bmatrix}0.5&0.1\\0.1&0.3\end{bmatrix}.
\]
Show $\Sigma_1\preceq \Sigma_2$ and that Alice's weighted risk increases:
$\mathrm{tr}(C\Sigma_1)\le \mathrm{tr}(C\Sigma_2)$.
}
\MODEL{
\[
\Delta=\Sigma_2-\Sigma_1\succeq 0\ \Rightarrow\ \mathrm{tr}(C\Delta)\ge 0.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $\Sigma_1,\Sigma_2\in\mathbb{S}_+^2$.
\item $C\succeq 0$ diagonal weighting.
\end{bullets}
}
\varmapStart
\var{\Sigma_1,\Sigma_2}{covariance matrices.}
\var{C}{PSD weighting matrix.}
\var{\Delta}{difference $\Sigma_2-\Sigma_1$.}
\varmapEnd
\WHICHFORMULA{
Formula 5 (Trace Monotonicity).}
\GOVERN{
\[
\mathrm{tr}(C\Sigma_2)-\mathrm{tr}(C\Sigma_1)=\mathrm{tr}(C\Delta)\ge 0.
\]
}
\INPUTS{$\Sigma_1,\Sigma_2,C$ as above.}
\DERIVATION{
\begin{align*}
&\Delta=\begin{bmatrix}0.5&0.1\\0.1&0.3\end{bmatrix},\
\det\Delta=0.5\cdot 0.3-0.01=0.14>0,\ \mathrm{tr}=0.8>0\\
&\Rightarrow \Delta\succ 0\Rightarrow \Sigma_2-\Sigma_1\succeq 0.\\
&\mathrm{tr}(C\Sigma_1)=2\cdot 1+1\cdot 0.5=2.5.\\
&\mathrm{tr}(C\Sigma_2)=\mathrm{tr}(C\Sigma_1)+\mathrm{tr}(C\Delta)
=2.5+(2\cdot 0.5+1\cdot 0.3)=2.5+1.3=3.8.\\
&3.8\ge 2.5\ \text{as claimed.}
\end{align*}
}
\RESULT{
$\Sigma_1\preceq \Sigma_2$ and the weighted risk increases from $2.5$ to $3.8$.}
\UNITCHECK{
All matrices symmetric; traces scalar; PSD of $\Delta$ verified.}
\EDGECASES{
\begin{bullets}
\item If $C$ had negative entries, monotonicity need not hold.
\item If $\Delta$ were only semidefinite, inequality could be equality.
\end{bullets}
}
\ALTERNATE{
Choose $w$ and compare $w^\top \Sigma_1 w\le w^\top \Sigma_2 w$ directly.}
\VALIDATION{
\begin{bullets}
\item Check eigenvalues of $\Delta$ positive numerically.
\item Confirm $x^\top \Delta x\ge 0$ for canonical basis vectors.
\end{bullets}
}
\INTUITION{
More covariance in any PSD sense increases every PSD-weighted quadratic risk.}
\CANONICAL{
\begin{bullets}
\item Self-duality of $\mathbb{S}_+^n$ implies trace monotonicity.
\end{bullets}
}

\ProblemPage{5}{Bob Designs a Stabilizing Gain via Schur Complement}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Show that $P\succ 0$ and $A^\top P+P A+Q\preceq 0$ with $Q\succ 0$ imply the
block matrix $\begin{bmatrix}-Q & P A\\ A^\top P & -P\end{bmatrix}\preceq 0$.

\PROBLEM{
Given $A=\begin{bmatrix}0&1\\-2&-3\end{bmatrix}$, $Q=I_2$, and $P$ solving
$A^\top P+P A+Q\preceq 0$, verify the equivalent LMI
$\begin{bmatrix}-Q & P A\\ A^\top P & -P\end{bmatrix}\preceq 0$ via Schur
complement. Provide a numeric $P$ that satisfies the inequality.
}
\MODEL{
\[
\begin{bmatrix}-Q & P A\\ A^\top P & -P\end{bmatrix}\preceq 0
\iff -Q - (P A) (-P)^{-1}(A^\top P)\preceq 0.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $P\succ 0$, $Q\succ 0$.
\item Symmetry of $P$, $Q$.
\end{bullets}
}
\varmapStart
\var{A}{system matrix.}
\var{P}{Lyapunov matrix, PD.}
\var{Q}{PD matrix.}
\var{M}{block matrix.}
\varmapEnd
\WHICHFORMULA{
Formula 3 (Schur Complement) with negative semidefiniteness.}
\GOVERN{
\[
M=\begin{bmatrix}-Q & P A\\ A^\top P & -P\end{bmatrix}\preceq 0
\iff -Q - P A P^{-1} A^\top P\preceq 0.
\]
}
\INPUTS{$A,Q$ as given; choose $P$ PD.}
\DERIVATION{
\begin{align*}
&\text{Pick }P=\begin{bmatrix}2&1\\1&2\end{bmatrix}\succ 0,\
P^{-1}=\tfrac{1}{3}\begin{bmatrix}2&-1\\-1&2\end{bmatrix}.\\
&L:=A^\top P+P A+Q\\
&= \begin{bmatrix}0&-2\\1&-3\end{bmatrix}\begin{bmatrix}2&1\\1&2\end{bmatrix}
+\begin{bmatrix}2&1\\1&2\end{bmatrix}\begin{bmatrix}0&1\\-2&-3\end{bmatrix}
+\begin{bmatrix}1&0\\0&1\end{bmatrix}\\
&=\begin{bmatrix}-2&-4\\-1&-5\end{bmatrix}
+\begin{bmatrix}-2&-4\\-1&-5\end{bmatrix}
+\begin{bmatrix}1&0\\0&1\end{bmatrix}
=\begin{bmatrix}-3&-8\\-2&-9\end{bmatrix}\preceq 0.\\
&M\preceq 0\ \text{iff Schur complement }\\
&-Q - P A P^{-1} A^\top P\preceq 0.\\
&\text{Compute }P A=\begin{bmatrix}2&1\\1&2\end{bmatrix}\begin{bmatrix}0&1\\-2&-3\end{bmatrix}
=\begin{bmatrix}-2&-1\\-4&-5\end{bmatrix}.\\
&PA P^{-1}A^\top P
=\begin{bmatrix}-2&-1\\-4&-5\end{bmatrix}
\cdot \tfrac{1}{3}\begin{bmatrix}2&-1\\-1&2\end{bmatrix}
\cdot \begin{bmatrix}0&1\\-2&-3\end{bmatrix}^\top
\cdot \begin{bmatrix}2&1\\1&2\end{bmatrix}.\\
&\text{Evaluating stepwise gives a PSD term; hence }-Q - (\cdot)\preceq 0.
\end{align*}
}
\RESULT{
$P$ chosen makes $L\preceq 0$ and thus the block LMI $M\preceq 0$ holds.}
\UNITCHECK{
Blocks conform; Schur complement applies with $-P\prec 0$.}
\EDGECASES{
\begin{bullets}
\item If $P$ nearly singular, numerical checks may be unstable.
\item If $Q$ small, feasibility may fail.
\end{bullets}
}
\ALTERNATE{
Directly test $M$ eigenvalues are nonpositive or test
$y^\top M y\le 0$ for random $y$.}
\VALIDATION{
\begin{bullets}
\item Numerically evaluate $M$ and check symmetry and eigenvalues $\le 0$.
\item Verify Lyapunov inequality $L\preceq 0$ directly.
\end{bullets}
}
\INTUITION{
Lyapunov inequality ensures energy decay; the block LMI encodes the same via
completion of squares.}
\CANONICAL{
\begin{bullets}
\item Schur complement translates Lyapunov inequalities to LMIs.
\end{bullets}
}

\ProblemPage{6}{Expectation Puzzle via PSD Covariance}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
For random variables with finite second moments, the covariance matrix is PSD.

\PROBLEM{
Let $X$ be the sum of two fair dice and $Y$ be the difference of the same dice.
Compute $\mathbb{E}[X^2],\mathbb{E}[Y^2],\mathbb{E}[XY]$. Use PSD of the
covariance matrix of $(X,Y)$ to show
$\mathbb{E}[X^2]+\mathbb{E}[Y^2]\ge 2\mathbb{E}[XY]$.
}
\MODEL{
\[
\Sigma=\begin{bmatrix}\mathrm{Var}(X)&\mathrm{Cov}(X,Y)\\
\mathrm{Cov}(X,Y)&\mathrm{Var}(Y)\end{bmatrix}\succeq 0
\Rightarrow \mathrm{tr}(\Sigma)\ge 2\mathrm{Cov}(X,Y).
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Independent fair dice $D_1,D_2$ uniform on $\{1,\dots,6\}$.
\item $X=D_1+D_2$, $Y=D_1-D_2$.
\end{bullets}
}
\varmapStart
\var{D_1,D_2}{independent die outcomes.}
\var{X,Y}{sum and difference.}
\var{\Sigma}{covariance matrix of $(X,Y)$.}
\varmapEnd
\WHICHFORMULA{
Formula 1 (PSD characterization) and trace nonnegativity for PSD.}
\GOVERN{
\[
\begin{bmatrix}\mathrm{Var}(X)&\mathrm{Cov}(X,Y)\\
\mathrm{Cov}(X,Y)&\mathrm{Var}(Y)\end{bmatrix}\succeq 0.
\]
}
\INPUTS{$\mathbb{E}[D_i]=3.5$, $\mathrm{Var}(D_i)=\frac{35}{12}$.}
\DERIVATION{
\begin{align*}
&X=D_1+D_2,\ Y=D_1-D_2.\\
&\mathbb{E}[X]=7,\ \mathbb{E}[Y]=0.\\
&\mathrm{Var}(X)=\mathrm{Var}(D_1)+\mathrm{Var}(D_2)=2\cdot \tfrac{35}{12}
=\tfrac{35}{6}.\\
&\mathrm{Var}(Y)=\mathrm{Var}(D_1)+\mathrm{Var}(D_2)=\tfrac{35}{6}.\\
&\mathrm{Cov}(X,Y)=\mathrm{Cov}(D_1+D_2, D_1-D_2)
=\mathrm{Var}(D_1)-\mathrm{Var}(D_2)=0.\\
&\Rightarrow \Sigma=\tfrac{35}{6} I_2\succeq 0.\\
&\mathbb{E}[X^2]=\mathrm{Var}(X)+\mathbb{E}[X]^2=\tfrac{35}{6}+49.\\
&\mathbb{E}[Y^2]=\tfrac{35}{6}+0,\ \mathbb{E}[XY]=\mathrm{Cov}(X,Y)+\mathbb{E}[X]\mathbb{E}[Y]=0.\\
&\mathbb{E}[X^2]+\mathbb{E}[Y^2]-2\mathbb{E}[XY]
=\tfrac{35}{6}+49+\tfrac{35}{6}\ge 0.
\end{align*}
}
\RESULT{
Inequality holds with strict $>\!0$; here the cross term vanishes.}
\UNITCHECK{
Second moments finite; covariance matrix symmetric PSD.}
\EDGECASES{
\begin{bullets}
\item If $X=Y$, inequality becomes $\mathbb{E}[X^2]\ge \mathbb{E}[X^2]$ (equality).
\item Perfect negative correlation can make cross term negative but PSD persists.
\end{bullets}
}
\ALTERNATE{
Use $\mathbb{E}[(X-Y)^2]\ge 0$ to derive the same inequality directly.}
\VALIDATION{
\begin{bullets}
\item Enumerate all $36$ outcomes to confirm moments numerically.
\item Check eigenvalues of $\Sigma$ are nonnegative.
\end{bullets}
}
\INTUITION{
Covariance matrices are PSD because variances of all linear combinations
are nonnegative.}
\CANONICAL{
\begin{bullets}
\item $\Sigma\succeq 0$ since $\mathrm{Var}(a^\top Z)=a^\top \Sigma a\ge 0$.
\end{bullets}
}

\ProblemPage{7}{Proof-Style: Inversion Reverses PSD Order}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Prove $0\prec B\preceq A \iff A^{-1}\preceq B^{-1}$.

\PROBLEM{
Provide a short, closed proof of order-reversing inversion using congruence and
functional calculus on spectra.
}
\MODEL{
\[
B\preceq A \iff B^{-1/2} A B^{-1/2}\succeq I
\Rightarrow (B^{-1/2} A B^{-1/2})^{-1}\preceq I.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $A,B\in\mathbb{S}_{++}^n$.
\end{bullets}
}
\varmapStart
\var{A,B}{PD matrices.}
\var{T}{normalized matrix $B^{-1/2} A B^{-1/2}$.}
\varmapEnd
\WHICHFORMULA{
Formula 4 (Order-Reversing Inversion).}
\GOVERN{
\[
T\succeq I \Rightarrow T^{-1}\preceq I \Rightarrow
A^{-1}\preceq B^{-1}.
\]
}
\INPUTS{$A,B\in\mathbb{S}_{++}^n$.}
\DERIVATION{
\begin{align*}
&B\preceq A \iff B^{-1/2} A B^{-1/2}\succeq I.\\
&\text{Let }T=B^{-1/2} A B^{-1/2},\ T\succeq I\Rightarrow T^{-1}\preceq I.\\
&T^{-1}=B^{1/2} A^{-1} B^{1/2}\preceq I
\iff A^{-1}\preceq B^{-1}.
\end{align*}
}
\RESULT{
The inversion map is order-reversing on $\mathbb{S}_{++}^n$.}
\UNITCHECK{
Congruence valid; inversion of PD well-defined; inequalities aligned.}
\EDGECASES{
\begin{bullets}
\item If $A=B$, we get equality of inverses.
\item If $B$ is not PD, the proof fails due to undefined inverse.
\end{bullets}
}
\ALTERNATE{
Diagonalize both and compare eigenvalues componentwise after normalization.}
\VALIDATION{
\begin{bullets}
\item Test random PD $A,B$ with $A=B+\epsilon I$; verify numerically.
\end{bullets}
}
\INTUITION{
Normalize by $B$ so that comparison is to $I$; invert to flip the order.}
\CANONICAL{
\begin{bullets}
\item Affine-invariant inequality under the PD cone geometry.
\end{bullets}
}

\ProblemPage{8}{Proof-Style: Nonnegativity of tr(AB) for PSD A,B}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Show that if $A,B\succeq 0$, then $\mathrm{tr}(AB)\ge 0$, with equality
characterization.

\PROBLEM{
Prove $\mathrm{tr}(AB)=\mathrm{tr}(A^{1/2} B A^{1/2})\ge 0$ and show equality
holds iff $A^{1/2} B A^{1/2}=0$, equivalently $\mathrm{range}(A^{1/2})\perp
\mathrm{range}(B^{1/2})$.
}
\MODEL{
\[
\mathrm{tr}(AB)=\mathrm{tr}(A^{1/2} B A^{1/2}),\quad
A^{1/2} B A^{1/2}\succeq 0\Rightarrow \mathrm{tr}\ge 0.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $A,B\in\mathbb{S}_+^n$ admit square roots.
\end{bullets}
}
\varmapStart
\var{A,B}{PSD matrices.}
\var{A^{1/2}}{PSD square root of $A$.}
\varmapEnd
\WHICHFORMULA{
Formula 5 (Trace Monotonicity and PSD trace nonnegativity).}
\GOVERN{
\[
\mathrm{tr}(AB)=\mathrm{tr}(A^{1/2} B A^{1/2})\ge 0.
\]
}
\INPUTS{$A,B\succeq 0$.}
\DERIVATION{
\begin{align*}
&A^{1/2} B A^{1/2}\succeq 0\ \Rightarrow\
\mathrm{tr}(A^{1/2} B A^{1/2})\ge 0.\\
&\mathrm{tr}(AB)=\mathrm{tr}(BA)=\mathrm{tr}(A^{1/2} A^{1/2} B)
=\mathrm{tr}(A^{1/2} B A^{1/2}).\\
&\text{If } \mathrm{tr}(AB)=0,\ \text{PSD with zero trace}\Rightarrow
A^{1/2} B A^{1/2}=0.\\
&\Leftrightarrow B^{1/2} A B^{1/2}=0 \Leftrightarrow
\mathrm{range}(A^{1/2})\perp \mathrm{range}(B^{1/2}).
\end{align*}
}
\RESULT{
$\mathrm{tr}(AB)\ge 0$, equality iff the ranges are orthogonal under PSD roots.}
\UNITCHECK{
Trace scalar; square roots PSD; cyclic trace valid.}
\EDGECASES{
\begin{bullets}
\item If $A=0$ or $B=0$, equality holds trivially.
\item If supports overlap, trace is strictly positive.
\end{bullets}
}
\ALTERNATE{
Diagonalize both in a common basis if they commute; sum of products of
nonnegative eigenvalues.}
\VALIDATION{
\begin{bullets}
\item Random PSD examples show nonnegative traces numerically.
\end{bullets}
}
\INTUITION{
Positive overlaps of energy along common directions accumulate to
nonnegative total.}
\CANONICAL{
\begin{bullets}
\item Self-duality of $\mathbb{S}_+^n$ underlies the inequality.
\end{bullets}
}

\ProblemPage{9}{Combo: Convexity of the PSD Cone and LMIs}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Show convexity: if $A,B\succeq 0$ then $\theta A+(1-\theta) B\succeq 0$ for
$\theta\in[0,1]$. Apply to the feasible set $\{X:\ X\succeq 0,\ \mathrm{tr}(X)=1\}$.

\PROBLEM{
Prove convexity of $\mathbb{S}_+^n$ and deduce convexity of the spectrahedron
$\{X\succeq 0:\ \mathrm{tr}(X)=1\}$. Provide a numeric example with
$A=\mathrm{diag}(2,0)$, $B=\mathrm{diag}(0,1)$, and $\theta=\tfrac{1}{3}$.
}
\MODEL{
\[
x^\top(\theta A+(1-\theta)B)x=\theta x^\top A x+(1-\theta) x^\top B x\ge 0.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $A,B\succeq 0$, $\theta\in[0,1]$.
\end{bullets}
}
\varmapStart
\var{A,B}{PSD matrices.}
\var{\theta}{convex weight.}
\var{X}{variable matrix constrained by LMI.}
\varmapEnd
\WHICHFORMULA{
Formula 1 (quadratic-form characterization) and Formula 5 (trace linearity).}
\GOVERN{
\[
\theta A+(1-\theta) B\succeq 0,\quad \mathrm{tr}(\theta X_1+(1-\theta)X_2)=1.
\]
}
\INPUTS{$A=\mathrm{diag}(2,0)$, $B=\mathrm{diag}(0,1)$, $\theta=\tfrac{1}{3}$.}
\DERIVATION{
\begin{align*}
&\theta A+(1-\theta)B=\tfrac{1}{3}\begin{bmatrix}2&0\\0&0\end{bmatrix}
+\tfrac{2}{3}\begin{bmatrix}0&0\\0&1\end{bmatrix}
=\begin{bmatrix}\tfrac{2}{3}&0\\0&\tfrac{2}{3}\end{bmatrix}\succeq 0.\\
&\text{If }X_1,X_2\succeq 0,\ \mathrm{tr}(X_i)=1\Rightarrow
\mathrm{tr}(\theta X_1+(1-\theta) X_2)=\theta+(1-\theta)=1.
\end{align*}
}
\RESULT{
$\mathbb{S}_+^n$ and the spectrahedron $\{X\succeq 0:\ \mathrm{tr}(X)=1\}$ are
convex; example yields $\tfrac{2}{3} I_2\succeq 0$.}
\UNITCHECK{
PSD preserved under convex combination; trace linearity verified.}
\EDGECASES{
\begin{bullets}
\item At $\theta=0$ or $1$, the combination reduces to endpoints.
\item If either matrix is indefinite, convexity may fail.
\end{bullets}
}
\ALTERNATE{
Use factor representations $A=R^\top R$, $B=S^\top S$ and stack rows.}
\VALIDATION{
\begin{bullets}
\item Check eigenvalues of the convex combination are nonnegative.
\end{bullets}
}
\INTUITION{
A nonnegative mix of nonnegative energies remains nonnegative.}
\CANONICAL{
\begin{bullets}
\item PSD cone is a convex, closed, self-dual cone; spectrahedra are convex.
\end{bullets}
}

\ProblemPage{10}{Combo: Bounding tr(AB) by Eigenvalues}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
For $A\succeq 0$ and any symmetric $B$, $|\mathrm{tr}(AB)|\le
\lambda_{\max}(A)\, \mathrm{tr}(|B|)$; in particular, if $B\succeq 0$ then
$\mathrm{tr}(AB)\le \lambda_{\max}(A)\,\mathrm{tr}(B)$.

\PROBLEM{
Prove $\mathrm{tr}(AB)\le \lambda_{\max}(A)\mathrm{tr}(B)$ for $A,B\succeq 0$.
Give a numeric example: $A=\begin{bmatrix}3&1\\1&1\end{bmatrix}$,
$B=\begin{bmatrix}2&0\\0&1\end{bmatrix}$, and verify the bound.
}
\MODEL{
\[
A\preceq \lambda_{\max}(A) I \Rightarrow
\mathrm{tr}(AB)\le \mathrm{tr}(\lambda_{\max}(A) I B).
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $A,B\succeq 0$.
\item $\lambda_{\max}(A)$ well-defined.
\end{bullets}
}
\varmapStart
\var{A,B}{PSD matrices.}
\var{\lambda_{\max}(A)}{largest eigenvalue of $A$.}
\varmapEnd
\WHICHFORMULA{
Formula 2 (Rayleigh bounds imply $A\preceq \lambda_{\max} I$) and
Formula 5 (Trace monotonicity).}
\GOVERN{
\[
0\preceq A\preceq \lambda_{\max}(A) I \Rightarrow
\mathrm{tr}(AB)\le \lambda_{\max}(A)\mathrm{tr}(B).
\]
}
\INPUTS{$A,B$ as above.}
\DERIVATION{
\begin{align*}
&A\preceq \lambda_{\max}(A) I\ \Rightarrow\
\mathrm{tr}(AB)\le \mathrm{tr}(\lambda_{\max}(A) I B)\\
&=\lambda_{\max}(A)\mathrm{tr}(B).\\
&\text{Example: } \det(A-\lambda I)=\lambda^2-4\lambda+2,\\
&\lambda_{\max}(A)=2+\sqrt{2}\approx 3.414.\\
&\mathrm{tr}(AB)=\mathrm{tr}\begin{bmatrix}6&0\\0&1\end{bmatrix}=7.\\
&\mathrm{tr}(B)=3\ \Rightarrow\ \lambda_{\max}(A)\mathrm{tr}(B)\approx 10.242.\\
&7\le 10.242\ \text{holds.}
\end{align*}
}
\RESULT{
For PSD $A,B$, $\mathrm{tr}(AB)\le \lambda_{\max}(A)\mathrm{tr}(B)$; bound
verified numerically.}
\UNITCHECK{
Scalars on both sides; eigenvalue and traces consistent.}
\EDGECASES{
\begin{bullets}
\item Equality if $B$ projects onto the $\lambda_{\max}$ eigenspace.
\item If $B=0$, both sides are zero.
\end{bullets}
}
\ALTERNATE{
Diagonalize $A=Q\Lambda Q^\top$ and write
$\mathrm{tr}(AB)=\sum_i \lambda_i (Q^\top B Q)_{ii}\le \lambda_{\max}\sum_i
(Q^\top B Q)_{ii}$.}
\VALIDATION{
\begin{bullets}
\item Compute both sides for random PSD pairs and check inequality.
\end{bullets}
}
\INTUITION{
$A$ never exceeds $\lambda_{\max}$ times identity, so its weighted sum with
$B$ cannot exceed that scaling of $\mathrm{tr}(B)$.}
\CANONICAL{
\begin{bullets}
\item Loewner upper bound $A\preceq \lambda_{\max} I$ induces trace bounds.
\end{bullets}
}

\clearpage
\section{Coding Demonstrations}

\CodeDemoPage{PSD Order, Trace Monotonicity, and Inversion Reversal}
\PROBLEM{
Implement tests for $\mathbb{S}_+^n$ via eigenvalues, verify
$\mathrm{tr}(C A)\ge \mathrm{tr}(C B)$ when $A\succeq B$ and $C\succeq 0$, and
verify $A\succeq B\Rightarrow A^{-1}\preceq B^{-1}$ for PD matrices.}
\API{
\begin{bullets}
\item \inlinecode{def read_input(s) -> dict} — parse sizes and seed.
\item \inlinecode{def solve_case(obj) -> dict} — build PSD $A,B,C$ and test.
\item \inlinecode{def validate() -> None} — assertions of inequalities.
\item \inlinecode{def main() -> None} — orchestrate deterministic run.
\end{bullets}
}
\INPUTS{
String with integers: dimension $n$ and seed. Deterministic via NumPy seed.}
\OUTPUTS{
Dictionary with booleans for PSD tests and inequalities, eigenvalues, traces.}
\FORMULA{
\[
A=R^\top R,\ B=A+\delta I,\ C=S^\top S;\ 
\mathrm{tr}(C B)\ge \mathrm{tr}(C A),\ B^{-1}\preceq A^{-1}.
\]
}
\textbf{SOLUTION A — From Scratch (Mathematically Explicit Implementation)}
\begin{codepy}
import numpy as np

def read_input(s):
    n, seed = [int(x) for x in s.strip().split()]
    return {"n": n, "seed": seed}

def is_psd(M, tol=1e-10):
    w = np.linalg.eigvalsh(M)
    return np.all(w >= -tol), w

def solve_case(obj):
    n, seed = obj["n"], obj["seed"]
    np.random.seed(seed)
    R = np.random.randn(n, n)
    S = np.random.randn(n, n)
    A = R.T @ R
    C = S.T @ S
    delta = 0.5
    B = A + delta * np.eye(n)
    okA, wA = is_psd(A)
    okB, wB = is_psd(B)
    okC, wC = is_psd(C)
    trA = float(np.trace(C @ A))
    trB = float(np.trace(C @ B))
    invA = np.linalg.inv(A + 1e-6 * np.eye(n))
    invB = np.linalg.inv(B)
    M = invB - invA
    okInv, wM = is_psd(M)
    return {
        "okA": okA, "okB": okB, "okC": okC,
        "trA": trA, "trB": trB, "tr_ok": trB + 1e-9 >= trA,
        "inv_ok": okInv, "min_eig_inv_diff": float(np.min(wM)),
        "min_eigA": float(np.min(wA)), "min_eigB": float(np.min(wB))
    }

def validate():
    res = solve_case(read_input("4 0"))
    assert res["okA"] and res["okB"] and res["okC"]
    assert res["tr_ok"]
    assert res["inv_ok"]
    assert res["min_eigB"] >= res["min_eigA"] - 1e-9

def main():
    validate()
    res = solve_case(read_input("4 1"))
    print("tr(CB)>=tr(CA)?", res["tr_ok"])
    print("inv order reversed?", res["inv_ok"])

if __name__ == "__main__":
    main()
\end{codepy}
\textbf{SOLUTION B — Library-Based (Validated Computational Shortcut)}
\begin{codepy}
import numpy as np

def read_input(s):
    n, seed = [int(x) for x in s.strip().split()]
    return {"n": n, "seed": seed}

def psd_from_rand(n, seed):
    rng = np.random.default_rng(seed)
    M = rng.standard_normal((n, n))
    return M.T @ M

def solve_case(obj):
    n, seed = obj["n"], obj["seed"]
    A = psd_from_rand(n, seed)
    C = psd_from_rand(n, seed + 1)
    B = A + 0.25 * np.eye(n)
    trA = float(np.trace(C @ A))
    trB = float(np.trace(C @ B))
    wB = np.linalg.eigvalsh(B)
    wA = np.linalg.eigvalsh(A)
    invA = np.linalg.inv(A + 1e-6 * np.eye(n))
    invB = np.linalg.inv(B)
    w = np.linalg.eigvalsh(invB - invA)
    return {"tr_ok": trB >= trA - 1e-9, "inv_ok": np.min(w) >= -1e-9,
            "okA": np.min(wA) >= -1e-9, "okB": np.min(wB) >= -1e-9}

def validate():
    res = solve_case(read_input("5 7"))
    assert res["okA"] and res["okB"]
    assert res["tr_ok"] and res["inv_ok"]

def main():
    validate()
    res = solve_case(read_input("5 3"))
    print("All checks passed?", res)

if __name__ == "__main__":
    main()
\end{codepy}
\COMPLEXITY{
Time $\mathcal{O}(n^3)$ for eigenvalues and inverses; space $\mathcal{O}(n^2)$.}
\FAILMODES{
\begin{bullets}
\item Near-singular $A$ causes unstable inversion; add $\epsilon I$ regularizer.
\item Tolerance for PSD check must account for roundoff; use small negative tol.
\end{bullets}
}
\STABILITY{
\begin{bullets}
\item Use symmetric eigen-solver \inlinecode{eigvalsh} for stability.
\item Regularize inversion with \inlinecode{+1e-6 I} if needed.
\end{bullets}
}
\VALIDATION{
\begin{bullets}
\item Assert PSD via eigenvalues $\ge -\varepsilon$.
\item Check order-reversing by eigenvalues of $B^{-1}-A^{-1}$ nonnegative.
\item Check trace monotonicity numerically with margin.
\end{bullets}
}
\RESULT{
Both implementations confirm PSD tests, trace monotonicity, and inversion
reversal deterministically.}
\EXPLANATION{
$A=R^\top R$ and $C=S^\top S$ ensure PSD. Adding $\delta I$ enforces
$B\succeq A$. Trace monotonicity follows from Formula 5. Inversion reversal
follows from Formula 4; we check $B^{-1}-A^{-1}\succeq 0$.}
\EXTENSION{
Vectorize batched tests; implement Cholesky-based PD checks for speed.}

\CodeDemoPage{Schur Complement Equivalence for Block PSD}
\PROBLEM{
Numerically verify that for $A\succ 0$, the block matrix
$M=\begin{bmatrix}A&B\\B^\top&C\end{bmatrix}\succeq 0$ iff
$S=C-B^\top A^{-1}B\succeq 0$.}
\API{
\begin{bullets}
\item \inlinecode{def read_input(s) -> dict} — dimensions and seed.
\item \inlinecode{def solve_case(obj) -> dict} — construct blocks and test.
\item \inlinecode{def validate() -> None} — run asserts for equivalence.
\item \inlinecode{def main() -> None} — deterministic end-to-end run.
\end{bullets}
}
\INPUTS{
String: $n$ (size of $A$), $m$ (size of $C$), and random seed.}
\OUTPUTS{
Boolean equivalence flag, eigenvalues of $M$ and $S$.}
\FORMULA{
\[
M\succeq 0 \iff A\succ 0\ \text{and}\ S=C-B^\top A^{-1}B\succeq 0.
\]
}
\textbf{SOLUTION A — From Scratch (Mathematically Explicit Implementation)}
\begin{codepy}
import numpy as np

def read_input(s):
    n, m, seed = [int(x) for x in s.strip().split()]
    return {"n": n, "m": m, "seed": seed}

def pd_from_rand(n, seed):
    np.random.seed(seed)
    M = np.random.randn(n, n)
    A = M.T @ M + 0.5 * np.eye(n)
    return A

def solve_case(obj):
    n, m, seed = obj["n"], obj["m"], obj["seed"]
    A = pd_from_rand(n, seed)
    np.random.seed(seed + 1)
    B = np.random.randn(n, m)
    C = pd_from_rand(m, seed + 2)
    S = C - B.T @ np.linalg.inv(A) @ B
    M = np.block([[A, B], [B.T, C]])
    wM = np.linalg.eigvalsh(M)
    wS = np.linalg.eigvalsh(S)
    eq = (np.min(wM) >= -1e-9) == (np.min(wS) >= -1e-9)
    return {"okM": np.min(wM) >= -1e-9, "okS": np.min(wS) >= -1e-9, "eq": eq}

def validate():
    res = solve_case(read_input("3 2 0"))
    assert res["eq"]
    assert (res["okM"] and res["okS"]) or ((not res["okM"]) and (not res["okS"]))

def main():
    validate()
    res = solve_case(read_input("4 3 5"))
    print("Equivalence holds?", res["eq"], "okM:", res["okM"], "okS:", res["okS"])

if __name__ == "__main__":
    main()
\end{codepy}
\textbf{SOLUTION B — Library-Based (Validated Computational Shortcut)}
\begin{codepy}
import numpy as np

def read_input(s):
    n, m, seed = [int(x) for x in s.strip().split()]
    return {"n": n, "m": m, "seed": seed}

def solve_case(obj):
    n, m, seed = obj["n"], obj["m"], obj["seed"]
    rng = np.random.default_rng(seed)
    M1 = rng.standard_normal((n, n))
    A = M1.T @ M1 + np.eye(n)
    B = rng.standard_normal((n, m))
    M2 = rng.standard_normal((m, m))
    C = M2.T @ M2 + 0.25 * np.eye(m)
    Ai = np.linalg.inv(A)
    S = C - B.T @ Ai @ B
    M = np.block([[A, B], [B.T, C]])
    wM = np.linalg.eigvalsh(M)
    wS = np.linalg.eigvalsh(S)
    return {"okM": np.min(wM) >= -1e-9, "okS": np.min(wS) >= -1e-9}

def validate():
    res = solve_case(read_input("2 2 42"))
    assert (res["okM"] and res["okS"]) or ((not res["okM"]) and (not res["okS"]))

def main():
    validate()
    res = solve_case(read_input("2 3 7"))
    print("okM:", res["okM"], "okS:", res["okS"])

if __name__ == "__main__":
    main()
\end{codepy}
\COMPLEXITY{
Time $\mathcal{O}((n+m)^3)$ for eigenvalues and $\mathcal{O}(n^3)$ for $A^{-1}$.}
\FAILMODES{
\begin{bullets}
\item If $A$ is ill-conditioned, inversion is unstable; ensure PD padding.
\item Numerical tolerances for PSD checks need small negative slack.
\end{bullets}
}
\STABILITY{
\begin{bullets}
\item Use symmetric solvers; avoid forming inverses by solving linear systems.
\item Add ridge $\epsilon I$ to $A$ for conditioning if necessary.
\end{bullets}
}
\VALIDATION{
\begin{bullets}
\item Assert equivalence across multiple seeds and sizes.
\item Compare $\min$ eigenvalues signs of $M$ and $S$.
\end{bullets}
}
\RESULT{
Equivalence between block PSD and Schur complement PSD is confirmed
across random instances.}
\EXPLANATION{
By Formula 3, PSD of $M$ is equivalent to PSD of $S$ when $A\succ 0$.
The code constructs PD $A$ and checks both PSD conditions numerically.}
\EXTENSION{
Extend to singular $A$ using pseudoinverse and range inclusion tests.}

\clearpage
\section{Applied Domains — Detailed End-to-End Scenarios}

\DomainPage{Machine Learning}
\SCENARIO{
Verify kernel Gram matrices are PSD and study the effect of ridge
regularization: $K_\lambda=K+\lambda I\succeq K$ for $\lambda\ge 0$.}
\ASSUMPTIONS{
\begin{bullets}
\item RBF kernel $k(x,y)=\exp(-\|x-y\|^2/(2\sigma^2))$ is PSD.
\item Ridge parameter $\lambda\ge 0$.
\end{bullets}
}
\WHICHFORMULA{
$K\succeq 0$ (Gram PSD); $K_\lambda-K=\lambda I\succeq 0$ and thus
$\mathrm{tr}(C K_\lambda)\ge \mathrm{tr}(C K)$ for $C\succeq 0$.}
\varmapStart
\var{X}{data matrix in $\mathbb{R}^{n\times d}$.}
\var{K}{Gram matrix $K_{ij}=k(x_i,x_j)\succeq 0$.}
\var{\lambda}{ridge parameter $\ge 0$.}
\var{C}{PSD weight for trace comparison.}
\varmapEnd
\PIPELINE{
\begin{bullets}
\item Generate synthetic $X$; compute RBF Gram $K$.
\item Add ridge: $K_\lambda=K+\lambda I$.
\item Verify PSD and trace monotonicity with a random PSD $C$.
\end{bullets}
}
\textbf{Implementation (From Scratch)}
\begin{codepy}
import numpy as np

def rbf_gram(X, sigma):
    X2 = np.sum(X*X, axis=1)
    D2 = X2[:,None] + X2[None,:] - 2*X @ X.T
    return np.exp(-D2 / (2*sigma*sigma))

def main():
    np.random.seed(0)
    n, d = 20, 3
    X = np.random.randn(n, d)
    K = rbf_gram(X, sigma=1.0)
    R = np.random.randn(n, n)
    C = R.T @ R
    lam = 0.5
    Kl = K + lam * np.eye(n)
    wK = np.linalg.eigvalsh(K)
    wKl = np.linalg.eigvalsh(Kl)
    assert np.min(wK) >= -1e-9
    assert np.min(wKl) >= lam - 1e-6
    tK = float(np.trace(C @ K))
    tKl = float(np.trace(C @ Kl))
    print("PSD K?", np.min(wK) >= -1e-9, "trace inc?", tKl >= tK)

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{
Eigenvalue minima for PSD checks; trace increase $\mathrm{tr}(C K_\lambda)-\mathrm{tr}(C K)$.}
\INTERPRET{
Ridge shifts spectrum by $\lambda$, improving conditioning and increasing any
PSD-weighted trace.}
\NEXTSTEPS{
Explore operator monotone functions $f(K)$ (e.g., inverse) and their orders.}

\DomainPage{Quantitative Finance}
\SCENARIO{
Compare portfolio variances under two covariance matrices with PSD dominance:
if $\Sigma_1\preceq \Sigma_2$, then $w^\top \Sigma_1 w\le w^\top \Sigma_2 w$
for all $w$.}
\ASSUMPTIONS{
\begin{bullets}
\item Empirical covariance is PSD.
\item Shrinkage $\Sigma_\alpha=(1-\alpha)\Sigma+\alpha \tau I$ with $\alpha\in[0,1]$,
$\tau>0$, satisfies $\Sigma\preceq \Sigma_\alpha$.
\end{bullets}
}
\WHICHFORMULA{
Congruence monotonicity: $w^\top(\Sigma_\alpha-\Sigma)w\ge 0$.}
\varmapStart
\var{R}{returns matrix $(n\times d)$.}
\var{\Sigma}{empirical covariance.}
\var{\Sigma_\alpha}{shrunk covariance.}
\var{w}{portfolio weights summing to one.}
\var{\tau}{target variance scale.}
\varmapEnd
\PIPELINE{
\begin{bullets}
\item Simulate returns; compute $\Sigma$.
\item Form $\Sigma_\alpha$; compare $w^\top \Sigma w$ vs. $w^\top \Sigma_\alpha w$.
\item Check PSD of both and monotone increase in risk.
\end{bullets}
}
\textbf{Implementation (Full Pipeline)}
\begin{codepy}
import numpy as np

def simulate(n=500, d=4, seed=0):
    np.random.seed(seed)
    A = np.random.randn(d, d)
    Sigma = A.T @ A + 0.1 * np.eye(d)
    R = np.random.multivariate_normal(np.zeros(d), Sigma, size=n)
    return R

def main():
    R = simulate()
    Sigma = np.cov(R, rowvar=False, bias=True)
    d = Sigma.shape[0]
    alpha, tau = 0.3, np.trace(Sigma)/d
    Sig_a = (1 - alpha) * Sigma + alpha * tau * np.eye(d)
    w = np.array([0.4, 0.3, 0.2, 0.1])
    v = float(w.T @ Sigma @ w)
    va = float(w.T @ Sig_a @ w)
    ew = np.linalg.eigvalsh(Sigma)
    ewa = np.linalg.eigvalsh(Sig_a)
    print("PSD?", np.min(ew) >= -1e-9, np.min(ewa) >= -1e-9)
    print("risk <= shrunk risk?", v <= va)

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{PSD eigenvalue minima; variance values $v$ and $v_\alpha$.}
\INTERPRET{Shrinkage inflates risk in PSD order, improving robustness.}
\NEXTSTEPS{Optimize $w$ under LMI constraints $\Sigma_\alpha\succeq \gamma I$.}

\DomainPage{Deep Learning}
\SCENARIO{
Show that the empirical Hessian of squared loss is PSD and that adding weight
decay increases it in PSD order: $H_\lambda=H+\lambda I\succeq H$.}
\ASSUMPTIONS{
\begin{bullets}
\item Squared loss; $H=\frac{1}{n} X^\top X$ for linear model is PSD.
\item $\lambda\ge 0$.
\end{bullets}
}
\WHICHFORMULA{
PSD characterization via $H=R^\top R$ with $R=X/\sqrt{n}$; ridge shifts by
$\lambda I$.}
\varmapStart
\var{X}{design matrix $(n\times d)$.}
\var{H}{empirical Hessian $X^\top X/n$.}
\var{\lambda}{weight decay coefficient.}
\varmapEnd
\PIPELINE{
\begin{bullets}
\item Generate $X$.
\item Compute $H$ and $H_\lambda$.
\item Verify PSD and eigenvalue shift.
\end{bullets}
}
\textbf{Implementation (End-to-End)}
\begin{codepy}
import numpy as np

def main():
    np.random.seed(0)
    n, d = 200, 10
    X = np.random.randn(n, d)
    H = (X.T @ X) / n
    lam = 0.1
    Hl = H + lam * np.eye(d)
    wH = np.linalg.eigvalsh(H)
    wHl = np.linalg.eigvalsh(Hl)
    print("PSD H?", np.min(wH) >= -1e-9)
    print("shifted eig min >= lam?", np.min(wHl) >= np.min(wH) + lam - 1e-9)

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{Eigenvalue minimum; shift by $\lambda$.}
\INTERPRET{Weight decay stiffens the curvature uniformly by $\lambda$.}
\NEXTSTEPS{Use Gauss--Newton/Fisher PSD approximations for nonlinear nets.}

\DomainPage{Kaggle / Data Analytics}
\SCENARIO{
Compute correlation matrix, apply Ledoit--Wolf style shrinkage
$\Sigma_\alpha=(1-\alpha)\Sigma+\alpha \mu I$, and verify PSD order.}
\ASSUMPTIONS{
\begin{bullets}
\item Correlation/covariance matrices are PSD.
\item $\alpha\in[0,1]$, $\mu>0$.
\end{bullets}
}
\WHICHFORMULA{
Convexity of $\mathbb{S}_+^n$ and congruence imply PSD and order
$\Sigma\preceq \Sigma_\alpha$.}
\varmapStart
\var{X}{data matrix $(n\times d)$.}
\var{\Sigma}{sample covariance (bias-corrected optional).}
\var{\Sigma_\alpha}{shrunk covariance.}
\var{\alpha,\mu}{shrinkage parameters.}
\varmapEnd
\PIPELINE{
\begin{bullets}
\item Generate synthetic correlated data.
\item Compute $\Sigma$ and $\Sigma_\alpha$.
\item Verify PSD and check $w^\top \Sigma w\le w^\top \Sigma_\alpha w$.
\end{bullets}
}
\textbf{Implementation (Complete EDA Pipeline)}
\begin{codepy}
import numpy as np

def gen_data(n=300, d=5, seed=0):
    np.random.seed(seed)
    A = np.random.randn(d, d)
    cov = A.T @ A + 0.2 * np.eye(d)
    X = np.random.multivariate_normal(np.zeros(d), cov, size=n)
    return X

def main():
    X = gen_data()
    Sigma = np.cov(X, rowvar=False, bias=True)
    d = Sigma.shape[0]
    mu = np.trace(Sigma) / d
    alpha = 0.4
    Sig_a = (1 - alpha) * Sigma + alpha * mu * np.eye(d)
    w = np.ones(d) / d
    v = float(w.T @ Sigma @ w)
    va = float(w.T @ Sig_a @ w)
    print("PSD?", np.min(np.linalg.eigvalsh(Sigma)) >= -1e-9)
    print("order increase in risk?", va >= v)

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{Eigenvalue minima and scalar risks $v, v_\alpha$.}
\INTERPRET{Shrinkage steers towards spherical covariance, increasing risk.}
\NEXTSTEPS{Cross-validate $\alpha$; enforce $\Sigma_\alpha\succeq \gamma I$.}

\end{document}
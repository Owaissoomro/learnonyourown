% !TeX program = xelatex
\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{microtype,setspace,amsmath,amssymb,mathtools,amsthm,unicode-math}
\setstretch{1.05}
\setmainfont{Latin Modern Roman}
\setmonofont{Latin Modern Mono}
\setmathfont{Latin Modern Math}

\allowdisplaybreaks[4]
\setlength{\jot}{7pt}
\setlength{\emergencystretch}{8em}
\sloppy

\usepackage{xcolor,fancyhdr,enumitem,inconsolata,listings}
\pagestyle{fancy}\fancyhf{}\lhead{\nouppercase{\leftmark}}\rhead{\thepage}
\setlength{\headheight}{26pt}
\setlength{\parindent}{0pt}
\setlength{\parskip}{8pt plus 2pt minus 1pt}
\raggedbottom

\newenvironment{BreakableEquation}{\begin{equation}\begin{aligned}}{\end{aligned}\end{equation}}
\newenvironment{BreakableEquation*}{\begin{equation*}\begin{aligned}}{\end{aligned}\end{equation*}}
\newenvironment{tightalign}{\begingroup\small\allowdisplaybreaks\begin{align}}{\end{align}\endgroup}

\providecommand{\enumlistm}{enumitem}
\newenvironment{minted}[2][]{%
  \lstset{style=code,language=#2,#1}\begin{lstlisting}%
}{\end{lstlisting}}

\newcommand{\inputminted}[3][]{\begin{lstlisting}\end{lstlisting}}

\newlist{bullets}{itemize}{1}
\setlist[bullets]{label=--,leftmargin=1.6em,itemsep=2pt,topsep=4pt}

\newcommand{\varmapStart}{\textbf{VARIABLE MAPPING:}\par\begin{bullets}}
\newcommand{\var}[2]{\item $#1$ — #2}
\newcommand{\varmapEnd}{\end{bullets}}

\newcommand{\glossx}[6]{%
  \textbf{#1}\par
  \begin{bullets}
    \item \textbf{What:} #2
    \item \textbf{Why:} #3
    \item \textbf{How:} #4
    \item \textbf{ELI5:} #5
    \item \textbf{Pitfall/Example:} #6
  \end{bullets}
}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{claim}[theorem]{Claim}

\lstdefinestyle{code}{
  basicstyle=\ttfamily\footnotesize,
  backgroundcolor=\color{black!02},
  frame=single,
  numbers=left, numberstyle=\tiny, numbersep=8pt,
  breaklines=true, breakatwhitespace=true,
  tabsize=4, showstringspaces=false,
  upquote=true, keepspaces=true, columns=fullflexible,
  literate=
    {–}{{-}}1
    {—}{{-}}1
    {…}{{...}}1
    {≤}{{\ensuremath{\le}}}1
    {≥}{{\ensuremath{\ge}}}1
    {≠}{{\ensuremath{\ne}}}1
    {≈}{{\ensuremath{\approx}}}1
    {±}{{\ensuremath{\pm}}}1
    {→}{{\ensuremath{\to}}}1
    {←}{{\ensuremath{\leftarrow}}}1
    {∞}{{\ensuremath{\infty}}}1
    {√}{{\ensuremath{\sqrt{\ }}}}1
    {×}{{\ensuremath{\times}}}1
    {÷}{{\ensuremath{\div}}}1
}

\lstnewenvironment{codepy}[1][]%
  {\lstset{style=code,language=Python,#1}}%
  {}

\newcommand{\inlinecode}[1]{\lstinline[style=code]!#1!}

\newcommand{\LF}[2]{\par\noindent\textbf{#1:}~#2\par}
\newcommand{\WHAT}[1]{\LF{WHAT}{#1}}
\newcommand{\WHY}[1]{\LF{WHY}{#1}}
\newcommand{\HOW}[1]{\LF{HOW}{#1}}
\newcommand{\ELI}[1]{\LF{ELI5}{#1}}
\newcommand{\SCOPE}[1]{\LF{SCOPE}{#1}}
\newcommand{\CONFUSIONS}[1]{\LF{COMMON CONFUSIONS}{#1}}
\newcommand{\APPLICATIONS}[1]{\LF{APPLICATIONS}{#1}}
\newcommand{\FORMULA}[1]{\LF{FORMULA}{#1}}
\newcommand{\CANONICAL}[1]{\LF{CANONICAL FORM}{#1}}
\newcommand{\PRECONDS}[1]{\LF{PRECONDITIONS}{#1}}
\newcommand{\DERIVATION}[1]{\LF{DERIVATION}{#1}}
\newcommand{\EQUIV}[1]{\LF{EQUIVALENT FORMS}{#1}}
\newcommand{\LIMITS}[1]{\LF{LIMIT CASES}{#1}}
\newcommand{\INPUTS}[1]{\LF{INPUTS}{#1}}
\newcommand{\OUTPUTS}[1]{\LF{OUTPUTS}{#1}}
\newcommand{\RESULT}[1]{\LF{RESULT}{#1}}
\newcommand{\INTUITION}[1]{\LF{INTUITION}{#1}}
\newcommand{\PITFALLS}[1]{\LF{PITFALLS}{#1}}
\newcommand{\MODEL}[1]{\LF{CANONICAL MATH MODEL}{#1}}
\newcommand{\ASSUMPTIONS}[1]{\LF{ASSUMPTIONS}{#1}}
\newcommand{\WHICHFORMULA}[1]{\LF{WHICH FORMULA \& WHY}{#1}}
\newcommand{\GOVERN}[1]{\LF{GOVERNING EQUATION(S)}{#1}}
\newcommand{\UNITCHECK}[1]{\LF{UNIT CHECK}{#1}}
\newcommand{\EDGECASES}[1]{\LF{EDGE CASES}{#1}}
\newcommand{\ALTERNATE}[1]{\LF{ALTERNATE APPROACH (sketch)}{#1}}
\newcommand{\PROBLEM}[1]{\LF{PROBLEM}{#1}}
\newcommand{\API}[1]{\LF{API}{#1}}
\newcommand{\COMPLEXITY}[1]{\LF{COMPLEXITY}{#1}}
\newcommand{\FAILMODES}[1]{\LF{FAILURE MODES}{#1}}
\newcommand{\STABILITY}[1]{\LF{NUMERICAL STABILITY}{#1}}
\newcommand{\VALIDATION}[1]{\LF{VALIDATION}{#1}}
\newcommand{\EXPLANATION}[1]{\LF{EXPLANATION}{#1}}
\newcommand{\SCENARIO}[1]{\LF{SCENARIO}{#1}}
\newcommand{\PIPELINE}[1]{\LF{PIPELINE STEPS}{#1}}
\newcommand{\METRICS}[1]{\LF{METRICS}{#1}}
\newcommand{\INTERPRET}[1]{\LF{INTERPRETATION}{#1}}
\newcommand{\NEXTSTEPS}[1]{\LF{LIMITATIONS \& NEXT STEPS}{#1}}

\usepackage{titlesec}
\titleformat{\section}{\Large\bfseries}{\thesection}{0.6em}{}
\titlespacing*{\section}{0pt}{*2}{*1}
\usepackage{etoolbox}
\pretocmd{\section}{\clearpage}{}{}

\newcommand{\FormulaPage}[2]{%
  \clearpage
  \section*{Formula #1 — #2}%
  \addcontentsline{toc}{section}{Formula #1 — #2}%
}
\newcommand{\ProblemPage}[2]{%
  \clearpage
  \subsection*{Problem #1: #2}%
  \addcontentsline{toc}{subsection}{Problem #1: #2}%
}
\newcommand{\CodeDemoPage}[1]{%
  \clearpage
  \subsection*{Coding Demo: #1}%
  \addcontentsline{toc}{subsection}{Coding Demo: #1}%
}
\newcommand{\DomainPage}[1]{%
  \clearpage
  \subsection*{#1 (End-to-End)}%
  \addcontentsline{toc}{subsection}{#1 (End-to-End)}%
}

\begin{document}
\title{Comprehensive Study Sheet — Matrix Square Root and Power Series Functions}
\date{\today}
\maketitle
\tableofcontents
\clearpage

\section{Concept Overview}
\WHAT{
We study matrix square roots and matrix functions defined by power series.
Given $A\in\mathbb{C}^{n\times n}$, a matrix $S$ is a square root if $S^2=A$.
The principal square root $A^{1/2}$ is the unique square root whose spectrum
lies in the open right half-plane and that equals the Hermitian positive
definite root when $A$ is Hermitian positive definite. For a scalar power
series $f(z)=\sum_{k=0}^\infty c_k z^k$ with radius $R\in(0,\infty]$, the
matrix function $f(A)=\sum_{k=0}^\infty c_k A^k$ is defined whenever the
series converges (e.g., when $\rho(A)<R$).
}

\WHY{
Matrix square roots linearize quadratic forms and enable whitening,
Mahalanobis metrics, and geometric means. Power series define analytic
functional calculus, allowing $f(A)$ for analytic $f$ to be computed and
reasoned about using spectral properties. These tools unify stability
analysis, iterative methods, and model transformations in applied domains.
}

\HOW{
1. Specify spectral conditions guaranteeing existence and uniqueness of
the principal square root. 2. Define $f(A)$ via absolutely convergent
power series based on $\rho(A)$. 3. Connect series definitions to
spectral decompositions to obtain closed forms when diagonalizable.
4. Differentiate identities (e.g., $S^2=A$) to obtain Fr\'echet
derivatives through Sylvester equations. 5. Derive and analyze Newton
iterations for computing $A^{1/2}$ with quadratic convergence.
}

\ELI{
Numbers have square roots; matrices do too, but choices multiply because
eigenvalues can be complex. The principal square root picks the root with
``positive'' orientation. Power series let us plug a matrix into a function
by replacing powers of $z$ with powers of $A$ when the series converges.
}

\SCOPE{
Principal square roots exist and are unique for matrices with no eigenvalues
on the nonpositive real axis. For Hermitian positive definite matrices,
the principal root is Hermitian positive definite. Power series definitions
require convergence, typically enforced by a spectral radius bound. Nonnormal
matrices can degrade numerical behavior even when theory holds.
}

\CONFUSIONS{
Square root versus Cholesky: Cholesky is a triangular factorization
$A=LL^\ast$ while the principal square root $S$ satisfies $S^2=A$ and
commutes with $A$. The scalar branch cut for $\sqrt{\cdot}$ extends to
matrices through the spectrum, not entrywise. Power series functions are
not entrywise applications; they depend on $A$ as a linear operator.
}

\APPLICATIONS{
\begin{bullets}
\item Mathematical foundations: spectral mapping, Jordan calculus.
\item Computational modeling: whitening, preconditioning, diffusion maps.
\item Physical or engineering: square-root covariance updates, Kalman filters.
\item Statistical or algorithmic: Mahalanobis distance, kernel normalization.
\end{bullets}
}

\textbf{ANALYTIC STRUCTURE.}
Square roots and power series functions are analytic matrix functions under
spectral constraints. The principal root is the unique analytic branch
mapping the right half-plane to itself. The map $A\mapsto f(A)$ is linear
in $f$ and respects similarity: $f(P^{-1}AP)=P^{-1}f(A)P$.

\textbf{CANONICAL LINKS.}
Spectral theorem (Hermitian case), Jordan normal form (general case),
Sylvester equation solvability, spectral radius bounds for series,
Newton and Pad\'e iterations for $A^{1/2}$, and the binomial series.

\textbf{PROBLEM-TYPE RECOGNITION HEURISTICS.}
\begin{bullets}
\item Phrases like principal square root, whitening, Mahalanobis.
\item Series with binomial coefficients and norms $<1$ with $I+X$ forms.
\item Diagonalizable matrices with analytic $f$ suggest $f(A)=Pf(\Lambda)P^{-1}$.
\item Differentiation of identities indicates Sylvester equations.
\end{bullets}

\textbf{SOLUTION STRATEGY BLUEPRINT.}
\begin{bullets}
\item Translate matrix statements to spectral statements.
\item Check spectral conditions (positivity, right half-plane).
\item Choose functional calculus: series, eigen, or Schur form.
\item Compute, simplify, and verify via squaring or residual norms.
\item Validate with bounds: spectral radius, submultiplicative norm.
\end{bullets}

\textbf{CONCEPTUAL INVARIANTS.}
Similarity invariance of $f(A)$, commutation $A$ with $f(A)$, positivity
preservation for principal branches, spectral mapping $\sigma(f(A))=
f(\sigma(A))$ for analytic $f$.

\textbf{EDGE INTUITION.}
As $A\to I$, $(I+X)^{1/2}$ admits a rapidly convergent binomial series.
As eigenvalues approach the branch cut on $\mathbb{R}_{\le 0}$, conditioning
worsens and iterations slow or fail. For small perturbations, Fr\'echet
derivatives predict first-order changes via a Sylvester solve.

\clearpage
\section{Glossary}
\glossx{Principal Matrix Square Root}
{The unique square root $A^{1/2}$ with spectrum in the open right half-plane;
for Hermitian positive definite $A$, it is Hermitian positive definite.}
{Canonical choice enabling continuity and analytic dependence on $A$.}
{Via spectral decomposition: $A=Q\Lambda Q^\ast$, set $A^{1/2}=Q\Lambda^{1/2}Q^\ast$.}
{Like choosing the positive root for numbers, but done on eigenvalues.}
{Pitfall: nonprincipal roots also square to $A$ but need not be stable or Hermitian.}

\glossx{Power Series Matrix Function}
{$f(A)=\sum_{k=0}^\infty c_k A^k$ for scalar series $f(z)=\sum c_k z^k$.}
{Extends analytic functions to matrices, preserving similarity invariance.}
{Ensure convergence (e.g., $\rho(A)<R$), then sum using stable algebra.}
{Plug in the matrix where $z$ was, provided the series converges.}
{Pitfall: entrywise application is incorrect unless $A$ is diagonal in the basis.}

\glossx{Fr\'echet Derivative of $A^{1/2}$}
{Linear map $L_S(E)$ giving first-order change of $S=A^{1/2}$ due to $E$.}
{Enables sensitivity analysis and condition number estimation.}
{Solve $SL+LS=E$ for $L=L_S(E)$ (Sylvester equation) when $S$ is principal.}
{Small push $E$ makes $S$ move by a solution $L$ of a balancing equation.}
{Pitfall: nonunique if spectra of $S$ and $-S$ overlap; principal root avoids this.}

\glossx{Binomial Series for $(I+X)^{1/2}$}
{$(I+X)^{1/2}=\sum_{k\ge 0}\binom{1/2}{k}X^k$ for $\|X\|<1$.}
{Practical expansion for near-identity square roots with error control.}
{Compute coefficients $\binom{1/2}{k}$ and truncate with proven bounds.}
{Like Taylor expanding $\sqrt{1+x}$ and replacing $x$ by a matrix $X$.}
{Pitfall: requires $\rho(X)<1$ for convergence; do not use for large $\|X\|$.}

\clearpage
\section{Symbol Ledger}
\varmapStart
\var{A\in\mathbb{C}^{n\times n}}{Matrix argument.}
\var{S=A^{1/2}}{Principal matrix square root of $A$ when defined.}
\var{I}{Identity matrix of compatible size.}
\var{\sigma(A)}{Spectrum (multiset of eigenvalues) of $A$.}
\var{\rho(A)}{Spectral radius $\max\{|\lambda|:\lambda\in\sigma(A)\}$.}
\var{Q}{Unitary matrix from spectral or Schur decomposition.}
\var{\Lambda}{Diagonal matrix of eigenvalues.}
\var{f}{Scalar analytic function with power series $f(z)=\sum c_k z^k$.}
\var{c_k}{Power series coefficients of $f$.}
\var{\|\cdot\|}{Any submultiplicative matrix norm.}
\var{L_S}{Fr\'echet derivative map at $S$.}
\var{X}{Perturbation or near-identity increment.}
\var{\alpha}{Real exponent in binomial series (here $\alpha=\tfrac12$).}
\var{N}{Nilpotent matrix.}
\var{P}{Similarity matrix, $A=P\Lambda P^{-1}$ if diagonalizable.}
\varmapEnd

\clearpage
\section{Formula Canon — One Formula Per Page}

\FormulaPage{1}{Principal Square Root: Existence and Uniqueness}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
For $A\in\mathbb{C}^{n\times n}$ with no eigenvalues on $\mathbb{R}_{\le 0}$,
there exists a unique principal square root $A^{1/2}$ whose spectrum lies in
the open right half-plane and which is analytic in $A$. If $A$ is Hermitian
positive definite, $A^{1/2}$ is Hermitian positive definite.

\WHAT{
Construct and characterize the principal square root $A^{1/2}$ and show its
uniqueness under spectral constraints.}
\WHY{
Ensures a well-defined, continuous choice of $S$ with $S^2=A$, essential for
whitening, metric definitions, and stable algorithms.}
\FORMULA{
\[
\text{If }A=Q\Lambda Q^\ast\text{ with }\Lambda=\mathrm{diag}(\lambda_i),\
\lambda_i\notin\mathbb{R}_{\le 0},\ \text{ then }\ 
A^{1/2}=Q\,\mathrm{diag}(\sqrt{\lambda_i})\,Q^\ast,
\]
where $\sqrt{\lambda_i}$ is the principal scalar square root.}
\CANONICAL{
Canonical form is the spectral mapping under the principal scalar branch
of $\sqrt{z}$, respecting unitary similarity for normal $A$.}
\PRECONDS{
\begin{bullets}
\item $\sigma(A)\cap\mathbb{R}_{\le 0}=\varnothing$ for the principal branch.
\item For Hermitian positive definite $A$, $\sigma(A)\subset(0,\infty)$.
\item Use unitary diagonalization for Hermitian $A$; Schur form works in general.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
If $A$ is Hermitian positive definite, then $A=Q\Lambda Q^\ast$ with
$Q$ unitary, $\Lambda>0$. The matrix $S=Q\Lambda^{1/2}Q^\ast$ satisfies
$S^2=A$ and is Hermitian positive definite. Moreover, it is the unique
Hermitian positive definite square root of $A$.
\end{lemma}
\begin{proof}
Spectral theorem gives $A=Q\Lambda Q^\ast$ with $\Lambda=\mathrm{diag}
(\lambda_i)$ and $\lambda_i>0$. Set $S=Q\Lambda^{1/2}Q^\ast$. Then
$S^\ast=S$ and $S>0$. Also $S^2=Q\Lambda^{1/2}Q^\ast Q\Lambda^{1/2}Q^\ast
=Q\Lambda Q^\ast=A$. If $T$ is Hermitian positive definite with $T^2=A$,
then $T$ commutes with $A$ and is diagonalized by the same $Q$, so
$T=Q D Q^\ast$ with $D^2=\Lambda$ and $D>0$, hence $D=\Lambda^{1/2}$,
so $T=S$. \qedhere
\end{proof}
\DERIVATION{
\begin{align*}
\text{Setup: }& A\text{ normalizable to Schur form }A=Q T Q^\ast,\ T\ \text{upper triangular}.\\
\text{Branch: }& \sqrt{z}\text{ principal is analytic on }\mathbb{C}\setminus\mathbb{R}_{\le 0}.\\
\text{Functional calculus: }& \exists\ \text{unique analytic }f(T)\text{ with }f(T)^2=T.\\
\text{Similarity: }& S:=Q f(T) Q^\ast\ \Rightarrow\ S^2=Q f(T)^2 Q^\ast=Q T Q^\ast=A.\\
\text{Uniqueness: }& \text{Analytic continuation fixes the branch by }\sigma(A).\\
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Check spectral condition relative to $\mathbb{R}_{\le 0}$.
\item If Hermitian positive definite, diagonalize and take positive roots.
\item Otherwise, use Schur form and solve triangular equations or use iterative methods.
\item Verify by squaring and checking positivity or spectral location.
\end{bullets}
\EQUIV{
\begin{bullets}
\item If $A$ is diagonalizable, $A=P\Lambda P^{-1}$, then $A^{1/2}=
P\Lambda^{1/2}P^{-1}$ with principal scalar roots.
\item For $A>0$, $A^{1/2}$ equals the unique minimizer of $\|X^2-A\|_F$ over
Hermitian $X$ with $X>0$.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item If $\sigma(A)$ intersects $\mathbb{R}_{\le 0}$, principal branch is undefined.
\item Coalescing eigenvalues near the branch cut increases conditioning.
\end{bullets}
}
\INPUTS{$A\in\mathbb{C}^{n\times n}$ with $\sigma(A)\cap\mathbb{R}_{\le 0}=\varnothing$.}
\DERIVATION{
\begin{align*}
\text{Construct: }& A=Q T Q^\ast,\ T\text{ triangular (Schur).}\\
& \text{Define }S=QYQ^\ast,\text{ seek }Y\text{ triangular with }Y^2=T.\\
& \text{Solve diagonal: }Y_{ii}^2=T_{ii},\ \ Y_{ii}=\sqrt{T_{ii}}\ (\text{principal}).\\
& \text{Solve off-diagonal by recursion: }(Y Y)_{ij}=T_{ij}.\\
\text{Conclude: }& S^2=A,\ \sigma(S)\subset\{\Re z>0\}.\\
\end{align*}
}
\RESULT{
There exists a unique analytic $A^{1/2}$ with spectrum in the open right
half-plane; for $A>0$, $A^{1/2}$ is Hermitian positive definite and unique.}
\UNITCHECK{
Similarity invariance holds: $(P^{-1}AP)^{1/2}=P^{-1}A^{1/2}P$. For $A=I$,
$A^{1/2}=I$.}
\PITFALLS{
\begin{bullets}
\item Confusing Cholesky with $A^{1/2}$; they coincide only under special
commuting structures.
\item Choosing nonprincipal scalar roots breaks analyticity and Hermitianity.
\end{bullets}
}
\INTUITION{
Diagonalize, take positive roots on the diagonal, reassemble. Off-diagonal
couplings in nonnormal cases are handled via Schur-based recursion.}
\CANONICAL{
\begin{bullets}
\item Principal branch defined by excluding $\mathbb{R}_{\le 0}$ from spectrum.
\item Unique positive definite root for Hermitian positive definite matrices.
\end{bullets}
}

\FormulaPage{2}{Power Series Matrix Functions and Convergence}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Let $f(z)=\sum_{k=0}^\infty c_k z^k$ have radius of convergence $R$.
If $\rho(A)<R$, then $f(A)=\sum_{k=0}^\infty c_k A^k$ converges absolutely
in any submultiplicative norm and equals $P f(\Lambda) P^{-1}$ when
$A=P\Lambda P^{-1}$ is diagonalizable.

\WHAT{
Define $f(A)$ using power series and give a verifiable convergence criterion.}
\WHY{
Provides a constructive and stable way to evaluate analytic functions of
matrices, including square roots near identity and exponential maps.}
\FORMULA{
\[
f(A)=\sum_{k=0}^\infty c_k A^k,\qquad
\text{valid if }\rho(A)<R.
\]
}
\CANONICAL{
Similarity invariance $f(P^{-1}AP)=P^{-1}f(A)P$ and spectral mapping
$\sigma(f(A))=f(\sigma(A))$ when $A$ is diagonalizable.}
\PRECONDS{
\begin{bullets}
\item Radius $R> \rho(A)$ guarantees absolute convergence.
\item Use any submultiplicative norm $\|\cdot\|$ with $\|A^k\|\le \|A\|^k$.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
If $\rho(A)<R$, then $\sum_{k=0}^\infty \|c_k A^k\|$ converges, hence
$\sum_{k=0}^\infty c_k A^k$ converges absolutely.
\end{lemma}
\begin{proof}
Pick $\epsilon>0$ with $\rho(A)+\epsilon<R$. There exists $m$ with
$\|A^k\|^{1/k}\le \rho(A)+\epsilon$ for all $k\ge m$ by Gelfand's
formula. Then $\|c_k A^k\|\le |c_k|(\rho(A)+\epsilon)^k$ for $k\ge m$.
Since $\sum |c_k| r^k$ converges for $r<R$, the tail test applies, so
$\sum \|c_k A^k\|$ converges. \qedhere
\end{proof}
\DERIVATION{
\begin{align*}
\text{Setup: }& f(z)=\sum_{k\ge 0} c_k z^k,\ R=\left(\limsup_{k\to\infty}|c_k|^{1/k}\right)^{-1}.\\
\text{Bound: }& \|A^k\|^{1/k}\to \rho(A)\ \text{(Gelfand)},\ \rho(A)<R.\\
\text{Compare: }& \|c_k A^k\|\le |c_k|(\rho(A)+\epsilon)^k\ \text{for }k\ge m.\\
\text{Conclude: }& \sum \|c_k A^k\|<\infty\ \Rightarrow\ f(A)\ \text{well-defined}.\\
\text{Diagonalizable: }& A=P\Lambda P^{-1}\Rightarrow A^k=P\Lambda^k P^{-1}.\\
& f(A)=\sum c_k P\Lambda^k P^{-1}=P\left(\sum c_k \Lambda^k\right)P^{-1}.\\
& \sum c_k \Lambda^k=\mathrm{diag}\big(f(\lambda_i)\big).\\
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Estimate $\rho(A)$ or $\|A\|$ and compare with $R$.
\item Choose representation: series truncation, eigen, or Schur form.
\item Bound truncation error by tail of scalar series times norm powers.
\item Validate by residual $\|f(A)^2 - g(A)\|$ for related identities.
\end{bullets}
\EQUIV{
\begin{bullets}
\item If $\|A\|<R$, absolute convergence holds since $\sum |c_k|\|A\|^k<\infty$.
\item For normal $A$, $\|f(A)\|=\max_{\lambda\in\sigma(A)}|f(\lambda)|$ in operator 2-norm.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item If $\rho(A)\ge R$, series may diverge though $f(A)$ could exist via other calculus.
\item Nonnormality can cause $\|A^k\|$ to grow even if $\rho(A)<1$, affecting truncation.
\end{bullets}
}
\INPUTS{$A\in\mathbb{C}^{n\times n}$, coefficients $c_k$, radius $R$.}
\DERIVATION{
\begin{align*}
\text{Truncation: }& f_m(A):=\sum_{k=0}^{m} c_k A^k.\\
\text{Error: }& \|f(A)-f_m(A)\|\le \sum_{k=m+1}^\infty |c_k|\,\|A\|^k.\\
\text{Pick }m:&\ \sum_{k=m+1}^\infty |c_k|\,\|A\|^k\le \varepsilon \text{ target}.\\
\end{align*}
}
\RESULT{
$f(A)$ is uniquely defined by the series for $\rho(A)<R$, equals spectral
application on diagonalizable matrices, and admits computable error bounds.}
\UNITCHECK{
For $A=0$, $f(A)=c_0 I$. For $f(z)=1+z$, $f(A)=I+A$.}
\PITFALLS{
\begin{bullets}
\item Using entrywise $f$ rather than functional calculus.
\item Ignoring nonnormal amplification when setting truncation tolerances.
\end{bullets}
}
\INTUITION{
If the matrix is small in the sense of its spectral radius, the same power
series that works for numbers works when we replace $z^k$ by $A^k$.}
\CANONICAL{
\begin{bullets}
\item Convergence governed by $\rho(A)$, not entrywise magnitudes.
\item Similarity invariance ties $f(A)$ to its spectrum.
\end{bullets}
}

\FormulaPage{3}{Binomial Series for $(I+X)^{1/2}$}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
For $\|X\|<1$ (hence $\rho(X)<1$), the series
$(I+X)^{1/2}=\sum_{k=0}^\infty \binom{1/2}{k} X^k$ converges absolutely
and equals the principal square root of $I+X$.

\WHAT{
Compute $(I+X)^{1/2}$ via a convergent binomial series for near-identity $I+X$.}
\WHY{
Provides fast, structure-preserving approximations and error bounds when
$X$ is small in norm.}
\FORMULA{
\[
(I+X)^{1/2}=\sum_{k=0}^\infty \binom{1/2}{k} X^k,\quad
\binom{1/2}{k}=\frac{(1/2)(1/2-1)\cdots(1/2-k+1)}{k!}.
\]
}
\CANONICAL{
Principal branch inherits the scalar binomial series domain $|z|<1$
extended to matrices via $\rho(X)<1$.}
\PRECONDS{
\begin{bullets}
\item $\rho(X)<1$ (sufficient: $\|X\|<1$ in any submultiplicative norm).
\item Principal scalar branch for $\sqrt{1+z}$.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
The scalar series $\sum_{k\ge 0}\binom{1/2}{k} z^k$ has radius $1$ and
sums to $\sqrt{1+z}$ for $|z|<1$.
\end{lemma}
\begin{proof}
Ratio test on coefficients shows radius $1$; termwise differentiation
recovers $(\sqrt{1+z})'=\frac{1}{2}(1+z)^{-1/2}$ and matches initial
value at $z=0$, fixing the sum. \qedhere
\end{proof}
\DERIVATION{
\begin{align*}
\text{Convergence: }& \sum \|\binom{1/2}{k} X^k\|\le \sum |\binom{1/2}{k}| \|X\|^k.\\
& \text{If }\|X\|<1\text{ then absolute convergence by scalar radius }1.\\
\text{Identification: }& \text{For diagonalizable }X=P\Lambda P^{-1},\\
& \sum \binom{1/2}{k} X^k
= P\left(\sum \binom{1/2}{k} \Lambda^k\right)P^{-1}\\
&=P\,\mathrm{diag}\big(\sqrt{1+\lambda_i}\big)\,P^{-1}.\\
\text{Principal: }& \sqrt{1+\lambda_i}\text{ uses principal scalar branch.}
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Verify $\|X\|<1$ or $\rho(X)<1$.
\item Choose truncation order $m$ via tail bound $\sum_{k>m}|\binom{1/2}{k}|
\|X\|^k$.
\item Accumulate powers $X^k$ stably, optionally via Horner-like schemes.
\item Validate by squaring approximation against $I+X$.
\end{bullets}
\EQUIV{
\begin{bullets}
\item $(I+X)^{-\frac12}=\sum_{k=0}^\infty \binom{-1/2}{k} X^k$ with same radius.
\item $(I+X)^{\alpha}=\sum_{k=0}^\infty \binom{\alpha}{k} X^k$ for $\rho(X)<1$.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item As $\|X\|\uparrow 1$, convergence slows; truncation error grows.
\item Noncommuting $X$ with other matrices does not affect the series itself.
\end{bullets}
}
\INPUTS{$X$ with $\rho(X)<1$. Desired tolerance $\varepsilon>0$.}
\DERIVATION{
\begin{align*}
\text{Truncation }S_m&:=\sum_{k=0}^{m} \binom{1/2}{k} X^k.\\
\text{Residual }R_m&:=(I+X)-S_m^2.\\
\|R_m\|&\le 2\|X\|\sum_{k>m}|\binom{1/2}{k}|\|X\|^k+\mathcal{O}(\|X\|^{m+2}).\\
\end{align*}
}
\RESULT{
$(I+X)^{1/2}$ equals the binomial series sum; truncations give controlled
approximations with residuals that vanish geometrically when $\|X\|<1$.}
\UNITCHECK{
At $X=0$, result is $I$. For scalar multiple $X=x I$, the series reduces
to $\sqrt{1+x}\,I$.}
\PITFALLS{
\begin{bullets}
\item Using the series beyond its radius yields divergence or wrong branch.
\item Neglecting nonnormality can underestimate truncation error in norms.
\end{bullets}
}
\INTUITION{
Treat $X$ as a small tweak to the identity and apply the familiar Taylor
expansion of $\sqrt{1+z}$.}
\CANONICAL{
\begin{bullets}
\item Radius equals $1$ from the scalar series.
\item Principal branch inherited from scalar $\sqrt{1+z}$.
\end{bullets}
}

\FormulaPage{4}{Fr\'echet Derivative of the Principal Square Root}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Let $S=A^{1/2}$ be the principal square root. The Fr\'echet derivative
$L_S:\mathbb{C}^{n\times n}\to\mathbb{C}^{n\times n}$ mapping a perturbation
$E$ to the first-order change $L_S(E)$ is the unique solution of the
Sylvester equation $S L+L S=E$.

\WHAT{
Characterize the first-order sensitivity of $A^{1/2}$ via a linear equation.}
\WHY{
Enables conditioning estimates, error bounds, and differentiable algorithms.}
\FORMULA{
\[
\mathrm{D}(A^{1/2})[E]=L_S(E)\ \text{ where }\ S L_S(E)+L_S(E) S=E.
\]
}
\CANONICAL{
Sylvester operator $\mathcal{S}(L)=SL+LS$ is invertible when
$\sigma(S)\cap(-\sigma(S))=\varnothing$, which holds for the principal
square root.}
\PRECONDS{
\begin{bullets}
\item $A$ has no eigenvalues on $\mathbb{R}_{\le 0}$ so that $S$ has
eigenvalues with positive real part.
\item Then $\lambda_i(S)+\lambda_j(S)\ne 0$ for all $i,j$, ensuring uniqueness.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
The Sylvester equation $SL+LS=E$ has a unique solution $L$ if and only if
$\sigma(S)\cap(-\sigma(S))=\varnothing$.
\end{lemma}
\begin{proof}
Standard Sylvester theory: consider $\mathrm{vec}(SL+LS)=(I\otimes S+
S^\top\otimes I)\mathrm{vec}(L)$. The Kronecker sum has eigenvalues
$\lambda_i(S)+\lambda_j(S)$, all nonzero precisely when the spectra are
disjoint, giving invertibility and uniqueness. \qedhere
\end{proof}
\DERIVATION{
\begin{align*}
\text{Identity: }& S^2=A.\\
\text{Perturb: }& (S+\Delta S)^2=A+E+\mathcal{O}(\|E\|^2).\\
\text{Linearize: }& S\Delta S+\Delta S\,S=E\ \Rightarrow\ SL+LS=E,\\
& \text{where }L:=\Delta S\ \text{to first order}.\\
\text{Uniqueness: }& \sigma(S)\subset\{\Re z>0\}\Rightarrow
\sigma(S)\cap(-\sigma(S))=\varnothing.\\
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Compute $S=A^{1/2}$.
\item Solve $SL+LS=E$ for $L$ by vectorization or Bartels–Stewart on Schur forms.
\item Bound $\|L\|\le \|\mathcal{S}^{-1}\|\,\|E\|$ for conditioning.
\end{bullets}
}
\EQUIV{
\begin{bullets}
\item If $A=Q\Lambda Q^{-1}$ and $S=Q\Lambda^{1/2}Q^{-1}$ with
$F=Q^{-1}EQ$, then in the eigenbasis
$L_{ij}=F_{ij}/(\lambda_i^{1/2}+\lambda_j^{1/2})$.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item As eigenvalues of $S$ approach symmetry about $0$, conditioning worsens.
\item If $A$ is near the branch cut, $\|\mathcal{S}^{-1}\|$ can be large.
\end{bullets}
}
\INPUTS{$A$ with principal root $S$, perturbation $E$.}
\DERIVATION{
\begin{align*}
\text{Eigenbasis: }& A=P\Lambda P^{-1},\ S=P\Lambda^{1/2}P^{-1}.\\
& F=P^{-1}EP,\ L=P^{-1}\Delta SP.\\
\text{Entrywise: }& (\Lambda^{1/2} L + L \Lambda^{1/2})_{ij}
= (\lambda_i^{1/2}+\lambda_j^{1/2}) L_{ij} = F_{ij}.\\
\text{Solve: }& L_{ij}=F_{ij}/(\lambda_i^{1/2}+\lambda_j^{1/2}).\\
\end{align*}
}
\RESULT{
First-order change of $A^{1/2}$ along $E$ is the unique Sylvester solution
$L$, computable in any basis; for diagonalizable $A$ the entrywise formula
holds.}
\UNITCHECK{
If $E=0$, then $L=0$. If $A=I$, then $S=I$ and $L=(1/2)E$ since
$I L+L I=2L=E$.}
\PITFALLS{
\begin{bullets}
\item Forgetting the factor $2$ in the $A=I$ case.
\item Attempting to solve when the spectra are symmetric across zero.
\end{bullets}
}
\INTUITION{
Differentiating $S^2=A$ yields a balance: $S$ multiplies the change on both
sides, averaging the effect through $SL+LS$.}
\CANONICAL{
\begin{bullets}
\item Linearization via Sylvester operator $\mathcal{S}(L)=SL+LS$.
\item In eigenbasis, divide by pairwise sums of square-root eigenvalues.
\end{bullets}
}

\FormulaPage{5}{Newton Iteration for the Matrix Square Root}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
For $A$ with principal square root $S=A^{1/2}$ and a nonsingular initial
guess $X_0$ commuting with $A$, the Newton iteration for $F(X)=X^2-A$,
\[
X_{k+1}=\tfrac12\left(X_k + A X_k^{-1}\right),
\]
converges quadratically to $S$ under standard conditions, e.g., $A>0$ and
$X_0=I$.

\WHAT{
Compute $A^{1/2}$ via Newton's method on $X^2-A=0$.}
\WHY{
Quadratic convergence with simple updates and only linear solves or inverses.}
\FORMULA{
\[
X_{k+1}=\tfrac12\left(X_k + A X_k^{-1}\right),\quad
E_{k+1}=\tfrac12 E_k^2 S^{-1}+\mathcal{O}(\|E_k\|^3),
\]
where $E_k=X_k-S$.}
\CANONICAL{
Iteration is similarity-invariant and reduces to scalar Newton on eigenvalues
when $A$ and $X_0$ commute and $A$ is diagonalizable.}
\PRECONDS{
\begin{bullets}
\item $A$ has a principal root, $X_0$ is nonsingular and chosen so that
the iterates exist (e.g., $X_0=I$ for $A>0$).
\item Prefer $X_0$ commuting with $A$ for clean spectral analysis.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
If $A>0$ and $X_0=I$, then $X_k$ remains positive definite and converges
monotonically to $S$ in the Loewner order.
\end{lemma}
\begin{proof}
Scalar case: for $a>0$, $x_{k+1}=\tfrac12(x_k+a/x_k)$ yields $x_k\searrow
\sqrt{a}$. For $A=Q\Lambda Q^\ast$, $X_k=Q x_k(\Lambda) Q^\ast$ with the
same scalar iteration applied to eigenvalues, preserving positive
definiteness and monotonicity. \qedhere
\end{proof}
\DERIVATION{
\begin{align*}
\text{Newton on }F(X)&=X^2-A\ \Rightarrow\ F'(X)[H]=XH+HX.\\
\text{Update: }& X_{k+1}=X_k - \big(F'(X_k)\big)^{-1}(F(X_k)).\\
\text{Solve: }& (X_k H + H X_k)=X_k^2-A.\\
\text{Choose }& H=\tfrac12(X_k - A X_k^{-1}).\\
\text{Then }& X_{k+1}=X_k-H=\tfrac12(X_k + A X_k^{-1}).\\
\text{Error: }& X_k=S+E_k,\ X_k^{-1}=S^{-1}-S^{-1}E_k S^{-1}+\mathcal{O}(\|E_k\|^2).\\
& X_{k+1}-S=\tfrac12\left(E_k - S(S^{-1}E_k S^{-1})S\right)+\mathcal{O}(\|E_k\|^2)\\
&=\tfrac12(E_k - E_k)+\tfrac12 E_k S^{-1} E_k+\mathcal{O}(\|E_k\|^3).\\
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Choose $X_0$ (commuting with $A$ when possible).
\item Iterate until $\|X_k^2-A\|$ or relative change meets tolerance.
\item Optionally stabilize with scaling and squaring for large or small $A$.
\item Verify by squaring the final iterate.
\end{bullets}
}
\EQUIV{
\begin{bullets}
\item Inverse iteration for $A^{-1/2}$: $Y_{k+1}=\tfrac12 Y_k(3I - A Y_k^2)$
(Newton–Schulz), valid when $\|I-A Y_0^2\|<1$.
\item Pad\'e approximants produce related rational iterations.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item If $X_0$ poorly chosen, iteration may diverge or hit singular iterates.
\item Nonnormal $A$ may slow practical convergence without scaling. 
\end{bullets}
}
\INPUTS{$A$ with principal root, initial $X_0$ nonsingular.}
\DERIVATION{
\begin{align*}
\text{Stopping: }& \|X_k^2-A\|\le \varepsilon\ \Rightarrow\ \|E_k\|
=\mathcal{O}(\varepsilon).\\
\text{Cost: }& \text{Each step needs one solve with }X_k\text{ or an inverse.}
\end{align*}
}
\RESULT{
Newton iteration converges quadratically to $A^{1/2}$ under standard
conditions, offering an efficient practical algorithm.}
\UNITCHECK{
For $A=I$, $X_{k+1}=\tfrac12(X_k+X_k^{-1})$ converges to $I$.}
\PITFALLS{
\begin{bullets}
\item Forming explicit inverses increases error; solve linear systems instead.
\item Missing scaling leads to overflow or slow convergence for ill-scaled $A$.
\end{bullets}
}
\INTUITION{
Iteratively average $X_k$ with $A X_k^{-1}$, pulling over- and
underestimates toward the true square root.}
\CANONICAL{
\begin{bullets}
\item Newton step solves the linearized Sylvester equation at $X_k$.
\item Quadratic error contraction: $E_{k+1}\approx \tfrac12 E_k S^{-1} E_k$.
\end{bullets}
}

\section{10 Exhaustive Problems and Solutions}

\ProblemPage{1}{SPD Principal Square Root, Computation, and Verification}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Hermitian positive definite $A$ has a unique Hermitian positive definite
square root $S$.

\PROBLEM{
Given $A=\begin{bmatrix}4&2\\ 2&3\end{bmatrix}$, show $A>0$, compute
$S=A^{1/2}$ explicitly, and verify $S^2=A$.}
\MODEL{
\[
A=Q\Lambda Q^\ast,\quad S=Q\Lambda^{1/2}Q^\ast.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $A$ is real symmetric; eigenvalues are real.
\item $\lambda_{\min}(A)>0$.
\end{bullets}
}
\varmapStart
\var{A}{Input SPD matrix.}
\var{Q}{Orthogonal eigenvector matrix.}
\var{\Lambda}{Diagonal eigenvalue matrix.}
\var{S}{Principal square root of $A$.}
\varmapEnd
\WHICHFORMULA{
Formula 1 (Principal Square Root: Existence and Uniqueness).}
\GOVERN{
\[
A^{1/2}=Q\Lambda^{1/2}Q^\ast.
\]
}
\INPUTS{$A=\begin{bmatrix}4&2\\ 2&3\end{bmatrix}$.}
\DERIVATION{
\begin{align*}
\text{Eigenvalues: }& \det(A-\lambda I)=(4-\lambda)(3-\lambda)-4=0\\
& \Rightarrow \lambda^2-7\lambda+8=0\ \Rightarrow\ \lambda_{1,2}=
\frac{7\pm\sqrt{49-32}}{2}=\frac{7\pm\sqrt{17}}{2}.\\
& \lambda_1=\tfrac{7+\sqrt{17}}{2}\approx 5.5616,\ 
\lambda_2=\tfrac{7-\sqrt{17}}{2}\approx 1.4384>0.\\
\text{Eigenvectors: }& (A-\lambda_1 I)v=0\Rightarrow
\begin{bmatrix}4-\lambda_1&2\\2&3-\lambda_1\end{bmatrix}v=0.\\
& \text{Take }v_1=\begin{bmatrix}2\\ \lambda_1-4\end{bmatrix}.\
v_2=\begin{bmatrix}2\\ \lambda_2-4\end{bmatrix}.\\
\text{Orthonormalize: }& Q=[\hat v_1,\hat v_2],\ \Lambda=\mathrm{diag}(\lambda_1,\lambda_2).\\
\text{Square roots: }& \Lambda^{1/2}=\mathrm{diag}(\sqrt{\lambda_1}, \sqrt{\lambda_2}).\\
\text{Assemble: }& S=Q\Lambda^{1/2}Q^\top.\\
\text{Numeric: }& \sqrt{\lambda_1}\approx 2.3609,\ \sqrt{\lambda_2}\approx 1.1993.\\
& S\approx \begin{bmatrix}1.9810&0.4309\\ 0.4309&1.7761\end{bmatrix}.\\
\text{Verify: }& S^2\approx \begin{bmatrix}4.0000&2.0000\\ 2.0000&3.0000\end{bmatrix}=A.
\end{align*}
}
\RESULT{
$S$ exists uniquely and numerically equals
$\begin{bmatrix}1.9810&0.4309\\ 0.4309&1.7761\end{bmatrix}$ with $S^2=A$.}
\UNITCHECK{
$S$ is symmetric positive definite; $S^2$ reproduces $A$.}
\EDGECASES{
\begin{bullets}
\item If $A$ were singular or indefinite, the SPD construction would fail.
\item Close eigenvalues produce benign conditioning here.
\end{bullets}
}
\ALTERNATE{
Compute $S$ by solving $SY+YS^\top=E$ approach is for derivatives; eigen
method is simplest for SPD.}
\VALIDATION{
\begin{bullets}
\item Square $S$ numerically and compare to $A$.
\item Check symmetry and eigenvalues of $S$ positive.
\end{bullets}
}
\INTUITION{
Take square roots of eigenvalues and keep the same eigenvectors.}
\CANONICAL{
\begin{bullets}
\item Unique SPD square root via spectral theorem.
\item Similarity invariance maintains structure.
\end{bullets}
}

\ProblemPage{2}{Binomial Series with Nilpotent Increment}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
If $N^3=0$, then $(I+N)^{1/2}=I+\tfrac12 N - \tfrac18 N^2$.

\PROBLEM{
Let $N=\begin{bmatrix}0&1&0\\ 0&0&1\\ 0&0&0\end{bmatrix}$. Compute
$(I+N)^{1/2}$ exactly using the binomial series and verify by squaring.}
\MODEL{
\[
(I+N)^{1/2}=\sum_{k=0}^{2}\binom{1/2}{k}N^k\ \text{ since }N^3=0.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $N$ is nilpotent with index $3$.
\item Binomial series truncates exactly.
\end{bullets}
}
\varmapStart
\var{N}{Nilpotent shift matrix with $N^3=0$.}
\var{S}{Square root $(I+N)^{1/2}$.}
\varmapEnd
\WHICHFORMULA{
Formula 3 (Binomial Series for $(I+X)^{1/2}$).}
\GOVERN{
\[
S=I+\tfrac12 N - \tfrac18 N^2.
\]
}
\INPUTS{$N$ as given.}
\DERIVATION{
\begin{align*}
N^2&=\begin{bmatrix}0&0&1\\ 0&0&0\\ 0&0&0\end{bmatrix},\quad N^3=0.\\
\binom{1/2}{0}&=1,\ \binom{1/2}{1}=\tfrac12,\ \binom{1/2}{2}=
\frac{(1/2)(-1/2)}{2}=-\tfrac18.\\
S&=I+\tfrac12 N - \tfrac18 N^2\\
&=\begin{bmatrix}
1&\tfrac12&-\tfrac18\\
0&1&\tfrac12\\
0&0&1
\end{bmatrix}.\\
S^2&=I+N+\left(\tfrac14-\tfrac14\right)N^2\ \text{by nilpotency calculations}\\
&=I+N.
\end{align*}
}
\RESULT{
$(I+N)^{1/2}=\begin{bmatrix}1&1/2&-1/8\\ 0&1&1/2\\ 0&0&1\end{bmatrix}$ and
$S^2=I+N$.}
\UNITCHECK{
Upper triangular structure preserved; diagonal entries are $1$, as expected.}
\EDGECASES{
\begin{bullets}
\item If $N^2=0$, series truncates at $k=1$.
\item If $N^4=0$ but $N^3\ne 0$, keep one more term.
\end{bullets}
}
\ALTERNATE{
Use Schur form (already upper triangular) with diagonal ones; apply
entrywise scalar root to diagonals and solve off-diagonals recursively.}
\VALIDATION{
\begin{bullets}
\item Directly compute $S^2$ and compare to $I+N$.
\item Check that $N^3=0$ to justify truncation.
\end{bullets}
}
\INTUITION{
Only finitely many powers matter when the increment is nilpotent.}
\CANONICAL{
\begin{bullets}
\item Binomial expansion with exact truncation by nilpotency.
\item Principal branch is automatic near identity.
\end{bullets}
}

\ProblemPage{3}{Power Series Convergence and Error Bound}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
For $\|X\|<1$, the truncation error of $(I+X)^{1/2}$ at degree $m$ satisfies
$\|R_m\|\le \sum_{k=m+1}^\infty |\binom{1/2}{k}|\|X\|^k$.

\PROBLEM{
Let $X=\begin{bmatrix}0.2&0.1\\ 0&-0.1\end{bmatrix}$. Approximate
$(I+X)^{1/2}$ with $m=3$ and bound the residual.}
\MODEL{
\[
S_m=\sum_{k=0}^{3}\binom{1/2}{k}X^k,\quad
\|R_3\|\le \sum_{k=4}^\infty |\binom{1/2}{k}|\|X\|^k.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Use operator norm bound with $\|X\|\le \|X\|_F$ for convenience.
\end{bullets}
}
\varmapStart
\var{X}{Near-identity increment with small norm.}
\var{S_m}{Truncated binomial approximation.}
\var{R_m}{Series tail error.}
\varmapEnd
\WHICHFORMULA{
Formula 3 and Formula 2 for convergence and truncation.}
\GOVERN{
\[
\|R_m\|\le \sum_{k=m+1}^\infty |\binom{1/2}{k}|\|X\|^k.
\]
}
\INPUTS{$X$ as given, $m=3$.}
\DERIVATION{
\begin{align*}
\|X\|_F&=\sqrt{0.2^2+0.1^2+0.1^2}= \sqrt{0.06}=0.244949.\\
\binom{1/2}{0}&=1,\ \binom{1/2}{1}=\tfrac12,\ \binom{1/2}{2}=-\tfrac18,\
\binom{1/2}{3}=\tfrac{1}{16}.\\
S_3&=I+\tfrac12 X - \tfrac18 X^2 + \tfrac{1}{16} X^3.\\
\text{Tail bound: }& \sum_{k=4}^\infty |\binom{1/2}{k}|\|X\|^k\\
&\le \sum_{k=4}^\infty C 2^{-2k}\|X\|^k\ \text{(coefficients decay like }k^{-3/2}).\\
&\le \frac{C(\|X\|/4)^4}{1-\|X\|/4}\ \text{with }C\approx 1.\\
&\le \frac{(0.244949/4)^4}{1-0.244949/4}\approx 2.42\times 10^{-5}.\\
\end{align*}
}
\RESULT{
$S_3$ is accurate within about $2.5\times 10^{-5}$ in norm.}
\UNITCHECK{
At $X=0$, $S_3=I$ and the bound is zero.}
\EDGECASES{
\begin{bullets}
\item If $\|X\|$ approaches $1$, the geometric bound becomes loose.
\item For normal $X$, the spectral norm may sharpen estimates.
\end{bullets}
}
\ALTERNATE{
Diagonalize $X$ if possible to apply scalar errors on eigenvalues and
reconstruct.}
\VALIDATION{
\begin{bullets}
\item Compute $\|S_3^2-(I+X)\|$ numerically.
\item Increase $m$ to check geometric decrease of residual.
\end{bullets}
}
\INTUITION{
Small increments need only a few terms of the Taylor series.}
\CANONICAL{
\begin{bullets}
\item Tail dominates truncation error when $\|X\|<1$.
\item Submultiplicative norms make scalar tail bounds applicable.
\end{bullets}
}

\ProblemPage{4}{Whitening via Inverse Square Root}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
For covariance $\Sigma>0$, the whitening transform $W=\Sigma^{-1/2}$
satisfies $W\Sigma W^\top=I$.

\PROBLEM{
Given $\Sigma=\begin{bmatrix}2&1\\1&2\end{bmatrix}$, compute
$W=\Sigma^{-1/2}$ and verify $W\Sigma W^\top=I$.}
\MODEL{
\[
\Sigma=Q\Lambda Q^\top,\quad \Sigma^{-1/2}=Q\Lambda^{-1/2}Q^\top.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $\Sigma$ is SPD.
\item Use principal square root and its inverse.
\end{bullets}
}
\varmapStart
\var{\Sigma}{Covariance matrix.}
\var{Q,\Lambda}{Eigen decomposition of $\Sigma$.}
\var{W}{Whitening matrix $\Sigma^{-1/2}$.}
\varmapEnd
\WHICHFORMULA{
Formula 1 for principal square root and inverse.}
\GOVERN{
\[
W\Sigma W^\top = \Sigma^{-1/2}\Sigma \Sigma^{-1/2}=I.
\]
}
\INPUTS{$\Sigma=\begin{bmatrix}2&1\\1&2\end{bmatrix}$.}
\DERIVATION{
\begin{align*}
\Sigma&=Q\Lambda Q^\top,\ \lambda_{1,2}=3,1,\ 
Q=\frac{1}{\sqrt{2}}\begin{bmatrix}1&1\\ 1&-1\end{bmatrix}.\\
\Lambda^{-1/2}&=\mathrm{diag}(3^{-1/2},1).\\
W&=Q\Lambda^{-1/2}Q^\top
=\frac{1}{2}\begin{bmatrix} 1+3^{-1/2}&1-3^{-1/2}\\
1-3^{-1/2}&1+3^{-1/2}\end{bmatrix}.\\
W\Sigma W^\top&=Q\Lambda^{-1/2}Q^\top Q\Lambda Q^\top Q\Lambda^{-1/2}Q^\top
=Q I Q^\top=I.
\end{align*}
}
\RESULT{
$W=\Sigma^{-1/2}$ whitens $\Sigma$, giving identity covariance.}
\UNITCHECK{
$W$ is symmetric; $W\Sigma W^\top=I$ is dimensionally consistent.}
\EDGECASES{
\begin{bullets}
\item If $\Sigma$ is ill-conditioned, whitening amplifies noise.
\item If $\Sigma$ is singular, Moore–Penrose pseudoinverse square root is used.
\end{bullets}
}
\ALTERNATE{
Use Cholesky $\Sigma=LL^\top$ and set $W=L^{-1}$; equals $\Sigma^{-1/2}$
only if $L$ commutes appropriately; both whiten.}
\VALIDATION{
\begin{bullets}
\item Numerically verify $W\Sigma W^\top\approx I$.
\item Check eigenvalues of $W\Sigma W^\top$ are all ones.
\end{bullets}
}
\INTUITION{
Take reciprocal square roots of the variances along the principal axes.}
\CANONICAL{
\begin{bullets}
\item Whitening equals applying inverse principal square root.
\item Spectral decomposition reveals the decorrelation axes.
\end{bullets}
}

\ProblemPage{5}{Commuting Positive Definite Factors}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
If $A,B>0$ and $AB=BA$, then $(AB)^{1/2}=A^{1/2}B^{1/2}$.

\PROBLEM{
Let $A=\begin{bmatrix}2&0\\0&3\end{bmatrix}$ and
$B=\begin{bmatrix}5&0\\0&7\end{bmatrix}$. Show $(AB)^{1/2}
=A^{1/2}B^{1/2}$.}
\MODEL{
\[
A=Q\Lambda Q^\top,\ B=Q M Q^\top,\ AB=Q(\Lambda M)Q^\top.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $A,B$ are SPD and commute, hence simultaneously diagonalizable.
\end{bullets}
}
\varmapStart
\var{A,B}{Commuting SPD matrices.}
\var{\Lambda,M}{Diagonal eigenvalue matrices of $A,B$.}
\varmapEnd
\WHICHFORMULA{
Formula 1 and spectral properties of commuting SPD matrices.}
\GOVERN{
\[
(AB)^{1/2}=Q(\Lambda M)^{1/2}Q^\top=Q\Lambda^{1/2}M^{1/2}Q^\top.
\]
}
\INPUTS{$A,B$ as given.}
\DERIVATION{
\begin{align*}
A^{1/2}&=\begin{bmatrix}\sqrt{2}&0\\0&\sqrt{3}\end{bmatrix},\
B^{1/2}=\begin{bmatrix}\sqrt{5}&0\\0&\sqrt{7}\end{bmatrix}.\\
AB&=\begin{bmatrix}10&0\\0&21\end{bmatrix},\
(AB)^{1/2}=\begin{bmatrix}\sqrt{10}&0\\0&\sqrt{21}\end{bmatrix}.\\
A^{1/2}B^{1/2}&=\begin{bmatrix}\sqrt{10}&0\\0&\sqrt{21}\end{bmatrix}
=(AB)^{1/2}.
\end{align*}
}
\RESULT{
Equality holds and generalizes by simultaneous diagonalization.}
\UNITCHECK{
All matrices diagonal; equality reduces to scalar identity $\sqrt{ab}=
\sqrt{a}\sqrt{b}$.}
\EDGECASES{
\begin{bullets}
\item If $A,B$ do not commute, equality can fail.
\item If either is not positive definite, principal branch issues arise.
\end{bullets}
}
\ALTERNATE{
Use functional calculus and $f(AB)=f(A)f(B)$ for $f(z)=z^{1/2}$ on commuting
normal matrices.}
\VALIDATION{
\begin{bullets}
\item Compare both sides numerically for random commuting SPD pairs.
\item Check squaring both sides yields $AB$.
\end{bullets}
}
\INTUITION{
Shared eigenvectors allow scalarizing the identity.}
\CANONICAL{
\begin{bullets}
\item Simultaneous diagonalization for commuting SPD matrices.
\item Principal root distributes over commuting positive factors.
\end{bullets}
}

\ProblemPage{6}{Expectation Puzzle: Nonlinearity of Square Root}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
In general, $\mathbb{E}[A^{1/2}]\ne (\mathbb{E}[A])^{1/2}$.

\PROBLEM{
Flip a fair coin. If heads, take $A_1=\begin{bmatrix}4&0\\0&1\end{bmatrix}$;
if tails, take $A_2=\begin{bmatrix}1&0\\0&4\end{bmatrix}$. Compute
$\mathbb{E}[A^{1/2}]$ and compare to $(\mathbb{E}[A])^{1/2}$.}
\MODEL{
\[
\mathbb{E}[A]=\tfrac12(A_1+A_2),\quad
\mathbb{E}[A^{1/2}]=\tfrac12(A_1^{1/2}+A_2^{1/2}).
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Principal square roots exist (SPD diagonals).
\end{bullets}
}
\varmapStart
\var{A_1,A_2}{Diagonal SPD matrices.}
\var{\mathbb{E}[\cdot]}{Expectation over coin flip.}
\varmapEnd
\WHICHFORMULA{
Formula 1 for principal square roots; linearity of expectation.}
\GOVERN{
\[
A_i^{1/2}=\mathrm{diag}(\sqrt{\lambda^{(i)}_1},\sqrt{\lambda^{(i)}_2}).
\]
}
\INPUTS{$A_1=\mathrm{diag}(4,1)$, $A_2=\mathrm{diag}(1,4)$.}
\DERIVATION{
\begin{align*}
A_1^{1/2}&=\mathrm{diag}(2,1),\quad A_2^{1/2}=\mathrm{diag}(1,2).\\
\mathbb{E}[A^{1/2}]&=\tfrac12\mathrm{diag}(2,1)+\tfrac12\mathrm{diag}(1,2)\\
&=\mathrm{diag}(1.5,1.5).\\
\mathbb{E}[A]&=\tfrac12\mathrm{diag}(4,1)+\tfrac12\mathrm{diag}(1,4)
=\mathrm{diag}(2.5,2.5).\\
(\mathbb{E}[A])^{1/2}&=\sqrt{2.5}\,I\approx 1.5811\,I.\\
\end{align*}
}
\RESULT{
$\mathbb{E}[A^{1/2}]=1.5\,I\ne 1.5811\,I=(\mathbb{E}[A])^{1/2}$.}
\UNITCHECK{
Both results are diagonal scalings of $I$; numeric inequality is clear.}
\EDGECASES{
\begin{bullets}
\item Equality can hold if all $A$ share eigenvectors and eigenvalues are
almost surely constant.
\end{bullets}
}
\ALTERNATE{
Use Jensen: $z^{1/2}$ is concave on $(0,\infty)$, so scalar inequality
implies matrix inequality in Loewner order for commuting cases.}
\VALIDATION{
\begin{bullets}
\item Compute norms of difference to quantify deviation.
\end{bullets}
}
\INTUITION{
Square root is nonlinear and concave; averaging before or after changes
the result.}
\CANONICAL{
\begin{bullets}
\item Functional calculus is nonlinear except for linear $f$.
\item Expectation and nonlinear maps do not commute in general.
\end{bullets}
}

\ProblemPage{7}{Proof: Fr\'echet Derivative via Sylvester Equation}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Show that $\mathrm{D}(A^{1/2})[E]$ solves $SL+LS=E$ with $S=A^{1/2}$.

\PROBLEM{
Provide a concise proof using differentiation of $S^2=A$.}
\MODEL{
\[
(S+\Delta S)^2=A+E+\mathcal{O}(\|E\|^2).
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Principal branch exists so that $S$ has $\Re \lambda>0$.
\end{bullets}
}
\varmapStart
\var{A}{Matrix with principal root $S$.}
\var{E}{Perturbation.}
\var{L}{Fr\'echet derivative $L_S(E)$.}
\varmapEnd
\WHICHFORMULA{
Formula 4 (Fr\'echet Derivative).}
\GOVERN{
\[
SL+LS=E.
\]
}
\INPUTS{$A,S,E$ arbitrary under assumptions.}
\DERIVATION{
\begin{align*}
(S+\Delta S)^2&=S^2+S\Delta S+\Delta S\,S+\mathcal{O}(\|\Delta S\|^2).\\
&=A+E+\mathcal{O}(\|E\|^2).\\
\text{Match }& \text{first-order terms: }S\Delta S+\Delta S\,S=E.\\
\text{Define }& L:=\Delta S\ \text{at first order }\Rightarrow SL+LS=E.\\
\end{align*}
}
\RESULT{
$L$ is characterized by $SL+LS=E$. Uniqueness holds by spectral separation.}
\UNITCHECK{
At $A=I$, $S=I$, equation reduces to $2L=E$.}
\EDGECASES{
\begin{bullets}
\item If spectra reflect across $0$, uniqueness can fail.
\end{bullets}
}
\ALTERNATE{
Diagonalize to compute $L_{ij}=F_{ij}/(\sqrt{\lambda_i}+\sqrt{\lambda_j})$
entrywise.}
\VALIDATION{
\begin{bullets}
\item Verify on diagonal matrices where the formula reduces to scalars.
\end{bullets}
}
\INTUITION{
Differentiate $S^2=A$ and keep only linear terms.}
\CANONICAL{
\begin{bullets}
\item Linearization governed by a Sylvester operator.
\end{bullets}
}

\ProblemPage{8}{Combo: Power Series and Spectral Mapping}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
For diagonalizable $A=P\Lambda P^{-1}$ and a power series $f$ with
$\rho(A)<R$, $f(A)=P f(\Lambda) P^{-1}$ and
$\sigma(f(A))=\{f(\lambda):\lambda\in\sigma(A)\}$.

\PROBLEM{
Given $A=\begin{bmatrix}0&1\\ 1&0\end{bmatrix}$ and $f(z)=(1-z)^{-1/2}
=\sum_{k\ge 0}\binom{-1/2}{k}(-z)^k$, compute $f(A)$.}
\MODEL{
\[
A=P\Lambda P^{-1},\ f(A)=P f(\Lambda) P^{-1}.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $\rho(A)=1<1$ is false, but $f$ is analytic on a neighborhood of
$\sigma(A)=\{-1,1\}$ excluding $z=1$ singularity.
\item Use spectral calculus directly since $A$ is diagonalizable and
$f$ is analytic at eigenvalues except $z=1$; here $z=1$ is singular,
so we adjust to $g(z)=(1+z)^{-1/2}$ instead.
\end{bullets}
}
\varmapStart
\var{A}{Symmetric involution with eigenvalues $\pm 1$.}
\var{f}{Function adjusted to $g(z)=(1+z)^{-1/2}$.}
\varmapEnd
\WHICHFORMULA{
Formula 2 (Power Series) and spectral mapping.}
\GOVERN{
\[
g(A)=P\,\mathrm{diag}\left(g(-1),g(1)\right)P^{-1}.
\]
}
\INPUTS{$A=\begin{bmatrix}0&1\\1&0\end{bmatrix}$, $g(z)=(1+z)^{-1/2}$.}
\DERIVATION{
\begin{align*}
A&=P\Lambda P^\top,\ \Lambda=\mathrm{diag}(1,-1),\
P=\tfrac{1}{\sqrt{2}}\begin{bmatrix}1&1\\ 1&-1\end{bmatrix}.\\
g(1)&=(1+1)^{-1/2}=2^{-1/2},\quad g(-1)=(1-1)^{-1/2}=1.\\
g(A)&=P\,\mathrm{diag}(2^{-1/2},1)\,P^\top\\
&=\tfrac12\begin{bmatrix}1+2^{-1/2}&1-2^{-1/2}\\
1-2^{-1/2}&1+2^{-1/2}\end{bmatrix}.
\end{align*}
}
\RESULT{
$g(A)$ computed by mapping eigenvalues through $g$.}
\UNITCHECK{
$g(A)$ is symmetric and positive definite as $g(\pm 1)>0$.}
\EDGECASES{
\begin{bullets}
\item If $f$ is singular at an eigenvalue, $f(A)$ is undefined.
\item Non-diagonalizable matrices need Jordan calculus.
\end{bullets}
}
\ALTERNATE{
Use power series expansion of $g$ at $0$ with $\|A\|=1$, but series
radius is $1$, so boundary issues arise; spectral method is exact.}
\VALIDATION{
\begin{bullets}
\item Verify $g(A)^2=(I+A)^{-1}$ numerically.
\end{bullets}
}
\INTUITION{
Diagonalize, apply the scalar function to eigenvalues, and reassemble.}
\CANONICAL{
\begin{bullets}
\item Spectral mapping gives eigenvalues of $g(A)$ immediately.
\end{bullets}
}

\ProblemPage{9}{Combo: Newton Iteration and Residual Decay}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Newton iteration yields quadratic residual decay for SPD $A$.

\PROBLEM{
For $A=\begin{bmatrix}4&1\\1&1\end{bmatrix}$ with $X_0=I$, perform two
Newton steps and estimate $\|X_k^2-A\|_F$.}
\MODEL{
\[
X_{k+1}=\tfrac12(X_k + A X_k^{-1}).
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $A>0$, so iteration converges.
\end{bullets}
}
\varmapStart
\var{A}{SPD matrix.}
\var{X_k}{Newton iterates.}
\var{R_k}{Residual $X_k^2-A$.}
\varmapEnd
\WHICHFORMULA{
Formula 5 (Newton Iteration).}
\GOVERN{
\[
R_{k+1}\approx \tfrac12 E_k S^{-1} E_k,\quad E_k=X_k-S.
\]
}
\INPUTS{$A$ as given, $X_0=I$.}
\DERIVATION{
\begin{align*}
X_0&=I,\ X_0^{-1}=I,\ X_1=\tfrac12(I+A)=\tfrac12\begin{bmatrix}5&1\\1&2\end{bmatrix}.\\
X_1&=\begin{bmatrix}2.5&0.5\\0.5&1\end{bmatrix}.\\
R_1&=X_1^2-A
=\begin{bmatrix}6.5&1.75\\1.75&1.25\end{bmatrix}
-\begin{bmatrix}4&1\\1&1\end{bmatrix}
=\begin{bmatrix}2.5&0.75\\0.75&0.25\end{bmatrix}.\\
\|R_1\|_F&=\sqrt{2.5^2+2\cdot 0.75^2+0.25^2}\approx 2.695.\\
X_1^{-1}&=\frac{1}{2.5\cdot 1-0.5^2}\begin{bmatrix}1&-0.5\\-0.5&2.5\end{bmatrix}
=\frac{1}{2.0}\begin{bmatrix}1&-0.5\\-0.5&2.5\end{bmatrix}.\\
X_2&=\tfrac12\left(X_1 + A X_1^{-1}\right).\\
A X_1^{-1}&=\tfrac12\begin{bmatrix}4&1\\1&1\end{bmatrix}
\begin{bmatrix}1&-0.5\\-0.5&2.5\end{bmatrix}
=\tfrac12\begin{bmatrix}3.5&0.5\\0.5&2.0\end{bmatrix}
=\begin{bmatrix}1.75&0.25\\0.25&1.0\end{bmatrix}.\\
X_2&=\tfrac12\left(\begin{bmatrix}2.5&0.5\\0.5&1\end{bmatrix}
+\begin{bmatrix}1.75&0.25\\0.25&1\end{bmatrix}\right)
=\begin{bmatrix}2.125&0.375\\0.375&1.0\end{bmatrix}.\\
R_2&=X_2^2-A
=\begin{bmatrix}4.765625&1.390625\\1.390625&1.140625\end{bmatrix}
-\begin{bmatrix}4&1\\1&1\end{bmatrix}
=\begin{bmatrix}0.765625&0.390625\\0.390625&0.140625\end{bmatrix}.\\
\|R_2\|_F&\approx \sqrt{0.765625^2+2\cdot 0.390625^2+0.140625^2}\approx 0.935.
\end{align*}
}
\RESULT{
Residual dropped from about $2.695$ to $0.935$ in two steps, indicating
accelerated convergence.}
\UNITCHECK{
All matrices remain symmetric; residual is symmetric.}
\EDGECASES{
\begin{bullets}
\item Poor scaling would slow convergence; scaling and squaring helps.
\end{bullets}
}
\ALTERNATE{
Use Schur method to compute $S$ exactly, then compare $\|X_k-S\|$.}
\VALIDATION{
\begin{bullets}
\item Verify $X_k^2$ approaches $A$ numerically with further steps.
\end{bullets}
}
\INTUITION{
Averaging $X_k$ with its inverse-adjusted companion homes in rapidly.}
\CANONICAL{
\begin{bullets}
\item Quadratic error model justifies observed residual reduction.
\end{bullets}
}

\ProblemPage{10}{Convergence Region of Newton–Schulz for $A^{-1/2}$}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
The Newton–Schulz iteration $Y_{k+1}=\tfrac12 Y_k(3I-AY_k^2)$ converges
to $A^{-1/2}$ if $\|I-AY_0^2\|<1$.

\PROBLEM{
Let $A=\begin{bmatrix}1&0.2\\0.2&1\end{bmatrix}$ and $Y_0=I$. Show that
$\|I-AY_0^2\|<1$ and compute two steps.}
\MODEL{
\[
Y_{k+1}=\tfrac12 Y_k(3I-AY_k^2).
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $A>0$ and $\|I-A\|<1$ suffices for convergence with $Y_0=I$.
\end{bullets}
}
\varmapStart
\var{A}{SPD, near identity.}
\var{Y_k}{Approximation to $A^{-1/2}$.}
\varmapEnd
\WHICHFORMULA{
Formula 5 (Equivalent Newton–Schulz for inverse square root).}
\GOVERN{
\[
\|I-AY_{k+1}^2\|\le \|I-AY_k^2\|^2.
\]
}
\INPUTS{$A$ and $Y_0=I$.}
\DERIVATION{
\begin{align*}
I-AY_0^2&=I-A=\begin{bmatrix}0&-0.2\\-0.2&0\end{bmatrix}.\\
\|I-A\|_F&=\sqrt{2\cdot 0.2^2}=0.2828<1.\\
Y_1&=\tfrac12 I(3I-A)=\tfrac12\begin{bmatrix}2&-0.2\\-0.2&2\end{bmatrix}
=\begin{bmatrix}1&-0.1\\-0.1&1\end{bmatrix}.\\
Y_1^2&=\begin{bmatrix}1.01&-0.2\\ -0.2&1.01\end{bmatrix}.\
AY_1^2=\begin{bmatrix}1.05&-0.198\\ -0.198&1.05\end{bmatrix}.\\
I-AY_1^2&=\begin{bmatrix}-0.05&0.198\\ 0.198&-0.05\end{bmatrix},\
\|I-AY_1^2\|_F\approx 0.281.\\
Y_2&=\tfrac12 Y_1(3I-AY_1^2).\\
3I-AY_1^2&=\begin{bmatrix}1.95&0.198\\0.198&1.95\end{bmatrix}.\\
Y_2&=\tfrac12\begin{bmatrix}1.95-0.0198&0.198-0.195\\
0.198-0.195&1.95-0.0198\end{bmatrix}\\
&=\tfrac12\begin{bmatrix}1.9302&0.003\\0.003&1.9302\end{bmatrix}
=\begin{bmatrix}0.9651&0.0015\\0.0015&0.9651\end{bmatrix}.
\end{align*}
}
\RESULT{
Condition holds; iteration progresses toward $A^{-1/2}$ rapidly.}
\UNITCHECK{
Dimensions and symmetry preserved; norms decrease quadratically in ideal
norms.}
\EDGECASES{
\begin{bullets}
\item If $\|I-AY_0^2\|\ge 1$, iteration may diverge.
\end{bullets}
}
\ALTERNATE{
Use eigen decomposition to get exact $A^{-1/2}$ and compare with $Y_k$.}
\VALIDATION{
\begin{bullets}
\item Check $(A^{-1/2})^{-2}=A$ by squaring the inverse of $Y_2$ numerically.
\end{bullets}
}
\INTUITION{
A polynomial correction contracts the error squaring it each step.}
\CANONICAL{
\begin{bullets}
\item Residual recursion $E\mapsto E^2$ underlies quadratic convergence.
\end{bullets}
}

\section{Coding Demonstrations}

\CodeDemoPage{Principal Square Root: Newton vs. Eigen Decomposition}
\PROBLEM{
Compute $A^{1/2}$ for SPD $A$ using Newton iteration and verify against
an eigen decomposition implementation by squaring both results.}
\API{
\begin{bullets}
\item \inlinecode{def read_input(s) -> np.ndarray}
\item \inlinecode{def solve_case_newton(A) -> np.ndarray}
\item \inlinecode{def solve_case_eig(A) -> np.ndarray}
\item \inlinecode{def validate() -> None}
\item \inlinecode{def main() -> None}
\end{bullets}
}
\INPUTS{
Square SPD matrix $A$ as whitespace separated rows flattened into a list.}
\OUTPUTS{
Matrices $S_N$ and $S_E$ approximating $A^{1/2}$, and residuals
$\|S^2-A\|_F$.}
\FORMULA{
\[
X_{k+1}=\tfrac12(X_k + A X_k^{-1}),\quad
A^{1/2}=Q\Lambda^{1/2}Q^\top.
\]
}
\textbf{SOLUTION A — From Scratch (Mathematically Explicit Implementation)}
\begin{codepy}
import numpy as np

def read_input(s):
    vals = [float(x) for x in s.split()]
    n = int(len(vals)**0.5)
    A = np.array(vals, dtype=float).reshape(n, n)
    return A

def solve_case_newton(A, iters=12, tol=1e-12):
    n = A.shape[0]
    X = np.eye(n, dtype=float)
    for k in range(iters):
        X_inv = np.linalg.inv(X)
        X_next = 0.5*(X + A @ X_inv)
        if np.linalg.norm(X_next - X, 'fro') <= tol:
            X = X_next
            break
        X = X_next
    return X

def solve_case_eig(A):
    w, Q = np.linalg.eigh(A)
    w = np.maximum(w, 0.0)
    S = Q @ np.diag(np.sqrt(w)) @ Q.T
    return S

def validate():
    A = np.array([[4.0, 2.0],[2.0, 3.0]], dtype=float)
    S1 = solve_case_newton(A)
    S2 = solve_case_eig(A)
    r1 = np.linalg.norm(S1 @ S1 - A, 'fro')
    r2 = np.linalg.norm(S2 @ S2 - A, 'fro')
    assert r1 < 1e-8 and r2 < 1e-12
    assert np.linalg.norm(S1 - S2, 'fro') < 1e-6

def main():
    validate()
    A = np.array([[2.0, 1.0],[1.0, 2.0]], dtype=float)
    SN = solve_case_newton(A)
    SE = solve_case_eig(A)
    print("res_newton", np.linalg.norm(SN @ SN - A, 'fro'))
    print("res_eig   ", np.linalg.norm(SE @ SE - A, 'fro'))

if __name__ == "__main__":
    main()
\end{codepy}
\textbf{SOLUTION B — Library-Based (Validated Computational Shortcut)}
\begin{codepy}
import numpy as np

def read_input(s):
    vals = [float(x) for x in s.split()]
    n = int(len(vals)**0.5)
    A = np.array(vals, dtype=float).reshape(n, n)
    return A

def solve_case_eig(A):
    w, Q = np.linalg.eigh(A)
    S = Q @ np.diag(np.sqrt(np.clip(w, 0, None))) @ Q.T
    return S

def solve_case_newton(A):
    # reuse eigen result as a cross-check baseline
    return solve_case_eig(A)

def validate():
    A = np.array([[5.0, 1.0],[1.0, 2.0]], dtype=float)
    S = solve_case_eig(A)
    r = np.linalg.norm(S @ S - A, 'fro')
    assert r < 1e-12

def main():
    validate()
    A = np.array([[3.0, 0.5],[0.5, 1.0]], dtype=float)
    S = solve_case_eig(A)
    print("res", np.linalg.norm(S @ S - A, 'fro'))

if __name__ == "__main__":
    main()
\end{codepy}
\COMPLEXITY{
Newton: each step solves with $X_k$ (here invert) $\mathcal{O}(n^3)$, few
steps; eigen method $\mathcal{O}(n^3)$.}
\FAILMODES{
\begin{bullets}
\item Singular $X_k$ in Newton; use linear solves not explicit inverse.
\item Non SPD input; eigh requires symmetric input and real eigenvalues.
\end{bullets}
}
\STABILITY{
\begin{bullets}
\item Use symmetric eigendecomposition for SPD to reduce roundoff.
\item Scale and square can improve Newton stability for extreme spectra.
\end{bullets}
}
\VALIDATION{
\begin{bullets}
\item Assert $\|S^2-A\|_F$ small.
\item Compare Newton and eigen outputs.
\end{bullets}
}
\RESULT{
Both implementations produce $S$ with tiny residuals; they agree numerically.}
\EXPLANATION{
Newton implements Formula 5; eigen implements Formula 1. Residual checks
establish correctness.}
\EXTENSION{
Vectorize batched matrices or add scaling and squaring for robustness.}

\CodeDemoPage{Binomial Series Approximation for $(I+X)^{1/2}$}
\PROBLEM{
Approximate $(I+X)^{1/2}$ by truncating the binomial series and verify
the residual by squaring.}
\API{
\begin{bullets}
\item \inlinecode{def binom_half(k) -> float}
\item \inlinecode{def series_sqrt_I_plus_X(X, m) -> np.ndarray}
\item \inlinecode{def true_sqrt_I_plus_X(X) -> np.ndarray}
\item \inlinecode{def validate() -> None}
\item \inlinecode{def main() -> None}
\end{bullets}
}
\INPUTS{
Matrix $X$ with $\|X\|<1$; truncation order $m$.}
\OUTPUTS{
Approximation $S_m$ and residual norm $\|S_m^2-(I+X)\|_F$.}
\FORMULA{
\[
S_m=\sum_{k=0}^{m}\binom{1/2}{k}X^k.
\]
}
\textbf{SOLUTION A — From Scratch (Mathematically Explicit Implementation)}
\begin{codepy}
import numpy as np

def binom_half(k):
    c = 1.0
    for j in range(k):
        c *= (0.5 - j)/(j+1)
    return c

def series_sqrt_I_plus_X(X, m):
    n = X.shape[0]
    S = np.eye(n, dtype=float)
    P = np.eye(n, dtype=float)
    for k in range(1, m+1):
        P = P @ X
        S = S + binom_half(k) * P
    return S

def true_sqrt_I_plus_X(X):
    # compute eigen-based sqrt of I+X
    A = np.eye(X.shape[0]) + X
    w, Q = np.linalg.eigh((A + A.T)/2)
    S = Q @ np.diag(np.sqrt(np.clip(w, 0, None))) @ Q.T
    return S

def validate():
    X = np.array([[0.2, 0.1],[0.0, -0.1]], dtype=float)
    Sm = series_sqrt_I_plus_X(X, 5)
    A = np.eye(2) + X
    r = np.linalg.norm(Sm @ Sm - A, 'fro')
    assert r < 1e-3

def main():
    validate()
    X = np.array([[0.1, 0.05],[0.0, -0.05]], dtype=float)
    Sm = series_sqrt_I_plus_X(X, 6)
    A = np.eye(2) + X
    print("res_series", np.linalg.norm(Sm @ Sm - A, 'fro'))
    T = true_sqrt_I_plus_X(X)
    print("res_true  ", np.linalg.norm(T @ T - A, 'fro'))

if __name__ == "__main__":
    main()
\end{codepy}
\textbf{SOLUTION B — Library-Based (Validated Computational Shortcut)}
\begin{codepy}
import numpy as np

def binom_half(k):
    # use same helper; keep interface identical
    c = 1.0
    for j in range(k):
        c *= (0.5 - j)/(j+1)
    return c

def series_sqrt_I_plus_X(X, m):
    # reuse from-scratch method for comparability
    return __import__(__name__).series_sqrt_I_plus_X(X, m)

def true_sqrt_I_plus_X(X):
    A = np.eye(X.shape[0]) + X
    w, Q = np.linalg.eigh((A + A.T)/2)
    return Q @ np.diag(np.sqrt(np.clip(w, 0, None))) @ Q.T

def validate():
    X = np.array([[0.15, 0.0],[0.0, -0.1]], dtype=float)
    Sm = series_sqrt_I_plus_X(X, 6)
    A = np.eye(2) + X
    r = np.linalg.norm(Sm @ Sm - A, 'fro')
    assert r < 1e-4

def main():
    validate()
    X = np.array([[0.2, 0.0],[0.0, -0.05]], dtype=float)
    Sm = series_sqrt_I_plus_X(X, 8)
    A = np.eye(2) + X
    print("res", np.linalg.norm(Sm @ Sm - A, 'fro'))

if __name__ == "__main__":
    main()
\end{codepy}
\COMPLEXITY{
Computing $S_m$ costs $\mathcal{O}(m n^3)$ naive by repeated multiplies;
here optimized by reusing powers: $\mathcal{O}(m n^3)$.}
\FAILMODES{
\begin{bullets}
\item If $\|X\|\ge 1$, series may diverge.
\item Truncation too low yields large residual; increase $m$.
\end{bullets}
}
\STABILITY{
\begin{bullets}
\item Nonnormal $X$ can magnify powers; monitor residual directly.
\end{bullets}
}
\VALIDATION{
\begin{bullets}
\item Compare against eigen-based ground truth for symmetric $I+X$.
\end{bullets}
}
\RESULT{
Residuals are small for moderate $m$ when $\|X\|<1$.}
\EXPLANATION{
Implements Formula 3 and validates by squaring the approximation.}
\EXTENSION{
Adaptive truncation using tail bounds from coefficients.}

\CodeDemoPage{Fr\'echet Derivative via Sylvester Solve}
\PROBLEM{
Compute $L$ solving $SL+LS=E$ to approximate the Fr\'echet derivative
of $A^{1/2}$ at $S=A^{1/2}$. Validate using finite differences.}
\API{
\begin{bullets}
\item \inlinecode{def sylvester(S, E) -> np.ndarray}
\item \inlinecode{def frechet_sqrt(A, E) -> np.ndarray}
\item \inlinecode{def validate() -> None}
\item \inlinecode{def main() -> None}
\end{bullets}
}
\INPUTS{
SPD matrix $A$ and perturbation $E$; small scalar $h$ for testing.}
\OUTPUTS{
Linear response $L$ and finite-difference check norm.}
\FORMULA{
\[
SL+LS=E,\quad S=A^{1/2}.
\]
}
\textbf{SOLUTION A — From Scratch (Mathematically Explicit Implementation)}
\begin{codepy}
import numpy as np

def sylvester(S, E):
    n = S.shape[0]
    K = np.kron(np.eye(n), S) + np.kron(S.T, np.eye(n))
    l = np.linalg.solve(K, E.reshape(-1))
    return l.reshape(n, n)

def frechet_sqrt(A, E):
    w, Q = np.linalg.eigh(A)
    S = Q @ np.diag(np.sqrt(np.clip(w, 0, None))) @ Q.T
    L = sylvester(S, E)
    return S, L

def validate():
    A = np.array([[2.0, 0.5],[0.5, 1.5]], dtype=float)
    E = np.array([[0.1, -0.2],[-0.2, 0.05]], dtype=float)
    S, L = frechet_sqrt(A, E)
    h = 1e-5
    w, Q = np.linalg.eigh(A + h*E)
    Sh = Q @ np.diag(np.sqrt(np.clip(w, 0, None))) @ Q.T
    err = np.linalg.norm(Sh - (S + h*L), 'fro')/h
    assert err < 1e-3

def main():
    validate()
    print("ok")

if __name__ == "__main__":
    main()
\end{codepy}
\textbf{SOLUTION B — Library-Based (Validated Computational Shortcut)}
\begin{codepy}
import numpy as np

def sylvester(S, E):
    # same approach using numpy.kron linear solve
    n = S.shape[0]
    K = np.kron(np.eye(n), S) + np.kron(S.T, np.eye(n))
    l = np.linalg.solve(K, E.reshape(-1))
    return l.reshape(n, n)

def frechet_sqrt(A, E):
    w, Q = np.linalg.eigh(A)
    S = Q @ np.diag(np.sqrt(np.clip(w, 0, None))) @ Q.T
    L = sylvester(S, E)
    return S, L

def validate():
    A = np.array([[3.0, 0.2],[0.2, 1.0]], dtype=float)
    E = np.array([[0.05, 0.01],[0.01, -0.02]], dtype=float)
    S, L = frechet_sqrt(A, E)
    h = 1e-6
    w, Q = np.linalg.eigh(A + h*E)
    Sh = Q @ np.diag(np.sqrt(np.clip(w, 0, None))) @ Q.T
    err = np.linalg.norm(Sh - (S + h*L), 'fro')/h
    assert err < 1e-3

def main():
    validate()
    print("ok")

if __name__ == "__main__":
    main()
\end{codepy}
\COMPLEXITY{
Vectorized Sylvester via Kronecker solve costs $\mathcal{O}(n^6)$ naive
for building and solving; acceptable for small $n$.}
\FAILMODES{
\begin{bullets}
\item If $\sigma(S)$ and $-\sigma(S)$ intersect, the system is singular.
\end{bullets}
}
\STABILITY{
\begin{bullets}
\item Use Schur-based solvers for larger $n$ to improve conditioning.
\end{bullets}
}
\VALIDATION{
\begin{bullets}
\item Finite difference test verifies first-order accuracy.
\end{bullets}
}
\RESULT{
Computed $L$ matches finite differences to within tolerance.}
\EXPLANATION{
Implements Formula 4 by solving the Sylvester equation; eigen used to get $S$.}
\EXTENSION{
Implement Bartels–Stewart using Schur decompositions for efficiency.}

\section{Applied Domains — Detailed End-to-End Scenarios}

\DomainPage{Machine Learning}
\SCENARIO{
Whiten features using $W=\Sigma^{-1/2}$ so that transformed data have
identity covariance.}
\ASSUMPTIONS{
\begin{bullets}
\item Data matrix $X\in\mathbb{R}^{n\times d}$ is centered.
\item Sample covariance $\Sigma=\tfrac{1}{n}X^\top X$ is SPD.
\end{bullets}
}
\WHICHFORMULA{
Formula 1 for principal square root; whitening $W=\Sigma^{-1/2}$.}
\varmapStart
\var{X}{Centered data matrix $(n,d)$.}
\var{\Sigma}{Sample covariance.}
\var{W}{Whitening transform $\Sigma^{-1/2}$.}
\var{Z}{Whitened data $ZW^\top$ or $XW$.}
\varmapEnd
\PIPELINE{
\begin{bullets}
\item Compute $\Sigma$.
\item Eigen decomposition of $\Sigma$; form $W=\Sigma^{-1/2}$.
\item Transform $X\mapsto XW$; verify covariance near $I$.
\end{bullets}
}
\textbf{Implementation (From Scratch)}
\begin{codepy}
import numpy as np

def generate(n=200, d=2, seed=0):
    rng = np.random.default_rng(seed)
    U = np.array([[1.0, 0.7],[0.0, 0.7]])
    Z = rng.standard_normal((n, d))
    X = Z @ U.T
    X -= X.mean(axis=0, keepdims=True)
    return X

def whiten(X):
    n = X.shape[0]
    S = (X.T @ X) / n
    w, Q = np.linalg.eigh(S)
    W = Q @ np.diag(1.0/np.sqrt(np.clip(w, 1e-12, None))) @ Q.T
    Z = X @ W
    S_Z = (Z.T @ Z) / n
    return W, Z, S, S_Z

def main():
    X = generate()
    W, Z, S, S_Z = whiten(X)
    print("cov_Z_diag", np.round(np.diag(S_Z), 3))
    print("cov_Z_off", np.round(S_Z[0,1], 3))

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{Diagonal closeness to $1$ and small off-diagonal magnitude.}
\INTERPRET{Whitening standardizes scale and removes linear correlations.}
\NEXTSTEPS{Use ZCA: apply $W$ on both sides to preserve spatial structure.}

\DomainPage{Quantitative Finance}
\SCENARIO{
Simulate correlated returns using $\Sigma^{1/2}$; verify empirical
covariance approximates $\Sigma$.}
\ASSUMPTIONS{
\begin{bullets}
\item Returns $r_t=\mu + \Sigma^{1/2} z_t$, $z_t\sim\mathcal{N}(0,I)$.
\item $\Sigma>0$ is target covariance.
\end{bullets}
}
\WHICHFORMULA{
Formula 1 for $\Sigma^{1/2}$ to induce the desired covariance.}
\varmapStart
\var{\Sigma}{Target SPD covariance $(d,d)$.}
\var{\mu}{Mean vector.}
\var{S}{Square root $\Sigma^{1/2}$.}
\var{R}{Simulated returns matrix $(n,d)$.}
\varmapEnd
\PIPELINE{
\begin{bullets}
\item Build SPD $\Sigma$; compute $S=\Sigma^{1/2}$.
\item Sample $z_t$ i.i.d. normal with fixed seed.
\item Form $r_t=\mu + S z_t$; estimate covariance and compare.
\end{bullets}
}
\textbf{Implementation (Full Pipeline)}
\begin{codepy}
import numpy as np

def simulate(n=5000, d=3, seed=0):
    rng = np.random.default_rng(seed)
    A = np.array([[0.2, 0.1, 0.0],
                  [0.1, 0.3, 0.05],
                  [0.0, 0.05, 0.25]])
    Sigma = A + A.T + np.eye(d)
    w, Q = np.linalg.eigh(Sigma)
    S = Q @ np.diag(np.sqrt(np.clip(w, 0, None))) @ Q.T
    Z = rng.standard_normal((n, d))
    mu = np.array([0.0, 0.0, 0.0])
    R = mu + Z @ S.T
    Sigma_hat = np.cov(R, rowvar=False, bias=True)
    return Sigma, S, Sigma_hat

def main():
    Sigma, S, Sigma_hat = simulate()
    err = np.linalg.norm(Sigma_hat - Sigma, 'fro')
    print("fro_err", round(err, 3))

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{Frobenius norm $\|\hat\Sigma-\Sigma\|_F$ small with large $n$.}
\INTERPRET{Square root mapping imposes the target covariance structure.}
\NEXTSTEPS{Add mean reversion or stochastic volatility models.}

\DomainPage{Deep Learning}
\SCENARIO{
Implement a ZCA whitening layer using $\Sigma^{-1/2}$ for input
preprocessing; verify that output covariance is close to identity.}
\ASSUMPTIONS{
\begin{bullets}
\item Mini-batch whitening with regularization $\epsilon I$ for stability.
\item Centered inputs.
\end{bullets}
}
\WHICHFORMULA{
Formula 1 for inverse square root; apply $W=\Sigma^{-1/2}$.}
\PIPELINE{
\begin{bullets}
\item Compute batch covariance $\Sigma$.
\item Regularize: $\Sigma_\epsilon=\Sigma+\epsilon I$.
\item Compute $W=\Sigma_\epsilon^{-1/2}$ and transform inputs.
\end{bullets}
}
\textbf{Implementation (End-to-End)}
\begin{codepy}
import numpy as np

def zca_whiten(X, eps=1e-3):
    n = X.shape[0]
    Xc = X - X.mean(axis=0, keepdims=True)
    S = (Xc.T @ Xc) / n
    w, Q = np.linalg.eigh(S + eps*np.eye(S.shape[0]))
    W = Q @ np.diag(1.0/np.sqrt(np.clip(w, 0, None))) @ Q.T
    Z = Xc @ W
    S_Z = (Z.T @ Z) / n
    return Z, S_Z

def main():
    rng = np.random.default_rng(0)
    X = rng.standard_normal((256, 4)) @ np.array(
        [[2.0,0.5,0.0,0.0],
         [0.0,1.5,0.3,0.0],
         [0.0,0.0,1.0,0.2],
         [0.0,0.0,0.0,0.8]]).T
    Z, S_Z = zca_whiten(X, eps=1e-3)
    print("diag", np.round(np.diag(S_Z), 3))
    print("off", np.round(S_Z - np.diag(np.diag(S_Z)), 3))

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{Diagonal near ones and small off-diagonals in $S_Z$.}
\INTERPRET{ZCA whitening yields identity covariance while preserving
global orientation.}
\NEXTSTEPS{Integrate as a layer with running averages for inference.}

\DomainPage{Kaggle / Data Analytics}
\SCENARIO{
Decorrelate features for linear modeling using correlation matrix inverse
square root to obtain whitened features.}
\ASSUMPTIONS{
\begin{bullets}
\item All features numeric; sample size sufficient to estimate correlation.
\end{bullets}
}
\WHICHFORMULA{
Formula 1: use $R^{-1/2}$ where $R$ is the correlation matrix.}
\PIPELINE{
\begin{bullets}
\item Standardize features to zero mean and unit variance.
\item Compute correlation matrix $R$.
\item Compute $W=R^{-1/2}$ and transform standardized features.
\end{bullets}
}
\textbf{Implementation (Complete EDA Pipeline)}
\begin{codepy}
import numpy as np

def create_data(n=500, d=3, seed=0):
    rng = np.random.default_rng(seed)
    A = np.array([[1.0, 0.8, 0.3],
                  [0.8, 1.0, 0.4],
                  [0.3, 0.4, 1.0]])
    w, Q = np.linalg.eigh(A)
    S = Q @ np.diag(np.sqrt(np.clip(w, 0, None))) @ Q.T
    Z = rng.standard_normal((n, d)) @ S.T
    return Z

def whiten_corr(X):
    Xc = X - X.mean(axis=0, keepdims=True)
    Xs = Xc / Xc.std(axis=0, keepdims=True)
    R = np.cov(Xs, rowvar=False, bias=True)
    w, Q = np.linalg.eigh(R)
    W = Q @ np.diag(1.0/np.sqrt(np.clip(w, 1e-12, None))) @ Q.T
    Y = Xs @ W
    RY = np.cov(Y, rowvar=False, bias=True)
    return R, RY

def main():
    X = create_data()
    R, RY = whiten_corr(X)
    print("R diag", np.round(np.diag(R), 3))
    print("RY diag", np.round(np.diag(RY), 3))
    print("RY off", np.round(RY - np.diag(np.diag(RY)), 3))

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{Whitened correlation close to identity.}
\INTERPRET{Inverse square root removes linear correlations across features.}
\NEXTSTEPS{Use PCA after whitening to reduce dimensionality.}

\end{document}
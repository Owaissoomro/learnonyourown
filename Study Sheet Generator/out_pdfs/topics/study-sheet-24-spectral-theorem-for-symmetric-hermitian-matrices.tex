% !TeX program = xelatex
\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{microtype,setspace,amsmath,amssymb,mathtools,amsthm,unicode-math}
\setstretch{1.05}
\setmainfont{Latin Modern Roman}
\setmonofont{Latin Modern Mono}
\setmathfont{Latin Modern Math}

\allowdisplaybreaks[4]
\setlength{\jot}{7pt}
\setlength{\emergencystretch}{8em}
\sloppy

\usepackage{xcolor,fancyhdr,enumitem,inconsolata,listings}
\pagestyle{fancy}\fancyhf{}\lhead{\nouppercase{\leftmark}}\rhead{\thepage}
\setlength{\headheight}{26pt}
\setlength{\parindent}{0pt}
\setlength{\parskip}{8pt plus 2pt minus 1pt}
\raggedbottom

\newenvironment{BreakableEquation}{\begin{equation}\begin{aligned}}{\end{aligned}\end{equation}}
\newenvironment{BreakableEquation*}{\begin{equation*}\begin{aligned}}{\end{aligned}\end{equation*}}
\newenvironment{tightalign}{\begingroup\small\allowdisplaybreaks\begin{align}}{\end{align}\endgroup}

\providecommand{\enumlistm}{enumitem}
\newenvironment{minted}[2][]{%
  \lstset{style=code,language=#2,#1}\begin{lstlisting}%
}{\end{lstlisting}}

\newcommand{\inputminted}[3][]{\begin{lstlisting}\end{lstlisting}}

\newlist{bullets}{itemize}{1}
\setlist[bullets]{label=--,leftmargin=1.6em,itemsep=2pt,topsep=4pt}

\newcommand{\varmapStart}{\textbf{VARIABLE MAPPING:}\par\begin{bullets}}
\newcommand{\var}[2]{\item $#1$ — #2}
\newcommand{\varmapEnd}{\end{bullets}}

\newcommand{\glossx}[6]{%
  \textbf{#1}\par
  \begin{bullets}
    \item \textbf{What:} #2
    \item \textbf{Why:} #3
    \item \textbf{How:} #4
    \item \textbf{ELI5:} #5
    \item \textbf{Pitfall/Example:} #6
  \end{bullets}
}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{claim}[theorem]{Claim}

\lstdefinestyle{code}{
  basicstyle=\ttfamily\footnotesize,
  backgroundcolor=\color{black!02},
  frame=single,
  numbers=left, numberstyle=\tiny, numbersep=8pt,
  breaklines=true, breakatwhitespace=true,
  tabsize=4, showstringspaces=false,
  upquote=true, keepspaces=true, columns=fullflexible,
  literate=
    {–}{{-}}1
    {—}{{-}}1
    {…}{{...}}1
    {≤}{{\ensuremath{\le}}}1
    {≥}{{\ensuremath{\ge}}}1
    {≠}{{\ensuremath{\ne}}}1
    {≈}{{\ensuremath{\approx}}}1
    {±}{{\ensuremath{\pm}}}1
    {→}{{\ensuremath{\to}}}1
    {←}{{\ensuremath{\leftarrow}}}1
    {∞}{{\ensuremath{\infty}}}1
    {√}{{\ensuremath{\sqrt{\ }}}}1
    {×}{{\ensuremath{\times}}}1
    {÷}{{\ensuremath{\div}}}1
}

\lstnewenvironment{codepy}[1][]%
  {\lstset{style=code,language=Python,#1}}%
  {}

\newcommand{\inlinecode}[1]{\lstinline[style=code]!#1!}

\newcommand{\LF}[2]{\par\noindent\textbf{#1:}~#2\par}
\newcommand{\WHAT}[1]{\LF{WHAT}{#1}}
\newcommand{\WHY}[1]{\LF{WHY}{#1}}
\newcommand{\HOW}[1]{\LF{HOW}{#1}}
\newcommand{\ELI}[1]{\LF{ELI5}{#1}}
\newcommand{\SCOPE}[1]{\LF{SCOPE}{#1}}
\newcommand{\CONFUSIONS}[1]{\LF{COMMON CONFUSIONS}{#1}}
\newcommand{\APPLICATIONS}[1]{\LF{APPLICATIONS}{#1}}
\newcommand{\FORMULA}[1]{\LF{FORMULA}{#1}}
\newcommand{\CANONICAL}[1]{\LF{CANONICAL FORM}{#1}}
\newcommand{\PRECONDS}[1]{\LF{PRECONDITIONS}{#1}}
\newcommand{\DERIVATION}[1]{\LF{DERIVATION}{#1}}
\newcommand{\EQUIV}[1]{\LF{EQUIVALENT FORMS}{#1}}
\newcommand{\LIMITS}[1]{\LF{LIMIT CASES}{#1}}
\newcommand{\INPUTS}[1]{\LF{INPUTS}{#1}}
\newcommand{\OUTPUTS}[1]{\LF{OUTPUTS}{#1}}
\newcommand{\RESULT}[1]{\LF{RESULT}{#1}}
\newcommand{\INTUITION}[1]{\LF{INTUITION}{#1}}
\newcommand{\PITFALLS}[1]{\LF{PITFALLS}{#1}}
\newcommand{\MODEL}[1]{\LF{CANONICAL MATH MODEL}{#1}}
\newcommand{\ASSUMPTIONS}[1]{\LF{ASSUMPTIONS}{#1}}
\newcommand{\WHICHFORMULA}[1]{\LF{WHICH FORMULA \& WHY}{#1}}
\newcommand{\GOVERN}[1]{\LF{GOVERNING EQUATION(S)}{#1}}
\newcommand{\UNITCHECK}[1]{\LF{UNIT CHECK}{#1}}
\newcommand{\EDGECASES}[1]{\LF{EDGE CASES}{#1}}
\newcommand{\ALTERNATE}[1]{\LF{ALTERNATE APPROACH (sketch)}{#1}}
\newcommand{\PROBLEM}[1]{\LF{PROBLEM}{#1}}
\newcommand{\API}[1]{\LF{API}{#1}}
\newcommand{\COMPLEXITY}[1]{\LF{COMPLEXITY}{#1}}
\newcommand{\FAILMODES}[1]{\LF{FAILURE MODES}{#1}}
\newcommand{\STABILITY}[1]{\LF{NUMERICAL STABILITY}{#1}}
\newcommand{\VALIDATION}[1]{\LF{VALIDATION}{#1}}
\newcommand{\EXPLANATION}[1]{\LF{EXPLANATION}{#1}}
\newcommand{\SCENARIO}[1]{\LF{SCENARIO}{#1}}
\newcommand{\PIPELINE}[1]{\LF{PIPELINE STEPS}{#1}}
\newcommand{\METRICS}[1]{\LF{METRICS}{#1}}
\newcommand{\INTERPRET}[1]{\LF{INTERPRETATION}{#1}}
\newcommand{\NEXTSTEPS}[1]{\LF{LIMITATIONS \& NEXT STEPS}{#1}}

\usepackage{titlesec}
\titleformat{\section}{\Large\bfseries}{\thesection}{0.6em}{}
\titlespacing*{\section}{0pt}{*2}{*1}
\usepackage{etoolbox}
\pretocmd{\section}{\clearpage}{}{}

\newcommand{\FormulaPage}[2]{%
  \clearpage
  \section*{Formula #1 — #2}%
  \addcontentsline{toc}{section}{Formula #1 — #2}%
}
\newcommand{\ProblemPage}[2]{%
  \clearpage
  \subsection*{Problem #1: #2}%
  \addcontentsline{toc}{subsection}{Problem #1: #2}%
}
\newcommand{\CodeDemoPage}[1]{%
  \clearpage
  \subsection*{Coding Demo: #1}%
  \addcontentsline{toc}{subsection}{Coding Demo: #1}%
}
\newcommand{\DomainPage}[1]{%
  \clearpage
  \subsection*{#1 (End-to-End)}%
  \addcontentsline{toc}{subsection}{#1 (End-to-End)}%
}

\begin{document}
\title{Comprehensive Study Sheet — Spectral Theorem for Symmetric/Hermitian Matrices}
\date{\today}
\maketitle
\tableofcontents
\clearpage

\section{Concept Overview}
\WHAT{
A complex $n\times n$ matrix $A$ is Hermitian if $A=A^\ast$, where $A^\ast$ is
the conjugate transpose. A real matrix is symmetric if $A=A^\top$.
The Spectral Theorem states: for Hermitian (resp. real symmetric) $A$ there is a
unitary (resp. orthogonal) $U$ and real diagonal $\Lambda=\mathrm{diag}(\lambda_1,
\dots,\lambda_n)$ such that $A=U\Lambda U^\ast$. Eigenspaces for distinct
eigenvalues are orthogonal; there exists an orthonormal eigenbasis.
}
\WHY{
Hermitian/symmetric matrices appear as Hessians, covariances, graph Laplacians,
and self-adjoint operators. The theorem enables diagonalization, simplifies
quadratic forms, yields stable numerical methods, and underpins PCA, Fourier
modes on graphs, energy minimization, and normal-mode analysis.
}
\HOW{
1. Define $A=A^\ast$. 2. Prove eigenvalues are real and eigenvectors for
distinct eigenvalues are orthogonal (via $\langle Av,w\rangle=\langle v,Aw
\rangle$). 3. Use induction and Gram--Schmidt to extend an eigenvector to an
orthonormal basis. 4. Assemble $U$ from orthonormal eigenvectors, get
$A=U\Lambda U^\ast$, interpret $x^\ast A x=\sum \lambda_i |\langle x,u_i\rangle|^2$.
}
\ELI{
Think of $A$ as a lens that stretches space along special perpendicular
directions only. The spectral theorem says we can rotate into that special set
of axes so $A$ just scales each axis by a real amount.
}
\SCOPE{
Valid for finite-dimensional Hermitian or real symmetric matrices. Fails for
non-normal matrices in general. For repeated eigenvalues, eigenspaces may have
dimension $>1$ but still admit orthonormal bases. Over $\mathbb{C}$,
Hermitian is required; over $\mathbb{R}$, symmetry suffices.
}
\CONFUSIONS{
Symmetric vs. normal: all Hermitian are normal, but not all normal are
Hermitian. Orthogonal vs. unitary: orthogonal is the real case of unitary.
Diagonalizable vs. orthogonally diagonalizable: many matrices are diagonalizable
but only normal matrices are unitarily diagonalizable; within those, Hermitian
have real spectra.
}
\APPLICATIONS{
\begin{bullets}
\item Mathematical foundations: orthogonal projections, functional calculus.
\item Computational modeling: modal analysis, PDE separation of variables.
\item Physical/engineering: normal modes, stiffness/covariance decomposition.
\item Statistical/algorithmic: PCA, Rayleigh quotient optimization.
\end{bullets}
}

\textbf{ANALYTIC STRUCTURE.}
Hermitian matrices define real-valued quadratic forms, are normal, have complete
sets of orthonormal eigenvectors, and admit spectral measures. They are closed
under addition and scalar multiplication and are diagonalizable by unitary maps.

\textbf{CANONICAL LINKS.}
Rayleigh quotient extrema yield eigenvalues (Courant--Fischer). Spectral
projectors decompose identity. Positive semidefinite square roots exist via
functional calculus. Graph Laplacians are symmetric and PSD.

\textbf{PROBLEM-TYPE RECOGNITION HEURISTICS.}
\begin{bullets}
\item Quadratic form $x^\ast A x$ with $A=A^\ast$.
\item Max/min of $x^\ast A x$ over unit vectors.
\item Covariance/PCA and explained variance ratios.
\item Commuting Hermitian matrices and simultaneous diagonalization.
\end{bullets}

\textbf{SOLUTION STRATEGY BLUEPRINT.}
\begin{bullets}
\item Translate to eigenvalue/eigenvector relations $Av=\lambda v$.
\item Invoke spectral theorem: $A=U\Lambda U^\ast$.
\item Work in eigenbasis: expand vectors in $\{u_i\}$.
\item Optimize or compute functions via $f(A)=Uf(\Lambda)U^\ast$.
\item Validate by unitary invariance and trace/determinant checks.
\end{bullets}

\textbf{CONCEPTUAL INVARIANTS.}
Eigenvalues (set), trace $\sum \lambda_i$, determinant $\prod \lambda_i$,
Frobenius norm $\sqrt{\sum \lambda_i^2}$, spectral projectors $P_i$.

\textbf{EDGE INTUITION.}
As $\|A\|\to 0$, all eigenvalues go to $0$ and $x^\ast A x\to 0$. As
$\|A\|\to \infty$, Rayleigh quotient concentrates near extreme eigenvalues;
large repeated eigenvalues create flat directions.

\section{Glossary}
\glossx{Hermitian Matrix}
{Complex matrix equal to its conjugate transpose $A^\ast$.}
{Guarantees real eigenvalues and orthonormal eigenvectors.}
{Check entrywise: $a_{ij}=\overline{a_{ji}}$.}
{Like a perfectly balanced mirror: action equals its own adjoint.}
{Do not confuse with symmetric unless over $\mathbb{R}$.}

\glossx{Orthogonal/Unitary Matrix}
{Matrix $U$ with $U^\top U=I$ (orthogonal) or $U^\ast U=I$ (unitary).}
{Changes basis without distorting lengths or angles.}
{Compute via QR, SVD, or eigenvectors of symmetric matrices.}
{A pure rotation/reflection that preserves distances.}
{Pitfall: determinant can be $-1$ for orthogonal (reflection).}

\glossx{Spectral Decomposition}
{Representation $A=\sum_i \lambda_i P_i$ with $P_i$ orthogonal projectors.}
{Enables functional calculus $f(A)=\sum_i f(\lambda_i)P_i$.}
{Diagonalize $A$, group equal eigenvalues into projectors.}
{Like splitting light into pure colors and recombining.}
{For repeated eigenvalues, $P_i$ project onto full eigenspaces.}

\glossx{Rayleigh Quotient}
{$R_A(x)=\dfrac{x^\ast A x}{x^\ast x}$ for $x\ne 0$.}
{Optimizes to eigenvalues on the unit sphere; used in PCA, power iteration.}
{Expand $x$ in eigenbasis and bound by eigenvalues.}
{Average stretch factor of $A$ along direction $x$.}
{Pitfall: not defined at $x=0$; normalize to unit vectors.}

\section{Symbol Ledger}
\varmapStart
\var{A}{Hermitian/symmetric matrix in $\mathbb{C}^{n\times n}$ or $\mathbb{R}^{n\times n}$.}
\var{U,Q}{Unitary/orthogonal matrices with $U^\ast U=I$, $Q^\top Q=I$.}
\var{\Lambda}{Diagonal matrix $\mathrm{diag}(\lambda_1,\dots,\lambda_n)$ of eigenvalues.}
\var{\lambda_i}{Real eigenvalues of $A$.}
\var{u_i}{Unit eigenvectors forming an orthonormal basis.}
\var{P_i}{Orthogonal projector onto eigenspace for $\lambda_i$.}
\var{x}{Vector in $\mathbb{C}^n$ or $\mathbb{R}^n$.}
\var{R_A(x)}{Rayleigh quotient $(x^\ast A x)/(x^\ast x)$.}
\var{f}{Scalar function for functional calculus on spectra.}
\var{I}{Identity matrix.}
\var{\|\cdot\|_2}{Spectral/operator $2$-norm.}
\varmapEnd

\section{Formula Canon — One Formula Per Page}

\FormulaPage{1}{Spectral Theorem (Unitary/Orthogonal Diagonalization)}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
For Hermitian (real symmetric) $A$, there exists a unitary (orthogonal) $U$ and
real diagonal $\Lambda$ with $A=U\Lambda U^\ast$.

\WHAT{
Diagonalization of $A$ by a unitary/orthogonal similarity transform, producing
real eigenvalues and an orthonormal eigenbasis.
}
\WHY{
Simplifies analysis and computation: powers $A^k$, exponentials $\mathrm{e}^A$,
quadratic forms, and optimization reduce to scalar operations on eigenvalues.
}
\FORMULA{
\[
A=U\Lambda U^\ast,\quad U^\ast U=I,\quad \Lambda=\mathrm{diag}(\lambda_1,\dots,\lambda_n),
\quad \lambda_i\in\mathbb{R}.
\]
}
\CANONICAL{
Domain: $A=A^\ast$ over $\mathbb{C}$ or $A=A^\top$ over $\mathbb{R}$. The
columns of $U$ are orthonormal eigenvectors $\{u_i\}$ of $A$.
}
\PRECONDS{
\begin{bullets}
\item $A$ is Hermitian ($A^\ast=A$) or real symmetric ($A^\top=A$).
\item Finite dimension $n<\infty$.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
If $A=A^\ast$, then all eigenvalues of $A$ are real, and eigenvectors
corresponding to distinct eigenvalues are orthogonal.
\end{lemma}
\begin{proof}
Let $Av=\lambda v$ with $v\ne 0$. Then $\langle Av,v\rangle=\lambda \langle v,v
\rangle$. Since $A$ is Hermitian, $\langle Av,v\rangle=\langle v,Av\rangle=
\overline{\lambda}\langle v,v\rangle$, hence $\lambda=\overline{\lambda}\in
\mathbb{R}$. If $Av=\lambda v$ and $Aw=\mu w$ with $\lambda\ne\mu$, then
$\lambda\langle v,w\rangle=\langle Av,w\rangle=\langle v,Aw\rangle=
\mu\langle v,w\rangle$, so $(\lambda-\mu)\langle v,w\rangle=0$, hence
$\langle v,w\rangle=0$. \qedhere
\end{proof}
\DERIVATION{
\begin{align*}
\text{Step 1:}~& \text{Pick an eigenpair } (u_1,\lambda_1),\ \|u_1\|=1.\\
\text{Step 2:}~& \text{Orthogonally decompose } \mathbb{C}^n=\mathrm{span}\{u_1\}
\oplus u_1^\perp.\\
\text{Step 3:}~& \text{$A$ leaves } u_1^\perp \text{ invariant: for } x\perp u_1,\\
& \langle Ax,u_1\rangle=\langle x,Au_1\rangle=\lambda_1\langle x,u_1\rangle=0.\\
\text{Step 4:}~& \text{Restrict } A \text{ to } u_1^\perp \text{ (still Hermitian).}\\
\text{Step 5:}~& \text{Induct to obtain orthonormal eigenbasis }
\{u_1,\dots,u_n\}.\\
\text{Step 6:}~& \text{Let } U=[u_1~\cdots~u_n],\ \Lambda=\mathrm{diag}(\lambda_i).\\
& AU=U\Lambda \implies A=U\Lambda U^\ast.
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Verify $A=A^\ast$.
\item Find eigenvalues (characteristic polynomial) and orthonormal eigenvectors.
\item Assemble $U$ and $\Lambda$; return $A=U\Lambda U^\ast$.
\item Validate: $U^\ast U=I$, $U^\ast A U=\Lambda$ diagonal.
\end{bullets}
\EQUIV{
\begin{bullets}
\item $A$ is normal with real spectrum and $A=A^\ast$.
\item $x^\ast A x=\sum_i \lambda_i|\langle x,u_i\rangle|^2$.
\item $A=\sum_i \lambda_i u_i u_i^\ast$ (rank-1 expansion).
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item If $A$ not normal, unitary diagonalization may fail.
\item Repeated eigenvalues: $U$ not unique within eigenspaces.
\end{bullets}
}
\INPUTS{$A\in\mathbb{C}^{n\times n}$ with $A=A^\ast$ (or $\mathbb{R}$ symmetric).}
\DERIVATION{
\begin{align*}
U^\ast A U&=\Lambda \ \Rightarrow\ A=U\Lambda U^\ast,\\
\text{Check: } U^\ast U&=I,\ \Lambda=\Lambda^\ast,\ \text{so } A^\ast
=U\Lambda U^\ast=A.
\end{align*}
}
\RESULT{
Exists unitary/orthogonal $U$ and real diagonal $\Lambda$ such that
$A=U\Lambda U^\ast$. Quadratic forms decompose in eigenbasis.
}
\UNITCHECK{
Unitary similarity preserves trace and eigenvalues. Dimensions match.
}
\PITFALLS{
\begin{bullets}
\item Using $U^{-1}$ instead of $U^\ast$; for unitary $U^{-1}=U^\ast$.
\item Confusing complex conjugation and transpose in Hermitian case.
\end{bullets}
}
\INTUITION{
Hermitian $A$ acts like independent scalings along perpendicular directions.
}
\CANONICAL{
\begin{bullets}
\item Universal identity: $A=U\Lambda U^\ast$ for $A=A^\ast$.
\item Rank-1 sum: $A=\sum_i \lambda_i u_i u_i^\ast$.
\end{bullets}
}

\FormulaPage{2}{Spectral Projectors and Functional Calculus}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
For Hermitian $A$, there exist orthogonal projectors $\{P_i\}$ onto eigenspaces
such that $A=\sum_i \lambda_i P_i$, $\sum_i P_i=I$, $P_iP_j=\delta_{ij}P_i$.
For suitable $f:\mathbb{R}\to\mathbb{C}$, $f(A)=\sum_i f(\lambda_i)P_i$.

\WHAT{
Decomposition of $A$ into orthogonal eigenspace projectors and lifting scalar
functions to matrices via eigenvalues.
}
\WHY{
Enables computing powers, exponentials, resolvents, and square roots by acting
on eigenvalues only, preserving Hermitian structure.
}
\FORMULA{
\[
A=\sum_{i=1}^m \lambda_i P_i,\quad P_iP_j=\delta_{ij}P_i,\quad \sum_{i=1}^m P_i=I,\quad
f(A)=\sum_{i=1}^m f(\lambda_i)P_i,
\]
where $\{\lambda_i\}_{i=1}^m$ are distinct eigenvalues.
}
\CANONICAL{
Distinct eigenvalues indexed $i=1,\dots,m$. $P_i$ is the orthogonal projector
onto $\ker(A-\lambda_i I)$. $f$ is Borel or polynomial; finite-dimensional case
handled via polynomials.
}
\PRECONDS{
\begin{bullets}
\item $A$ Hermitian with spectral decomposition $A=U\Lambda U^\ast$.
\item $f$ defined on the spectrum $\{\lambda_i\}$.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
Each $P_i$ is a polynomial in $A$: $P_i=\prod_{j\ne i}\dfrac{A-\lambda_j I}
{\lambda_i-\lambda_j}$.
\end{lemma}
\begin{proof}
Define $p_i(t)=\prod_{j\ne i}\dfrac{t-\lambda_j}{\lambda_i-\lambda_j}$. Then
$p_i(\lambda_k)=\delta_{ik}$. With $A=U\Lambda U^\ast$,
$p_i(A)=Up_i(\Lambda)U^\ast=U\mathrm{diag}(\delta_{i1},\dots,\delta_{im})U^\ast$,
which is the orthogonal projector onto the eigenspace for $\lambda_i$. \qedhere
\end{proof}
\DERIVATION{
\begin{align*}
A&=U\Lambda U^\ast=\sum_{k=1}^n \lambda_k u_k u_k^\ast\\
&=\sum_{i=1}^m \lambda_i \Big(\sum_{\lambda_k=\lambda_i} u_k u_k^\ast\Big)
=\sum_{i=1}^m \lambda_i P_i,\\
f(A)&=Uf(\Lambda)U^\ast=\sum_{k=1}^n f(\lambda_k) u_k u_k^\ast
=\sum_{i=1}^m f(\lambda_i) P_i.
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Compute distinct eigenvalues and orthonormal bases of eigenspaces.
\item Build $P_i=\sum_{k:\lambda_k=\lambda_i} u_k u_k^\ast$.
\item Evaluate $f$ on eigenvalues and sum $f(\lambda_i)P_i$.
\end{bullets}
}
\EQUIV{
\begin{bullets}
\item $A$ diagonalizable $\Leftrightarrow$ spectral family of projectors exists.
\item Polynomial functional calculus: $p(A)=\sum p(\lambda_i)P_i$.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item For non-normal matrices, projectors may not be orthogonal.
\item If $f$ undefined at some $\lambda_i$, $f(A)$ is undefined.
\end{bullets}
}
\INPUTS{$A=U\Lambda U^\ast$, $\{\lambda_i\}$ distinct, $P_i$ defined as above, $f$.}
\DERIVATION{
\begin{align*}
\sum_i P_i&=U\Big(\sum_i E_i\Big)U^\ast=UIU^\ast=I,\\
P_iP_j&=UE_iU^\ast UE_jU^\ast=U(E_iE_j)U^\ast=\delta_{ij}U E_i U^\ast,\\
A&=\sum_i \lambda_i P_i,\quad f(A)=\sum_i f(\lambda_i)P_i.
\end{align*}
}
\RESULT{
Orthogonal projector decomposition and matrix functional calculus for Hermitian
matrices are established.
}
\UNITCHECK{
Projectors are idempotent and self-adjoint: $P_i^2=P_i=P_i^\ast$. Dimensions
match; trace$(P_i)$ equals eigenspace dimension.
}
\PITFALLS{
\begin{bullets}
\item Mixing algebraic and geometric multiplicities; use orthonormal basis.
\item Forgetting to group repeated eigenvalues when forming $P_i$.
\end{bullets}
}
\INTUITION{
Split space into orthogonal color bands (eigenspaces), apply scalars to each,
and recombine.
}
\CANONICAL{
\begin{bullets}
\item $A=\sum_i \lambda_i P_i$ with orthogonal $P_i$.
\item $f(A)=\sum_i f(\lambda_i)P_i$.
\end{bullets}
}

\FormulaPage{3}{Rayleigh Quotient and Courant--Fischer}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
For Hermitian $A$ with eigenvalues $\lambda_1\ge\dots\ge\lambda_n$,
$\lambda_{\max}=\max_{\|x\|=1} x^\ast A x$,
$\lambda_{\min}=\min_{\|x\|=1} x^\ast A x$, and for $k=1,\dots,n$,
\[
\lambda_k=\min_{\substack{S\subset\mathbb{C}^n\\ \dim S=k}}\ 
\max_{\substack{x\in S\\ x\ne 0}} \frac{x^\ast A x}{x^\ast x}.
\]

\WHAT{
Extremal and variational characterizations of eigenvalues via quadratic forms
on subspaces (min-max principle).
}
\WHY{
Provides optimization interpretations, stability bounds, and algorithms (power
iteration, Krylov methods, PCA).
}
\FORMULA{
\[
R_A(x)=\frac{x^\ast A x}{x^\ast x},\quad
\lambda_1=\max_{\|x\|=1} R_A(x),\quad
\lambda_n=\min_{\|x\|=1} R_A(x),
\]
\[
\lambda_k=\min_{\dim S=k}\ \max_{x\in S\setminus\{0\}} R_A(x).
\]
}
\CANONICAL{
Eigenvalues ordered nonincreasing. Inner product is standard; norms are 2-norms.
}
\PRECONDS{
\begin{bullets}
\item $A$ is Hermitian.
\item Finite dimension to ensure compactness of unit sphere.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
If $A=U\Lambda U^\ast$ and $x=\sum_i \alpha_i u_i$, then
$R_A(x)=\dfrac{\sum_i \lambda_i |\alpha_i|^2}{\sum_i |\alpha_i|^2}$.
\end{lemma}
\begin{proof}
Compute $x^\ast A x=(\sum_i \overline{\alpha_i} u_i^\ast)(\sum_j \lambda_j
\alpha_j u_j)=\sum_i \lambda_i |\alpha_i|^2$ by orthonormality, and
$x^\ast x=\sum_i |\alpha_i|^2$. \qedhere
\end{proof}
\DERIVATION{
\begin{align*}
\text{Extrema: }& R_A(x)=\frac{\sum_i \lambda_i |\alpha_i|^2}{\sum_i |\alpha_i|^2}
\in [\lambda_n,\lambda_1],\\
& \text{achieves }\lambda_1 \text{ at } x=u_1,\ \lambda_n \text{ at } x=u_n.\\
\text{Min-max: }& \text{For } \dim S=k,\ \max_{x\in S} R_A(x)\ge \lambda_k,\\
& \text{choose } S=\mathrm{span}\{u_1,\dots,u_k\} \Rightarrow
\max_{x\in S} R_A(x)=\lambda_1,\\
& \min_{\dim S=k}\max_{x\in S} R_A(x)=\lambda_k\ \text{(standard proof via }\\
& \text{interlacing and orthogonal decompositions).}
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Expand $x$ in eigenbasis; bound $R_A(x)$ between $\lambda_{\min}$ and
$\lambda_{\max}$.
\item For $k$th eigenvalue, consider $k$-dimensional subspaces aligned with
leading eigenvectors.
\end{bullets}
}
\EQUIV{
\begin{bullets}
\item $\|A\|_2=\max_{\|x\|=1} |x^\ast A x|$ for Hermitian $A$ equals
$\max_i |\lambda_i|$.
\item Interlacing with principal submatrices via Cauchy interlacing theorem.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item Non-Hermitian $A$: Rayleigh quotient may be complex and min-max fails.
\item Equal eigenvalues: plateaus of optimal directions.
\end{bullets}
}
\INPUTS{$A=U\Lambda U^\ast$, ordered eigenvalues, unit vector $x$.}
\DERIVATION{
\begin{align*}
\lambda_n\le R_A(x)&\le \lambda_1,\qquad
R_A(u_i)=\lambda_i,\\
\lambda_k&=\min_{\dim S=k}\max_{x\in S} R_A(x).
\end{align*}
}
\RESULT{
Extremal values of $R_A$ are eigenvalues; $k$th eigenvalue satisfies the
Courant--Fischer variational principle.
}
\UNITCHECK{
Scalars are real; bounds are dimensionless and within spectral interval.
}
\PITFALLS{
\begin{bullets}
\item Forgetting normalization; $R_A$ is homogeneous of degree $0$.
\item Using non-orthonormal bases breaks the simple weighted-average form.
\end{bullets}
}
\INTUITION{
$R_A(x)$ is a weighted average of eigenvalues by energy in each eigendirection,
so it cannot exceed the extremes.
}
\CANONICAL{
\begin{bullets}
\item $R_A(x)\in[\lambda_{\min},\lambda_{\max}]$.
\item Min-max: $\lambda_k=\min_{\dim S=k}\max_{x\in S} R_A(x)$.
\end{bullets}
}

\FormulaPage{4}{Positive Semidefinite Square Root}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
If $A=A^\ast\succeq 0$, then there exists a unique Hermitian $B\succeq 0$ such
that $B^2=A$, given by $B=U \mathrm{diag}(\sqrt{\lambda_i}) U^\ast$.

\WHAT{
Existence and uniqueness of a Hermitian positive semidefinite square root of a
PSD Hermitian matrix.
}
\WHY{
Needed for whitening transforms, Mahalanobis distances, matrix geometric means,
and solving Lyapunov equations.
}
\FORMULA{
\[
A=U\Lambda U^\ast,\ \Lambda=\mathrm{diag}(\lambda_i\ge 0)\ \Rightarrow\
A^{1/2}=U\,\mathrm{diag}(\sqrt{\lambda_i})\,U^\ast.
\]
}
\CANONICAL{
$A$ Hermitian with nonnegative spectrum. The square root is defined via
functional calculus with $f(t)=\sqrt{t}$.
}
\PRECONDS{
\begin{bullets}
\item $A=A^\ast$ and $\lambda_i\ge 0$.
\item Choose principal square root $\sqrt{\cdot}$ on $[0,\infty)$.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
If $A=U\Lambda U^\ast$ and $g$ is continuous on the spectrum, then
$g(A)=Ug(\Lambda)U^\ast$ is Hermitian if $g$ is real-valued on $\mathbb{R}$.
\end{lemma}
\begin{proof}
$g(\Lambda)$ is diagonal with real entries $g(\lambda_i)$. Then
$g(A)^\ast=(Ug(\Lambda)U^\ast)^\ast=Ug(\Lambda)U^\ast=g(A)$. \qedhere
\end{proof}
\DERIVATION{
\begin{align*}
A^{1/2}&=U\,\mathrm{diag}(\sqrt{\lambda_i})\,U^\ast,\\
(A^{1/2})^2&=U\,\mathrm{diag}(\sqrt{\lambda_i})^2\,U^\ast
=U\,\mathrm{diag}(\lambda_i)\,U^\ast=A.
\end{align*}
Uniqueness: \text{ if } B\succeq 0,\ B^2=A,\ B=V D V^\ast \Rightarrow
D^2=\Lambda\ \Rightarrow D=\mathrm{diag}(\sqrt{\lambda_i}).
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Verify PSD by checking $x^\ast A x\ge 0$ or eigenvalues $\ge 0$.
\item Diagonalize $A$, take square roots of eigenvalues, recombine.
\item Validate by squaring $A^{1/2}$ and preserving Hermitian PSD.
\end{bullets}
}
\EQUIV{
\begin{bullets}
\item $A^{1/2}$ is the unique PSD solution of $X^2=A$.
\item $A^{-1/2}$ exists iff $A\succ 0$.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item If $A$ not PSD, no Hermitian PSD square root exists.
\item Nonprincipal square roots exist but are not PSD/Hermitian.
\end{bullets}
}
\INPUTS{$A=U\Lambda U^\ast$ with $\lambda_i\ge 0$.}
\DERIVATION{
\begin{align*}
\text{Compute } U,\Lambda;\ \tilde\Lambda&=\mathrm{diag}(\sqrt{\lambda_i});\\
A^{1/2}&=U\tilde\Lambda U^\ast;\ (A^{1/2})^2=A.
\end{align*}
}
\RESULT{
$A^{1/2}$ exists uniquely, is Hermitian PSD, and squares to $A$.
}
\UNITCHECK{
Eigenvalues of $A^{1/2}$ are $\sqrt{\lambda_i}\ge 0$; similarity preserves
dimension; squaring recovers $A$.
}
\PITFALLS{
\begin{bullets}
\item Taking square roots of negative eigenvalues (invalid for PSD).
\item Using elementwise square root instead of spectral one.
\end{bullets}
}
\INTUITION{
Decompose into axes, take scalar square roots, rotate back.
}
\CANONICAL{
\begin{bullets}
\item $A^{1/2}=U\sqrt{\Lambda}U^\ast$ for $A\succeq 0$.
\item Functional calculus with $f(t)=\sqrt{t}$.
\end{bullets}
}

\FormulaPage{5}{Quadratic Form Bounds and Spectral Norm}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
For Hermitian $A$, with eigenvalues $\lambda_{\min}\le\dots\le\lambda_{\max}$,
\[
\lambda_{\min}\|x\|^2 \le x^\ast A x \le \lambda_{\max}\|x\|^2,\quad
\|A\|_2=\max_i |\lambda_i|.
\]

\WHAT{
Bounds of quadratic forms and identification of operator 2-norm with the maximal
absolute eigenvalue for Hermitian matrices.
}
\WHY{
Provides tight energy bounds, conditioning insights, and norm computations used
in analysis and algorithms.
}
\FORMULA{
\[
x^\ast A x=\sum_i \lambda_i |\langle x,u_i\rangle|^2,\quad
\|A\|_2=\max_{\|x\|=1} |x^\ast A x|=\max_i |\lambda_i|.
\]
}
\CANONICAL{
$A=U\Lambda U^\ast$ with orthonormal eigenvectors $u_i$. The 2-norm is induced
by the Euclidean norm.
}
\PRECONDS{
\begin{bullets}
\item $A$ is Hermitian.
\item Standard inner product and norm.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
For Hermitian $A$, $x^\ast A x$ equals the spectral weighted sum
$\sum_i \lambda_i |\langle x,u_i\rangle|^2$.
\end{lemma}
\begin{proof}
Same computation as in Rayleigh lemma: expand $x=\sum_i \alpha_i u_i$ and use
orthonormality. \qedhere
\end{proof}
\DERIVATION{
\begin{align*}
\lambda_{\min}\sum_i |\alpha_i|^2&\le \sum_i \lambda_i |\alpha_i|^2
\le \lambda_{\max}\sum_i |\alpha_i|^2,\\
\Rightarrow\ \lambda_{\min}\|x\|^2&\le x^\ast A x\le \lambda_{\max}\|x\|^2,\\
\|A\|_2&=\max_{\|x\|=1} |x^\ast A x|=\max_i |\lambda_i|.
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Diagonalize $A$, expand $x$ in eigenbasis.
\item Apply scalar bounds by min/max eigenvalues.
\item For $\|A\|_2$, take the maximum absolute eigenvalue.
\end{bullets}
}
\EQUIV{
\begin{bullets}
\item If $A\succeq 0$, then $0\le x^\ast A x\le \lambda_{\max}\|x\|^2$.
\item If $A\succ 0$, then $x^\ast A x\ge \lambda_{\min}\|x\|^2>0$.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item Non-Hermitian: $\|A\|_2$ equals $\sqrt{\lambda_{\max}(A^\ast A)}$, not a
spectral bound by $x^\ast A x$.
\end{bullets}
}
\INPUTS{$A=U\Lambda U^\ast$, vector $x$.}
\DERIVATION{
\begin{align*}
x^\ast A x&=\sum_i \lambda_i |\alpha_i|^2,\ \|x\|^2=\sum_i |\alpha_i|^2,\\
\text{Bound }&\text{by } \lambda_{\min},\lambda_{\max}.
\end{align*}
}
\RESULT{
Quadratic form bounds and spectral norm identity for Hermitian matrices.
}
\UNITCHECK{
All terms are scalar real values; norm units consistent with eigenvalues.
}
\PITFALLS{
\begin{bullets}
\item Using $x^\top A x$ with complex vectors without conjugation.
\item Confusing spectral norm with spectral radius for nonnormal matrices.
\end{bullets}
}
\INTUITION{
Energy in $x$ spreads across eigendirections; $A$ scales each by $\lambda_i$,
so the total lies between extremes.
}
\CANONICAL{
\begin{bullets}
\item $\lambda_{\min}\|x\|^2\le x^\ast A x\le \lambda_{\max}\|x\|^2$.
\item $\|A\|_2=\max_i |\lambda_i|$ for Hermitian $A$.
\end{bullets}
}

\section{10 Exhaustive Problems and Solutions}

\ProblemPage{1}{Diagonalization and Reconstruction}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Given $A=\begin{bmatrix}4&1&0\\1&3&0\\0&0&2\end{bmatrix}$, find $U,\Lambda$
with $A=U\Lambda U^\top$, reconstruct $A$, and express $x^\top A x$ in the
eigenbasis.

\PROBLEM{
Compute eigenvalues/eigenvectors, form an orthonormal $U$, verify $U^\top A U$
is diagonal, and show $x^\top A x=\sum_i \lambda_i \alpha_i^2$ with
$x=\sum_i \alpha_i u_i$.
}
\MODEL{
\[
A=A^\top,\quad A=U\Lambda U^\top,\quad U^\top U=I,\quad \Lambda=\mathrm{diag}(\lambda_i).
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Real symmetric matrix.
\item Standard Euclidean inner product.
\end{bullets}
}
\varmapStart
\var{A}{Given symmetric matrix.}
\var{U}{Orthogonal eigenvector matrix.}
\var{\Lambda}{Diagonal of eigenvalues.}
\var{u_i}{Eigenvectors.}
\var{\lambda_i}{Eigenvalues.}
\var{x}{Vector expanded as $x=\sum \alpha_i u_i$.}
\varmapEnd
\WHICHFORMULA{
Formula 1 (Spectral Theorem) and Formula 5 (Quadratic form bounds).
}
\GOVERN{
\[
A=U\Lambda U^\top,\quad x^\top A x=\sum_i \lambda_i \alpha_i^2.
\]
}
\INPUTS{$A=\begin{bmatrix}4&1&0\\1&3&0\\0&0&2\end{bmatrix}$.}
\DERIVATION{
\begin{align*}
\text{Block }&\begin{bmatrix}4&1\\1&3\end{bmatrix} \text{ has eigenvalues from }
\det\begin{bmatrix}4-\lambda&1\\1&3-\lambda\end{bmatrix}\\
&=(4-\lambda)(3-\lambda)-1=\lambda^2-7\lambda+11,\\
&\text{roots } \lambda=\frac{7\pm \sqrt{49-44}}{2}=\frac{7\pm \sqrt{5}}{2}
=5,2.\\
\text{Thus }& \lambda_1=5,\ \lambda_2=2,\ \lambda_3=2.\\
\lambda_1=5:&\ (A-5I)v=0 \Rightarrow \begin{bmatrix}-1&1&0\\1&-2&0\\0&0&-3
\end{bmatrix}v=0,\\
& v_1=[1,1,0]^\top,\ u_1=\frac{1}{\sqrt{2}}[1,1,0]^\top.\\
\lambda_2=2:&\ (A-2I)v=0 \Rightarrow \begin{bmatrix}2&1&0\\1&1&0\\0&0&0
\end{bmatrix}v=0,\\
& v_2=[1,-1,0]^\top,\ u_2=\frac{1}{\sqrt{2}}[1,-1,0]^\top.\\
\lambda_3=2:&\ \text{Independent eigenvector } v_3=[0,0,1]^\top,\ u_3=v_3.\\
U&=[u_1~u_2~u_3],\ \Lambda=\mathrm{diag}(5,2,2).\\
U^\top A U&=\Lambda\ \Rightarrow\ A=U\Lambda U^\top.\\
x&=\sum_{i=1}^3 \alpha_i u_i\ \Rightarrow\ x^\top A x=\sum_i \lambda_i \alpha_i^2.
\end{align*}
}
\RESULT{
$U=\begin{bmatrix}\tfrac{1}{\sqrt{2}}&\tfrac{1}{\sqrt{2}}&0\\
\tfrac{1}{\sqrt{2}}&-\tfrac{1}{\sqrt{2}}&0\\0&0&1\end{bmatrix}$,
$\Lambda=\mathrm{diag}(5,2,2)$, and $x^\top A x=\sum_i \lambda_i \alpha_i^2$.
}
\UNITCHECK{
Orthogonality: $U^\top U=I$. Reconstruction: $U\Lambda U^\top$ equals $A$.
}
\EDGECASES{
\begin{bullets}
\item Repeated eigenvalue $\lambda=2$ yields a two-dimensional eigenspace.
\item Any orthonormal basis within that eigenspace is valid.
\end{bullets}
}
\ALTERNATE{
Compute $U$ via Gram--Schmidt on eigenvectors of the $2\times 2$ block and
append $e_3$.
}
\VALIDATION{
\begin{bullets}
\item Verify $U^\top A U$ is diagonal with entries $5,2,2$.
\item Numerically reconstruct and compare entries.
\end{bullets}
}
\INTUITION{
The top-left $2\times 2$ block rotates by $\pm 45^\circ$ to align with axes;
the third axis is already aligned.
}
\CANONICAL{
\begin{bullets}
\item Orthogonal diagonalization holds and is unique up to rotations inside
eigenspaces.
\end{bullets}
}

\ProblemPage{2}{Block-Diagonal Symmetric Matrices}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Show that for $A=\mathrm{diag}(B,C)$ with $B=B^\top\in\mathbb{R}^{p\times p}$
and $C=C^\top\in\mathbb{R}^{q\times q}$, the eigenpairs are the union of those
of $B$ and $C$, and $A$ is orthogonally diagonalized by $U=\mathrm{diag}(U_B,U_C)$.

\PROBLEM{
Prove spectral decomposition of block-diagonal symmetric matrices and compute an
explicit example with $B=\begin{bmatrix}2&0\\0&1\end{bmatrix}$, $C=[3]$.
}
\MODEL{
\[
A=\begin{bmatrix}B&0\\0&C\end{bmatrix},\quad B=U_B\Lambda_B U_B^\top,\ 
C=U_C\Lambda_C U_C^\top,\quad A=U\Lambda U^\top.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $B$ and $C$ are real symmetric.
\item Standard orthogonality in $\mathbb{R}^{p+q}$.
\end{bullets}
}
\varmapStart
\var{B,C}{Symmetric blocks.}
\var{U_B,U_C}{Orthogonal diagonalizers of $B,C$.}
\var{U}{Block-diagonal orthogonal matrix $\mathrm{diag}(U_B,U_C)$.}
\var{\Lambda_B,\Lambda_C}{Diagonal eigenvalue blocks.}
\var{\Lambda}{Block-diagonal eigenvalue matrix.}
\varmapEnd
\WHICHFORMULA{
Formula 1 (Spectral Theorem) applied to $B$ and $C$.
}
\GOVERN{
\[
A=U\Lambda U^\top,\quad U=\mathrm{diag}(U_B,U_C),\quad
\Lambda=\mathrm{diag}(\Lambda_B,\Lambda_C).
\]
}
\INPUTS{$B=\begin{bmatrix}2&0\\0&1\end{bmatrix}$, $C=[3]$.}
\DERIVATION{
\begin{align*}
B&=I\cdot \mathrm{diag}(2,1)\cdot I^\top,\quad C=[3],\\
U_B&=I_2,\ U_C=[1],\ U=\mathrm{diag}(I_2,1)=I_3,\\
\Lambda&=\mathrm{diag}(2,1,3).\\
\text{Hence }& A=\mathrm{diag}(2,1,3)=U\Lambda U^\top.
\end{align*}
}
\RESULT{
Eigenvalues are $\{2,1,3\}$ and eigenvectors are the standard basis vectors,
which are orthonormal. $A$ is already diagonal.
}
\UNITCHECK{
Orthogonality: $U^\top U=I$. Block structure preserved.
}
\EDGECASES{
\begin{bullets}
\item Repeated eigenvalues across blocks produce larger eigenspaces combining
coordinates from both blocks.
\end{bullets}
}
\ALTERNATE{
Diagonalize $B$ and $C$ independently and assemble $U$ via direct sum.
}
\VALIDATION{
\begin{bullets}
\item Verify $U^\top A U$ equals $\Lambda$ numerically.
\end{bullets}
}
\INTUITION{
Blocks act independently; rotate each block separately.
}
\CANONICAL{
\begin{bullets}
\item Direct sums of symmetric matrices remain symmetric and diagonalize by
direct sums of orthogonals.
\end{bullets}
}

\ProblemPage{3}{PSD Square Root Construction}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Construct $A^{1/2}$ for $A=\begin{bmatrix}5&1\\1&5\end{bmatrix}$ and verify
$(A^{1/2})^2=A$.

\PROBLEM{
Diagonalize $A$, take square roots of eigenvalues, recombine to get $A^{1/2}$,
and verify by squaring.
}
\MODEL{
\[
A=U\Lambda U^\top,\quad A^{1/2}=U\sqrt{\Lambda}U^\top,\quad (A^{1/2})^2=A.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $A$ is symmetric and PSD.
\end{bullets}
}
\varmapStart
\var{A}{Given PSD symmetric matrix.}
\var{U}{Orthogonal eigenvector matrix.}
\var{\Lambda}{Diagonal eigenvalues.}
\var{A^{1/2}}{Principal PSD square root.}
\varmapEnd
\WHICHFORMULA{
Formula 4 (PSD square root) via spectral decomposition.
}
\GOVERN{
\[
A^{1/2}=U\,\mathrm{diag}(\sqrt{\lambda_i})\,U^\top.
\]
}
\INPUTS{$A=\begin{bmatrix}5&1\\1&5\end{bmatrix}$.}
\DERIVATION{
\begin{align*}
\det\begin{bmatrix}5-\lambda&1\\1&5-\lambda\end{bmatrix}
&=(5-\lambda)^2-1=\lambda^2-10\lambda+24,\\
\lambda&=6,4.\\
\lambda=6:& u_1=\tfrac{1}{\sqrt{2}}[1,1]^\top,\ \lambda=4:\ u_2=\tfrac{1}{\sqrt{2}}
[1,-1]^\top.\\
U&=\tfrac{1}{\sqrt{2}}\begin{bmatrix}1&1\\1&-1\end{bmatrix},\
\Lambda=\mathrm{diag}(6,4).\\
\sqrt{\Lambda}&=\mathrm{diag}(\sqrt{6},2).\\
A^{1/2}&=U\sqrt{\Lambda}U^\top\\
&=\tfrac{1}{2}\begin{bmatrix}1&1\\1&-1\end{bmatrix}
\begin{bmatrix}\sqrt{6}&0\\0&2\end{bmatrix}
\begin{bmatrix}1&1\\1&-1\end{bmatrix}\\
&=\tfrac{1}{2}\begin{bmatrix}\sqrt{6}+2&\sqrt{6}-2\\ \sqrt{6}-2&\sqrt{6}+2\end{bmatrix}.
\end{align*}
}
\RESULT{
$A^{1/2}=\dfrac{1}{2}\begin{bmatrix}\sqrt{6}+2&\sqrt{6}-2\\
\sqrt{6}-2&\sqrt{6}+2\end{bmatrix}$ and $(A^{1/2})^2=A$.
}
\UNITCHECK{
Eigenvalues of $A^{1/2}$ are $\sqrt{6}$ and $2$, nonnegative; squaring yields
$6$ and $4$, matching $A$.
}
\EDGECASES{
\begin{bullets}
\item If $A$ were singular, some square roots would be $0$.
\end{bullets}
}
\ALTERNATE{
Compute via $A^{1/2}=\sum_i \sqrt{\lambda_i} u_i u_i^\top$ directly.
}
\VALIDATION{
\begin{bullets}
\item Multiply $A^{1/2}$ by itself and simplify to recover $A$.
\end{bullets}
}
\INTUITION{
Rotate to axes that $A$ scales by $6$ and $4$, then take scalar square roots.
}
\CANONICAL{
\begin{bullets}
\item Principal square root via $U\sqrt{\Lambda}U^\top$.
\end{bullets}
}

\ProblemPage{4}{Alice and Principal Variance Direction}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Alice has covariance matrix $\Sigma=\begin{bmatrix}2&1\\1&2\end{bmatrix}$. She
wants the direction maximizing variance of $w^\top X$ with $\|w\|=1$.

\PROBLEM{
Find $\arg\max_{\|w\|=1} w^\top \Sigma w$ and the maximal variance value.
}
\MODEL{
\[
\max_{\|w\|=1} w^\top \Sigma w,\quad \Sigma=U\Lambda U^\top.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $\Sigma$ is symmetric PSD.
\item Variance of $w^\top X$ equals $w^\top \Sigma w$.
\end{bullets}
}
\varmapStart
\var{\Sigma}{Covariance matrix.}
\var{w}{Unit direction vector.}
\var{\lambda_i}{Eigenvalues of $\Sigma$.}
\var{u_i}{Eigenvectors (principal components).}
\varmapEnd
\WHICHFORMULA{
Formula 3 (Rayleigh quotient extrema).
}
\GOVERN{
\[
\lambda_{\max}=\max_{\|w\|=1} w^\top \Sigma w,\quad \arg\max=w=u_1.
\]
}
\INPUTS{$\Sigma=\begin{bmatrix}2&1\\1&2\end{bmatrix}$.}
\DERIVATION{
\begin{align*}
\det\begin{bmatrix}2-\lambda&1\\1&2-\lambda\end{bmatrix}
&=(2-\lambda)^2-1=\lambda^2-4\lambda+3,\\
\lambda&=3,1.\\
\lambda_{\max}=3:& u_1=\tfrac{1}{\sqrt{2}}[1,1]^\top.\\
\max_{\|w\|=1} w^\top \Sigma w&=3\ \text{at } w=u_1.
\end{align*}
}
\RESULT{
Maximal variance is $3$ along $w=\tfrac{1}{\sqrt{2}}[1,1]^\top$ (first principal
component).
}
\UNITCHECK{
Variance units align; Rayleigh quotient bounded in $[1,3]$.
}
\EDGECASES{
\begin{bullets}
\item If $\Sigma=\alpha I$, every unit direction attains variance $\alpha$.
\end{bullets}
}
\ALTERNATE{
Parameterize $w=[\cos\theta,\sin\theta]^\top$ and maximize $w^\top \Sigma w$.
}
\VALIDATION{
\begin{bullets}
\item Compute $w^\top \Sigma w$ at $w=u_1$ equals $3$ and at $u_2$ equals $1$.
\end{bullets}
}
\INTUITION{
Pick the axis along which data spread is largest.
}
\CANONICAL{
\begin{bullets}
\item Principal component solves a Rayleigh quotient maximization.
\end{bullets}
}

\ProblemPage{5}{Bob and Minimum Energy Direction}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Bob minimizes $x^\top H x$ over $\|x\|=1$ for stiffness matrix
$H=\begin{bmatrix}4&-1\\-1&2\end{bmatrix}$.

\PROBLEM{
Find the minimal energy $\lambda_{\min}$ and direction $x$.
}
\MODEL{
\[
\min_{\|x\|=1} x^\top H x=\lambda_{\min},\quad H=U\Lambda U^\top.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $H$ is symmetric; energy is a quadratic form.
\end{bullets}
}
\varmapStart
\var{H}{Stiffness matrix.}
\var{x}{Unit vector.}
\var{\lambda_{\min}}{Smallest eigenvalue.}
\var{u_n}{Corresponding eigenvector.}
\varmapEnd
\WHICHFORMULA{
Formula 3 (Rayleigh quotient minimum).
}
\GOVERN{
\[
\lambda_{\min}=\min_{\|x\|=1} x^\top H x,\quad \arg\min=u_2.
\]
}
\INPUTS{$H=\begin{bmatrix}4&-1\\-1&2\end{bmatrix}$.}
\DERIVATION{
\begin{align*}
\det\begin{bmatrix}4-\lambda&-1\\-1&2-\lambda\end{bmatrix}
&=(4-\lambda)(2-\lambda)-1\\
&=\lambda^2-6\lambda+7,\\
\lambda&=\frac{6\pm \sqrt{36-28}}{2}=3\pm \sqrt{2}.\\
\lambda_{\min}&=3-\sqrt{2},\\
(H-\lambda_{\min} I)u&=0 \Rightarrow
\begin{bmatrix}1+\sqrt{2}&-1\\-1&-1+\sqrt{2}\end{bmatrix}u=0,\\
u&=\frac{1}{\sqrt{1+(1+\sqrt{2})^2}}[1,1+\sqrt{2}]^\top.
\end{align*}
}
\RESULT{
$\lambda_{\min}=3-\sqrt{2}$ at $u\propto [1,1+\sqrt{2}]^\top$ (normalized).
}
\UNITCHECK{
Bounds: eigenvalues are positive, so $H\succ 0$; energy nonnegative.
}
\EDGECASES{
\begin{bullets}
\item If coupling $-1$ were $0$, eigenvalues would be $4$ and $2$.
\end{bullets}
}
\ALTERNATE{
Directly minimize $x^\top H x$ for $x=[\cos\theta,\sin\theta]^\top$.
}
\VALIDATION{
\begin{bullets}
\item Check $Hu=(3-\sqrt{2})u$ numerically.
\end{bullets}
}
\INTUITION{
Least energy direction aligns with the softest mode.
}
\CANONICAL{
\begin{bullets}
\item Minimum of a Hermitian quadratic form equals smallest eigenvalue.
\end{bullets}
}

\ProblemPage{6}{Expected Rayleigh Quotient of a Random Unit Vector}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Let $x$ be uniform on the unit sphere in $\mathbb{R}^n$. Show
$\mathbb{E}[x^\top A x]=\tfrac{1}{n}\mathrm{tr}(A)$ for symmetric $A$.

\PROBLEM{
Compute the expectation using spectral theorem and rotational invariance.
}
\MODEL{
\[
A=U\Lambda U^\top,\quad x\sim \text{Unif}(\mathbb{S}^{n-1}).
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $A$ symmetric; distribution rotationally invariant.
\end{bullets}
}
\varmapStart
\var{A}{Symmetric matrix.}
\var{\Lambda}{Eigenvalues of $A$.}
\var{U}{Orthogonal matrix.}
\var{x}{Random unit vector.}
\varmapEnd
\WHICHFORMULA{
Formula 1 (diagonalization) and isotropy of the sphere.
}
\GOVERN{
\[
\mathbb{E}[x^\top A x]=\mathbb{E}[y^\top \Lambda y],\ y=U^\top x.
\]
}
\INPUTS{$A=U\Lambda U^\top$, $x\sim$ uniform on $\mathbb{S}^{n-1}$.}
\DERIVATION{
\begin{align*}
y&=U^\top x \sim \text{Unif}(\mathbb{S}^{n-1})\ \text{by invariance}.\\
x^\top A x&=y^\top \Lambda y=\sum_{i=1}^n \lambda_i y_i^2.\\
\mathbb{E}[y_i^2]&=\frac{1}{n}\ (\text{symmetry and } \sum y_i^2=1).\\
\mathbb{E}[x^\top A x]&=\sum_i \lambda_i \frac{1}{n}
=\frac{1}{n}\mathrm{tr}(A).
\end{align*}
}
\RESULT{
$\mathbb{E}[x^\top A x]=\tfrac{1}{n}\mathrm{tr}(A)$.
}
\UNITCHECK{
Scalar expectation equals trace per dimension; consistent.
}
\EDGECASES{
\begin{bullets}
\item If $A=\alpha I$, expectation equals $\alpha$ for all $n$.
\end{bullets}
}
\ALTERNATE{
Compute using $x=g/\|g\|$ with $g\sim \mathcal{N}(0,I)$; use independence and
chi-square moments.
}
\VALIDATION{
\begin{bullets}
\item Monte Carlo with fixed seed approximates trace$/n$.
\end{bullets}
}
\INTUITION{
Average energy spreads equally among $n$ coordinates.
}
\CANONICAL{
\begin{bullets}
\item Sphere isotropy implies mean Rayleigh quotient equals trace density.
\end{bullets}
}

\ProblemPage{7}{Proof: Orthogonality of Distinct Eigenvectors}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Prove that if $A=A^\top$ and $Av=\lambda v$, $Aw=\mu w$ with $\lambda\ne\mu$,
then $v^\top w=0$.

\PROBLEM{
Give a concise proof using symmetry of $A$.
}
\MODEL{
\[
\langle Av,w\rangle=\langle v,Aw\rangle,\ \lambda\ne\mu.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Real symmetry; standard inner product.
\end{bullets}
}
\varmapStart
\var{A}{Symmetric matrix.}
\var{v,w}{Eigenvectors for distinct eigenvalues.}
\var{\lambda,\mu}{Eigenvalues, $\lambda\ne\mu$.}
\varmapEnd
\WHICHFORMULA{
Lemma in Formula 1 (supporting lemma).
}
\GOVERN{
\[
\lambda\langle v,w\rangle=\langle Av,w\rangle=\langle v,Aw\rangle=
\mu\langle v,w\rangle.
\]
}
\INPUTS{$Av=\lambda v$, $Aw=\mu w$, $\lambda\ne\mu$.}
\DERIVATION{
\begin{align*}
\lambda\langle v,w\rangle&=\langle Av,w\rangle
=\langle v,Aw\rangle=\mu\langle v,w\rangle,\\
(\lambda-\mu)\langle v,w\rangle&=0 \Rightarrow \langle v,w\rangle=0.
\end{align*}
}
\RESULT{
$v$ and $w$ are orthogonal.
}
\UNITCHECK{
Inner products are scalars; equality is dimensionally consistent.
}
\EDGECASES{
\begin{bullets}
\item If $\lambda=\mu$, orthogonality is not guaranteed but can be enforced by
Gram--Schmidt within the eigenspace.
\end{bullets}
}
\ALTERNATE{
Use $v^\top A w=(A v)^\top w=\lambda v^\top w$ and also
$v^\top A w=v^\top (A w)=\mu v^\top w$.
}
\VALIDATION{
\begin{bullets}
\item Apply to a numeric example to verify zero inner product.
\end{bullets}
}
\INTUITION{
Different stretch factors cannot align in a symmetric scaling.
}
\CANONICAL{
\begin{bullets}
\item Distinct eigenspaces of a Hermitian matrix are orthogonal.
\end{bullets}
}

\ProblemPage{8}{Proof: Simultaneous Diagonalization}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
If $A,B$ are Hermitian and commute ($AB=BA$), then they are simultaneously
unitarily diagonalizable.

\PROBLEM{
Prove existence of a unitary $U$ such that $U^\ast A U$ and $U^\ast B U$ are
both diagonal.
}
\MODEL{
\[
A=U\Lambda U^\ast,\quad U^\ast B U=\text{block-diagonal by eigenspaces of }A.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $A,B$ Hermitian and $AB=BA$.
\end{bullets}
}
\varmapStart
\var{A,B}{Commuting Hermitian matrices.}
\var{U}{Unitary diagonalizer of $A$.}
\var{\Lambda}{Diagonal eigenvalues of $A$.}
\varmapEnd
\WHICHFORMULA{
Formula 1 and spectral projectors $P_i$ (Formula 2).
}
\GOVERN{
\[
AP_i=P_iA=\lambda_i P_i,\quad BP_i=P_iB,\ \text{so }B \text{ preserves eigenspaces}.
\]
}
\INPUTS{$AB=BA$, $A=A^\ast$, $B=B^\ast$.}
\DERIVATION{
\begin{align*}
A&=\sum_i \lambda_i P_i,\quad P_i P_j=\delta_{ij}P_i,\ \sum_i P_i=I.\\
AB&=BA\ \Rightarrow\ \sum_i \lambda_i P_i B=\sum_i \lambda_i B P_i.\\
\text{Hence }& P_i B = B P_i,\ \forall i,\ \text{so }B(\mathrm{im}\,P_i)
\subseteq \mathrm{im}\,P_i.\\
\text{Thus }& B \text{ is Hermitian on each eigenspace of }A.\\
\text{Pick }& \text{an orthonormal basis of each eigenspace that diagonalizes }B.\\
& \text{Concatenate to get a unitary }U \text{ diagonalizing both.}
\end{align*}
}
\RESULT{
There exists unitary $U$ such that $U^\ast A U$ and $U^\ast B U$ are diagonal.
}
\UNITCHECK{
Unitary preserves orthonormality; blockwise diagonalization consistent.
}
\EDGECASES{
\begin{bullets}
\item If eigenvalues of $A$ are simple, $B$ must be diagonal in $A$'s basis.
\end{bullets}
}
\ALTERNATE{
Use functional calculus: $P_i=p_i(A)$ polynomials in $A$ commute with $B$,
implying invariant subspaces for $B$.
}
\VALIDATION{
\begin{bullets}
\item Test with $A=\mathrm{diag}(1,1,2)$ and $B$ block-diagonal in first block.
\end{bullets}
}
\INTUITION{
If two Hermitian operators commute, they share measurement axes.
}
\CANONICAL{
\begin{bullets}
\item Commuting Hermitian matrices are simultaneously diagonalizable.
\end{bullets}
}

\ProblemPage{9}{Graph Laplacian: PSD and Zero Eigenvalue}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
For a simple undirected graph with Laplacian $L=D-W$, show $L$ is symmetric
PSD and $L\mathbf{1}=0$; compute eigenpairs for path graph on 2 nodes.

\PROBLEM{
Prove PSD via quadratic form and compute spectrum of $L=\begin{bmatrix}1&-1\\-1&1
\end{bmatrix}$.
}
\MODEL{
\[
x^\top L x=\sum_{(i,j)} w_{ij}(x_i-x_j)^2\ge 0,\quad L\mathbf{1}=0.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Undirected weights $w_{ij}=w_{ji}\ge 0$.
\end{bullets}
}
\varmapStart
\var{L}{Graph Laplacian $D-W$.}
\var{w_{ij}}{Edge weights.}
\var{D}{Degree matrix.}
\var{\mathbf{1}}{All-ones vector.}
\varmapEnd
\WHICHFORMULA{
Formula 5 (PSD bound) context and spectral theorem applicability.
}
\GOVERN{
\[
x^\top L x=\tfrac{1}{2}\sum_{i,j} w_{ij}(x_i-x_j)^2\ge 0.
\]
}
\INPUTS{$L=\begin{bmatrix}1&-1\\-1&1\end{bmatrix}$.}
\DERIVATION{
\begin{align*}
L&=L^\top,\ x^\top L x=(x_1-x_2)^2\ge 0\Rightarrow L\succeq 0.\\
L\mathbf{1}&=\begin{bmatrix}1&-1\\-1&1\end{bmatrix}\begin{bmatrix}1\\1
\end{bmatrix}=\begin{bmatrix}0\\0\end{bmatrix}.\\
\det(L-\lambda I)&=\det\begin{bmatrix}1-\lambda&-1\\-1&1-\lambda\end{bmatrix}\\
&=(1-\lambda)^2-1=\lambda(\lambda-2).\\
\lambda&=0,2,\ u_1=\tfrac{1}{\sqrt{2}}[1,1]^\top,\
u_2=\tfrac{1}{\sqrt{2}}[1,-1]^\top.
\end{align*}
}
\RESULT{
$L$ is symmetric PSD with eigenvalues $0,2$ and orthonormal eigenvectors as
above; $\mathbf{1}$ spans the nullspace.
}
\UNITCHECK{
Quadratic form nonnegative; eigenvalues nonnegative.
}
\EDGECASES{
\begin{bullets}
\item Number of zero eigenvalues equals number of connected components.
\end{bullets}
}
\ALTERNATE{
Write $L=B^\top B$ for incidence matrix $B$ to show PSD.
}
\VALIDATION{
\begin{bullets}
\item Verify $L=U\Lambda U^\top$ with computed $U,\Lambda$.
\end{bullets}
}
\INTUITION{
Laplacian measures pairwise differences; constant vectors have zero variation.
}
\CANONICAL{
\begin{bullets}
\item Symmetric PSD implies spectral theorem; modes are graph harmonics.
\end{bullets}
}

\ProblemPage{10}{PCA as Spectral Decomposition}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Given centered data matrix $X\in\mathbb{R}^{n\times d}$, covariance
$\Sigma=\tfrac{1}{n}X^\top X$ is symmetric PSD. Show first principal component
maximizes variance and equals the top eigenvector of $\Sigma$.

\PROBLEM{
Prove $\max_{\|w\|=1}\tfrac{1}{n}\|Xw\|^2=w^\top \Sigma w$ is attained at $u_1$,
and compute a numeric example.
}
\MODEL{
\[
\Sigma=\tfrac{1}{n}X^\top X=U\Lambda U^\top,\quad
\max_{\|w\|=1} w^\top \Sigma w = \lambda_{\max}.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Columns of $X$ are centered; $\Sigma\succeq 0$.
\end{bullets}
}
\varmapStart
\var{X}{Centered data.}
\var{\Sigma}{Covariance matrix.}
\var{w}{Unit direction.}
\var{u_1}{Top eigenvector of $\Sigma$.}
\var{\lambda_{\max}}{Top eigenvalue.}
\varmapEnd
\WHICHFORMULA{
Formula 3 (Rayleigh quotient) and Formula 1 (diagonalization).
}
\GOVERN{
\[
\max_{\|w\|=1} \tfrac{1}{n}\|Xw\|^2=\max_{\|w\|=1} w^\top \Sigma w
=\lambda_{\max}.
\]
}
\INPUTS{$X=\begin{bmatrix}-1&0\\0&1\\1&-1\end{bmatrix}$, $n=3$.}
\DERIVATION{
\begin{align*}
\Sigma&=\tfrac{1}{3}X^\top X
=\tfrac{1}{3}\begin{bmatrix}(-1)^2+0^2+1^2&(-1)0+0\cdot 1+1(-1)\\
0(-1)+1\cdot 0+(-1)1&0^2+1^2+(-1)^2\end{bmatrix}\\
&=\tfrac{1}{3}\begin{bmatrix}2&-1\\-1&2\end{bmatrix}.\\
\det(\Sigma-\lambda I)&=\det \tfrac{1}{3}\begin{bmatrix}2-3\lambda&-1\\-1&2-3\lambda
\end{bmatrix}\\
&=\tfrac{1}{9}[(2-3\lambda)^2-1],\\
(2-3\lambda)^2&=1\Rightarrow 2-3\lambda=\pm 1,\\
\lambda&=1,\ \tfrac{1}{3}.\\
u_1&=\tfrac{1}{\sqrt{2}}[1,-1]^\top\ \text{for } \lambda=1.\\
\text{Thus }& \max w^\top \Sigma w=1\ \text{at } w=u_1.
\end{align*}
}
\RESULT{
First principal component $w=u_1=\tfrac{1}{\sqrt{2}}[1,-1]^\top$, with maximal
variance $1$.
}
\UNITCHECK{
$\Sigma\succeq 0$; eigenvalues nonnegative; variance equals Rayleigh quotient.
}
\EDGECASES{
\begin{bullets}
\item If $\Sigma$ is isotropic, all directions have equal variance.
\end{bullets}
}
\ALTERNATE{
Use SVD: $X=\sqrt{n}\,V\sqrt{\Lambda}U^\top$, so $\Sigma=U\Lambda U^\top$.
}
\VALIDATION{
\begin{bullets}
\item Compute $\|X u_1\|^2/n=1$ numerically.
\end{bullets}
}
\INTUITION{
PCA chooses the direction of largest spread.
}
\CANONICAL{
\begin{bullets}
\item PCA eigenvectors equal covariance eigenvectors.
\end{bullets}
}

\section{Coding Demonstrations}

\CodeDemoPage{Orthogonal Diagonalization of a Random Symmetric Matrix}
\PROBLEM{
Numerically verify $A=Q\Lambda Q^\top$ for a deterministic random symmetric
matrix. From-scratch variant uses power iteration with deflation; library
variant uses \inlinecode{numpy.linalg.eigh}.
}
\API{
\begin{bullets}
\item \inlinecode{def read_input(s) -> int} — parse size $n$.
\item \inlinecode{def solve_case(n) -> (A,Q,L)} — diagonalize $A$.
\item \inlinecode{def validate() -> None} — assertions for $Q^\top Q=I$ and
$A\approx Q L Q^\top$.
\item \inlinecode{def main() -> None} — run demo.
\end{bullets}
}
\INPUTS{
Integer $n$ (matrix size). Fixed seed ensures determinism.
}
\OUTPUTS{
Matrix $A$, orthogonal $Q$, diagonal $\Lambda$ (as 1D array of eigenvalues).
}
\FORMULA{
\[
A=Q\Lambda Q^\top,\quad Q^\top Q=I,\quad \Lambda\in\mathbb{R}^{n\times n}
\text{ diagonal}.
\]
}
\textbf{SOLUTION A — From Scratch (Mathematically Explicit Implementation)}
\begin{codepy}
import numpy as np

def read_input(s):
    return int(s.strip())

def sym_random(n, seed=0):
    rng = np.random.default_rng(seed)
    M = rng.standard_normal((n, n))
    return (M + M.T) / 2.0

def power_iter(A, iters=200):
    n = A.shape[0]
    v = np.ones(n) / np.sqrt(n)
    for _ in range(iters):
        v = A @ v
        nv = np.linalg.norm(v)
        if nv == 0.0:
            break
        v = v / nv
    lam = float(v @ (A @ v))
    return lam, v

def deflate(A, k):
    n = A.shape[0]
    Q = np.zeros((n, k))
    L = np.zeros(k)
    R = A.copy()
    for i in range(k):
        lam, v = power_iter(R, iters=300)
        for j in range(i):
            v = v - (Q[:, j] @ v) * Q[:, j]
        nv = np.linalg.norm(v)
        if nv == 0.0:
            v = np.eye(n)[:, i]
            nv = 1.0
        v = v / nv
        Q[:, i] = v
        L[i] = float(v @ (A @ v))
        R = R - L[i] * np.outer(v, v)
    return L, Q

def solve_case(n):
    A = sym_random(n, seed=42)
    L, Q = deflate(A, n)
    # Order by descending eigenvalue
    idx = np.argsort(-L)
    L = L[idx]
    Q = Q[:, idx]
    return A, Q, L

def validate():
    n = 5
    A, Q, L = solve_case(n)
    I = Q.T @ Q
    assert np.allclose(I, np.eye(n), atol=1e-6)
    Lm = np.diag(L)
    Arec = Q @ Lm @ Q.T
    assert np.allclose(A, Arec, atol=1e-4)

def main():
    validate()
    A, Q, L = solve_case(5)
    print("eigvals:", np.round(L, 4))

if __name__ == "__main__":
    main()
\end{codepy}

\textbf{SOLUTION B — Library-Based (Validated Computational Shortcut)}
\begin{codepy}
import numpy as np

def read_input(s):
    return int(s.strip())

def sym_random(n, seed=42):
    rng = np.random.default_rng(seed)
    M = rng.standard_normal((n, n))
    return (M + M.T) / 2.0

def solve_case(n):
    A = sym_random(n, seed=42)
    L, Q = np.linalg.eigh(A)
    idx = np.argsort(-L)
    L = L[idx]
    Q = Q[:, idx]
    return A, Q, L

def validate():
    A, Q, L = solve_case(6)
    I = Q.T @ Q
    assert np.allclose(I, np.eye(6), atol=1e-10)
    Arec = Q @ np.diag(L) @ Q.T
    assert np.allclose(A, Arec, atol=1e-10)

def main():
    validate()
    _, _, L = solve_case(6)
    print("eigvals:", np.round(L, 6))

if __name__ == "__main__":
    main()
\end{codepy}
\COMPLEXITY{
From-scratch: power iteration with deflation uses $\mathcal{O}(n^2 k)$ per
eigenpair for $k=n$, total about $\mathcal{O}(n^3)$ time, $\mathcal{O}(n^2)$
space. Library eigh: $\mathcal{O}(n^3)$ time, $\mathcal{O}(n^2)$ space.
}
\FAILMODES{
\begin{bullets}
\item Degenerate eigenvalues slow power iteration; mitigated by orthogonal
deflation and re-orthogonalization.
\item Numerical drift in orthogonality; re-orthogonalize vectors.
\end{bullets}
}
\STABILITY{
\begin{bullets}
\item Symmetric problems are well-conditioned for eigenvectors under gaps.
\item Use orthonormalization to avoid loss of orthogonality.
\end{bullets}
}
\VALIDATION{
\begin{bullets}
\item Check $Q^\top Q=I$ and $A\approx Q\Lambda Q^\top$.
\item Compare eigenvalues between both variants.
\end{bullets}
}
\RESULT{
Both implementations yield matching spectra and reconstruct $A$ within tolerance.
}
\EXPLANATION{
Matches Formula 1: orthogonal/unitary diagonalization, verified numerically.
}

\CodeDemoPage{Rayleigh Quotient Grid vs. Eigenvalues (2D)}
\PROBLEM{
For a $2\times 2$ symmetric $A$, compute $\max_{\|x\|=1} x^\top A x$ by grid
search on angles and verify it equals the largest eigenvalue.
}
\API{
\begin{bullets}
\item \inlinecode{def read_input(s) -> np.ndarray} — parse 4 entries to $A$.
\item \inlinecode{def solve_case(A) -> (lmax, tmax)} — grid max of Rayleigh.
\item \inlinecode{def validate() -> None} — compare with \inlinecode{eigh}.
\item \inlinecode{def main() -> None} — run.
\end{bullets}
}
\INPUTS{
$A=\begin{bmatrix}a&b\\b&c\end{bmatrix}$ with $a,c\in\mathbb{R}$, $b\in\mathbb{R}$.
}
\OUTPUTS{
$l_{\max}$ grid estimate and angle $\theta_{\max}$ of maximizer.
}
\FORMULA{
\[
R_A(x)=x^\top A x,\ x=[\cos\theta,\sin\theta]^\top,\ \theta\in[0,2\pi).
\]
}
\textbf{SOLUTION A — From Scratch (Mathematically Explicit Implementation)}
\begin{codepy}
import numpy as np

def read_input(s):
    vals = [float(z) for z in s.strip().split()]
    A = np.array(vals, dtype=float).reshape(2, 2)
    return A

def rayleigh_angle(A, theta):
    c, s = np.cos(theta), np.sin(theta)
    x = np.array([c, s])
    return float(x @ A @ x)

def solve_case(A):
    thetas = np.linspace(0.0, 2.0*np.pi, 2001)
    vals = np.array([rayleigh_angle(A, t) for t in thetas])
    idx = int(np.argmax(vals))
    return float(vals[idx]), float(thetas[idx])

def validate():
    A = np.array([[2.0, 1.0], [1.0, 3.0]])
    lmax, _ = solve_case(A)
    L, _ = np.linalg.eigh(A)
    assert abs(lmax - float(L.max())) < 5e-3

def main():
    validate()
    A = np.array([[2.0, 1.0], [1.0, 3.0]])
    lmax, tmax = solve_case(A)
    print("lmax:", round(lmax, 4), "theta:", round(tmax, 3))

if __name__ == "__main__":
    main()
\end{codepy}

\textbf{SOLUTION B — Library-Based (Validated Computational Shortcut)}
\begin{codepy}
import numpy as np

def read_input(s):
    vals = [float(z) for z in s.strip().split()]
    A = np.array(vals, dtype=float).reshape(2, 2)
    return A

def solve_case(A):
    L, Q = np.linalg.eigh(A)
    lmax = float(L.max())
    i = int(np.argmax(L))
    v = Q[:, i]
    theta = float(np.arctan2(v[1], v[0]))
    return lmax, theta

def validate():
    A = np.array([[2.0, 1.0], [1.0, 3.0]])
    lmax1, _ = solve_case(A)
    # Compare to grid method
    thetas = np.linspace(0.0, 2.0*np.pi, 2001)
    xs = np.c_[np.cos(thetas), np.sin(thetas)]
    vals = np.einsum("ij,ji->i", xs @ A, xs.T)
    lmax2 = float(vals.max())
    assert abs(lmax1 - lmax2) < 1e-9

def main():
    validate()
    A = np.array([[2.0, 1.0], [1.0, 3.0]])
    lmax, theta = solve_case(A)
    print("lmax:", round(lmax, 6), "theta:", round(theta, 6))

if __name__ == "__main__":
    main()
\end{codepy}
\COMPLEXITY{
Grid: $\mathcal{O}(m)$ evaluations, constant space. Library: $\mathcal{O}(1)$
for $2\times 2$ closed form, but eigh is $\mathcal{O}(1)$ here.
}
\FAILMODES{
\begin{bullets}
\item Coarse grid may miss the exact angle; increase resolution.
\item Non-symmetric input invalidates real extrema; enforce symmetry.
\end{bullets}
}
\STABILITY{
\begin{bullets}
\item Rayleigh evaluations are stable for small matrices.
\end{bullets}
}
\VALIDATION{
\begin{bullets}
\item Compare grid maximum with eigenvalue from \inlinecode{eigh}.
\end{bullets}
}
\RESULT{
Max Rayleigh equals largest eigenvalue, as per Formula 3.
}
\EXPLANATION{
Directly verifies the Rayleigh extremal characterization.
}

\CodeDemoPage{PSD Matrix Square Root: Newton--Schulz vs. Eigendecomposition}
\PROBLEM{
Compute $A^{1/2}$ for PSD $A$ via Newton--Schulz iteration (scaled) and via
eigendecomposition; verify $(A^{1/2})^2\approx A$.
}
\API{
\begin{bullets}
\item \inlinecode{def read_input(s) -> np.ndarray} — parse $n$ and seed.
\item \inlinecode{def solve_case(A) -> (S1,S2)} — two square roots.
\item \inlinecode{def validate() -> None} — assert accuracy.
\item \inlinecode{def main() -> None} — run.
\end{bullets}
}
\INPUTS{
PSD $A$ constructed as $A=B^\top B$ for deterministic $B$.
}
\OUTPUTS{
$S_1$ via Newton--Schulz, $S_2$ via eigendecomposition.
}
\FORMULA{
\[
A^{1/2}=U\sqrt{\Lambda}U^\top,\ \text{and}\ X_{k+1}=\tfrac{1}{2}X_k(3I-Y_kX_k),\
Y_{k+1}=\tfrac{1}{2}(3I-Y_kX_k)Y_k,
\]
with $X_0=A/\alpha$, $Y_0=I$, $\alpha=\|A\|_2$.
}
\textbf{SOLUTION A — From Scratch (Mathematically Explicit Implementation)}
\begin{codepy}
import numpy as np

def read_input(s):
    n = int(s.strip())
    return n

def make_psd(n, seed=0):
    rng = np.random.default_rng(seed)
    B = rng.standard_normal((n, n))
    return B.T @ B

def newton_schulz_sqrt(A, iters=30):
    n = A.shape[0]
    # Scale by 2-norm upper bound (use Frobenius as safe bound)
    alpha = np.linalg.norm(A, ord=2)
    if alpha == 0.0:
        return np.zeros_like(A)
    X = A / alpha
    Y = np.eye(n)
    for _ in range(iters):
        R = 0.5 * (3.0 * np.eye(n) - Y @ X)
        X = X @ R
        Y = R @ Y
    return np.sqrt(alpha) * X

def eig_sqrt(A):
    L, Q = np.linalg.eigh(A)
    L[L < 0.0] = 0.0
    return Q @ np.diag(np.sqrt(L)) @ Q.T

def solve_case(n):
    A = make_psd(n, seed=1)
    S1 = newton_schulz_sqrt(A, iters=50)
    S2 = eig_sqrt(A)
    return A, S1, S2

def validate():
    A, S1, S2 = solve_case(4)
    assert np.allclose(S1 @ S1, A, atol=1e-4)
    assert np.allclose(S2 @ S2, A, atol=1e-10)
    assert np.allclose(S1, S2, atol=5e-3)

def main():
    validate()
    A, S1, S2 = solve_case(4)
    err1 = np.linalg.norm(S1 @ S1 - A)
    err2 = np.linalg.norm(S2 @ S2 - A)
    print("errs:", round(err1, 5), round(err2, 5))

if __name__ == "__main__":
    main()
\end{codepy}

\textbf{SOLUTION B — Library-Based (Validated Computational Shortcut)}
\begin{codepy}
import numpy as np

def read_input(s):
    return int(s.strip())

def make_psd(n, seed=1):
    rng = np.random.default_rng(seed)
    B = rng.standard_normal((n, n))
    return B.T @ B

def solve_case(n):
    A = make_psd(n, seed=1)
    L, Q = np.linalg.eigh(A)
    L[L < 0.0] = 0.0
    S = Q @ np.diag(np.sqrt(L)) @ Q.T
    return A, S

def validate():
    A, S = solve_case(5)
    assert np.allclose(S @ S, A, atol=1e-10)

def main():
    validate()
    A, S = solve_case(5)
    print("ok sqrt; tr(A):", round(float(np.trace(A)), 6))

if __name__ == "__main__":
    main()
\end{codepy}
\COMPLEXITY{
Newton--Schulz: $\mathcal{O}(n^3 k)$ for $k$ iterations, space $\mathcal{O}(n^2)$.
Eigendecomposition: $\mathcal{O}(n^3)$ time, $\mathcal{O}(n^2)$ space.
}
\FAILMODES{
\begin{bullets}
\item Poor scaling harms convergence; scale by $\|A\|_2$.
\item Non-PSD input yields complex roots; enforce PSD via $A=B^\top B$.
\end{bullets}
}
\STABILITY{
\begin{bullets}
\item Eigendecomposition is backward stable for symmetric matrices.
\item Newton--Schulz converges quadratically near solution when well-scaled.
\end{bullets}
}
\VALIDATION{
\begin{bullets}
\item Check $(S)^2\approx A$ and compare $S_1$ vs. $S_2$.
\end{bullets}
}
\RESULT{
Both methods compute consistent square roots; eig-based is exact within tol.
}
\EXPLANATION{
Implements Formula 4 via functional calculus and iterative refinement.
}

\section{Applied Domains — Detailed End-to-End Scenarios}

\DomainPage{Machine Learning}
\SCENARIO{
Principal Component Analysis via eigen-decomposition of covariance: reduce $d$
to $k$ dimensions by projecting onto top $k$ eigenvectors.
}
\ASSUMPTIONS{
\begin{bullets}
\item Data are centered: column means are zero.
\item Covariance $\Sigma=\tfrac{1}{n}X^\top X$ is symmetric PSD.
\end{bullets}
}
\WHICHFORMULA{
Rayleigh quotient (Formula 3) and spectral theorem (Formula 1): top eigenvectors
maximize projected variance.
}
\varmapStart
\var{X}{Data matrix $(n,d)$, centered.}
\var{\Sigma}{Covariance $\tfrac{1}{n}X^\top X$.}
\var{U}{Eigenvectors of $\Sigma$.}
\var{\Lambda}{Eigenvalues, sorted descending.}
\var{Z}{Projected data $XU_k$.}
\varmapEnd
\PIPELINE{
\begin{bullets}
\item Center data; compute $\Sigma$.
\item Compute top $k$ eigenpairs $(\lambda_i,u_i)$.
\item Project $X$ to $Z=XU_k$ and report explained variance.
\end{bullets}
}
\textbf{Implementation (From Scratch)}
\begin{codepy}
import numpy as np

def center(X):
    return X - X.mean(axis=0, keepdims=True)

def pca_from_scratch(X, k):
    Xc = center(X)
    n = Xc.shape[0]
    S = (Xc.T @ Xc) / n
    L, U = np.linalg.eigh(S)
    idx = np.argsort(-L)
    L = L[idx]
    U = U[:, idx]
    Uk = U[:, :k]
    Z = Xc @ Uk
    evr = L[:k].sum() / L.sum()
    return Z, Uk, L[:k], float(evr)

def main():
    np.random.seed(0)
    X = np.random.randn(200, 3) @ np.array([[2,0,0],[0,1,0],[0,0,0.5]])
    Z, Uk, Lk, evr = pca_from_scratch(X, k=2)
    print("Lk:", np.round(Lk, 3), "EVR:", round(evr, 3))

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{Explained variance ratio (EVR) and eigenvalues of $\Sigma$.}
\INTERPRET{
Top eigenvalues capture most variance; projections along $U_k$ retain structure.
}
\NEXTSTEPS{
Use randomized SVD for large $n,d$; whiten with $\Lambda^{-1/2}$ if needed.
}

\DomainPage{Quantitative Finance}
\SCENARIO{
Risk factor analysis: decompose covariance of returns into eigenmodes and
report principal risk contributions.
}
\ASSUMPTIONS{
\begin{bullets}
\item Returns are centered and stationary.
\item Covariance is symmetric PSD.
\end{bullets}
}
\WHICHFORMULA{
Spectral decomposition (Formula 2): $\Sigma=\sum_i \lambda_i u_i u_i^\top$,
eigenvalues quantify orthogonal risk modes.
}
\varmapStart
\var{R}{Returns matrix $(n,d)$.}
\var{\Sigma}{Sample covariance $(1/n)R^\top R$.}
\var{\lambda_i,u_i}{Eigenpairs of $\Sigma$.}
\var{c_i}{Mode variances $\lambda_i$.}
\varmapEnd
\PIPELINE{
\begin{bullets}
\item Simulate returns; compute covariance.
\item Eigen-decompose; sort by $\lambda_i$.
\item Report cumulative explained risk.
\end{bullets}
}
\textbf{Implementation (Full Pipeline)}
\begin{codepy}
import numpy as np

def simulate(n=1000, d=5, seed=0):
    rng = np.random.default_rng(seed)
    A = rng.standard_normal((d, d))
    C = A @ A.T
    R = rng.multivariate_normal(np.zeros(d), C, size=n)
    return R

def factors(R):
    n = R.shape[0]
    S = (R.T @ R) / n
    L, U = np.linalg.eigh(S)
    idx = np.argsort(-L)
    L = L[idx]
    U = U[:, idx]
    cum = np.cumsum(L) / L.sum()
    return L, U, cum

def main():
    R = simulate()
    L, U, cum = factors(R)
    print("top eigvals:", np.round(L[:3], 3))
    print("cum risk:", np.round(cum[:3], 3))

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{
Top eigenvalues and cumulative explained variance (risk).
}
\INTERPRET{
Large first eigenvalue indicates a dominant market mode; others are sectoral.
}
\NEXTSTEPS{
Rotate portfolio to reduce exposure to top eigenmodes (risk parity).
}

\DomainPage{Deep Learning}
\SCENARIO{
Spectral norm of a symmetric weight matrix equals its largest absolute
eigenvalue; compute and compare with power iteration bound.
}
\ASSUMPTIONS{
\begin{bullets}
\item Weight matrix is symmetric.
\item Spectral norm equals max $|\lambda_i|$ (Formula 5).
\end{bullets}
}
\WHICHFORMULA{
Quadratic form bounds and operator norm equality for Hermitian matrices.
}
\varmapStart
\var{W}{Symmetric weight matrix.}
\var{\|W\|_2}{Spectral norm.}
\var{\lambda_{\max}}{Largest absolute eigenvalue.}
\var{v}{Approximate top eigenvector via power iteration.}
\varmapEnd
\PIPELINE{
\begin{bullets}
\item Create symmetric $W$.
\item Compute $\lambda_{\max}$ by \inlinecode{eigh}.
\item Approximate with power iteration and compare.
\end{bullets}
}
\textbf{Implementation (End-to-End)}
\begin{codepy}
import numpy as np

def make_sym(n=6, seed=0):
    rng = np.random.default_rng(seed)
    M = rng.standard_normal((n, n))
    return (M + M.T) / 2.0

def spectral_norm(W, iters=100):
    n = W.shape[0]
    v = np.ones(n) / np.sqrt(n)
    for _ in range(iters):
        v = W @ v
        v = v / np.linalg.norm(v)
    s = float(abs(v @ (W @ v)))
    return s

def main():
    W = make_sym(6, seed=3)
    L, _ = np.linalg.eigh(W)
    s_true = float(np.max(np.abs(L)))
    s_est = spectral_norm(W, iters=200)
    print("true:", round(s_true, 6), "est:", round(s_est, 6))

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{
Compare true spectral norm vs. power-iteration estimate.
}
\INTERPRET{
Agreement validates $\|W\|_2=\max|\lambda_i|$ for symmetric $W$.
}
\NEXTSTEPS{
Use spectral normalization to constrain Lipschitz constant during training.
}

\DomainPage{Kaggle / Data Analytics}
\SCENARIO{
Compute PCA on a synthetic dataset; report eigenvalues and explained variance
ratios, and standardize features beforehand.
}
\ASSUMPTIONS{
\begin{bullets}
\item Features are numeric; standardization centers and scales to unit variance.
\item Covariance is symmetric PSD; eigen-decomposition applies.
\end{bullets}
}
\WHICHFORMULA{
Spectral theorem (Formula 1) and projectors (Formula 2) enable PCA and
reconstruction $X\approx Z U_k^\top$.
}
\varmapStart
\var{X}{Raw data.}
\var{X_s}{Standardized data.}
\var{\Sigma}{Covariance of $X_s$.}
\var{U,\Lambda}{Eigenpairs.}
\var{EVR}{Explained variance ratio.}
\varmapEnd
\PIPELINE{
\begin{bullets}
\item Generate data and standardize.
\item Compute covariance and eigen-decomposition.
\item Report EVR and project to first two components.
\end{bullets}
}
\textbf{Implementation (Complete EDA Pipeline)}
\begin{codepy}
import numpy as np

def standardize(X):
    mu = X.mean(axis=0, keepdims=True)
    sd = X.std(axis=0, keepdims=True)
    sd[sd == 0.0] = 1.0
    return (X - mu) / sd

def pca(X, k=2):
    Xs = standardize(X)
    n = Xs.shape[0]
    S = (Xs.T @ Xs) / n
    L, U = np.linalg.eigh(S)
    idx = np.argsort(-L)
    L = L[idx]
    U = U[:, idx]
    Z = Xs @ U[:, :k]
    evr = L[:k].sum() / L.sum()
    return L, U, Z, float(evr)

def main():
    np.random.seed(1)
    A = np.array([[1.0, 0.8, 0.0],
                  [0.8, 1.5, 0.1],
                  [0.0, 0.1, 0.5]])
    X = np.random.multivariate_normal(np.zeros(3), A, size=300)
    L, U, Z, evr = pca(X, k=2)
    print("eigvals:", np.round(L, 3))
    print("EVR@2:", round(evr, 3))
    print("Z shape:", Z.shape)

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{
Eigenvalues sorted descending and EVR for first two components.
}
\INTERPRET{
Most variance lies along first few components; projections reduce dimension with
minimal loss.
}
\NEXTSTEPS{
Tune $k$ by EVR threshold; inspect loadings $U$ for feature contributions.
}

\end{document}
% !TeX program = xelatex
\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{microtype,setspace,amsmath,amssymb,mathtools,amsthm,unicode-math}
\setstretch{1.05}
\setmainfont{Latin Modern Roman}
\setmonofont{Latin Modern Mono}
\setmathfont{Latin Modern Math}
\allowdisplaybreaks[4]
\setlength{\jot}{7pt}
\setlength{\emergencystretch}{8em}
\sloppy
\usepackage{xcolor,fancyhdr,enumitem,inconsolata,listings}
\pagestyle{fancy}\fancyhf{}\lhead{\nouppercase{\leftmark}}\rhead{\thepage}
\setlength{\headheight}{26pt}
\setlength{\parindent}{0pt}
\setlength{\parskip}{8pt plus 2pt minus 1pt}
\raggedbottom
\newenvironment{BreakableEquation}{\begin{equation}\begin{aligned}}{\end{aligned}\end{equation}}
\newenvironment{BreakableEquation*}{\begin{equation*}\begin{aligned}}{\end{aligned}\end{equation*}}
\newenvironment{tightalign}{\begingroup\small\allowdisplaybreaks\begin{align}}{\end{align}\endgroup}
\providecommand{\enumlistm}{enumitem}
\newenvironment{minted}[2][]{%
  \lstset{style=code,language=#2,#1}\begin{lstlisting}%
}{\end{lstlisting}}
\newcommand{\inputminted}[3][]{\begin{lstlisting}\end{lstlisting}}
\newlist{bullets}{itemize}{1}
\setlist[bullets]{label=--,leftmargin=1.6em,itemsep=2pt,topsep=4pt}
\newcommand{\varmapStart}{\textbf{VARIABLE MAPPING:}\par\begin{bullets}}
\newcommand{\var}[2]{\item $#1$ — #2}
\newcommand{\varmapEnd}{\end{bullets}}
\newcommand{\glossx}[6]{%
  \textbf{#1}\par
  \begin{bullets}
    \item \textbf{What:} #2
    \item \textbf{Why:} #3
    \item \textbf{How:} #4
    \item \textbf{ELI5:} #5
    \item \textbf{Pitfall/Example:} #6
  \end{bullets}
}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{claim}[theorem]{Claim}
\lstdefinestyle{code}{
  basicstyle=\ttfamily\footnotesize,
  backgroundcolor=\color{black!02},
  frame=single,
  numbers=left, numberstyle=\tiny, numbersep=8pt,
  breaklines=true, breakatwhitespace=true,
  tabsize=4, showstringspaces=false,
  upquote=true, keepspaces=true, columns=fullflexible,
  literate=
    {–}{{-}}1
    {—}{{-}}1
    {…}{{...}}1
    {≤}{{\ensuremath{\le}}}1
    {≥}{{\ensuremath{\ge}}}1
    {≠}{{\ensuremath{\ne}}}1
    {≈}{{\ensuremath{\approx}}}1
    {±}{{\ensuremath{\pm}}}1
    {→}{{\ensuremath{\to}}}1
    {←}{{\ensuremath{\leftarrow}}}1
    {∞}{{\ensuremath{\infty}}}1
    {√}{{\ensuremath{\sqrt{\ }}}}1
    {×}{{\ensuremath{\times}}}1
    {÷}{{\ensuremath{\div}}}1
}
\lstnewenvironment{codepy}[1][]%
  {\lstset{style=code,language=Python,#1}}%
  {}
\newcommand{\inlinecode}[1]{\lstinline[style=code]!#1!}
\newcommand{\LF}[2]{\par\noindent\textbf{#1:}~#2\par}
\newcommand{\WHAT}[1]{\LF{WHAT}{#1}}
\newcommand{\WHY}[1]{\LF{WHY}{#1}}
\newcommand{\HOW}[1]{\LF{HOW}{#1}}
\newcommand{\ELI}[1]{\LF{ELI5}{#1}}
\newcommand{\SCOPE}[1]{\LF{SCOPE}{#1}}
\newcommand{\CONFUSIONS}[1]{\LF{COMMON CONFUSIONS}{#1}}
\newcommand{\APPLICATIONS}[1]{\LF{APPLICATIONS}{#1}}
\newcommand{\FORMULA}[1]{\LF{FORMULA}{#1}}
\newcommand{\CANONICAL}[1]{\LF{CANONICAL FORM}{#1}}
\newcommand{\PRECONDS}[1]{\LF{PRECONDITIONS}{#1}}
\newcommand{\DERIVATION}[1]{\LF{DERIVATION}{#1}}
\newcommand{\EQUIV}[1]{\LF{EQUIVALENT FORMS}{#1}}
\newcommand{\LIMITS}[1]{\LF{LIMIT CASES}{#1}}
\newcommand{\INPUTS}[1]{\LF{INPUTS}{#1}}
\newcommand{\OUTPUTS}[1]{\LF{OUTPUTS}{#1}}
\newcommand{\RESULT}[1]{\LF{RESULT}{#1}}
\newcommand{\INTUITION}[1]{\LF{INTUITION}{#1}}
\newcommand{\PITFALLS}[1]{\LF{PITFALLS}{#1}}
\newcommand{\MODEL}[1]{\LF{CANONICAL MATH MODEL}{#1}}
\newcommand{\ASSUMPTIONS}[1]{\LF{ASSUMPTIONS}{#1}}
\newcommand{\WHICHFORMULA}[1]{\LF{WHICH FORMULA \& WHY}{#1}}
\newcommand{\GOVERN}[1]{\LF{GOVERNING EQUATION(S)}{#1}}
\newcommand{\UNITCHECK}[1]{\LF{UNIT CHECK}{#1}}
\newcommand{\EDGECASES}[1]{\LF{EDGE CASES}{#1}}
\newcommand{\ALTERNATE}[1]{\LF{ALTERNATE APPROACH (sketch)}{#1}}
\newcommand{\PROBLEM}[1]{\LF{PROBLEM}{#1}}
\newcommand{\API}[1]{\LF{API}{#1}}
\newcommand{\COMPLEXITY}[1]{\LF{COMPLEXITY}{#1}}
\newcommand{\FAILMODES}[1]{\LF{FAILURE MODES}{#1}}
\newcommand{\STABILITY}[1]{\LF{NUMERICAL STABILITY}{#1}}
\newcommand{\VALIDATION}[1]{\LF{VALIDATION}{#1}}
\newcommand{\EXPLANATION}[1]{\LF{EXPLANATION}{#1}}
\newcommand{\SCENARIO}[1]{\LF{SCENARIO}{#1}}
\newcommand{\PIPELINE}[1]{\LF{PIPELINE STEPS}{#1}}
\newcommand{\METRICS}[1]{\LF{METRICS}{#1}}
\newcommand{\INTERPRET}[1]{\LF{INTERPRETATION}{#1}}
\newcommand{\NEXTSTEPS}[1]{\LF{LIMITATIONS \& NEXT STEPS}{#1}}
\usepackage{titlesec}
\titleformat{\section}{\Large\bfseries}{\thesection}{0.6em}{}
\titlespacing*{\section}{0pt}{*2}{*1}
\usepackage{etoolbox}
\pretocmd{\section}{\clearpage}{}{}
\newcommand{\FormulaPage}[2]{%
  \clearpage
  \section*{Formula #1 — #2}%
  \addcontentsline{toc}{section}{Formula #1 — #2}%
}
\newcommand{\ProblemPage}[2]{%
  \clearpage
  \subsection*{Problem #1: #2}%
  \addcontentsline{toc}{subsection}{Problem #1: #2}%
}
\newcommand{\CodeDemoPage}[1]{%
  \clearpage
  \subsection*{Coding Demo: #1}%
  \addcontentsline{toc}{subsection}{Coding Demo: #1}%
}
\newcommand{\DomainPage}[1]{%
  \clearpage
  \subsection*{#1 (End-to-End)}%
  \addcontentsline{toc}{subsection}{#1 (End-to-End)}%
}
\begin{document}
\title{Comprehensive Study Sheet — QR Algorithm for Eigenvalues}
\date{\today}
\maketitle
\tableofcontents
\clearpage
\section{Concept Overview}
\WHAT{
Let $A\in\mathbb{R}^{n\times n}$. The QR algorithm constructs a sequence
$A_0=A$, and for $k\ge 0$ performs a QR factorization $A_k=Q_kR_k$ with
$Q_k$ orthogonal and $R_k$ upper triangular, then sets $A_{k+1}=R_kQ_k$.
For shifted QR, given a shift $\mu_k\in\mathbb{R}$, factor
$A_k-\mu_k I=Q_kR_k$ and set $A_{k+1}=R_kQ_k+\mu_k I$.
Under broad conditions, $A_k$ converges to an upper triangular (Schur) form
whose diagonal entries are the eigenvalues of $A$. For symmetric $A$,
$A_k$ converges to a diagonal matrix and the columns of the cumulative
orthogonal factor converge to eigenvectors.}
\WHY{
Eigenvalues underpin stability, oscillations, variance, and graph spectra.
The QR algorithm is the standard, backward-stable method for dense eigenvalue
computation. It reduces general matrices to Hessenberg form for efficiency,
uses shifts for rapid convergence, and reveals spectral structure through
orthogonal similarities, preserving conditioning and norms.}
\HOW{
1. Reduce $A$ to similar Hessenberg $H=U^\top AU$ via Householder.
2. Iterate implicit shifted QR steps on $H$:
factor $H-\mu I=Q R$, form $H^+=R Q+\mu I$. This preserves Hessenberg.
3. Detect decoupling when subdiagonal entries are small; deflate blocks.
4. For symmetric $A$, the iteration equals simultaneous orthogonal iteration,
ensuring convergence to a diagonal matrix.}
\ELI{
Think of $A$ as a machine that twists and stretches space. QR peels off a
pure rotation $Q$ from $A$, leaving a pure stretch $R$. Then it puts the
stretch before the rotation to better align with principal axes. Repeating
this realigns the machine to act mainly along its eigen-directions, revealing
its eigenvalues on the diagonal.}
\SCOPE{
Applies to real or complex square matrices. For real arithmetic, complex
eigenpairs appear as $2\times 2$ blocks in real Schur form. Convergence
without shifts can be slow; shifts are essential for practical speed.
Breakdown does not occur for full-rank $Q,R$; subdiagonal deflation must
be handled numerically using thresholds.}
\CONFUSIONS{
QR factorization versus QR algorithm: the former is a one-shot factorization,
the latter iterates QR on a sequence of similar matrices. Power iteration
versus QR: power finds a dominant eigenpair; QR finds all eigenvalues.}
\APPLICATIONS{
\begin{bullets}
\item Mathematical foundations: Schur decomposition, spectral theorem.
\item Computational modeling: vibration modes, PDE discretizations.
\item Engineering: modal analysis, control poles, filter design.
\item Data science: PCA eigenvalues via covariance matrices.
\end{bullets}
}
\textbf{ANALYTIC STRUCTURE.}
Orthogonal similarities preserve spectral invariants and norms; Hessenberg
structure is invariant under QR steps. For symmetric matrices, the sequence
is monotone in off-diagonal Frobenius norm and converges to diagonal.
\textbf{CANONICAL LINKS.}
Householder reflections yield Hessenberg reduction. Schur decomposition is
the limit of QR steps. Rayleigh quotient and Wilkinson shift accelerate
convergence.
\textbf{PROBLEM-TYPE RECOGNITION HEURISTICS.}
\begin{bullets}
\item Asking for all eigenvalues numerically with stability hints at QR.
\item Presence of Hessenberg or Givens/Householder transformations.
\item Questions about shifts, deflation, or real Schur form.
\end{bullets}
\textbf{SOLUTION STRATEGY BLUEPRINT.}
\begin{bullets}
\item Express algorithm as orthogonal similarities.
\item Reduce to Hessenberg; argue preservation and deflation.
\item Use shifts; relate to Rayleigh quotient or trailing block eigenvalues.
\item Prove convergence via orthogonal iteration or interlacing.
\end{bullets}
\textbf{CONCEPTUAL INVARIANTS.}
Spectrum, determinant, trace, Frobenius norm; Hessenberg bandwidth.
\textbf{EDGE INTUITION.}
If a subdiagonal entry tends to $0$, the matrix nearly decouples and an
eigenvalue has converged. Very close eigenvalues slow unshifted QR but
shifts restore rapid convergence.
\clearpage
\section{Glossary}
\glossx{QR Factorization}{
Factorization $A=QR$ with $Q$ orthogonal and $R$ upper triangular.}{
Enables stable decompositions and the core update in QR iteration.}{
Compute $Q$ via Householder or Givens; form $R=Q^\top A$.}{
Like rotating coordinates so the matrix becomes upper triangular.}{
Pitfall: sign conventions; enforce $r_{ii}\ge 0$ to make $R$ unique.}
\glossx{Upper Hessenberg Matrix}{
Square matrix with zeros below the first subdiagonal.}{
Enables $\mathcal{O}(n^2)$-cost QR steps and is preserved by QR.}{
Reduce dense $A$ to $H=U^\top AU$ via Householder reflections.}{
Almost triangular, so QR is cheap and structured.}{
Pitfall: forgetting that Hessenberg structure is preserved by QR.}
\glossx{Rayleigh Shift}{
Shift $\mu_k=x_k^\top A_k x_k$ or a trailing diagonal $a_{nn}$ used in QR.}{
Accelerates convergence, often cubically for symmetric matrices.}{
Factor $A_k-\mu_k I=Q_kR_k$ and set $A_{k+1}=R_kQ_k+\mu_k I$.}{
Like guessing today where a spinning top points and adjusting to it.}{
Pitfall: for nonsymmetric matrices Rayleigh shift can be unstable.}
\glossx{Wilkinson Shift}{
Shift equal to the eigenvalue of the trailing $2\times 2$ block closest
to $a_{nn}$.}{
Achieves near-quadratic or cubic convergence and avoids wrong locks.}{
Compute eigenvalues of $\begin{bmatrix}a&b\\c&d\end{bmatrix}$ and pick the
one nearest $d$.}{
A smart guess from the last two entries of the matrix.}{
Pitfall: choosing the other root causes slow or stalled convergence.}
\clearpage
\section{Symbol Ledger}
\varmapStart
\var{A\in\mathbb{R}^{n\times n}}{input matrix.}
\var{A_k}{iterate at step $k$.}
\var{Q_k}{orthogonal factor at step $k$.}
\var{R_k}{upper triangular factor at step $k$.}
\var{\mu_k}{shift at step $k$.}
\var{H}{upper Hessenberg form similar to $A$.}
\var{U}{orthogonal reducer to Hessenberg.}
\var{\lambda_i}{eigenvalues of $A$.}
\var{V}{matrix of eigenvectors (when diagonalizable).}
\var{T}{real Schur form (upper quasi-triangular).}
\var{\rho(x)}{Rayleigh quotient $x^\top Ax/(x^\top x)$.}
\var{\epsilon}{deflation tolerance for subdiagonal entries.}
\var{\|\cdot\|_F}{Frobenius norm.}
\varmapEnd
\clearpage
\section{Formula Canon — One Formula Per Page}
\FormulaPage{1}{Unshifted QR Iteration and Spectral Invariance}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
For $A_0=A$ and $A_k=Q_kR_k$ the QR factorization with $Q_k^\top Q_k=I$,
define $A_{k+1}=R_kQ_k$. Then $A_{k+1}=Q_k^\top A_k Q_k$ is similar to
$A_k$; the sequence preserves eigenvalues and converges to Schur form
under mild conditions.
\WHAT{
Defines the fundamental QR update and shows similarity, implying invariant
characteristic polynomial and eigenvalues across iterations.}
\WHY{
Similarity ensures we do not change the eigenvalues while improving
triangularity. It is the backbone of Schur-form convergence.}
\FORMULA{
\[
A_k=Q_kR_k,\quad A_{k+1}=R_kQ_k=Q_k^\top A_k Q_k,\quad
\chi_{A_{k+1}}=\chi_{A_k}.
\]
}
\CANONICAL{
$A\in\mathbb{R}^{n\times n}$ arbitrary. $Q_k$ orthogonal, $R_k$ upper
triangular. The update is an orthogonal similarity, so $\{A_k\}$ is a
sequence of orthogonally similar matrices.}
\PRECONDS{
\begin{bullets}
\item Existence of a QR factorization for each $A_k$ (always holds).
\item Orthogonal $Q_k$ (Householder or Givens) for stability.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
If $B=Q^\top A Q$ with $Q$ orthogonal, then $A$ and $B$ have the same
characteristic polynomial and eigenvalues. 
\end{lemma}
\begin{proof}
$\det(\lambda I-B)=\det(\lambda I-Q^\top A Q)
=\det(Q^\top(\lambda I-A)Q)=\det(\lambda I-A)$ since $\det(Q^\top Q)=1$.
Hence $\chi_B=\chi_A$ and the spectra coincide.\qedhere
\end{proof}
\DERIVATION{
\begin{align*}
\text{Given }A_k&=Q_kR_k,\ \ Q_k^\top Q_k=I,\\
A_{k+1}&=R_kQ_k=(Q_k^\top A_k)Q_k=Q_k^\top A_k Q_k,\\
\chi_{A_{k+1}}&=\chi_{A_k}\quad\text{by the lemma},\\
\text{Triangularization: }&\text{as }k\to\infty,\ A_k\to T\text{ upper
triangular (Schur form),}\\
&\text{so }\operatorname{diag}(T)\ \text{lists the eigenvalues of }A.
\end{align*}
}
\EQUIV{
\begin{bullets}
\item Orthogonal iteration on $A^\top A$ relates to QR on $A$.
\item Unshifted QR on $A$ equals repeated $A_k=Q_k^\top A_k Q_k$.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item Without shifts, convergence can be linear and slow when eigenvalues
have similar magnitudes.
\item For symmetric $A$, $A_k$ converges to diagonal and off-diagonal norm
decreases monotonically.
\end{bullets}
}
\INPUTS{$A\in\mathbb{R}^{n\times n}$; optional tolerance $\epsilon>0$ for deflation.}
\RESULT{
Unshifted QR preserves eigenvalues and tends to upper triangular form.
For symmetric $A$, $A_k\to \operatorname{diag}(\lambda_1,\dots,\lambda_n)$.}
\PITFALLS{
\begin{bullets}
\item Confusing $A_{k+1}=R_kQ_k$ with $A_{k+1}=Q_kR_k$; only the former
preserves $A_k$ via similarity.
\item Losing orthogonality if $Q_k$ is not accumulated stably.
\end{bullets}
}
\ELI{
Peel off the rotation from $A_k$ and then put the stretch before the
rotation. That is the same as changing coordinates by $Q_k$ and never
changes the eigenvalues.}
\FormulaPage{2}{Shifted QR Iteration (Rayleigh/Wilkinson Shift)}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Given a shift $\mu_k$, factor $A_k-\mu_k I=Q_kR_k$, then
$A_{k+1}=R_kQ_k+\mu_k I=Q_k^\top A_k Q_k$. Properly chosen shifts
accelerate convergence dramatically.
\WHAT{
Introduces a shift to the QR step to cluster the spectrum and deflate
faster, mirroring Rayleigh quotient iteration behavior.}
\WHY{
Shifts move the spectrum so that the target eigenvalue behaves like a
dominant one for the QR step, yielding quadratic or cubic convergence.}
\FORMULA{
\[
A_k-\mu_k I=Q_kR_k,\quad A_{k+1}=R_kQ_k+\mu_k I.
\]
}
\CANONICAL{
$\mu_k\in\mathbb{R}$; common choices: Rayleigh shift $\mu_k=x^\top A_k x$
with a current eigenvector estimate $x$, trailing diagonal $\mu_k=a_{nn}$,
or Wilkinson shift from the trailing $2\times 2$ block.}
\PRECONDS{
\begin{bullets}
\item $A_k-\mu_k I$ nonsingular (generic except at eigenvalues).
\item $Q_k$ orthogonal and $R_k$ upper triangular.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
Let $A_k-\mu I=Q R$. Then $R Q+\mu I=Q^\top A_k Q$. 
\end{lemma}
\begin{proof}
From $A_k-\mu I=Q R$ we have $A_k=Q R+\mu I$. Multiply by $Q^\top$ on the
left and $Q$ on the right:
$Q^\top A_k Q=Q^\top(Q R+\mu I)Q=R Q+\mu I$ since $Q^\top Q=I$.\qedhere
\end{proof}
\DERIVATION{
\begin{align*}
&\text{Choose }\mu_k\text{ (e.g., Wilkinson).}\\
&A_k-\mu_k I=Q_kR_k,\quad A_{k+1}=R_kQ_k+\mu_k I.\\
&\text{By the lemma, }A_{k+1}=Q_k^\top A_k Q_k\text{, hence eigenvalues
are preserved.}\\
&\text{Heuristic: if }\mu_k\approx \lambda_i,\text{ the }(n,n)\text{ entry
of }A_{k+1}\text{ converges rapidly to }\lambda_i,\\
&\text{and }a_{n,n-1}\to 0\text{ enabling deflation.}
\end{align*}
}
\EQUIV{
\begin{bullets}
\item For symmetric $A$, Rayleigh quotient shift QR is equivalent to one
step of Rayleigh quotient iteration applied implicitly to all vectors.
\item Wilkinson shift equals choosing the eigenvalue of the trailing
$2\times 2$ closest to $a_{nn}$.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item If $\mu_k$ hits an eigenvalue exactly, the factorization becomes
singular; in practice, tiny perturbations avoid breakdown.
\item Poor shift choice can slow convergence or cause shift-locking.
\end{bullets}
}
\INPUTS{$A_k\in\mathbb{R}^{n\times n}$, choice of $\mu_k\in\mathbb{R}$.}
\RESULT{
Shifted QR preserves spectrum and, with Wilkinson shift on symmetric
matrices, typically yields quadratic or cubic convergence and rapid
deflation.}
\PITFALLS{
\begin{bullets}
\item Using $a_{nn}$ as a shift when the trailing $2\times 2$ block is
ill-conditioned can stall; use Wilkinson shift instead.
\item Accumulating $Q_k$ explicitly without reorthogonalization can lose
orthogonality in finite precision.
\end{bullets}
}
\ELI{
Subtract a good guess of an eigenvalue, triangularize, then add it back.
Like tuning a radio near the station so the signal locks in quickly.}
\FormulaPage{3}{Hessenberg Reduction and Invariance under QR}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Any $A\in\mathbb{R}^{n\times n}$ is orthogonally similar to an upper
Hessenberg matrix $H=U^\top A U$. A QR step on $H$ produces another upper
Hessenberg matrix $\tilde H$; thus Hessenberg structure is invariant.
\WHAT{
Reduces QR cost from $\mathcal{O}(n^3)$ to $\mathcal{O}(n^2)$ per step,
and shows structure preservation enabling efficient implementations.}
\WHY{
Householder transformations annihilate entries below the first subdiagonal,
and QR of Hessenberg requires only local Givens operations.}
\FORMULA{
\[
H=U^\top A U,\quad H\text{ upper Hessenberg},\quad
H-\mu I=Q R,\ \ \tilde H=R Q+\mu I\ \text{ is upper Hessenberg.}
\]
}
\CANONICAL{
$U$ orthogonal product of $n-2$ Householder reflectors. $Q$ orthogonal,
$R$ upper triangular with banded structure when factored from Hessenberg.}
\PRECONDS{
\begin{bullets}
\item $A$ finite dimension; Householder reflectors exist.
\item QR of an upper Hessenberg matrix is performed by Givens rotations.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
If $H$ is upper Hessenberg and $H=Q R$ is its QR factorization, then
$R Q$ is upper Hessenberg. 
\end{lemma}
\begin{proof}
Form $Q$ as a product of Givens rotations that zero successive subdiagonal
entries of each column. Each Givens affects only adjacent rows, so the
bandwidth is preserved: $Q^\top H$ is upper triangular, hence $R$. Then
$R Q$ multiplies $R$ by a product of adjacent-row rotations on the right,
which can only create a bulge at one position that is chased down to the
bottom by subsequent rotations, restoring upper Hessenberg structure. The
final product is upper Hessenberg.\qedhere
\end{proof}
\DERIVATION{
\begin{align*}
&\text{Householder: }U_1,\dots,U_{n-2}\ \text{such that }H=U^\top A U,\\
&\text{with }H_{ij}=0\ \text{for }i>j+1.\\
&H-\mu I=Q R\ \Rightarrow\ \tilde H=R Q+\mu I,\\
&\text{by the lemma }\tilde H\text{ is upper Hessenberg, enabling
}\mathcal{O}(n^2)\text{ steps.}
\end{align*}
}
\EQUIV{
\begin{bullets}
\item Similarity to $H$ can be replaced by reduction to real Schur form
via continued QR steps.
\item Givens versus Householder are equivalent up to orthogonality.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item For symmetric $A$, reduction yields tridiagonal $T$ and even cheaper
QR steps $\mathcal{O}(n)$ per iteration.
\item Finite precision introduces fill-in smaller than machine epsilon that
is re-zeroed by bulge chasing.
\end{bullets}
}
\INPUTS{$A\in\mathbb{R}^{n\times n}$; choose reduction to $H=U^\top A U$.}
\RESULT{
Efficient, structure-preserving QR iteration on $H$ yields upper Hessenberg
iterates, enabling fast eigenvalue computation.}
\PITFALLS{
\begin{bullets}
\item Performing dense QR on $A$ each step is wasteful; reduce once.
\item Ignoring bulge chasing order breaks Hessenberg. Follow adjacent-row
rotations strictly.
\end{bullets}
}
\ELI{
First trim the matrix so it is almost triangular. Then each QR step nudges
a small bump down the matrix until the form is neat again.}
\FormulaPage{4}{Symmetric QR Convergence and Orthogonal Iteration}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
For symmetric $A=V\Lambda V^\top$ with distinct eigenvalues, shifted QR
iteration converges to a diagonal matrix. The accumulated orthogonal factor
converges to $V$ up to signs or permutations.
\WHAT{
Establishes convergence behavior on the core case where eigenvectors are
orthogonal and the Schur form is diagonal.}
\WHY{
Symmetric problems pervade applications; QR is globally convergent here,
with rapid rates under shifts.}
\FORMULA{
\[
A_k=Q_kR_k,\quad A_{k+1}=R_kQ_k,\quad
\widehat Q_k=Q_0Q_1\cdots Q_{k-1},\quad
A_k=\widehat Q_k^\top A \widehat Q_k\to \Lambda.
\]
}
\CANONICAL{
$A$ real symmetric, eigen-decomposition $A=V\Lambda V^\top$ with simple
spectrum. Use Wilkinson or Rayleigh shifts.}
\PRECONDS{
\begin{bullets}
\item $A$ symmetric; distinct eigenvalues ensure unique invariant subspaces.
\item Orthogonal QR steps and proper deflation strategy.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
Orthogonal iteration on $A$ converges to $V$; QR iteration is equivalent
to orthogonal iteration for symmetric $A$. 
\end{lemma}
\begin{proof}
Orthogonal iteration: start with $Q^{(0)}$ orthogonal; iterate
$Z^{(k)}=A Q^{(k)}$, then $Q^{(k+1)}R^{(k)}=Z^{(k)}$ via QR. Writing
$A=V\Lambda V^\top$ and $Q^{(k)}=V Y^{(k)}$ with $Y^{(k)}$ orthogonal,
we get $Z^{(k)}=V\Lambda Y^{(k)}$. QR of $Z^{(k)}$ yields
$Y^{(k+1)}R^{(k)}=\Lambda Y^{(k)}$ whose columns scale by eigenvalues;
the columns of $Y^{(k)}$ converge to the identity up to signs and order,
hence $Q^{(k)}\to V$. QR iteration produces the same $Q^{(k)}$ because
$A_{k+1}=R^{(k)}Q^{(k)}$ implies $A_{k}=(Q^{(0)}\cdots Q^{(k-1)})^\top
A (Q^{(0)}\cdots Q^{(k-1)})$, matching orthogonal iteration. Therefore
the equivalence and convergence hold.\qedhere
\end{proof}
\DERIVATION{
\begin{align*}
&A=V\Lambda V^\top,\ \Lambda=\operatorname{diag}(\lambda_i).\\
&A_k=\widehat Q_k^\top A \widehat Q_k,\ \widehat Q_k=Q_0\cdots Q_{k-1}.\\
&\text{By the lemma, }\widehat Q_k\to V P D,\text{ where }P\text{ is a
permutation and }D\text{ a diagonal sign matrix}.\\
&\Rightarrow A_k\to D^\top P^\top V^\top V \Lambda V^\top V P D=\Lambda.
\end{align*}
}
\EQUIV{
\begin{bullets}
\item Tridiagonal QR with Wilkinson shift equals implicit symmetric QR.
\item Power iteration on $(A-\mu I)^{-1}$ aligns with single-vector RQI.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item Repeated eigenvalues lead to subspace convergence; any orthonormal
basis of the eigenspace may result.
\item Without shifts, rate can be linear; with Rayleigh shift, cubic local
convergence for simple eigenvalues.
\end{bullets}
}
\INPUTS{$A$ symmetric; tolerance $\epsilon$; shift strategy.}
\RESULT{
$A_k$ converges to a diagonal matrix of eigenvalues; the columns of
$\widehat Q_k$ converge to eigenvectors.}
\PITFALLS{
\begin{bullets}
\item Forgetting to reduce to tridiagonal form leads to $\mathcal{O}(n^3)$
per step rather than $\mathcal{O}(n)$.
\item Not deflating when a subdiagonal is tiny wastes iterations.
\end{bullets}
}
\ELI{
On a frictionless surface, the symmetric matrix behaves like independent
springs aligned with orthogonal axes. QR keeps turning the coordinate
frame until it perfectly aligns with those axes.}
\clearpage
\section{10 Exhaustive Problems and Solutions}
\ProblemPage{1}{Unshifted QR on a $2\times 2$ Symmetric Matrix}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Show unshifted QR converges to diagonal for $A=\begin{bmatrix}4&1\\1&3\end{bmatrix}$
and compute two iterations explicitly.
\PROBLEM{
Perform two unshifted QR steps and demonstrate that off-diagonal entries
decrease. Identify eigenvalue approximations after these steps.}
\MODEL{
\[
A_0=\begin{bmatrix}4&1\\1&3\end{bmatrix},\quad A_k=Q_kR_k,\quad
A_{k+1}=R_kQ_k.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $A$ symmetric, distinct eigenvalues.
\item QR computed with orthogonal $Q_k$.
\end{bullets}
}
\varmapStart
\var{A_k}{iterate}
\var{Q_k}{orthogonal factor}
\var{R_k}{upper triangular factor}
\varmapEnd
\WHICHFORMULA{
Formula 1: Unshifted QR similarity $A_{k+1}=Q_k^\top A_k Q_k$.}
\GOVERN{
\[
A_k=Q_kR_k,\quad A_{k+1}=R_kQ_k.
\]
}
\INPUTS{$A_0=\begin{bmatrix}4&1\\1&3\end{bmatrix}$.}
\DERIVATION{
\begin{align*}
&\text{Step 1: }A_0=\begin{bmatrix}4&1\\1&3\end{bmatrix}.\\
&\text{QR of }A_0:
\ \|a_1\|=\sqrt{4^2+1^2}=\sqrt{17}.\
q_1=\tfrac{1}{\sqrt{17}}(4,1)^\top.\\
&r_{12}=q_1^\top a_2=\tfrac{1}{\sqrt{17}}(4,1)\cdot(1,3)=\tfrac{7}{\sqrt{17}}.\\
&q_2=\frac{a_2-r_{12}q_1}{\|a_2-r_{12}q_1\|}
=\frac{(1,3)^\top-\tfrac{7}{17}(4,1)^\top}{\sqrt{\tfrac{33}{17}}}
=\sqrt{\tfrac{17}{33}}\left(\tfrac{-11}{17},\tfrac{44}{17}\right)^\top\\
&\quad=\frac{1}{\sqrt{33}}(-\sqrt{11},4\sqrt{11})^\top
\ \text{(any consistent normalization).}\\
&Q_0=[q_1,q_2],\quad R_0=Q_0^\top A_0.\\
&A_1=R_0Q_0=Q_0^\top A_0 Q_0=
\begin{bmatrix} \alpha&\beta\\ \beta&\delta\end{bmatrix},\
\text{with }|\beta|<1.\\
&\text{Numerically (rounded): }Q_0\approx
\begin{bmatrix}0.9701 & -0.2425\\ 0.2425 & 0.9701\end{bmatrix},\\
&R_0\approx \begin{bmatrix}4.1231&1.6971\\0&2.6673\end{bmatrix},\ 
A_1=R_0Q_0\approx
\begin{bmatrix}4.2355&0.4124\\0.4124&2.7645\end{bmatrix}.\\
&\text{Second step: QR of }A_1\text{ yields }A_2\approx
\begin{bmatrix}4.3260&0.1642\\0.1642&2.6740\end{bmatrix}.
\end{align*}
}
\RESULT{
Off-diagonal decreased from $1$ to $0.4124$ to $0.1642$. Diagonal entries
approach eigenvalues $\lambda\approx 4.4142$ and $2.5858$.}
\UNITCHECK{
Symmetry and trace preserved: $\operatorname{tr}(A_k)=7$, 
$\det(A_k)=11$.}
\EDGECASES{
\begin{bullets}
\item If $a_{12}=0$ initially, the method is already diagonal.
\item Near-equal eigenvalues slow the decrease in off-diagonal terms.
\end{bullets}
}
\ALTERNATE{
Use Givens rotation to zero the $(2,1)$ entry of $A_0$ directly and
proceed with $R Q$.}
\VALIDATION{
\begin{bullets}
\item Compare diagonals with exact eigenvalues of $A_0$:
$3.5\pm 0.5\sqrt{2}$.
\item Confirm constant trace and determinant.
\end{bullets}
}
\INTUITION{
The rotation $Q_0$ aligns the basis closer to eigenvectors, reducing the
coupling between coordinates.}
\CANONICAL{
\begin{bullets}
\item $A_{k+1}=Q_k^\top A_k Q_k$ preserves spectrum.
\item Off-diagonal norm decreases for symmetric $A$.
\end{bullets}
}
\ProblemPage{2}{Hessenberg Reduction for a $4\times 4$ Matrix}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Reduce $A=\begin{bmatrix}1&4&2&2\\3&3&5&1\\2&1&4&3\\1&2&3&4\end{bmatrix}$ to
Hessenberg form $H=U^\top A U$ with one Householder reflector, and show
one QR step preserves Hessenberg structure.
\PROBLEM{
Construct $U_1$ eliminating entries $(3,1)$ and $(4,1)$, form $H$, then
perform one unshifted QR step on $H$ and verify upper Hessenberg form.}
\MODEL{
\[
U_1=I-2vv^\top,\quad v=\frac{x-\alpha e_1}{\|x-\alpha e_1\|},\
x=(a_{21},a_{31},a_{41})^\top.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Standard Householder construction with sign to avoid cancellation.
\item Unshifted QR with Givens on Hessenberg.
\end{bullets}
}
\varmapStart
\var{U_1}{Householder reflector}
\var{v}{Householder vector}
\var{H}{upper Hessenberg form}
\var{Q,R}{QR of $H$}
\varmapEnd
\WHICHFORMULA{
Formula 3: Hessenberg reduction and invariance under QR.}
\GOVERN{
\[
H=U^\top A U,\quad H=Q R,\quad \tilde H=R Q.
\]
}
\INPUTS{$A$ given.}
\DERIVATION{
\begin{align*}
&x=(3,2,1)^\top,\ \alpha=\|x\|=\sqrt{14}.\
v=\frac{x-\alpha e_1}{\|x-\alpha e_1\|}.\\
&U_1=\begin{bmatrix}1&0\\0&I-2vv^\top\end{bmatrix}.\
H=U_1^\top A U_1\ \text{is upper Hessenberg.}\\
&\text{Compute }H\ \text{numerically (rounded)}:\
H\approx\begin{bmatrix}
1&4.89898&1.24035&2.79459\\
\sqrt{14}&3.51754&5.07288&-0.10725\\
0&2.27951&4.10620&2.58734\\
0&0.66262&2.25866&3.37626
\end{bmatrix}.\\
&\text{QR by Givens on subdiagonal }(\sqrt{14},0,0)^\top\ \text{etc.}\\
&\tilde H=R Q\ \text{has zeros below first subdiagonal by bulge chasing}.
\end{align*}
}
\RESULT{
$H$ upper Hessenberg; after one QR step $\tilde H$ remains upper Hessenberg.}
\UNITCHECK{
Orthogonality: $U_1^\top U_1=I$. Similarity preserves trace and determinant.}
\EDGECASES{
\begin{bullets}
\item If $x$ already parallel to $e_1$, $U_1=I$ and column is unchanged.
\item Nearly zero subdiagonals need careful deflation thresholds.
\end{bullets}
}
\ALTERNATE{
Use successive Givens to zero $(3,1)$ and $(4,1)$ then accumulate into a
single orthogonal $U$.}
\VALIDATION{
\begin{bullets}
\item Check $\|H-\text{Hessenberg}(H)\|_F=0$.
\item After QR, verify zeros for entries $i>j+1$ numerically.
\end{bullets}
}
\INTUITION{
A single mirror aligns the first column, leaving a nearly triangular shape
that QR steps preserve.}
\CANONICAL{
\begin{bullets}
\item Upper Hessenberg form exists for all $A$ via orthogonal similarity.
\item QR on Hessenberg stays Hessenberg.
\end{bullets}
}
\ProblemPage{3}{Shifted vs. Unshifted QR Convergence on a $3\times 3$ Symmetric}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Compare two iterations of unshifted and Wilkinson-shifted QR on
$A=\begin{bmatrix}2&1&0\\1&2&1\\0&1&2\end{bmatrix}$.
\PROBLEM{
Compute two steps for each method and compare the magnitude of subdiagonal
entries to show acceleration with shifts.}
\MODEL{
\[
A_0=A,\ A_{k+1}=R_kQ_k\ \text{(unshifted)};\ 
A_{k+1}=R_kQ_k+\mu_k I\ \text{(shifted)}.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Symmetric tridiagonal input.
\item Wilkinson shift from trailing $2\times 2$ block.
\end{bullets}
}
\varmapStart
\var{A_k}{iterate}
\var{\mu_k}{Wilkinson shift}
\var{Q_k,R_k}{QR factors}
\varmapEnd
\WHICHFORMULA{
Formula 2 and 4: Shifted QR on symmetric matrices.}
\GOVERN{
\[
\mu_k=\text{eig of }\begin{bmatrix}a_{n-1,n-1}&a_{n-1,n}\\
a_{n,n-1}&a_{n,n}\end{bmatrix}\text{ closest to }a_{nn}.
\]
}
\INPUTS{$A$ as given; two iterations.}
\DERIVATION{
\begin{align*}
&\text{Unshifted step 1 (rounded): }A_1\approx
\begin{bmatrix}2.6180&0.6180&0\\0.6180&2.0000&0.6180\\0&0.6180&1.3820\end{bmatrix}.\\
&\text{Unshifted step 2: }A_2\approx
\begin{bmatrix}2.7321&0.3249&0\\0.3249&2.0000&0.3249\\0&0.3249&1.2679\end{bmatrix}.\\
&\text{Shifted step 1: trailing block }
\begin{bmatrix}2&1\\1&2\end{bmatrix}\ \Rightarrow\ \mu_0=3\ \text{or }1.\\
&\text{Closest to }a_{33}=2\text{ is }\mu_0=3.\\
&A_0-3I=\begin{bmatrix}-1&1&0\\1&-1&1\\0&1&-1\end{bmatrix}=Q_0 R_0.\\
&A_1=R_0Q_0+3I\approx
\begin{bmatrix}2.9571&0.2899&0\\0.2899&2.0000&0.2899\\0&0.2899&1.0429\end{bmatrix}.\\
&\text{Shifted step 2: }\mu_1\approx 2.9999\ \Rightarrow\
A_2\approx
\begin{bmatrix}2.9899&0.0839&0\\0.0839&2.0000&0.0839\\0&0.0839&1.0101\end{bmatrix}.
\end{align*}
}
\RESULT{
Subdiagonal norms after two steps:
unshifted $\approx \sqrt{2}\cdot 0.3249\approx 0.4594$,
shifted $\approx \sqrt{2}\cdot 0.0839\approx 0.1187$. Shifts accelerate
convergence by about a factor of 4 here.}
\UNITCHECK{
Trace preserved $=6$; determinant preserved $=1$. Symmetry preserved.}
\EDGECASES{
\begin{bullets}
\item If trailing block is nearly defective, rounding can flip the chosen
Wilkinson root; tie-break consistently.
\item If $a_{n,n-1}=0$, deflate the trailing $1\times 1$ block.
\end{bullets}
}
\ALTERNATE{
Use Rayleigh quotient shift with the last column of $\widehat Q_k$ as the
vector $x$ to choose $\mu_k=x^\top A_k x$.}
\VALIDATION{
\begin{bullets}
\item Compare approximated eigenvalues with exact
$2+2\cos(\tfrac{k\pi}{4})$, $k=1,2,3$.
\item Verify reduction in off-diagonal Frobenius norm.
\end{bullets}
}
\INTUITION{
The shift lines up the last eigenvalue so the bulge dies out quickly.}
\CANONICAL{
\begin{bullets}
\item Wilkinson shift from trailing $2\times 2$ accelerates deflation.
\item Symmetric tridiagonal QR is efficient and rapidly convergent.
\end{bullets}
}
\ProblemPage{4}{Alice and Bob: Hidden Equivalence to Inverse Iteration}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Alice performs one shifted QR step with $\mu=\rho(x)$, where $x$ is the
current last column of $\widehat Q_k$. Bob performs one step of inverse
iteration with Rayleigh shift on $A_k$. Show both produce the same next
$A_{k+1}$ and direction $x^+$.
\PROBLEM{
Prove that Rayleigh-shifted QR equals performing inverse iteration for the
last vector followed by re-triangularization, on symmetric $A_k$.}
\MODEL{
\[
A_k-\mu I=Q R,\ A_{k+1}=R Q+\mu I,\
(A_k-\mu I) y = x,\ x^+ = \frac{y}{\|y\|}.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $A_k$ symmetric, $\mu=\rho(x)$ with $x$ normalized.
\item Nondefective simple eigenvalues.
\end{bullets}
}
\varmapStart
\var{x}{current vector}
\var{\mu}{Rayleigh quotient shift}
\var{y}{inverse iteration vector}
\var{Q,R}{shifted QR factors}
\varmapEnd
\WHICHFORMULA{
Formula 2 and 4; equivalence via orthogonal iteration.}
\GOVERN{
\[
A_{k+1}=Q^\top A_k Q,\quad y=R^{-1}Q^\top x.
\]
}
\INPUTS{$A_k$, unit $x$, $\mu=\rho(x)$.}
\DERIVATION{
\begin{align*}
&A_k-\mu I=Q R\ \Rightarrow\ A_k=Q R+\mu I.\\
&\text{Let }x=e_n\ \text{(last column in }Q\text{-basis). Then }Q^\top x=e_n.\\
&y=R^{-1}Q^\top x=R^{-1} e_n\ \Rightarrow\ x^+=\frac{y}{\|y\|}.\\
&\text{In the }Q\text{-basis, }A_{k+1}=R Q+\mu I\text{ and the last column
of }Q\text{ tends to the eigenvector.}\\
&\text{Thus inverse iteration update direction matches the implicit
shifted QR update.}
\end{align*}
}
\RESULT{
Rayleigh-shifted QR implicitly performs inverse iteration on the trailing
eigenvector while preserving the whole orthogonal basis.}
\UNITCHECK{
Orthogonality preserved; normalization keeps $\|x^+\|=1$.}
\EDGECASES{
\begin{bullets}
\item If $\mu$ equals an eigenvalue, inverse iteration is singular; use a
perturbed shift.
\item For clustered eigenvalues, more iterations may be needed.
\end{bullets}
}
\ALTERNATE{
Derive from the commuting diagram for orthogonal iteration in the shifted
basis.}
\VALIDATION{
\begin{bullets}
\item Numerically compare $x^+$ from both methods on a random symmetric
matrix; confirm alignment.
\end{bullets}
}
\INTUITION{
Shift near an eigenvalue makes the inverse map expand the target direction;
QR carries this out for all columns at once.}
\CANONICAL{
\begin{bullets}
\item Shifted QR equals many coupled Rayleigh quotient iterations.
\item The trailing vector is governed by the trailing block shift.
\end{bullets}
}
\ProblemPage{5}{Wilkinson Shift Choice from Trailing $2\times 2$}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Bob claims the Wilkinson shift is the eigenvalue of the trailing $2\times 2$
closest to $a_{nn}$. Prove and compute it for
$B=\begin{bmatrix}b&c\\c&d\end{bmatrix}$ embedded as trailing block.
\PROBLEM{
Derive the explicit formula for the Wilkinson shift and evaluate it for
$b=2.7$, $c=0.2$, $d=3.0$.}
\MODEL{
\[
\mu=d+\frac{c^2}{\tau+\operatorname{sign}(\tau)\sqrt{\tau^2+c^2}},
\ \tau=\frac{b-d}{2}.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Symmetric trailing block.
\item Choose root closer to $d$.
\end{bullets}
}
\varmapStart
\var{b,c,d}{entries of trailing $2\times 2$}
\var{\mu}{Wilkinson shift}
\var{\tau}{half-difference $(b-d)/2$}
\varmapEnd
\WHICHFORMULA{
Formula 2: Wilkinson shift from trailing block eigenvalues.}
\GOVERN{
\[
\lambda_{\pm}=d+\tau\pm \sqrt{\tau^2+c^2},\quad
\mu=\arg\min_{\lambda\in\{\lambda_\pm\}}|\lambda-d|.
\]
}
\INPUTS{$b=2.7$, $c=0.2$, $d=3.0$.}
\DERIVATION{
\begin{align*}
&\tau=(2.7-3.0)/2=-0.15,\ \sqrt{\tau^2+c^2}=\sqrt{0.0225+0.04}=0.25.\\
&\lambda_\pm=3.0-0.15\pm 0.25=\{3.10,2.60\}.\\
&|\lambda_+-d|=0.10,\ |\lambda_--d|=0.40\ \Rightarrow\ \mu=3.10.\\
&\text{Stable formula: }\mu=d+\frac{c^2}{\tau+\operatorname{sign}(\tau)\,
\sqrt{\tau^2+c^2}}\\
&=3.0+\frac{0.04}{-0.15-0.25}=3.0-0.1=2.9\ \text{(this is the other root)}.\\
&\text{Since }\tau<0,\ \operatorname{sign}(\tau)=-1\text{ picks closer root
to }d\text{ as }\lambda_+=3.10.\\
&\text{To avoid cancellation use }\mu=d-\frac{c^2}{|\tau|+\sqrt{\tau^2+c^2}}
=3.0-\frac{0.04}{0.4}=2.9\ \text{for the farther root.}\\
&\text{Thus pick }\lambda_+=3.10\text{ as Wilkinson shift.}
\end{align*}
}
\RESULT{
Wilkinson shift for the block is $\mu=3.10$, the eigenvalue nearer to $d$.}
\UNITCHECK{
Eigenvalues sum $b+d=5.7$; product $bd-c^2=8.06$.}
\EDGECASES{
\begin{bullets}
\item If $c=0$, then $\mu=d$ and deflation occurs immediately.
\item If $b=d$, $\mu=d+\operatorname{sign}(c)\,|c|$ choice is symmetric.
\end{bullets}
}
\ALTERNATE{
Compute both eigenvalues explicitly and compare distances to $d$.}
\VALIDATION{
\begin{bullets}
\item Verify $\lambda_+\approx 3.10$ numerically and that it is closer to
$d$ than $\lambda_-$. 
\end{bullets}
}
\INTUITION{
Pick the root that best matches the last diagonal entry to accelerate
annihilating the last subdiagonal.}
\CANONICAL{
\begin{bullets}
\item Shift from the trailing block stabilizes convergence.
\item Root selection uses proximity to $a_{nn}$.
\end{bullets}
}
\ProblemPage{6}{Expectation Puzzle: Random Givens Choice in $2\times 2$ Symmetric QR}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
A coin flip chooses one of two Givens rotations to factor
$A=\begin{bmatrix}a&b\\b&d\end{bmatrix}$: rotate by $\theta$ or by $-\theta$,
where $\theta$ zeros the $(2,1)$ entry in QR. What is the expected squared
off-diagonal of $A_1=R Q$ after one unshifted QR step?
\PROBLEM{
Compute $\mathbb{E}[A_1(1,2)^2]$ over the two equiprobable signs.}
\MODEL{
\[
A=Q R,\ Q=\begin{bmatrix}c&s\\-s&c\end{bmatrix},\ c=\cos\theta,\
s=\sin\theta,\ \tan\theta=\frac{b}{a}.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $a\neq 0$ and $|b|<|a|$ so $\theta\in(-\tfrac{\pi}{4},\tfrac{\pi}{4})$.
\item Equiprobable $\theta$ and $-\theta$ choices.
\end{bullets}
}
\varmapStart
\var{a,b,d}{entries of $A$}
\var{\theta}{Givens angle}
\var{A_1}{post-step matrix}
\varmapEnd
\WHICHFORMULA{
Formula 1: $A_1=R Q=Q^\top A Q$ for symmetric $A$.}
\GOVERN{
\[
A_1=Q^\top A Q=
\begin{bmatrix}
a c^2+2 b c s+d s^2 & (d-a) c s+b(c^2-s^2)\\
(d-a) c s+b(c^2-s^2) & a s^2-2 b c s+d c^2
\end{bmatrix}.
\]
}
\INPUTS{$a,b,d$ with $a\neq 0$.}
\DERIVATION{
\begin{align*}
&\text{Let }t=\tan\theta=b/a,\ c=\frac{1}{\sqrt{1+t^2}},\
s=\frac{t}{\sqrt{1+t^2}}.\\
&A_{12}=(d-a) c s+b(c^2-s^2)\\
&=\frac{(d-a)t}{1+t^2}+b\frac{1-t^2}{1+t^2}
=\frac{(d-a)t+b(1-t^2)}{1+t^2}.\\
&\text{With }-\theta,\ t\mapsto -t\Rightarrow A_{12}(-t)
=\frac{-(d-a)t+b(1-t^2)}{1+t^2}.\\
&\Rightarrow A_{12}(t)^2\ \text{and }A_{12}(-t)^2\ \text{average to}\\
&\frac{1}{2}\left(\frac{(d-a)t+b(1-t^2)}{1+t^2}\right)^2
+\frac{1}{2}\left(\frac{-(d-a)t+b(1-t^2)}{1+t^2}\right)^2\\
&=\frac{(d-a)^2 t^2+b^2(1-t^2)^2}{(1+t^2)^2}.\\
&\text{Since }t=b/a,\ \mathbb{E}[A_{12}^2]
=\frac{(d-a)^2 (b/a)^2+b^2(1-(b/a)^2)^2}{(1+(b/a)^2)^2}.
\end{align*}
}
\RESULT{
\[
\mathbb{E}[A_1(1,2)^2]=
\frac{(d-a)^2 \frac{b^2}{a^2}+b^2\left(1-\frac{b^2}{a^2}\right)^2}
{\left(1+\frac{b^2}{a^2}\right)^2}.
\]
}
\UNITCHECK{
Dimensionless in terms of $a,b,d$; reduces to $0$ if $b=0$.}
\EDGECASES{
\begin{bullets}
\item If $a=d$, then $\mathbb{E}[A_{12}^2]=
\frac{b^2(1-\frac{b^2}{a^2})^2}{(1+\frac{b^2}{a^2})^2}$.
\item If $|b|\ll |a|$, then $\mathbb{E}[A_{12}^2]\approx b^2\left(1-
3\frac{b^2}{a^2}\right)$.
\end{bullets}
}
\ALTERNATE{
Work directly with $\theta$ using double-angle identities:
$c^2-s^2=\cos 2\theta$, $2 c s=\sin 2\theta$.}
\VALIDATION{
\begin{bullets}
\item Check symmetry in $t\leftrightarrow -t$.
\item Numerical test with sample $(a,b,d)$ values.
\end{bullets}
}
\INTUITION{
Flipping the rotation sign cancels the linear term in $t$ on average,
leaving only even powers.}
\CANONICAL{
\begin{bullets}
\item $A_1=Q^\top A Q$; off-diagonal depends on $\sin 2\theta$ and
$\cos 2\theta$.
\item Averaging over sign yields a quadratic expression.
\end{bullets}
}
\ProblemPage{7}{Proof: Implicit Q Theorem (Symmetric Tridiagonal Case)}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
If two orthogonal matrices $Q$ and $\tilde Q$ satisfy
$Q^\top (T-\mu I) Q=R$ and $\tilde Q^\top (T-\mu I) \tilde Q=\tilde R$
being upper triangular for symmetric tridiagonal $T$, then
$Q$ and $\tilde Q$ differ by a sequence of Givens rotations that act
within the tridiagonal bandwidth; the resulting $R Q+\mu I$ equals
$\tilde R \tilde Q+\mu I$ up to signs. 
\PROBLEM{
Prove uniqueness of the shifted QR step up to signs within the tridiagonal
structure.}
\MODEL{
\[
T-\mu I=Q R=\tilde Q \tilde R,\ \text{all orthogonal-triangular.}
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $T$ symmetric tridiagonal with nonzero subdiagonals.
\item $Q,\tilde Q$ orthogonal, $R,\tilde R$ upper triangular.
\end{bullets}
}
\varmapStart
\var{T}{symmetric tridiagonal}
\var{\mu}{shift}
\var{Q,\tilde Q}{orthogonal factors}
\var{R,\tilde R}{upper triangular factors}
\varmapEnd
\WHICHFORMULA{
Formula 3: Hessenberg invariance; implicit Q theorem.}
\GOVERN{
\[
(T-\mu I)=Q R=\tilde Q \tilde R\Rightarrow
\tilde Q^\top Q = \tilde R \, R^{-1}.
\]
}
\INPUTS{$T,\mu$.}
\DERIVATION{
\begin{align*}
&\tilde Q^\top Q=\tilde R R^{-1}\ \text{is upper triangular and orthogonal.}\\
&\text{An orthogonal upper triangular matrix is diagonal with }\pm 1
\text{ on the diagonal.}\\
&\Rightarrow \tilde Q=Q D,\ \tilde R=D R,\ D=\operatorname{diag}(\pm 1).\\
&\text{In the tridiagonal case, }Q\text{ and }\tilde Q\text{ are products
of adjacent Givens; the sign flips are absorbed locally.}\\
&\text{Hence }R Q+\mu I=\tilde R \tilde Q+\mu I\ \text{up to the same
sign pattern.}
\end{align*}
}
\RESULT{
The shifted QR step on symmetric tridiagonal matrices is uniquely
determined up to harmless signs; implementations via bulge chasing are
equivalent.}
\UNITCHECK{
Orthogonality and triangularity preserved; bandwidth unchanged.}
\EDGECASES{
\begin{bullets}
\item If a subdiagonal is zero, the matrix decouples and uniqueness holds
per block.
\end{bullets}
}
\ALTERNATE{
Appeal to uniqueness of the $Q R$ factorization with positive diagonal in
$R$, which fixes the sign ambiguity.}
\VALIDATION{
\begin{bullets}
\item Numerically compare two implementations; results match up to signs.
\end{bullets}
}
\INTUITION{
Only one way to chase the bulge down a narrow corridor; any path is the
same up to flipping mirrors.}
\CANONICAL{
\begin{bullets}
\item Implicit QR is well-defined on structured matrices.
\item Sign choices do not change $R Q+\mu I$ beyond trivial symmetries.
\end{bullets}
}
\ProblemPage{8}{Proof: Convergence to Real Schur Form for Normal Matrices}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
For a normal matrix $A$ ($A^\top A=A A^\top$), the unshifted QR algorithm
converges to a real Schur form; with shifts, convergence is accelerated.
\PROBLEM{
Prove that the sequence $A_k$ approaches an upper quasi-triangular matrix
whose diagonal blocks are $1\times 1$ or $2\times 2$ representing real
and complex conjugate pairs.}
\MODEL{
\[
A=V \Lambda V^\top,\ V\text{ orthogonal},\ \Lambda\text{ block diagonal};
A_{k+1}=Q_k^\top A_k Q_k.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $A$ normal; orthogonal similarity diagonalizes $A$ over $\mathbb{C}$.
\item Real arithmetic yields real Schur form.
\end{bullets}
}
\varmapStart
\var{A}{normal matrix}
\var{V}{unitary/orthogonal eigenbasis}
\var{\Lambda}{diagonal over $\mathbb{C}$}
\varmapEnd
\WHICHFORMULA{
Formula 1 and 4: orthogonal similarities and convergence.}
\GOVERN{
\[
A_k=\widehat Q_k^\top A \widehat Q_k,\ 
\widehat Q_k=Q_0\cdots Q_{k-1}.
\]
}
\INPUTS{$A$ normal.}
\DERIVATION{
\begin{align*}
&A=V \Lambda V^\top\ \text{with }V^\top V=I.\\
&A_k=\widehat Q_k^\top V \Lambda V^\top \widehat Q_k
=(V^\top \widehat Q_k)^\top \Lambda (V^\top \widehat Q_k).\\
&\text{Let }W_k=V^\top \widehat Q_k\ \text{orthogonal. Then }
A_k=W_k^\top \Lambda W_k.\\
&\text{By orthogonal iteration arguments, }W_k\text{ tends to block
triangularizers of }\Lambda,\\
&\text{yielding an upper quasi-triangular limit }T\text{ (real Schur).}
\end{align*}
}
\RESULT{
$A_k\to T$, an upper quasi-triangular matrix; its $1\times 1$ and
$2\times 2$ diagonal blocks encode the eigenvalues.}
\UNITCHECK{
Similarity invariants preserved: trace, determinant, spectrum.}
\EDGECASES{
\begin{bullets}
\item If $A$ has a defective eigenvalue (non-normal), real Schur still
exists but convergence analysis is subtler.
\end{bullets}
}
\ALTERNATE{
Apply complex Schur first, then map to real Schur by $2\times 2$ blocks.}
\VALIDATION{
\begin{bullets}
\item Numerically observe subdiagonals tending to zero except inside
$2\times 2$ blocks associated with complex pairs.
\end{bullets}
}
\INTUITION{
Normal matrices are orthogonally diagonalizable; QR keeps rotating toward
that diagonal form while respecting real arithmetic.}
\CANONICAL{
\begin{bullets}
\item Orthogonal similarity preserves normality.
\item Real Schur form is the fixed-point structure of QR.
\end{bullets}
}
\ProblemPage{9}{Companion Matrix and Polynomial Roots via QR}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Use QR on the companion matrix $C$ of a monic polynomial
$p(z)=z^3-6 z^2+11 z-6$ to compute its roots.
\PROBLEM{
Construct $C$, reduce to Hessenberg (already companion), run shifted QR
until convergence, and read eigenvalues as polynomial roots.}
\MODEL{
\[
C=\begin{bmatrix}
0&0&6\\1&0&-11\\0&1&6
\end{bmatrix}.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Real arithmetic; real roots expected for this polynomial.
\item Wilkinson shifts applied for speed.
\end{bullets}
}
\varmapStart
\var{p}{monic cubic}
\var{C}{companion matrix}
\var{\lambda_i}{roots/eigenvalues}
\varmapEnd
\WHICHFORMULA{
Formula 2 and 3: shifted QR on Hessenberg to get eigenvalues.}
\GOVERN{
\[
\text{eig}(C)=\{\text{roots of }p\},\quad A_{k+1}=R_k Q_k+\mu_k I.
\]
}
\INPUTS{$p(z)=z^3-6 z^2+11 z-6$.}
\DERIVATION{
\begin{align*}
&\text{Companion }C\text{ as above; Hessenberg already.}\\
&\text{Running shifted QR converges to upper triangular with diagonal }
\{1,2,3\}.\\
&\text{Indeed }p(z)=(z-1)(z-2)(z-3).
\end{align*}
}
\RESULT{
Roots are $\{1,2,3\}$; QR on $C$ reveals them as eigenvalues.}
\UNITCHECK{
Sum of roots $=6$ equals trace of $C$ $=6$; product $=6$ equals $(-1)^3$
times constant term $=6$.}
\EDGECASES{
\begin{bullets}
\item Multiple roots lead to slower convergence; shift strategy crucial.
\item Non-monic polynomials require scaling to monic form.
\end{bullets}
}
\ALTERNATE{
Use Jenkins–Traub; still relies on structured QR-like steps.}
\VALIDATION{
\begin{bullets}
\item Evaluate $p(1)=p(2)=p(3)=0$ to confirm solutions.
\end{bullets}
}
\INTUITION{
Polynomial root finding reduces to eigenvalue problems via a structured
matrix whose action mimics multiplication by $z$.}
\CANONICAL{
\begin{bullets}
\item Companion matrices are upper Hessenberg; QR applies efficiently.
\item Eigenvalues equal polynomial roots for monic polynomials.
\end{bullets}
}
\ProblemPage{10}{SVD via QR on $A^\top A$ (Combo with Least Squares)}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Compute singular values of $A=\begin{bmatrix}3&1\\0&2\\0&0\end{bmatrix}$
via QR on $B=A^\top A$.
\PROBLEM{
Form $B$, run two QR steps (conceptually) to reach near-diagonal, and read
singular values as square roots of eigenvalues of $B$.}
\MODEL{
\[
B=A^\top A=\begin{bmatrix}9&3\\3&5\end{bmatrix}.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $B$ symmetric positive definite.
\item Shifted QR with Wilkinson shift.
\end{bullets}
}
\varmapStart
\var{A}{rectangular matrix}
\var{B}{normal equations matrix}
\var{\sigma_i}{singular values}
\varmapEnd
\WHICHFORMULA{
Formula 4: symmetric QR convergence on $B$.}
\GOVERN{
\[
\sigma_i=\sqrt{\lambda_i(B)},\quad A_{k+1}=R_k Q_k.
\]
}
\INPUTS{$A$ as given.}
\DERIVATION{
\begin{align*}
&B=\begin{bmatrix}9&3\\3&5\end{bmatrix}.\
\text{Eigenvalues } \lambda=\frac{14\pm \sqrt{196-144}}{2}=\frac{14\pm
\sqrt{52}}{2}.\\
&\lambda_{1,2}=7\pm \sqrt{13}\approx 10.6056,\ 3.3944.\\
&\sigma_{1,2}=\sqrt{\lambda_{1,2}}\approx 3.2566,\ 1.8424.\\
&\text{QR would converge to } \operatorname{diag}(\lambda_1,\lambda_2).
\end{align*}
}
\RESULT{
Singular values are approximately $3.2566$ and $1.8424$.}
\UNITCHECK{
$\operatorname{tr}(B)=14$ equals sum of eigenvalues; $\det(B)=36$ equals
product of eigenvalues; $\sigma_1\sigma_2=\sqrt{36}=6$.}
\EDGECASES{
\begin{bullets}
\item For ill-conditioned $A$, use symmetric tridiagonal QR after
Cholesky or Golub–Kahan bidiagonalization instead.
\end{bullets}
}
\ALTERNATE{
Compute SVD via bidiagonalization and symmetric QR on bidiagonal.}
\VALIDATION{
\begin{bullets}
\item Directly compute $\|A\|_2$ by power iteration; matches $\sigma_1$.
\end{bullets}
}
\INTUITION{
Eigenvalues of $A^\top A$ encode squared stretching factors of $A$.}
\CANONICAL{
\begin{bullets}
\item Symmetric QR on $A^\top A$ yields singular values.
\item Prefer bidiagonal QR in practice for stability.
\end{bullets}
}
\clearpage
\section{Coding Demonstrations}
\CodeDemoPage{Unshifted and Shifted QR for Symmetric Matrices}
\PROBLEM{
Implement unshifted and Wilkinson-shifted QR for symmetric tridiagonal
matrices, verify convergence to diagonal, and compare rates.}
\API{
\begin{bullets}
\item \inlinecode{def read_input(s) -> tuple} — parse size and seed.
\item \inlinecode{def solve_case(n, seed) -> dict} — run both algorithms.
\item \inlinecode{def validate() -> None} — correctness checks.
\item \inlinecode{def main() -> None} — run and print summary.
\end{bullets}
}
\INPUTS{
$n$ size (int), $seed$ random seed (int). Generates a symmetric
tridiagonal Toeplitz matrix with $2$ on diagonal and $1$ on off-diagonals.}
\OUTPUTS{
Dictionary with eigenvalue approximations and subdiagonal norms per method.}
\FORMULA{
\[
A_{k+1}=R_kQ_k\quad\text{and}\quad
A_{k+1}=R_kQ_k+\mu_k I,\ \mu_k=\text{Wilkinson shift}.
\]
}
\textbf{SOLUTION A — From Scratch (Mathematically Explicit Implementation)}
\begin{codepy}
import numpy as np

def tridiag_toeplitz(n, a=2.0, b=1.0):
    A = np.zeros((n, n))
    for i in range(n):
        A[i, i] = a
        if i+1 < n:
            A[i+1, i] = b
            A[i, i+1] = b
    return A

def givens(a, b):
    if b == 0:
        return 1.0, 0.0
    if abs(b) > abs(a):
        t = -a/b
        s = 1.0/np.sqrt(1+t*t)
        c = s*t
    else:
        t = -b/a
        c = 1.0/np.sqrt(1+t*t)
        s = c*t
    return c, s

def qr_tridiag(H):
    n = H.shape[0]
    Q = np.eye(n)
    R = H.copy()
    for i in range(n-1):
        c, s = givens(R[i, i], R[i+1, i])
        G = np.eye(n)
        G[i, i] = c; G[i, i+1] = s
        G[i+1, i] = -s; G[i+1, i+1] = c
        R = G @ R
        Q = Q @ G.T
    return Q, R

def wilkinson_shift(H):
    n = H.shape[0]
    b = H[n-2, n-2]; c = H[n-1, n-2]; d = H[n-1, n-1]
    tau = (b - d) / 2.0
    s = np.sqrt(tau*tau + c*c)
    lam1 = d + tau + s
    lam2 = d + tau - s
    if abs(lam1 - d) < abs(lam2 - d):
        return lam1
    else:
        return lam2

def run_qr(H, iters=20, shifted=False):
    A = H.copy()
    for _ in range(iters):
        mu = wilkinson_shift(A) if shifted else 0.0
        Q, R = qr_tridiag(A - mu*np.eye(A.shape[0]))
        A = R @ Q + mu*np.eye(A.shape[0])
    s = np.sqrt(np.sum(np.square(np.diag(A, k=-1))))
    return A, s

def solve_case(n=6, seed=0):
    np.random.seed(seed)
    H = tridiag_toeplitz(n, 2.0, 1.0)
    Au, su = run_qr(H, iters=12, shifted=False)
    As, ss = run_qr(H, iters=6, shifted=True)
    return {"unshifted": (np.diag(Au), su),
            "shifted": (np.diag(As), ss)}

def validate():
    out = solve_case(6, 0)
    du, su = out["unshifted"]
    ds, ss = out["shifted"]
    assert su > ss
    assert np.allclose(du.sum(), 12.0, atol=1e-6)
    assert np.allclose(ds.sum(), 12.0, atol=1e-6)

def main():
    validate()
    out = solve_case(6, 0)
    print("unshifted diag:", np.round(out["unshifted"][0], 6))
    print("unshifted subdiag norm:", np.round(out["unshifted"][1], 6))
    print("shifted diag:", np.round(out["shifted"][0], 6))
    print("shifted subdiag norm:", np.round(out["shifted"][1], 6))

if __name__ == "__main__":
    main()
\end{codepy}
\textbf{SOLUTION B — Library-Based (Validated Computational Shortcut)}
\begin{codepy}
import numpy as np

def solve_case(n=6, seed=0):
    np.random.seed(seed)
    a = 2.0*np.ones(n)
    b = 1.0*np.ones(n-1)
    H = np.diag(a) + np.diag(b, 1) + np.diag(b, -1)
    def qr_step(A, mu):
        Q, R = np.linalg.qr(A - mu*np.eye(n))
        return R @ Q + mu*np.eye(n)
    Au = H.copy()
    for _ in range(12):
        Au = qr_step(Au, 0.0)
    As = H.copy()
    for _ in range(6):
        B = As[-2:, -2:]
        tr = B[0, 0] + B[1, 1]
        det = B[0, 0]*B[1, 1] - B[0, 1]*B[1, 0]
        s = np.sqrt(max(tr*tr/4 - det, 0.0))
        lam1 = tr/2 + s; lam2 = tr/2 - s
        mu = lam1 if abs(lam1 - As[-1, -1]) < abs(lam2 - As[-1, -1]) else lam2
        As = qr_step(As, mu)
    su = np.sqrt(np.sum(np.square(np.diag(Au, k=-1))))
    ss = np.sqrt(np.sum(np.square(np.diag(As, k=-1))))
    return {"unshifted": (np.diag(Au), su),
            "shifted": (np.diag(As), ss)}

def validate():
    out = solve_case(6, 0)
    du, su = out["unshifted"]
    ds, ss = out["shifted"]
    assert su > ss
    assert np.allclose(du.sum(), 12.0, atol=1e-6)
    assert np.allclose(ds.sum(), 12.0, atol=1e-6)

def main():
    validate()
    out = solve_case(6, 0)
    print("OK; shifted subdiag <", out["unshifted"][1])

if __name__ == "__main__":
    main()
\end{codepy}
\COMPLEXITY{
Both variants: per iteration $\mathcal{O}(n)$ for tridiagonal QR; $\mathcal{O}(n^2)$
for general Hessenberg. Space $\mathcal{O}(n)$.}
\FAILMODES{
\begin{bullets}
\item Near-deflation may produce tiny negative under square roots; clip
to zero.
\item If subdiagonal is already zero, deflate to avoid division by zero in
Givens computation.
\end{bullets}
}
\STABILITY{
\begin{bullets}
\item Orthogonal transformations are backward stable.
\item Wilkinson shift avoids catastrophic cancellation in root choice.
\end{bullets}
}
\VALIDATION{
\begin{bullets}
\item Check trace invariance and decreasing subdiagonal norm.
\item Compare with numpy eigenvalues on final iterate.
\end{bullets}
}
\RESULT{
Shifted QR achieves smaller subdiagonal norm with fewer steps, matching the
theory of accelerated convergence.}
\EXPLANATION{
The code implements $A_{k+1}=R_k Q_k$ and its shifted variant, exactly
realizing the formulas and confirming eigenvalue invariance via trace and
rapid deflation via subdiagonal norm decay.}
\CodeDemoPage{QR on Companion Matrix for Polynomial Roots}
\PROBLEM{
Implement QR on the companion matrix of a cubic polynomial and verify that
the diagonal converges to its roots.}
\API{
\begin{bullets}
\item \inlinecode{def read_input(s) -> list} — parse coefficients.
\item \inlinecode{def solve_case(coefs) -> tuple} — run shifted QR on $C$.
\item \inlinecode{def validate() -> None} — check against known roots.
\item \inlinecode{def main() -> None} — orchestrate run.
\end{bullets}
}
\INPUTS{
Monic coefficients \inlinecode{[1, -6, 11, -6]} of $z^3-6 z^2+11 z-6$.}
\OUTPUTS{
Approximate eigenvalues of companion matrix; subdiagonal norm.}
\FORMULA{
\[
C=\begin{bmatrix}0&0&-c_0\\1&0&-c_1\\0&1&-c_2\end{bmatrix},\quad
\text{eig}(C)=\{\text{roots}\}.
\]
}
\textbf{SOLUTION A — From Scratch (Mathematically Explicit Implementation)}
\begin{codepy}
import numpy as np

def companion(coefs):
    assert abs(coefs[0]-1.0) < 1e-12
    n = len(coefs) - 1
    C = np.zeros((n, n))
    C[1:, :-1] = np.eye(n-1)
    C[:, -1] = -np.array(coefs[1:][::-1], dtype=float)
    return C

def qr_hessenberg(H):
    Q, R = np.linalg.qr(H)
    return Q, R

def wilkinson_shift(H):
    n = H.shape[0]
    if n == 1:
        return H[0, 0]
    B = H[-2:, -2:]
    tr = B[0, 0] + B[1, 1]
    det = B[0, 0]*B[1, 1] - B[0, 1]*B[1, 0]
    s = np.sqrt(max(tr*tr/4 - det, 0.0))
    lam1 = tr/2 + s; lam2 = tr/2 - s
    return lam1 if abs(lam1 - H[-1, -1]) < abs(lam2 - H[-1, -1]) else lam2

def run_qr(H, iters=25):
    A = H.copy()
    for _ in range(iters):
        mu = wilkinson_shift(A)
        Q, R = qr_hessenberg(A - mu*np.eye(A.shape[0]))
        A = R @ Q + mu*np.eye(A.shape[0])
    return A

def solve_case(coefs):
    C = companion(coefs)
    A = run_qr(C, iters=25)
    eigs = np.diag(A).copy()
    s = np.sqrt(np.sum(np.square(np.diag(A, k=-1))))
    return eigs, s

def validate():
    eigs, s = solve_case([1.0, -6.0, 11.0, -6.0])
    eigs = np.sort(np.round(eigs, 6))
    assert np.allclose(eigs, np.array([1.0, 2.0, 3.0]), atol=1e-3)
    assert s < 1e-3

def main():
    validate()
    eigs, s = solve_case([1.0, -6.0, 11.0, -6.0])
    print("roots:", np.round(np.sort(eigs), 6), "subdiag:", round(s, 6))

if __name__ == "__main__":
    main()
\end{codepy}
\textbf{SOLUTION B — Library-Based (Validated Computational Shortcut)}
\begin{codepy}
import numpy as np

def companion_np(coefs):
    n = len(coefs) - 1
    C = np.zeros((n, n))
    C[1:, :-1] = np.eye(n-1)
    C[:, -1] = -np.array(coefs[1:][::-1], dtype=float)
    return C

def solve_case(coefs):
    C = companion_np(coefs)
    w = np.linalg.eigvals(C)
    return np.sort(w)

def validate():
    w = solve_case([1.0, -6.0, 11.0, -6.0])
    w = np.sort(np.round(w.real, 6))
    assert np.allclose(w, np.array([1.0, 2.0, 3.0]), atol=1e-6)

def main():
    validate()
    print("eigvals ok")

if __name__ == "__main__":
    main()
\end{codepy}
\COMPLEXITY{
Per QR iteration on $n\times n$ companion (Hessenberg): $\mathcal{O}(n^2)$.
Space $\mathcal{O}(n^2)$ for dense arrays.}
\FAILMODES{
\begin{bullets}
\item Multiple roots lead to slow convergence; increase iterations.
\item Loss of orthogonality if QR factors are not computed stably.
\end{bullets}
}
\STABILITY{
\begin{bullets}
\item Orthogonal steps are backward stable; companion construction may be
ill-conditioned for high-degree polynomials.
\end{bullets}
}
\VALIDATION{
\begin{bullets}
\item Compare with numpy eigenvalues; check small subdiagonal norm.
\end{bullets}
}
\RESULT{
QR recovers roots $\{1,2,3\}$; subdiagonals vanish numerically.}
\EXPLANATION{
The companion matrix encodes the polynomial shift-operator. QR exposes its
eigenvalues, which equal the polynomial roots.}
\EXTENSION{
Apply balancing or Fiedler companions to reduce conditioning.}
\clearpage
\section{Applied Domains — Detailed End-to-End Scenarios}
\DomainPage{Machine Learning}
\SCENARIO{
Compute PCA eigenvalues of a covariance matrix using QR on its symmetric
tridiagonal reduction, verifying variance explained.}
\ASSUMPTIONS{
\begin{bullets}
\item Data centered; covariance symmetric positive semidefinite.
\item QR with Wilkinson shift on tridiagonal form.
\end{bullets}
}
\WHICHFORMULA{
Formula 4: symmetric QR convergence to diagonal on covariance matrices.}
\varmapStart
\var{X}{data matrix $(n,d)$, centered}
\var{S}{covariance $S=\tfrac{1}{n-1}X^\top X$}
\var{\lambda_i}{eigenvalues (variances)}
\varmapEnd
\PIPELINE{
\begin{bullets}
\item Generate centered synthetic data with known covariance.
\item Form $S$, reduce to tridiagonal by Lanczos (here small $d$, direct).
\item Run shifted QR; compute explained variance ratio.
\end{bullets}
}
\textbf{Implementation (From Scratch)}
\begin{codepy}
import numpy as np

def generate(n=200, d=4, seed=0):
    np.random.seed(seed)
    A = np.array([[2.0, 0.5, 0.0, 0.0],
                  [0.5, 1.5, 0.3, 0.0],
                  [0.0, 0.3, 1.0, 0.2],
                  [0.0, 0.0, 0.2, 0.5]])
    L = np.linalg.cholesky(A)
    X = np.random.randn(n, d) @ L.T
    X -= X.mean(axis=0, keepdims=True)
    return X

def qr_symmetric(S, iters=50):
    A = S.copy()
    for _ in range(iters):
        B = A[-2:, -2:]
        tr = B[0, 0] + B[1, 1]
        det = B[0, 0]*B[1, 1] - B[0, 1]*B[1, 0]
        s = np.sqrt(max(tr*tr/4 - det, 0.0))
        mu = tr/2 + s if abs(tr/2 + s - A[-1, -1]) < \
                       abs(tr/2 - s - A[-1, -1]) else tr/2 - s
        Q, R = np.linalg.qr(A - mu*np.eye(A.shape[0]))
        A = R @ Q + mu*np.eye(A.shape[0])
    return np.diag(A)

def main():
    X = generate()
    S = (X.T @ X) / (X.shape[0] - 1)
    vals = qr_symmetric(S, iters=60)
    vals = np.sort(vals)[::-1]
    ratio = vals / vals.sum()
    print("eigs:", np.round(vals, 4))
    print("ratio:", np.round(ratio, 4))

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{Eigenvalues of $S$ and explained variance ratios summing to $1$.}
\INTERPRET{
Largest eigenvalues capture principal variance directions.}
\NEXTSTEPS{
Compute eigenvectors via accumulated $Q$ and project data onto principal
components.}
\DomainPage{Quantitative Finance}
\SCENARIO{
Estimate eigenvalues of a $3\times 3$ asset-return covariance matrix using
QR and interpret principal risk factors.}
\ASSUMPTIONS{
\begin{bullets}
\item Returns centered; covariance symmetric positive semidefinite.
\item Use shifted QR directly on the small dense matrix.
\end{bullets}
}
\WHICHFORMULA{
Formula 4: symmetric QR convergence; eigenvalues are factor variances.}
\varmapStart
\var{R}{matrix of returns $(n,3)$}
\var{S}{covariance}
\var{\lambda_i}{principal variances}
\varmapEnd
\PIPELINE{
\begin{bullets}
\item Simulate correlated Gaussian returns.
\item Compute covariance and run shifted QR.
\item Report eigenvalues and contributions.
\end{bullets}
}
\textbf{Implementation (Full Pipeline)}
\begin{codepy}
import numpy as np

def simulate(n=500, seed=0):
    np.random.seed(seed)
    A = np.array([[0.1, 0.08, 0.05],
                  [0.08, 0.2, 0.06],
                  [0.05, 0.06, 0.15]])
    L = np.linalg.cholesky(A)
    R = np.random.randn(n, 3) @ L.T
    R -= R.mean(axis=0, keepdims=True)
    return R

def qr_sym(A, iters=80):
    M = A.copy()
    for _ in range(iters):
        B = M[-2:, -2:]
        tr = B[0, 0] + B[1, 1]
        det = B[0, 0]*B[1, 1] - B[0, 1]*B[1, 0]
        s = np.sqrt(max(tr*tr/4 - det, 0.0))
        lam1 = tr/2 + s; lam2 = tr/2 - s
        mu = lam1 if abs(lam1 - M[-1, -1]) < abs(lam2 - M[-1, -1]) else lam2
        Q, R = np.linalg.qr(M - mu*np.eye(M.shape[0]))
        M = R @ Q + mu*np.eye(M.shape[0])
    return np.diag(M)

def main():
    R = simulate()
    S = (R.T @ R) / (R.shape[0] - 1)
    vals = np.sort(qr_sym(S))[::-1]
    print("cov eigenvalues:", np.round(vals, 6))
    print("risk share:", np.round(vals/vals.sum(), 4))

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{Eigenvalues and their normalized shares.}
\INTERPRET{
Dominant eigenvalue indicates primary market mode of co-movement.}
\NEXTSTEPS{
Project returns on eigenvectors for factor exposures and risk parity.}
\DomainPage{Deep Learning}
\SCENARIO{
Compute spectral norm of a dense layer weight matrix via QR on $W^\top W$
and compare to power iteration.}
\ASSUMPTIONS{
\begin{bullets}
\item Weight matrix fixed; spectral norm equals largest singular value.
\item Use symmetric QR on $W^\top W$ to get $\sigma_{\max}$. 
\end{bullets}
}
\WHICHFORMULA{
Formula 4: symmetric QR convergence on $W^\top W$.}
\varmapStart
\var{W}{weight matrix $(m,d)$}
\var{\sigma_{\max}}{largest singular value}
\varmapEnd
\PIPELINE{
\begin{bullets}
\item Generate deterministic $W$.
\item Form $B=W^\top W$ and run QR.
\item Compare $\sqrt{\lambda_{\max}(B)}$ with power iteration estimate.
\end{bullets}
}
\textbf{Implementation (End-to-End)}
\begin{codepy}
import numpy as np

def spectral_qr(W, iters=60):
    B = W.T @ W
    M = B.copy()
    for _ in range(iters):
        Q, R = np.linalg.qr(M)
        M = R @ Q
    return np.sqrt(np.max(np.diag(M)))

def power_snorm(W, iters=80):
    x = np.ones(W.shape[1])
    for _ in range(iters):
        y = W @ x; z = W.T @ y
        nrm = np.linalg.norm(z)
        if nrm == 0:
            return 0.0
        x = z / nrm
    return np.linalg.norm(W @ x)

def main():
    np.random.seed(0)
    W = np.random.randn(64, 32) / 4.0
    s_qr = spectral_qr(W, iters=80)
    s_pw = power_snorm(W, iters=100)
    print("spectral norms:", round(s_qr, 6), round(s_pw, 6))
    assert abs(s_qr - s_pw) < 1e-3

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{Largest singular value from QR and power iteration; difference.}
\INTERPRET{
Spectral norm controls Lipschitz constant and gradient flow stability.}
\NEXTSTEPS{
Use Lanczos or randomized methods for larger matrices; add vector backprop.}
\DomainPage{Kaggle / Data Analytics}
\SCENARIO{
Use QR to compute PCA eigenvalues for a small feature set and report
explained variance ratios for EDA.}
\ASSUMPTIONS{
\begin{bullets}
\item Features standardized; covariance computed correctly.
\item QR with shifts yields eigenvalues reliably for small $d$.
\end{bullets}
}
\WHICHFORMULA{
Formula 4: eigenvalues of covariance via symmetric QR.}
\varmapStart
\var{X}{design matrix}
\var{S}{covariance}
\var{\lambda_i}{eigenvalues}
\varmapEnd
\PIPELINE{
\begin{bullets}
\item Create synthetic correlated features.
\item Standardize, compute $S$.
\item Run QR and report ratios.
\end{bullets}
}
\textbf{Implementation (Complete EDA Pipeline)}
\begin{codepy}
import numpy as np

def create_df(n=300, seed=0):
    np.random.seed(seed)
    A = np.array([[1.0, 0.8, 0.2],
                  [0.8, 1.0, 0.3],
                  [0.2, 0.3, 1.0]])
    L = np.linalg.cholesky(A)
    X = np.random.randn(n, 3) @ L.T
    X -= X.mean(axis=0, keepdims=True)
    X /= X.std(axis=0, keepdims=True)
    return X

def qr_eigs(S, iters=50):
    A = S.copy()
    for _ in range(iters):
        Q, R = np.linalg.qr(A)
        A = R @ Q
    return np.sort(np.diag(A))[::-1]

def main():
    X = create_df()
    S = (X.T @ X) / (X.shape[0] - 1)
    vals = qr_eigs(S, iters=80)
    ratio = vals / vals.sum()
    print("eigvals:", np.round(vals, 6))
    print("explained:", np.round(ratio, 4))

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{Eigenvalues and their normalized contributions.}
\INTERPRET{
Reveal dominant correlations and dimensionality for modeling.}
\NEXTSTEPS{
Compute eigenvectors and produce PCA scatter plots for components.}
\end{document}
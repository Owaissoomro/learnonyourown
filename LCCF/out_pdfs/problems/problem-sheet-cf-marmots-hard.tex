% !TeX program = xelatex
\documentclass{article}
\usepackage[margin=1in]{geometry}
\usepackage{microtype}
\usepackage{setspace}\setstretch{1.05}
\usepackage{fontspec}
\setmainfont{Latin Modern Roman}
\setmonofont{Latin Modern Mono}[BoldFont={Latin Modern Mono},ItalicFont={Latin Modern Mono}]
\usepackage{amsmath,mathtools,amsthm}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\usepackage{unicode-math}\setmathfont{Latin Modern Math}\allowdisplaybreaks[4]
% --- overflow and alignment slack ---
\setlength{\jot}{7pt}
\sloppy\emergencystretch=8em\hfuzz=1pt\vfuzz=2pt\raggedbottom
% --- breakable math helpers ---
\newenvironment{BreakableEquation}{\begin{equation}\begin{aligned}}{\end{aligned}\end{equation}}
\newenvironment{BreakableEquation*}{\begin{equation*}\begin{aligned}}{\end{aligned}\end{equation*}}
\newenvironment{tightalign}{\begingroup\small\allowdisplaybreaks\begin{align}}{\end{align}\endgroup}

\usepackage{xcolor}
\usepackage{xurl} % better URL wrapping
\usepackage[colorlinks=true,linkcolor=black,urlcolor=blue]{hyperref}
\usepackage{fancyhdr}
\pagestyle{fancy}\fancyhf{}\lhead{\nouppercase{\leftmark}}\rhead{\thepage}\setlength{\headheight}{26pt}
\usepackage{enumitem,booktabs,tabularx}
\newcolumntype{L}{>{\raggedright\arraybackslash}X}
\renewcommand{\arraystretch}{1.1}
\setlength{\parindent}{0pt}\setlength{\parskip}{8pt plus 2pt minus 1pt}

% Listings + upquote (no shell-escape needed)
\usepackage{listings}
\usepackage{upquote}
\lstdefinestyle{crisp}{
  basicstyle=\ttfamily\footnotesize,
  frame=single,
  breaklines=true,
  breakatwhitespace=false, % allow breaks inside long tokens
  tabsize=4,
  showstringspaces=false,
  numbers=left,
  numberstyle=\tiny,
  numbersep=8pt,
  keepspaces=true,
  columns=fullflexible,
  backgroundcolor=\color{black!02},
  aboveskip=8pt,
  belowskip=8pt
}
% Guarantee that 'python' exists as a language for listings
\lstdefinelanguage{python}{
  morekeywords={def,return,class,if,elif,else,for,while,try,except,raise,assert,pass,break,continue,lambda,nonlocal,global,yield,import,from,as,with,True,False,None},
  sensitive=true,
  morecomment=[l]\#,
  morestring=[b]",
  morestring=[b]'
}
% minted shim (robust; no shell-escape; uses listings' own environment)
\lstnewenvironment{minted}[2][]{\lstset{style=crisp,language=#2,#1}}{}

\usepackage[most]{tcolorbox}
\tcbset{colback=white,colframe=black!15,boxrule=0.4pt,arc=2pt,left=6pt,right=6pt,top=6pt,bottom=6pt,before skip=10pt,after skip=10pt,breakable}
\usepackage{titlesec}
\titleformat{\section}{\large\bfseries}{\thesection}{0.6em}{}
\titlespacing*{\section}{0pt}{*2.0}{*1.0}

% ===== CONSISTENCY TOOLKIT (macros) =====
\newlist{ledger}{enumerate}{1}
\setlist[ledger]{label=\textbullet,leftmargin=2em,itemsep=2pt,topsep=6pt}
\newcommand{\LEDGER}[1]{\textbf{ARITHMETIC LEDGER:}\par\begin{ledger}#1\end{ledger}}

\newlist{algosteps}{enumerate}{1}
\setlist[algosteps]{label=\arabic*.,leftmargin=2em,itemsep=2pt,topsep=6pt}

\newlist{bullets}{itemize}{1}
\setlist[bullets]{label=--,leftmargin=1.6em,itemsep=2pt,topsep=4pt}

\newcommand{\varmapStart}{\textbf{VARIABLE MAPPING:}\par\begin{bullets}}
\newcommand{\var}[2]{\item $#1$ — #2}
\newcommand{\varmapEnd}{\end{bullets}}

\newcommand{\LINE}[2]{\par\noindent\textbf{#1:}~#2\par}
\newcommand{\WHAT}[1]{\LINE{WHAT}{#1}}
\newcommand{\WHY}[1]{\LINE{WHY}{#1}}
\newcommand{\HOW}[1]{\LINE{HOW}{#1}}
\newcommand{\ELI}[1]{\LINE{ELI5}{#1}}
\newcommand{\STATEMENT}[1]{\LINE{STATEMENT}{#1}}
\newcommand{\BREAKDOWN}[1]{\LINE{PROBLEM BREAKDOWN}{#1}}
\newcommand{\MODEL}[1]{\LINE{CANONICAL MATHEMATICAL MODEL}{#1}}
\newcommand{\ASSUMPTIONS}[1]{\LINE{ASSUMPTIONS}{#1}}
\newcommand{\INVARIANTS}[1]{\LINE{INVARIANTS}{#1}}
\newcommand{\WHICHFORMULA}[1]{\LINE{WHICH FORMULA \& WHY}{#1}}
\newcommand{\GOVERN}[1]{\LINE{GOVERNING EQUATION(S)}{#1}}
\newcommand{\INPUTS}[1]{\LINE{INPUTS}{#1}}
\newcommand{\OUTPUTS}[1]{\LINE{OUTPUTS}{#1}}
\newcommand{\SAMPLES}[1]{\LINE{SAMPLES}{#1}}
\newcommand{\RESULT}[1]{\LINE{RESULT}{#1}}
\newcommand{\COMPLEXITY}[1]{\LINE{TIME/SPACE COMPLEXITY}{#1}}
\newcommand{\MEMORY}[1]{\LINE{MEMORY FOOTPRINT}{#1}}
\newcommand{\CORRECTNESS}[1]{\LINE{CORRECTNESS}{#1}}
\newcommand{\OPTIMALITY}[1]{\LINE{OPTIMALITY}{#1}}
\newcommand{\FAILMODES}[1]{\LINE{FAILURE MODES}{#1}}
\newcommand{\VALIDATION}[1]{\LINE{VALIDATION}{#1}}
\newcommand{\UNITCHECK}[1]{\LINE{UNIT CHECK}{#1}}
\newcommand{\EDGECASES}[1]{\LINE{EDGE CASES}{#1}}
\newcommand{\CHECKLIST}[1]{\LINE{CHECKLIST}{#1}}
\newcommand{\DERIV}[1]{\LINE{DERIVATION}{#1}}
\newcommand{\LEMMAHEAD}[1]{\LINE{SUPPORTING LEMMA}{#1}}
\newcommand{\PITFALLS}[1]{\LINE{PITFALLS}{#1}}

\usepackage{etoolbox}\pretocmd{\section}{\clearpage}{}{}

\newcommand{\FormulaPage}[2]{%
  \clearpage
  \section*{Formula #1 — #2}%
  \addcontentsline{toc}{section}{Formula #1 — #2}%
}
\newcommand{\ApproachPage}[2]{%
  \clearpage
  \subsection*{Approach #1 — #2}%
  \addcontentsline{toc}{subsection}{Approach #1 — #2}%
}

\begin{document}
\title{Interview Problem Sheet — Marmots (hard)}
\date{\today}
\author{}
\maketitle
\tableofcontents
\clearpage

% === Notes macro (BODY-ONLY; do not alter the preamble) ===
% Prints exactly N blank pages with empty headers/footers, then leaves you on the last blank page.
\newcommand{\NotePages}[1]{%
  \clearpage
  \begingroup
  \newcount\NPi \newcount\NPn
  \NPi=0 \NPn=#1
  \loop\ifnum\NPi<\NPn
    \advance\NPi by 1
    \mbox{}\thispagestyle{empty}%
    \ifnum\NPi<\NPn\clearpage\fi
  \repeat
  \endgroup
}

% ============ 1. Problem & Metadata (own page) ============
\section{Problem \& Metadata}
\LINE{PLATFORM}{CF}
\LINE{URL}{\url{https://codeforces.com/problemset/problem/802/F}}
\LINE{DIFFICULTY / RATING}{2800}
\STATEMENT{Your task is the exact same as for the easy version. But this time, the marmots subtract the village's population $P$ from their random number before responding to Heidi's request.

Also, there are now villages with as few as a single inhabitant, meaning that $1 \le P \le 1000$.

Can you help Heidi find out whether a village follows a Poisson or a uniform distribution?

Input: To make this statement self-contained and precise, we restate the format. The first line contains an integer $T$ (number of villages). Each test case consists of:
\begin{itemize}
\item One line with two integers $P$ and $N$ with $1 \le P \le 1000$ and $1 \le N \le 200{,}000$.
\item One line with $N$ space-separated integers $x_1,\ldots,x_N$, which are exactly the marmots' responses. Values may be negative.
\end{itemize}
For each village, the responses are i.i.d. from exactly one of the following two models:
\begin{itemize}
\item Poisson type: draw $Z \sim \mathrm{Poisson}(P)$ and respond $X = Z - P$ (thus $X \in \{-P,-P+1,\ldots\}$).
\item Uniform type: draw $U$ uniformly from the integers in $\{-P,-P+1,\ldots,0,\ldots,P\}$ and respond $X = U$ (thus $X \in \{-P,\ldots,P\}$).
\end{itemize}

Output: Output one line per village, in the same order as in the input. Print \texttt{poisson} if the village's distribution is of the Poisson type, and \texttt{uniform} if the answers came from a uniform distribution.}
\BREAKDOWN{We must decide between two integer distributions given samples: a shifted Poisson($P$) versus a discrete uniform on $\{-P,\ldots,P\}$. Outside-support observations decide immediately. Otherwise, compare models using moments or maximum likelihood.}
\ELI{If any answer lies outside the uniform range, it must be Poisson; otherwise pick the model that gives higher likelihood for the data.}
\NotePages{3}

% ============ 2. IO Contract (own page) ============
\section{IO Contract}
\INPUTS{
- Integer $T$.\par
- For each test $i=1..T$: integers $P$, $N$; then a length-$N$ list of integers $x_j$.\par
- Constraints: $1 \le P \le 1000$, $1 \le N \le 200{,}000$; $\sum N$ across tests is feasible for a single run.}
\OUTPUTS{For each test case, print a single line containing either \texttt{poisson} or \texttt{uniform}.}
\SAMPLES{
Example 1:\par
Input
\begin{verbatim}
2
3 5
-3 -1 0 1 3
5 4
-5 -4 8 0
\end{verbatim}
Output
\begin{verbatim}
uniform
poisson
\end{verbatim}
Explanation: In the second test, $8>P$ so uniform is impossible.\par
Example 2:\par
Input
\begin{verbatim}
1
2 6
-2 -1 0 0 1 2
\end{verbatim}
Output
\begin{verbatim}
uniform
\end{verbatim}
Here both models allow the support; a uniform draw over $\{-2,-1,0,1,2\}$ is more plausible than Poisson($2$)$-2$ for this tiny symmetric sample.}
\NotePages{3}

% ============ 3. Canonical Mathematical Model (own page) ============
\section{Canonical Mathematical Model}
\MODEL{Given $P \in \mathbb{N}$ and a multiset of observations $x_1,\ldots,x_N \in \mathbb{Z}$, decide between hypotheses:
\[
H_U:~X \sim \mathrm{Unif}\{-P,\ldots,P\},\quad
H_P:~X = Z-P,\; Z \sim \mathrm{Poisson}(P).
\]
We choose the hypothesis with larger likelihood; if $H_U$ is impossible for any sample, choose $H_P$.}
\varmapStart
\var{P}{village population parameter and model parameter}
\var{N}{number of observations in the village}
\var{x_j}{the $j$-th response}
\var{\ell_U}{log-likelihood under the uniform model}
\var{\ell_P}{log-likelihood under the Poisson model}
\varmapEnd
\GOVERN{
\[
\begin{aligned}
\ell_U &=
\begin{cases}
-N\log(2P{+}1), & \text{if } \max_j |x_j| \le P,\\
-\infty, & \text{otherwise},
\end{cases}\\[4pt]
\ell_P &= \sum_{j=1}^N \left[-P + (x_j{+}P)\log P - \log((x_j{+}P)!)\right],\quad \text{valid iff } \min_j x_j \ge -P.
\end{aligned}
\]
We decide $H_P$ if $\ell_P > \ell_U$, else $H_U$. Use $\log\Gamma$ as $\log(n!)=\log\Gamma(n{+}1)$.}
\ASSUMPTIONS{Observations are i.i.d. within each test. Inputs are integers. If an observation violates both supports (should not happen under the generative story), we still compute $\ell_P$ only if $x_j \ge -P$; otherwise we treat $\ell_P=-\infty$ and $\ell_U=-\infty$ and default to \texttt{poisson}.}
\INVARIANTS{
- If any $x_j > P$, then $H_U$ is impossible.\par
- If any $x_j < -P$, then $H_P$ is impossible (and $H_U$ is also impossible).}
\NotePages{3}

% ============ 4. Approach A — Baseline (own page) ============
\section{Approach A — Baseline}
\ApproachPage{A}{Brute Force / Direct}
\WHICHFORMULA{Use coarse diagnostics: support checks and variance comparison. For $H_U$, $\operatorname{Var}[X]=\tfrac{P(P+1)}{3}$. For $H_P$, $\operatorname{Var}[X]=P$.}
\ASSUMPTIONS{Enough samples that sample variance is informative. Fall back to Poisson on ties or ambiguous small $N$.}
\textbf{Algorithm Steps}
\begin{algosteps}
\item If any $x_j > P$, output \texttt{poisson}.
\item If any $x_j < -P$, output \texttt{poisson} (both models are impossible; default to Poisson).
\item Compute sample mean $\bar{x}$ and variance $s^2=\tfrac{1}{N}\sum (x_j-\bar{x})^2$.
\item Let $v_U=\tfrac{P(P+1)}{3}$ and $v_P=P$. If $|s^2-v_U|<|s^2-v_P|$, output \texttt{uniform}, else \texttt{poisson}.
\end{algosteps}
\COMPLEXITY{Single pass over the data, $O(N)$ time, $O(1)$ extra space.}
\[
\begin{aligned}
T(N) &= \Theta(N),\quad S(N)=\Theta(1).
\end{aligned}
\]
\CORRECTNESS{Outside-support observations force the decision. For valid supports, the variances differ for $P \ne 2$, so with sufficient samples the empirical variance concentrates near the true variance and selects the correct model with high probability.}
\EDGECASES{Small $N$, $P=1$ or $P=2$ where variances are close or equal; highly skewed Poisson samples may mislead variance on tiny $N$.}
\textbf{Code (Baseline)}
\begin{minted}{python}
import sys
import math
from typing import List, Tuple

def read_input() -> List[Tuple[int, List[int]]]:
    data = list(map(int, sys.stdin.read().strip().split()))
    if not data:
        return []
    it = iter(data)
    t = next(it)
    cases = []
    for _ in range(t):
        P = next(it)
        N = next(it)
        xs = [next(it) for _ in range(N)]
        cases.append((P, xs))
    return cases

def classify_baseline(P: int, xs: List[int]) -> str:
    # Support checks
    if any(x > P for x in xs):
        return "poisson"
    if any(x < -P for x in xs):
        return "poisson"  # both impossible; default to poisson
    N = len(xs)
    if N == 0:
        return "poisson"
    mean = sum(xs) / N
    var = sum((x - mean) * (x - mean) for x in xs) / N
    vU = P * (P + 1) / 3.0
    vP = float(P)
    # Compare squared distances to add a bit of stability
    dU = (var - vU) * (var - vU)
    dP = (var - vP) * (var - vP)
    return "uniform" if dU < dP else "poisson"

def solve_case_baseline(P: int, xs: List[int]) -> str:
    return classify_baseline(P, xs)

def solve_all_baseline(cases: List[Tuple[int, List[int]]]) -> List[str]:
    return [solve_case_baseline(P, xs) for (P, xs) in cases]

def main():
    # Basic self-checks (deterministic)
    assert classify_baseline(3, [-3, -1, 0, 1, 3]) == "uniform"
    assert classify_baseline(5, [-5, -4, 8, 0]) == "poisson"
    # Ambiguous tiny sample, still returns a label
    _ = classify_baseline(2, [-2, -1, 0, 1, 2])

    cases = read_input()
    if cases:
        ans = solve_all_baseline(cases)
        sys.stdout.write("\n".join(ans))

if __name__ == "__main__":
    main()
\end{minted}
\VALIDATION{Checked support-based decisions; verified that symmetric uniform-like samples return \texttt{uniform}.}
\NotePages{3}

% ============ 5. Approach B — Improved (own page) ============
\section{Approach B — Improved}
\ApproachPage{B}{Optimized Data Structure / Pruning / DP}
\WHICHFORMULA{Augment variance with skewness (third central moment). For $H_U$, the distribution is symmetric so $\mathbb{E}[(X-\mu)^3]=0$. For $H_P$, the third central moment equals $P$ (Poisson has $\mu_3=P$), so after shifting by $-P$ and centering by the sample mean, the sample third moment tends to be positive.}
\ASSUMPTIONS{Sample size not tiny so that the third central moment is informative; still retain support short-circuits.}
\textbf{Algorithm Steps}
\begin{algosteps}
\item If any $x_j > P$, output \texttt{poisson}; if any $x_j < -P$, output \texttt{poisson}.
\item Compute sample mean $\bar{x}$, variance $s^2$, and third central moment $m_3=\tfrac{1}{N}\sum (x_j-\bar{x})^3$.
\item If $|m_3|$ is small relative to $P$ and $s^2$ is closer to $v_U$ than $v_P$, choose \texttt{uniform}; otherwise choose \texttt{poisson}.
\end{algosteps}
\COMPLEXITY{Single pass accumulating sums for mean, variance, and third moment gives $O(N)$ time and $O(1)$ space, same as baseline, but with better discrimination at $P \approx 2$.}
\[
\begin{aligned}
T(N) &= \Theta(N).
\end{aligned}
\]
\CORRECTNESS{Uniform has zero skew; Poisson($P$)$-P$ has positive skew of order $P$. Combining support, variance, and skew robustly separates models with high probability.}
\textbf{Code (Improved)}
\begin{minted}{python}
import sys
import math
from typing import List, Tuple

def read_input() -> List[Tuple[int, List[int]]]:
    data = list(map(int, sys.stdin.read().strip().split()))
    if not data:
        return []
    it = iter(data)
    t = next(it)
    cases = []
    for _ in range(t):
        P = next(it)
        N = next(it)
        xs = [next(it) for _ in range(N)]
        cases.append((P, xs))
    return cases

def classify_skew(P: int, xs: List[int]) -> str:
    # Support checks
    if any(x > P for x in xs):
        return "poisson"
    if any(x < -P for x in xs):
        return "poisson"
    N = len(xs)
    if N == 0:
        return "poisson"
    # One-pass numerically stable moments (Welford / higher moments)
    mean = 0.0
    M2 = 0.0
    M3 = 0.0
    n = 0.0
    for x in xs:
        n1 = n
        n += 1.0
        delta = x - mean
        delta_n = delta / n
        term1 = delta * delta_n * n1
        M3 += term1 * delta_n * (n - 2.0) - 3.0 * delta_n * M2
        M2 += term1
        mean += delta_n
    var = M2 / N if N > 0 else 0.0
    m3 = M3 / N if N > 0 else 0.0

    vU = P * (P + 1) / 3.0
    vP = float(P)
    # Heuristic thresholds
    closer_to_uniform = (var - vU) ** 2 <= (var - vP) ** 2
    # Normalize m3 by max(1,P) to avoid tiny P dominating
    skew_signal = abs(m3) / max(1.0, float(P))
    if closer_to_uniform and skew_signal < 0.2:
        return "uniform"
    # If skew very strong and positive, prefer Poisson
    if m3 > 0.2 * max(1.0, float(P)):
        return "poisson"
    # Fallback to variance decision
    return "uniform" if closer_to_uniform else "poisson"

def solve_case_skew(P: int, xs: List[int]) -> str:
    return classify_skew(P, xs)

def solve_all_skew(cases: List[Tuple[int, List[int]]]) -> List[str]:
    return [solve_case_skew(P, xs) for (P, xs) in cases]

def main():
    # Deterministic checks
    assert classify_skew(3, [-3, -1, 0, 1, 3]) == "uniform"
    assert classify_skew(5, [-5, -4, 8, 0]) == "poisson"
    # Boundary P=2 often ambiguous; still returns a label
    _ = classify_skew(2, [-2, -1, 0, 1, 2])

    cases = read_input()
    if cases:
        ans = solve_all_skew(cases)
        sys.stdout.write("\n".join(ans))

if __name__ == "__main__":
    main()
\end{minted}
\VALIDATION{Adds skewness to disambiguate cases where variance is inconclusive; still honors immediate support contradictions.}
\NotePages{3}

% ============ 6. Approach C — Optimal (own page) ============
\section{Approach C — Optimal}
\ApproachPage{C}{Provably Optimal Method}
\WHICHFORMULA{Compute and compare exact log-likelihoods under both models using $\log\Gamma$ for factorial terms. This is the maximum likelihood decision between two simple hypotheses.}
\ASSUMPTIONS{Observations are i.i.d.; numeric stability through log-domain computations.}
\textbf{Algorithm Steps}
\begin{algosteps}
\item Check supports: if any $x_j > P$, then $\ell_U=-\infty$; if any $x_j < -P$, then $\ell_P=-\infty$.
\item Compute $\ell_U=-N\log(2P{+}1)$ if allowed, else $-\infty$.
\item Compute $\ell_P=\sum_j\bigl(-P+(x_j{+}P)\log P-\log\Gamma(x_j{+}P{+}1)\bigr)$ if allowed, else $-\infty$.
\item Output \texttt{poisson} if $\ell_P>\ell_U$, else \texttt{uniform}. Ties break to \texttt{uniform}.
\end{algosteps}
\OPTIMALITY{Among tests that select the higher likelihood for two fully specified distributions, this is the Neyman–Pearson most powerful test for each fixed $N$ at any threshold; choosing the larger likelihood is the MAP rule with equal priors.}
\COMPLEXITY{One pass accumulation of $O(1)$ work per item (constant-time $\log\Gamma$), so $O(N)$ time and $O(1)$ space.}
\[
\begin{aligned}
T(N) &= \Theta(N).
\end{aligned}
\]
\textbf{Code (Final Submission)}
\begin{minted}{python}
import sys
import math
from typing import List, Tuple

def read_input() -> List[Tuple[int, List[int]]]:
    data = list(map(int, sys.stdin.read().strip().split()))
    if not data:
        return []
    it = iter(data)
    t = next(it)
    cases = []
    for _ in range(t):
        P = next(it)
        N = next(it)
        xs = [next(it) for _ in range(N)]
        cases.append((P, xs))
    return cases

def poisson_loglik(P: int, xs: List[int]) -> float:
    # If any x < -P, Poisson(Z=P) - P is impossible
    if any(x < -P for x in xs):
        return float("-inf")
    if P <= 0:
        # Not expected under constraints, but handle gracefully
        return float("-inf")
    logP = math.log(P)
    # Sum [-P + (x+P)*log P - logGamma(x+P+1)]
    s = 0.0
    term_const = -float(P)
    for x in xs:
        k = x + P  # k >= 0 ensured by support check
        s += term_const + k * logP - math.lgamma(k + 1.0)
    return s

def uniform_loglik(P: int, xs: List[int]) -> float:
    # If any |x| > P, uniform is impossible
    if any(abs(x) > P for x in xs):
        return float("-inf")
    N = len(xs)
    return -N * math.log(2 * P + 1.0)

def solve_case(P: int, xs: List[int]) -> str:
    lU = uniform_loglik(P, xs)
    lP = poisson_loglik(P, xs)
    if lP > lU:
        return "poisson"
    else:
        return "uniform"

def solve_all(cases: List[Tuple[int, List[int]]]) -> List[str]:
    return [solve_case(P, xs) for (P, xs) in cases]

def main():
    # Deterministic mini-tests
    # 1) Support knockout for uniform
    assert solve_case(5, [-5, -4, 8, 0]) == "poisson"
    # 2) Symmetric uniform-like sample
    assert solve_case(3, [-3, -1, 0, 1, 3]) == "uniform"
    # 3) Typical Poisson-shifted mass near -P for small P
    assert solve_case(1, [-1, -1, 0, 0, 1]) in ("poisson", "uniform")  # valid, depends on likelihood

    cases = read_input()
    if cases:
        ans = solve_all(cases)
        sys.stdout.write("\n".join(ans))

if __name__ == "__main__":
    main()
\end{minted}
\VALIDATION{Exactly 3 asserts included. Verified behavior on support contradictions and symmetric samples.}
\RESULT{For each village, prints a single token: \texttt{poisson} or \texttt{uniform}, selecting the model with larger log-likelihood; ties break toward \texttt{uniform}.}
\NotePages{3}

% ============ 7. Testing & Final Reference Implementation (own page) ============
\section{Testing \& Final Reference Implementation}
\LINE{TEST PLAN}{Unit tests cover support contradictions and typical symmetric cases. For randomized checks (not included in asserts), compare decisions on synthetic data generated from both models at various $P$ and $N$.}
\LINE{CROSS-CHECKS}{On tiny crafted datasets, compare outputs of Approach A, B, and C; Approach C should dominate. Differences highlight where moments are insufficient but likelihood is decisive.}
\LINE{EDGE-CASE GENERATOR}{Generate extremes: $P=1$, $P=1000$; samples with some $x_j=\pm P$; all $x_j=-P$ (possible under Poisson) versus spread across $[-P,P]$ for uniform.}
\begin{minted}{python}
# Deterministic generators for boundaries, degenerates, adversarials
from typing import List
import math
import random

def gen_uniform(P: int, N: int, seed: int = 0) -> List[int]:
    rnd = random.Random(seed)
    return [rnd.randint(-P, P) for _ in range(N)]

def gen_poisson_shift(P: int, N: int, seed: int = 0) -> List[int]:
    rnd = random.Random(seed)
    # Knuth's algorithm for Poisson, good for small P; for larger P, use accept-reject
    # but keep deterministic and simple here.
    def sample_poisson(lmbda: float) -> int:
        L = math.exp(-lmbda)
        k = 0
        p = 1.0
        while True:
            k += 1
            p *= rnd.random()
            if p <= L:
                return k - 1
    return [sample_poisson(P) - P for _ in range(N)]

def quick_compare(P: int, N: int):
    from math import isfinite
    xs_u = gen_uniform(P, N, seed=123)
    xs_p = gen_poisson_shift(P, N, seed=123)
    # Reuse final classifier
    import sys as _sys
    import math as _math

    def lU(xs):
        return -len(xs) * _math.log(2 * P + 1.0)
    def lP(xs):
        if any(x < -P for x in xs):
            return float("-inf")
        logP = _math.log(P)
        return sum(-P + (x + P) * logP - _math.lgamma(x + P + 1.0) for x in xs)

    return ("uniform" if lU(xs_u) >= lP(xs_u) else "poisson",
            "poisson" if lP(xs_p) > lU(xs_p) else "uniform")
\end{minted}
\textbf{Reference Code (Ready to Submit)}
\begin{minted}{python}
import sys
import math
from typing import List, Tuple

def read_input() -> List[Tuple[int, List[int]]]:
    data = list(map(int, sys.stdin.read().strip().split()))
    if not data:
        return []
    it = iter(data)
    t = next(it)
    cases = []
    for _ in range(t):
        P = next(it)
        N = next(it)
        xs = [next(it) for _ in range(N)]
        cases.append((P, xs))
    return cases

def poisson_loglik(P: int, xs: List[int]) -> float:
    if any(x < -P for x in xs):
        return float("-inf")
    if P <= 0:
        return float("-inf")
    logP = math.log(P)
    s = 0.0
    term_const = -float(P)
    for x in xs:
        k = x + P
        s += term_const + k * logP - math.lgamma(k + 1.0)
    return s

def uniform_loglik(P: int, xs: List[int]) -> float:
    if any(abs(x) > P for x in xs):
        return float("-inf")
    return -len(xs) * math.log(2 * P + 1.0)

def solve_case(P: int, xs: List[int]) -> str:
    lU = uniform_loglik(P, xs)
    lP = poisson_loglik(P, xs)
    return "poisson" if lP > lU else "uniform"

def solve_all(cases: List[Tuple[int, List[int]]]) -> List[str]:
    return [solve_case(P, xs) for (P, xs) in cases]

def main():
    # Asserts (deterministic mini-tests)
    assert solve_case(5, [-5, -4, 8, 0]) == "poisson"
    assert solve_case(3, [-3, -1, 0, 1, 3]) == "uniform"
    assert solve_case(1, [-1, -1, 0, 0, 1]) in ("poisson", "uniform")

    cases = read_input()
    if cases:
        ans = solve_all(cases)
        sys.stdout.write("\n".join(ans))

if __name__ == "__main__":
    main()
\end{minted}
\NotePages{3}

% ============ 8. Review & Pitfalls (own page) ============
\section{Review \& Pitfalls}
\WHAT{Binary hypothesis test between shifted Poisson and discrete uniform on $\{-P,\ldots,P\}$.}
\WHY{Mixture identification and likelihood comparisons are common in interviews to test modeling, implementation care, and numerical stability.}
\CHECKLIST{
- Read $T$, then each $(P,N)$ and $N$ integers.\par
- Immediate support knockout: $|x|>P \Rightarrow$ not uniform; $x<-P \Rightarrow$ not Poisson.\par
- Compute log-likelihoods in log-domain; use $\log\Gamma$ for factorials.\par
- Compare and break ties consistently.}
\EDGECASES{
- $P=1$ or $P=2$ where variances are close.\par
- Tiny $N$ with coincidental symmetry.\par
- Observations equal to $\pm P$.\par
- All observations $=-P$ (possible for Poisson, unlikely for uniform).\par
- Mixed lines and whitespace in input.\par
- No cases or empty stdin (program should not crash).}
\PITFALLS{
- Using factorial on large integers instead of $\log\Gamma$ causing overflow.\par
- Forgetting that Poisson-shift forbids $x<-P$.\par
- Not handling $|x|>P$ as impossible for uniform.\par
- Floating-point underflow without log-domain.\par
- Off-by-one in uniform support size $(2P{+}1)$.\par
- Ignoring tie-breaking, leading to nondeterminism.}
\FAILMODES{Variance-only or skew-only heuristics can be fooled by small $N$; full likelihood handles all finite samples optimally under the model. The final method also gracefully handles immediate support contradictions.}
\ELI{Check whether any value lies outside the uniform window; if so, it must be Poisson. Otherwise, compute how probable the data are under each model and pick the one that fits better.}
\NotePages{3}

\end{document}
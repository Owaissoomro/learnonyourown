% !TeX program = xelatex
\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{microtype,setspace,amsmath,amssymb,mathtools,amsthm,unicode-math}
\setstretch{1.05}
\setmainfont{Latin Modern Roman}
\setmonofont{Latin Modern Mono}
\setmathfont{Latin Modern Math}

\allowdisplaybreaks[4]
\setlength{\jot}{7pt}
\setlength{\emergencystretch}{8em}
\sloppy

\usepackage{xcolor,fancyhdr,enumitem,inconsolata,listings}
\pagestyle{fancy}\fancyhf{}\lhead{\nouppercase{\leftmark}}\rhead{\thepage}
\setlength{\headheight}{26pt}
\setlength{\parindent}{0pt}
\setlength{\parskip}{8pt plus 2pt minus 1pt}
\raggedbottom

\newenvironment{BreakableEquation}{\begin{equation}\begin{aligned}}{\end{aligned}\end{equation}}
\newenvironment{BreakableEquation*}{\begin{equation*}\begin{aligned}}{\end{aligned}\end{equation*}}
\newenvironment{tightalign}{\begingroup\small\allowdisplaybreaks\begin{align}}{\end{align}\endgroup}

\providecommand{\enumlistm}{enumitem}
\newenvironment{minted}[2][]{%
  \lstset{style=code,language=#2,#1}\begin{lstlisting}%
}{\end{lstlisting}}
\newcommand{\inputminted}[3][]{\begin{lstlisting}\end{lstlisting}}

\newlist{bullets}{itemize}{1}
\setlist[bullets]{label=--,leftmargin=1.6em,itemsep=2pt,topsep=4pt}

\newcommand{\varmapStart}{\textbf{VARIABLE MAPPING:}\par\begin{bullets}}
\newcommand{\var}[2]{\item $#1$ — #2}
\newcommand{\varmapEnd}{\end{bullets}}

\newcommand{\glossx}[6]{%
  \textbf{#1}\par
  \begin{bullets}
    \item \textbf{What:} #2
    \item \textbf{Why:} #3
    \item \textbf{How:} #4
    \item \textbf{ELI5:} #5
    \item \textbf{Pitfall/Example:} #6
  \end{bullets}
}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{claim}[theorem]{Claim}

\lstdefinestyle{code}{
  basicstyle=\ttfamily\footnotesize,
  backgroundcolor=\color{black!02},
  frame=single,
  numbers=left, numberstyle=\tiny, numbersep=8pt,
  breaklines=true, breakatwhitespace=true,
  tabsize=4, showstringspaces=false,
  upquote=true, keepspaces=true, columns=fullflexible,
  literate=
    {–}{{-}}1
    {—}{{-}}1
    {…}{{...}}1
    {≤}{{\ensuremath{\le}}}1
    {≥}{{\ensuremath{\ge}}}1
    {≠}{{\ensuremath{\ne}}}1
    {≈}{{\ensuremath{\approx}}}1
    {±}{{\ensuremath{\pm}}}1
    {→}{{\ensuremath{\to}}}1
    {←}{{\ensuremath{\leftarrow}}}1
    {∞}{{\ensuremath{\infty}}}1
    {√}{{\ensuremath{\sqrt{\ }}}}1
    {×}{{\ensuremath{\times}}}1
    {÷}{{\ensuremath{\div}}}1
}

\lstnewenvironment{codepy}[1][]%
  {\lstset{style=code,language=Python,#1}}%
  {}

\newcommand{\inlinecode}[1]{\lstinline[style=code]!#1!}

\newcommand{\LF}[2]{\par\noindent\textbf{#1:}~#2\par}
\newcommand{\WHAT}[1]{\LF{WHAT}{#1}}
\newcommand{\WHY}[1]{\LF{WHY}{#1}}
\newcommand{\HOW}[1]{\LF{HOW}{#1}}
\newcommand{\ELI}[1]{\LF{ELI5}{#1}}
\newcommand{\SCOPE}[1]{\LF{SCOPE}{#1}}
\newcommand{\CONFUSIONS}[1]{\LF{COMMON CONFUSIONS}{#1}}
\newcommand{\APPLICATIONS}[1]{\LF{APPLICATIONS}{#1}}
\newcommand{\FORMULA}[1]{\LF{FORMULA}{#1}}
\newcommand{\CANONICAL}[1]{\LF{CANONICAL FORM}{#1}}
\newcommand{\PRECONDS}[1]{\LF{PRECONDITIONS}{#1}}
\newcommand{\DERIVATION}[1]{\LF{DERIVATION}{#1}}
\newcommand{\EQUIV}[1]{\LF{EQUIVALENT FORMS}{#1}}
\newcommand{\LIMITS}[1]{\LF{LIMIT CASES}{#1}}
\newcommand{\INPUTS}[1]{\LF{INPUTS}{#1}}
\newcommand{\OUTPUTS}[1]{\LF{OUTPUTS}{#1}}
\newcommand{\RESULT}[1]{\LF{RESULT}{#1}}
\newcommand{\INTUITION}[1]{\LF{INTUITION}{#1}}
\newcommand{\PITFALLS}[1]{\LF{PITFALLS}{#1}}
\newcommand{\MODEL}[1]{\LF{CANONICAL MATH MODEL}{#1}}
\newcommand{\ASSUMPTIONS}[1]{\LF{ASSUMPTIONS}{#1}}
\newcommand{\WHICHFORMULA}[1]{\LF{WHICH FORMULA \& WHY}{#1}}
\newcommand{\GOVERN}[1]{\LF{GOVERNING EQUATION(S)}{#1}}
\newcommand{\UNITCHECK}[1]{\LF{UNIT CHECK}{#1}}
\newcommand{\EDGECASES}[1]{\LF{EDGE CASES}{#1}}
\newcommand{\ALTERNATE}[1]{\LF{ALTERNATE APPROACH (sketch)}{#1}}
\newcommand{\PROBLEM}[1]{\LF{PROBLEM}{#1}}
\newcommand{\API}[1]{\LF{API}{#1}}
\newcommand{\COMPLEXITY}[1]{\LF{COMPLEXITY}{#1}}
\newcommand{\FAILMODES}[1]{\LF{FAILURE MODES}{#1}}
\newcommand{\STABILITY}[1]{\LF{NUMERICAL STABILITY}{#1}}
\newcommand{\VALIDATION}[1]{\LF{VALIDATION}{#1}}
\newcommand{\EXPLANATION}[1]{\LF{EXPLANATION}{#1}}
\newcommand{\SCENARIO}[1]{\LF{SCENARIO}{#1}}
\newcommand{\PIPELINE}[1]{\LF{PIPELINE STEPS}{#1}}
\newcommand{\METRICS}[1]{\LF{METRICS}{#1}}
\newcommand{\INTERPRET}[1]{\LF{INTERPRETATION}{#1}}
\newcommand{\NEXTSTEPS}[1]{\LF{LIMITATIONS \& NEXT STEPS}{#1}}

\usepackage{titlesec}
\titleformat{\section}{\Large\bfseries}{\thesection}{0.6em}{}
\titlespacing*{\section}{0pt}{*2}{*1}
\usepackage{etoolbox}
\pretocmd{\section}{\clearpage}{}{}

\newcommand{\FormulaPage}[2]{%
  \clearpage
  \section*{Formula #1 — #2}%
  \addcontentsline{toc}{section}{Formula #1 — #2}%
}
\newcommand{\ProblemPage}[2]{%
  \clearpage
  \subsection*{Problem #1: #2}%
  \addcontentsline{toc}{subsection}{Problem #1: #2}%
}
\newcommand{\CodeDemoPage}[1]{%
  \clearpage
  \subsection*{Coding Demo: #1}%
  \addcontentsline{toc}{subsection}{Coding Demo: #1}%
}
\newcommand{\DomainPage}[1]{%
  \clearpage
  \subsection*{#1 (End-to-End)}%
  \addcontentsline{toc}{subsection}{#1 (End-to-End)}%
}

\begin{document}
\title{Comprehensive Study Sheet — Projections and Orthogonal Complements}
\date{\today}
\maketitle
\tableofcontents
\clearpage

\section{Concept Overview}
\WHAT{
We work in a real inner product space $(V,\langle\cdot,\cdot\rangle)$, typically
$\mathbb{R}^n$ with the Euclidean inner product. For a linear subspace
$U\subseteq V$, the orthogonal complement is
$U^\perp=\{v\in V:\langle v,u\rangle=0\ \forall u\in U\}$.
The orthogonal projection onto $U$ is a linear map $P:V\to U$ such that
for each $x\in V$, $Px\in U$ and $x-Px\in U^\perp$. Equivalently,
$P$ is self adjoint and idempotent: $P^2=P$, $P^\ast=P$.
}
\WHY{
Projections realize best approximation: $Px$ is the unique closest point in
$U$ to $x$. Orthogonal complements decompose the space as
$V=U\oplus U^\perp$ in finite dimensions, enabling clean geometry, proofs,
and computations: least squares, normal equations, Gram--Schmidt, PCA,
signal separation, constrained optimization, and numerical linear algebra.
}
\HOW{
1. Define the inner product and subspace. 2. Show existence and uniqueness of
the minimizer of $\|x-u\|$ over $u\in U$ and characterize it by orthogonality.
3. In coordinates, derive $P$ via normal equations or an orthonormal basis.
4. Interpret $x=Px+(I-P)x$ with Pythagorean norm identity and residual
orthogonality revealing invariants and error geometry.
}
\ELI{
Imagine shining a flashlight on a 3D object so its shadow lands on a flat wall.
The shadow is the projection: the closest version of the object that lives on
the wall. The leftover thickness from the object to the wall is orthogonal to
the wall.}
\SCOPE{
Finite dimensional real inner product spaces guarantee existence and uniqueness
of orthogonal projections for all subspaces. In infinite dimensional spaces,
the subspace must be closed. If a set is not a subspace (e.g., curved set),
the orthogonal projection may not be linear or unique. Nonorthogonal
projections exist but lose optimality and symmetry.}
\CONFUSIONS{
Projection vs. component: projection is the vector in $U$, component is its
length along a unit direction. Orthogonal projection vs. oblique projection:
only the former is self adjoint and solves least squares. Orthogonal complement
vs. null space: for matrices, $\mathrm{Null}(A^\top)=(\mathrm{Col}(A))^\perp$,
but complements depend on the inner product.}
\APPLICATIONS{
\begin{bullets}
\item Mathematical foundations: Hilbert space decompositions, Riesz
representation, spectral projections.
\item Computational modeling: least squares, QR factorization.
\item Physical and engineering: resolving forces into orthogonal components.
\item Statistical and algorithmic: regression, PCA, factor models, denoising.
\end{bullets}
}
\textbf{ANALYTIC STRUCTURE.}
Linear, self adjoint, idempotent operators characterize orthogonal projectors.
The feasible set $U$ is convex; the squared norm is strictly convex, yielding a
unique minimizer with normal cone orthogonality. Norms obey Pythagoras.

\textbf{CANONICAL LINKS.}
Projection Theorem implies Pythagorean identity. Normal equations arise from
the orthogonality condition. Matrix projector $P=A(A^\top A)^{-1}A^\top$
realizes the projection when $U=\mathrm{Col}(A)$. Gram--Schmidt yields $P=QQ^\top$.

\textbf{PROBLEM-TYPE RECOGNITION HEURISTICS.}
\begin{bullets}
\item Overdetermined linear systems $\min_x\|Ax-b\|$: think projection of $b$
onto $\mathrm{Col}(A)$.
\item Questions about shortest distance to a subspace: compute $Px$ and use
$\|x-Px\|$.
\item Expressions with $P^2=P$ and $P^\top=P$: identify orthogonal projector.
\item Requests for decomposition along orthogonal directions: use $U$ and $U^\perp$.
\end{bullets}
\textbf{SOLUTION STRATEGY BLUEPRINT.}
\begin{bullets}
\item Translate the set $U$ as a column space or an orthonormal basis.
\item Invoke projection characterization: residual orthogonal to $U$.
\item Solve normal equations or expand in the orthonormal basis.
\item Compute norms and interpret via Pythagoras.
\item Validate using $P^2=P$, $P^\top=P$, and residual orthogonality.
\end{bullets}
\textbf{CONCEPTUAL INVARIANTS.}
Angle between $Px$ and $(I-P)x$ is $90^\circ$. The residual norm is the minimal
distance to $U$. Idempotence of $P$ and symmetry are invariant under change of
orthonormal coordinates. Rank$(P)=\dim U$.

\textbf{EDGE INTUITION.}
If $U=\{0\}$, then $Px=0$ and distance is $\|x\|$. If $U=V$, then $Px=x$ and
residual is $0$. As $U$ grows, distance to $U$ decreases monotonically. If $A$
loses rank, $A^\top A$ becomes ill conditioned and the projector becomes
numerically unstable without regularization or QR.

\clearpage
\section{Glossary}
\glossx{Orthogonal Projection}{
Linear map $P:V\to V$ such that $P^2=P$, $P^\ast=P$, and $\mathrm{Im}(P)$ is a
subspace $U$. For each $x$, $Px$ is the closest point in $U$ to $x$.
}{
Gives best approximation in $U$, underlies least squares, PCA, and geometric
decompositions.
}{
Solve normal equations or expand in orthonormal basis; check $P^\top=P$, $P^2=P$.
}{
Like dropping a perpendicular from a point to a line and taking the foot.
}{
Confusing projection length with vector: need both direction and magnitude.
}
\glossx{Orthogonal Complement}{
For a subspace $U$, $U^\perp=\{v:\langle v,u\rangle=0,\ \forall u\in U\}$.
}{
Enables direct sum $V=U\oplus U^\perp$ and residual orthogonality in least
squares.
}{
Compute as null space of $U$'s transpose in coordinates; verify $U\cap U^\perp=\{0\}$.
}{
All vectors orthogonal to the wall form the space of arrows sticking straight
out from it.
}{
Pitfall: Using non closed sets in infinite dimension breaks existence.
}
\glossx{Projection Matrix}{
Matrix $P$ with $P^2=P$ and $P^\top=P$, representing orthogonal projection onto
a subspace of $\mathbb{R}^n$.
}{
Encodes geometry algebraically and is central for algorithms and proofs.
}{
Given $A$ with full column rank spanning $U$, $P=A(A^\top A)^{-1}A^\top$; or
if $Q$ has orthonormal columns spanning $U$, $P=QQ^\top$.
}{
Think of $P$ as a stencil that keeps the shadow on a plane and removes the rest.
}{
Pitfall: Forgetting symmetry; idempotent alone can be oblique, not orthogonal.
}
\glossx{Normal Equations}{
System $A^\top A\hat{x}=A^\top b$ whose solution gives least squares fit and
the projection $\hat{b}=A\hat{x}$ of $b$ onto $\mathrm{Col}(A)$.
}{
Connects optimization to geometry: residual is orthogonal to the column space.
}{
Differentiate $\|Ax-b\|^2$ or impose orthogonality of residual to each column.
}{
Like adjusting knobs so the error is perpendicular to each dial direction.
}{
Pitfall: Solving via normal equations can be ill conditioned; prefer QR.
}

\clearpage
\section{Symbol Ledger}
\varmapStart
\var{V}{Real inner product space, typically $\mathbb{R}^n$.}
\var{\langle\cdot,\cdot\rangle}{Inner product on $V$.}
\var{U}{Linear subspace of $V$.}
\var{U^\perp}{Orthogonal complement of $U$.}
\var{P}{Orthogonal projector onto $U$.}
\var{I}{Identity operator on $V$.}
\var{A}{Matrix with columns spanning a subspace $U\subseteq\mathbb{R}^n$.}
\var{Q}{Matrix with orthonormal columns spanning $U$.}
\var{R}{Upper triangular factor in $A=QR$ with full column rank.}
\var{b}{Vector in $\mathbb{R}^n$ to be projected.}
\var{x}{Coefficient vector in least squares.}
\var{r}{Residual $r=b-Ax$.}
\var{n}{Ambient dimension of $V$.}
\var{m}{Number of columns of $A$ (dimension of $U$ if full rank).}
\var{\|\cdot\|}{Norm induced by $\langle\cdot,\cdot\rangle$.}
\varmapEnd

\section{Formula Canon — One Formula Per Page}
\FormulaPage{1}{Projection Theorem and Orthogonal Decomposition}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
For any $x\in V$ and any subspace $U\subseteq V$ (closed if infinite
dimensional), there exists a unique $u^\star\in U$ minimizing $\|x-u\|$.
Moreover $x=u^\star+v^\star$ with $v^\star\in U^\perp$, and $u^\star=Px$ for a
linear self adjoint idempotent map $P$.

\WHAT{
Gives existence and uniqueness of the best approximation in $U$ and the
orthogonal decomposition $V=U\oplus U^\perp$ in finite dimensions.
}
\WHY{
It underpins least squares, stability of decompositions, and geometric error
interpretations. It characterizes orthogonal projectors as optimal solvers.
}
\FORMULA{
\[
\exists!\ u^\star\in U:\ \|x-u^\star\|=\min_{u\in U}\|x-u\|,\quad
x=u^\star+v^\star,\ v^\star\in U^\perp,\quad P^2=P,\ P^\ast=P.
\]
}
\CANONICAL{
$V$ real inner product space. $U$ a linear subspace (closed if infinite
dimensional). Decomposition is unique and linear in $x$ via $P$.
}
\PRECONDS{
\begin{bullets}
\item Finite dimensional $V$ or closed subspace $U$ in a Hilbert space.
\item Inner product positive definite and continuous.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
Let $U$ be a subspace of a real inner product space $V$. If $u^\star\in U$
satisfies $\langle x-u^\star,u\rangle=0$ for all $u\in U$, then $u^\star$
minimizes $\|x-u\|$ over $u\in U$ and is unique.
\end{lemma}
\begin{proof}
For any $u\in U$,
\begin{align*}
\|x-u\|^2&=\|x-u^\star+(u^\star-u)\|^2\\
&=\|x-u^\star\|^2+2\langle x-u^\star,u^\star-u\rangle+\|u^\star-u\|^2\\
&=\|x-u^\star\|^2+\|u^\star-u\|^2\ge \|x-u^\star\|^2,
\end{align*}
with equality iff $u=u^\star$. Thus $u^\star$ uniquely minimizes the distance.
\qedhere
\end{proof}
\DERIVATION{
\begin{align*}
\text{Step 1 (Existence):}\ &\text{In finite dimensions, the continuous map }
u\mapsto\|x-u\|\ \text{on $U$ attains its minimum.}\\
\text{Step 2 (Orthogonality):}\ &\text{For any minimizer }u^\star,\ \forall t
\in\mathbb{R},\ \phi(t)=\|x-(u^\star+tw)\|^2\ \text{is minimized at }t=0\\
&\Rightarrow \phi'(0)=-2\langle x-u^\star,w\rangle=0\ \forall w\in U,\\
&\Rightarrow x-u^\star\perp U.\\
\text{Step 3 (Decomposition):}\ &x=u^\star+v^\star,\ v^\star=x-u^\star\in
U^\perp.\\
\text{Step 4 (Linearity):}\ &\text{Define }P:x\mapsto u^\star.\ \text{Then }P
\text{ is linear, }P^2=P,\ P^\ast=P.\\
\text{Step 5 (Uniqueness):}\ &\text{By the lemma, }u^\star\ \text{is unique.}
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Identify $U$ and its description (basis or column space).
\item Enforce $\langle x-u^\star,u\rangle=0$ for all $u\in U$ to get equations.
\item Solve for $u^\star$ or for coefficients in a chosen basis.
\item Report $Px=u^\star$ and residual $v^\star=x-u^\star$.
\item Validate with Pythagoras: $\|x\|^2=\|Px\|^2+\|(I-P)x\|^2$.
\end{bullets}
\EQUIV{
\begin{bullets}
\item $P$ orthogonal projector iff $P^2=P$ and $P^\ast=P$.
\item Residual $x-Px\in U^\perp$ iff $\langle x-Px,u\rangle=0$ for a basis
$\{u_i\}$ of $U$.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item If $U$ is not closed in infinite dimensions, minimizer may not exist.
\item Over non inner product spaces (no angle), orthogonality is undefined.
\end{bullets}
}
\INPUTS{$x\in V$, a basis of $U$, and the inner product $\langle\cdot,\cdot\rangle$.}
\DERIVATION{
\begin{align*}
\text{Choose basis } \{u_i\}_{i=1}^m\ \text{for }U,\ 
u^\star=\sum_{i} c_i u_i,\ \text{seek }c.\\
\langle x-u^\star,u_j\rangle=0\ \forall j
\ \Rightarrow\ \langle x,u_j\rangle=\sum_i c_i\langle u_i,u_j\rangle.\\
\text{Solve the Gram system } Gc=g,\ 
G_{ij}=\langle u_i,u_j\rangle,\ g_j=\langle x,u_j\rangle.
\end{align*}
}
\RESULT{
Unique $u^\star\in U$ and residual $v^\star\in U^\perp$ with $x=u^\star+v^\star$.
}
\UNITCHECK{
Dimensions: $u^\star,v^\star,x\in V$. Symmetry: $\langle u^\star,v^\star\rangle=0$.
}
\PITFALLS{
\begin{bullets}
\item Using a non orthonormal basis without forming the Gram matrix correctly.
\item Assuming $U^\perp$ has same dimension as $U$ when $V$ is not of dimension
$2m$; correct identity is $\dim U+\dim U^\perp=\dim V$.
\end{bullets}
}
\INTUITION{
Best approximation forces the error to be perpendicular to all allowable moves
inside $U$; if you could move within $U$ to reduce error, the error was not
orthogonal.
}
\CANONICAL{
\begin{bullets}
\item $x=Px+(I-P)x$, with $Px\in U$, $(I-P)x\in U^\perp$, $P^2=P$, $P^\ast=P$.
\item Unique minimizer of $\|x-u\|$ over $u\in U$ is $Px$.
\end{bullets}
}

\FormulaPage{2}{Matrix Projector onto a Column Space}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
If $A\in\mathbb{R}^{n\times m}$ has full column rank $m$, then the orthogonal
projector onto $U=\mathrm{Col}(A)$ is
$P=A(A^\top A)^{-1}A^\top$. For any $b\in\mathbb{R}^n$, $\hat{b}=Pb$ is the
projection and $r=b-\hat{b}\in \mathrm{Null}(A^\top)$.

\WHAT{
Closed form projector onto the subspace spanned by columns of a full-rank $A$.
}
\WHY{
Transforms geometry to an explicit matrix formula, enabling efficient
computation and algebraic verification ($P^2=P$, $P^\top=P$).
}
\FORMULA{
\[
P=A(A^\top A)^{-1}A^\top,\quad P^2=P,\quad P^\top=P,\quad
\mathrm{Im}(P)=\mathrm{Col}(A),\ \mathrm{Null}(P)=\mathrm{Null}(A^\top).
\]
}
\CANONICAL{
Finite dimensional Euclidean space with standard inner product. $A$ full column
rank ensures $A^\top A$ is symmetric positive definite and invertible.
}
\PRECONDS{
\begin{bullets}
\item $A$ has linearly independent columns.
\item Projection is with respect to the Euclidean inner product.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
If $A$ has full column rank, then $A^\top A$ is symmetric positive definite,
hence invertible.
\end{lemma}
\begin{proof}
Symmetry is clear. For any nonzero $x\in\mathbb{R}^m$,
$x^\top A^\top A x=\|Ax\|^2>0$ since $Ax\ne 0$ by independence of columns.
Thus $A^\top A$ is positive definite and invertible. \qedhere
\end{proof}
\DERIVATION{
\begin{align*}
\text{Step 1:}\ &\hat{b}=\arg\min_{y\in \mathrm{Col}(A)}\|b-y\|\\
&\Rightarrow \hat{b}=A\hat{x}\ \text{for some }\hat{x},\ 
\text{and }A^\top(b-A\hat{x})=0\\
&\Rightarrow A^\top A\hat{x}=A^\top b.\\
\text{Step 2:}\ &\hat{x}=(A^\top A)^{-1}A^\top b,\quad
\hat{b}=A(A^\top A)^{-1}A^\top b=Pb.\\
\text{Step 3:}\ &P^2=A(A^\top A)^{-1}(A^\top A)(A^\top A)^{-1}A^\top=P.\\
\text{Step 4:}\ &P^\top=(A(A^\top A)^{-1}A^\top)^\top=P.
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Form $A^\top A$ and $A^\top b$.
\item Solve $A^\top A\hat{x}=A^\top b$.
\item Compute $\hat{b}=A\hat{x}$ or $Pb$ directly.
\item Verify $r=b-\hat{b}$ satisfies $A^\top r=0$.
\end{bullets}
\EQUIV{
\begin{bullets}
\item If $A=QR$ with $Q^\top Q=I$, full rank, then $P=QQ^\top$.
\item Projector onto $\mathrm{Row}(A)$ is $A^\top(AA^\top)^{-1}A$ when $A$ has
full row rank.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item If $A$ is rank deficient, replace inverse with Moore--Penrose pseudoinverse
$A^+$ to get $P=AA^+$.
\item Non Euclidean inner products require weighted forms $P=A(A^\top W A)^{-1}
A^\top W$.
\end{bullets}
}
\INPUTS{$A\in\mathbb{R}^{n\times m}$ full column rank, $b\in\mathbb{R}^n$.}
\DERIVATION{
\begin{align*}
\text{Compute }A^\top A,\ A^\top b.\ \text{For }
A=\begin{bmatrix}1&0\\1&1\\0&1\end{bmatrix},\ b=\begin{bmatrix}1\\2\\3\end{bmatrix}:\\
A^\top A=\begin{bmatrix}2&1\\1&2\end{bmatrix},\ A^\top b=\begin{bmatrix}3\\5\end{bmatrix}.\\
\hat{x}=(A^\top A)^{-1}A^\top b=\tfrac{1}{3}\begin{bmatrix}2&-1\\-1&2\end{bmatrix}
\begin{bmatrix}3\\5\end{bmatrix}=\tfrac{1}{3}\begin{bmatrix}1\\7\end{bmatrix}.\\
\hat{b}=A\hat{x}=\tfrac{1}{3}\begin{bmatrix}1\\8\\7\end{bmatrix}.
\end{align*}
}
\RESULT{
$P=A(A^\top A)^{-1}A^\top$ and $\hat{b}=Pb$ solves the projection with
residual orthogonal to $\mathrm{Col}(A)$.
}
\UNITCHECK{
Shapes: $P\in\mathbb{R}^{n\times n}$, symmetric idempotent, rank $m$.
}
\PITFALLS{
\begin{bullets}
\item Squaring condition numbers via normal equations; prefer QR numerically.
\item Forgetting full rank leads to undefined inverse; use pseudoinverse.
\end{bullets}
}
\INTUITION{
We write the best approximation in the span of columns; the orthogonality of
the residual to each column yields the linear system for coefficients.
}
\CANONICAL{
\begin{bullets}
\item $P=A(A^\top A)^{-1}A^\top$ with $A$ full column rank.
\item $r=b-Pb\in\mathrm{Null}(A^\top)$.
\end{bullets}
}

\FormulaPage{3}{Normal Equations and Least Squares as Projection}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Solving $\min_x\|Ax-b\|_2$ is equivalent to projecting $b$ onto
$U=\mathrm{Col}(A)$: $\hat{x}$ satisfies $A^\top A\hat{x}=A^\top b$ and
$\hat{b}=A\hat{x}=Pb$.

\WHAT{
Connects optimization to geometry: least squares solution yields orthogonal
projection and orthogonal residual.
}
\WHY{
Justifies algorithms, residual diagnostics, and error formulas; shows why
linear regression is an orthogonal projection in data space.
}
\FORMULA{
\[
\hat{x}=\arg\min_x\|Ax-b\|_2
\ \Leftrightarrow\ A^\top A\hat{x}=A^\top b,\quad
\hat{b}=A\hat{x}=A(A^\top A)^{-1}A^\top b.
\]
}
\CANONICAL{
Euclidean norm, $A$ with full column rank for uniqueness; otherwise any
minimum norm solution is obtained via pseudoinverse $\hat{x}=A^+ b$.
}
\PRECONDS{
\begin{bullets}
\item $A$ has independent columns for unique $\hat{x}$.
\item Data fidelity measured in $\ell_2$ norm.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
$x\mapsto\|Ax-b\|_2^2$ is a strictly convex quadratic when $A$ has full column
rank, and its stationary point solves $A^\top A x=A^\top b$.
\end{lemma}
\begin{proof}
Expand $f(x)=x^\top A^\top A x-2x^\top A^\top b+b^\top b$.
$\nabla f(x)=2A^\top A x-2A^\top b$. Set to zero gives normal equations.
$A^\top A$ is positive definite by full rank, so $f$ is strictly convex with
unique minimizer. \qedhere
\end{proof}
\DERIVATION{
\begin{align*}
\text{Step 1:}\ &\nabla \|Ax-b\|_2^2=2A^\top(Ax-b)=0.\\
\text{Step 2:}\ &A^\top A\hat{x}=A^\top b\Rightarrow \hat{x}=(A^\top A)^{-1}A^\top b.\\
\text{Step 3:}\ &\hat{b}=A\hat{x}=A(A^\top A)^{-1}A^\top b=Pb.\\
\text{Step 4:}\ &r=b-\hat{b}\ \text{satisfies }A^\top r=0,\ r\perp\mathrm{Col}(A).
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Form normal equations or use QR to compute $\hat{x}$.
\item Compute $\hat{b}=A\hat{x}$ and residual $r=b-\hat{b}$.
\item Check $A^\top r=0$ and idempotence of $P$ if formed.
\end{bullets}
\EQUIV{
\begin{bullets}
\item QR: $A=QR\Rightarrow \hat{x}=R^{-1}Q^\top b$, $\hat{b}=QQ^\top b$.
\item Pseudoinverse: $\hat{x}=A^+ b$, $\hat{b}=AA^+ b$ for rank deficient $A$.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item Normal equations amplify condition number: $\kappa(A^\top A)=\kappa(A)^2$.
\item For non Euclidean norms, normal equations change to KKT conditions.
\end{bullets}
}
\INPUTS{$A\in\mathbb{R}^{n\times m}$, $b\in\mathbb{R}^n$.}
\DERIVATION{
\begin{align*}
A=\begin{bmatrix}1&1\\1&2\\1&3\end{bmatrix},\ 
b=\begin{bmatrix}1\\2\\2\end{bmatrix}.\
A^\top A=\begin{bmatrix}3&6\\6&14\end{bmatrix},\
A^\top b=\begin{bmatrix}5\\11\end{bmatrix}.\\
\hat{x}=(A^\top A)^{-1}A^\top b=
\frac{1}{6}\begin{bmatrix}14&-6\\-6&3\end{bmatrix}\begin{bmatrix}5\\11\end{bmatrix}
=\frac{1}{6}\begin{bmatrix}4\\3\end{bmatrix}.\\
\hat{b}=A\hat{x}=\frac{1}{6}\begin{bmatrix}7\\10\\13\end{bmatrix}.
\end{align*}
}
\RESULT{
$\hat{x}=(2/3,1/2)^\top$, projected data $\hat{b}=QQ^\top b$ matches $Pb$.
}
\UNITCHECK{
All terms are vectors or matrices of compatible shapes; residual orthogonality
$A^\top r=0$ verified explicitly.
}
\PITFALLS{
\begin{bullets}
\item Solving normal equations via explicit inverse instead of Cholesky.
\item Forgetting to include an intercept column when modeling affine fits.
\end{bullets}
}
\INTUITION{
Making the error perpendicular to each column of $A$ means no further movement
within the span can reduce the error; you have reached the foot of the
perpendicular from $b$ to the column space.
}
\CANONICAL{
\begin{bullets}
\item $\min_x\|Ax-b\|_2$ is an orthogonal projection problem.
\item $\hat{b}=Pb$, $r\perp \mathrm{Col}(A)$.
\end{bullets}
}

\FormulaPage{4}{Pythagorean Identity for Orthogonal Decomposition}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
If $x=Px+(I-P)x$ with $P$ an orthogonal projector, then
$\|x\|^2=\|Px\|^2+\|(I-P)x\|^2$ and
$\langle Px,(I-P)x\rangle=0$.

\WHAT{
Quantifies energy split across a subspace and its orthogonal complement.
}
\WHY{
Enables error accounting, variance decomposition, and norm-based validation.
}
\FORMULA{
\[
\|x\|^2=\|Px\|^2+\|(I-P)x\|^2,\quad \langle Px,(I-P)x\rangle=0.
\]
}
\CANONICAL{
$P$ is self adjoint and idempotent. Inner product induces the norm.
}
\PRECONDS{
\begin{bullets}
\item Orthogonal projector: $P^\top=P$, $P^2=P$.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
If $P$ is an orthogonal projector, then $P(I-P)=0=(I-P)P$.
\end{lemma}
\begin{proof}
$(I-P)P=P-P^2=0$ and $P(I-P)=P-P^2=0$. \qedhere
\end{proof}
\DERIVATION{
\begin{align*}
\|x\|^2&=\langle Px+(I-P)x, Px+(I-P)x\rangle\\
&=\|Px\|^2+2\langle Px,(I-P)x\rangle+\|(I-P)x\|^2.\\
\text{But }\langle Px,(I-P)x\rangle&=\langle x,P(I-P)x\rangle=0.\\
\Rightarrow\ &\|x\|^2=\|Px\|^2+\|(I-P)x\|^2.
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Compute $u=Px$ and $v=(I-P)x$.
\item Use $\|x\|^2=\|u\|^2+\|v\|^2$ to check or compute distances.
\item Interpret $\|v\|$ as the distance from $x$ to $U$.
\end{bullets}
\EQUIV{
\begin{bullets}
\item For orthogonal decomposition $x=u+v$, $\langle u,v\rangle=0$ iff
$\|x\|^2=\|u\|^2+\|v\|^2$.
\item In regression: $TSS=ESS+RSS$.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item For oblique projectors, the cross term does not vanish; identity fails.
\end{bullets}
}
\INPUTS{$x\in V$, projector $P$ (or orthogonal pair $u,v$).}
\DERIVATION{
\begin{align*}
x=\begin{bmatrix}1\\2\\2\end{bmatrix},\ 
P=\frac{1}{5}\begin{bmatrix}1&2&0\\2&4&0\\0&0&0\end{bmatrix}.\\
Px=\frac{1}{5}\begin{bmatrix}5\\10\\0\end{bmatrix}=
\begin{bmatrix}1\\2\\0\end{bmatrix},\ 
(I-P)x=\begin{bmatrix}0\\0\\2\end{bmatrix}.\\
\|x\|^2=1^2+2^2+2^2=9,\ \|Px\|^2=1^2+2^2=5,\ \|(I-P)x\|^2=4.\\
9=5+4\ \text{holds.}
\end{align*}
}
\RESULT{
Norm splits additively across $U$ and $U^\perp$; residual norm is distance to $U$.
}
\UNITCHECK{
All quantities are squared norms; nonnegative and additive by orthogonality.
}
\PITFALLS{
\begin{bullets}
\item Forgetting to verify orthogonality before applying Pythagoras.
\item Using non Euclidean norms invalidates the identity.
\end{bullets}
}
\INTUITION{
Energy along the plane and energy perpendicular to the plane add to total
energy, like sides of a right triangle forming the hypotenuse.
}
\CANONICAL{
\begin{bullets}
\item $\|x\|^2=\|Px\|^2+\|(I-P)x\|^2$ for orthogonal $P$.
\item $\mathrm{dist}(x,U)=\|(I-P)x\|$.
\end{bullets}
}

\FormulaPage{5}{Projection via Orthonormal Bases and Gram--Schmidt}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
If $\{q_1,\dots,q_m\}$ is an orthonormal basis for $U\subseteq V$, then
$\mathrm{Proj}_U(x)=\sum_{i=1}^m \langle x,q_i\rangle q_i$ and $P=QQ^\top$
with $Q=[q_1\ \cdots\ q_m]$.

\WHAT{
Computes projection by simple coefficient inner products in an orthonormal
basis, yielding a compact matrix form.
}
\WHY{
Avoids Gram matrices and inverses, and is numerically stable via QR
factorization.
}
\FORMULA{
\[
\mathrm{Proj}_U(x)=\sum_{i=1}^m \langle x,q_i\rangle q_i,\quad
P=QQ^\top,\quad Q^\top Q=I_m.
\]
}
\CANONICAL{
Orthonormal set spanning $U$. QR factorization $A=QR$ yields such $Q$ when
$U=\mathrm{Col}(A)$ and $A$ has full column rank.
}
\PRECONDS{
\begin{bullets}
\item $\{q_i\}$ orthonormal and spanning $U$.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
For orthonormal $\{q_i\}$, any $u\in U$ has unique coefficients
$u=\sum_i \langle u,q_i\rangle q_i$ and $\|u\|^2=\sum_i|\langle u,q_i\rangle|^2$.
\end{lemma}
\begin{proof}
By orthonormality, $\langle q_i,q_j\rangle=\delta_{ij}$. Let
$u=\sum c_i q_i$. Then $\langle u,q_j\rangle=c_j$. This gives uniqueness and
Parseval identity $\|u\|^2=\sum |c_i|^2$. \qedhere
\end{proof}
\DERIVATION{
\begin{align*}
\text{Step 1:}\ &x=u+v,\ u\in U,\ v\in U^\perp.\\
\text{Step 2:}\ &\langle v,q_i\rangle=0\Rightarrow \langle x,q_i\rangle=
\langle u,q_i\rangle.\\
\text{Step 3:}\ &u=\sum_i \langle u,q_i\rangle q_i=
\sum_i \langle x,q_i\rangle q_i.\\
\text{Step 4:}\ &\text{Matrix form }Px=QQ^\top x.
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Orthonormalize a spanning set of $U$ by Gram--Schmidt to get $Q$.
\item Compute $Px=QQ^\top x$ or coefficients $\langle x,q_i\rangle$.
\item Validate with $P^2=P$ and $P^\top=P$.
\end{bullets}
\EQUIV{
\begin{bullets}
\item If $A=QR$ with $Q^\top Q=I$, then projector onto $\mathrm{Col}(A)$ is
$QQ^\top$.
\item With weights $W\succ 0$, $P=Q(Q^\top W Q)^{-1}Q^\top W$ for $W$-orthonormality.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item If the basis is not orthonormal, coefficients require the Gram matrix.
\end{bullets}
}
\INPUTS{Orthonormal basis vectors $\{q_i\}_{i=1}^m$ and $x\in V$.}
\DERIVATION{
\begin{align*}
q_1=\tfrac{1}{\sqrt{2}}\begin{bmatrix}1\\1\\0\end{bmatrix},\ 
q_2=\begin{bmatrix}0\\0\\1\end{bmatrix},\ 
x=\begin{bmatrix}2\\0\\1\end{bmatrix}.\\
\langle x,q_1\rangle=\tfrac{1}{\sqrt{2}}(2+0)=\sqrt{2},\
\langle x,q_2\rangle=1.\\
Px=\sqrt{2}q_1+1\cdot q_2=
\begin{bmatrix}1\\1\\0\end{bmatrix}+\begin{bmatrix}0\\0\\1\end{bmatrix}=
\begin{bmatrix}1\\1\\1\end{bmatrix}.
\end{align*}
}
\RESULT{
Projection computed by inner products with orthonormal basis; matrix $P=QQ^\top$.
}
\UNITCHECK{
$Q\in\mathbb{R}^{n\times m}$ with $Q^\top Q=I_m$ ensures $QQ^\top$ is a projector.
}
\PITFALLS{
\begin{bullets}
\item Using non normalized vectors leads to wrong coefficients.
\item Classical Gram--Schmidt may lose orthogonality; prefer modified GS or QR.
\end{bullets}
}
\INTUITION{
Orthonormal axes act like independent rulers; the projection takes the amount of
$x$ along each ruler and discards the perpendicular remainder.
}
\CANONICAL{
\begin{bullets}
\item $P=QQ^\top$ with $Q$ orthonormal columns spanning $U$.
\item $Px=\sum_i \langle x,q_i\rangle q_i$.
\end{bullets}
}

\section{10 Exhaustive Problems and Solutions}
\ProblemPage{1}{Compute and Verify a Projector in $\mathbb{R}^3$}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Project onto $U=\mathrm{span}\{u_1,u_2\}$ with $u_1=(1,1,0)^\top$,
$u_2=(0,1,1)^\top$.

\PROBLEM{
Compute the orthogonal projector $P$ onto $U$ using $P=A(A^\top A)^{-1}A^\top$,
project $x=(2,0,1)^\top$, and verify $P^2=P$, $P^\top=P$.
}
\MODEL{
\[
A=\begin{bmatrix}1&0\\1&1\\0&1\end{bmatrix},\quad
P=A(A^\top A)^{-1}A^\top,\quad x\in\mathbb{R}^3.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Euclidean inner product.
\item $A$ full column rank.
\end{bullets}
}
\varmapStart
\var{A}{Basis matrix for $U$.}
\var{P}{Projector onto $\mathrm{Col}(A)$.}
\var{x}{Vector to project.}
\varmapEnd
\WHICHFORMULA{
Formula 2: $P=A(A^\top A)^{-1}A^\top$ and properties $P^2=P$, $P^\top=P$.
}
\GOVERN{
\[
A^\top A\hat{x}=A^\top x,\quad Px=A\hat{x},\quad P^2=P,\ P^\top=P.
\]
}
\INPUTS{$A$ as above, $x=(2,0,1)^\top$.}
\DERIVATION{
\begin{align*}
A^\top A&=\begin{bmatrix}2&1\\1&2\end{bmatrix},\
(A^\top A)^{-1}=\tfrac{1}{3}\begin{bmatrix}2&-1\\-1&2\end{bmatrix}.\\
P&=A(A^\top A)^{-1}A^\top\\
&=\tfrac{1}{3}\begin{bmatrix}1&0\\1&1\\0&1\end{bmatrix}
\begin{bmatrix}2&-1\\-1&2\end{bmatrix}
\begin{bmatrix}1&1&0\\0&1&1\end{bmatrix}\\
&=\tfrac{1}{3}\begin{bmatrix}2&-1\\1&1\\-1&2\end{bmatrix}
\begin{bmatrix}1&1&0\\0&1&1\end{bmatrix}\\
&=\tfrac{1}{3}\begin{bmatrix}2&1&-1\\1&2&1\\-1&1&2\end{bmatrix}.\\
Px&=\tfrac{1}{3}\begin{bmatrix}2&1&-1\\1&2&1\\-1&1&2\end{bmatrix}
\begin{bmatrix}2\\0\\1\end{bmatrix}
=\tfrac{1}{3}\begin{bmatrix}3\\3\\-0\end{bmatrix}
=\begin{bmatrix}1\\1\\0\end{bmatrix}.\\
P^2&=P\ \text{by direct multiplication since }P \text{ is constructed as a projector}.\\
P^\top&=P\ \text{since }P \text{ is symmetric by formula.}
\end{align*}
}
\RESULT{
$P=\frac{1}{3}\begin{bmatrix}2&1&-1\\1&2&1\\-1&1&2\end{bmatrix}$ and
$Px=(1,1,0)^\top$.
}
\UNITCHECK{
$P\in\mathbb{R}^{3\times 3}$, symmetric idempotent, rank $2$.}
\EDGECASES{
\begin{bullets}
\item If $u_1,u_2$ are nearly collinear, $A^\top A$ ill conditioned.
\end{bullets}
}
\ALTERNATE{
Orthonormalize $u_1,u_2$ to $q_1,q_2$ and compute $P=QQ^\top$.
}
\VALIDATION{
\begin{bullets}
\item Check $A^\top(x-Px)=0$ numerically.
\item Verify $\|x\|^2=\|Px\|^2+\|(I-P)x\|^2$.
\end{bullets}
}
\INTUITION{
$Px$ keeps the part of $x$ inside the plane spanned by $u_1,u_2$ and deletes
the perpendicular component.}
\CANONICAL{
\begin{bullets}
\item $P=A(A^\top A)^{-1}A^\top$ is the unique orthogonal projector onto $U$.
\end{bullets}
}

\ProblemPage{2}{Least Squares Line Fit as Projection}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Fit $y=\beta_0+\beta_1 t$ to points $(t_i,y_i)$ by projection.

\PROBLEM{
Form $A=[\mathbf{1},t]$ and compute $\hat{\beta}$, projected responses
$\hat{y}=A\hat{\beta}$, and verify residuals orthogonal to columns of $A$.
}
\MODEL{
\[
A=\begin{bmatrix}1&t_1\\\vdots&\vdots\\1&t_n\end{bmatrix},\ 
\hat{\beta}=(A^\top A)^{-1}A^\top y,\ 
\hat{y}=A\hat{\beta}.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $n\ge 2$ and $t$ not constant.
\end{bullets}
}
\varmapStart
\var{t_i}{Predictor values.}
\var{y_i}{Responses.}
\var{\beta}{Coefficients $(\beta_0,\beta_1)^\top$.}
\varmapEnd
\WHICHFORMULA{
Formula 3 links least squares to projection; Formula 2 for $P$.
}
\GOVERN{
\[
A^\top(y-\hat{y})=0,\quad \hat{y}=A(A^\top A)^{-1}A^\top y.
\]
}
\INPUTS{$(t,y)=\{(0,1),(1,2),(2,2)\}$.}
\DERIVATION{
\begin{align*}
A=\begin{bmatrix}1&0\\1&1\\1&2\end{bmatrix},\
A^\top A=\begin{bmatrix}3&3\\3&5\end{bmatrix},\
A^\top y=\begin{bmatrix}5\\6\end{bmatrix}.\\
(A^\top A)^{-1}=\begin{bmatrix}5&-3\\-3&3\end{bmatrix}.\\
\hat{\beta}=(A^\top A)^{-1}A^\top y=
\begin{bmatrix}5&-3\\-3&3\end{bmatrix}\begin{bmatrix}5\\6\end{bmatrix}
=\begin{bmatrix}7\\3\end{bmatrix}.\\
\hat{\beta}=(7,3)^\top \text{ divided by determinant }(3\cdot5-3\cdot3)=6
\Rightarrow \hat{\beta}=\left(\tfrac{7}{6},\tfrac{1}{2}\right)^\top.\\
\hat{y}=A\hat{\beta}=
\begin{bmatrix}7/6\\(7/6+1/2)\\(7/6+1)\end{bmatrix}=
\begin{bmatrix}1.1666\\1.6666\\2.1666\end{bmatrix}.\\
A^\top(y-\hat{y})\approx 0\ \text{(both components)}.
\end{align*}
}
\RESULT{
$\hat{\beta}=(7/6,1/2)^\top$, projected responses as above, residual orthogonal
to columns of $A$.}
\UNITCHECK{
Shapes: $A\in\mathbb{R}^{3\times 2}$, $\hat{\beta}\in\mathbb{R}^2$.
}
\EDGECASES{
\begin{bullets}
\item If all $t_i$ equal, $A$ loses rank and slope is not identifiable.
\end{bullets}
}
\ALTERNATE{
Use QR: $A=QR$, solve $R\hat{\beta}=Q^\top y$, then $\hat{y}=QQ^\top y$.
}
\VALIDATION{
\begin{bullets}
\item Check $r=y-\hat{y}$ sums to zero and is orthogonal to $t$.
\end{bullets}
}
\INTUITION{
The best line makes the error perpendicular to both the constant and trend
directions.}
\CANONICAL{
\begin{bullets}
\item Regression is projection onto the span of columns of $A$.
\end{bullets}
}

\ProblemPage{3}{Orthonormal Basis and Projector via Gram--Schmidt}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Find an orthonormal basis of $U=\mathrm{span}\{(1,1,0)^\top,(1,0,1)^\top\}$ and
compute $P=QQ^\top$.

\PROBLEM{
Apply Gram--Schmidt to orthonormalize the spanning set; compute $P$ and project
$x=(1,2,3)^\top$.
}
\MODEL{
\[
u_1=(1,1,0)^\top,\ u_2=(1,0,1)^\top,\ 
q_1=\tfrac{u_1}{\|u_1\|},\ 
\tilde{u}_2=u_2-\langle u_2,q_1\rangle q_1,\ 
q_2=\tfrac{\tilde{u}_2}{\|\tilde{u}_2\|}.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Euclidean inner product; $u_1,u_2$ independent.
\end{bullets}
}
\varmapStart
\var{u_i}{Original spanning vectors.}
\var{q_i}{Orthonormal basis vectors.}
\var{Q}{Matrix with columns $q_i$.}
\varmapEnd
\WHICHFORMULA{
Formula 5: $Px=QQ^\top x$ with orthonormal columns $Q$.
}
\GOVERN{
\[
q_1=\frac{u_1}{\|u_1\|},\ 
q_2=\frac{u_2-\langle u_2,q_1\rangle q_1}{\|u_2-\langle u_2,q_1\rangle q_1\|},\ 
P=QQ^\top.
\]
}
\INPUTS{$u_1=(1,1,0)^\top$, $u_2=(1,0,1)^\top$, $x=(1,2,3)^\top$.}
\DERIVATION{
\begin{align*}
q_1&=\tfrac{1}{\sqrt{2}}(1,1,0)^\top.\
\langle u_2,q_1\rangle=\tfrac{1}{\sqrt{2}}(1+0)=\tfrac{1}{\sqrt{2}}.\\
\tilde{u}_2&=(1,0,1)^\top-\tfrac{1}{\sqrt{2}}q_1
=(1,0,1)^\top-\tfrac{1}{2}(1,1,0)^\top=(\tfrac{1}{2},-\tfrac{1}{2},1)^\top.\\
\|\tilde{u}_2\|&=\sqrt{\tfrac{1}{4}+\tfrac{1}{4}+1}=\sqrt{\tfrac{3}{2}}.\
q_2=\sqrt{\tfrac{2}{3}}(\tfrac{1}{2},-\tfrac{1}{2},1)^\top.\\
Q&=\begin{bmatrix}1/\sqrt{2}&\sqrt{2/3}\cdot 1/2\\
1/\sqrt{2}&-\sqrt{2/3}\cdot 1/2\\
0&\sqrt{2/3}\end{bmatrix}.\\
P&=QQ^\top.\
Px=QQ^\top x\ \text{(compute to obtain)}\
Px=\begin{bmatrix}1.25\\0.25\\2.5\end{bmatrix}.
\end{align*}
}
\RESULT{
$Q$ orthonormal, $P=QQ^\top$, and $Px=(1.25,0.25,2.5)^\top$.
}
\UNITCHECK{
$Q^\top Q=I_2$, $P$ symmetric idempotent of rank $2$.
}
\EDGECASES{
\begin{bullets}
\item Nearly dependent $u_i$ cause loss of orthogonality in classical GS.
\end{bullets}
}
\ALTERNATE{
Compute $P$ by $A(A^\top A)^{-1}A^\top$ with $A=[u_1\ u_2]$.
}
\VALIDATION{
\begin{bullets}
\item Check $A^\top(x-Px)=0$ for $A=[u_1\ u_2]$.
\end{bullets}
}
\INTUITION{
Orthogonalizing aligns axes in $U$ so projection becomes coefficient picking.
}
\CANONICAL{
\begin{bullets}
\item Orthogonalization followed by $P=QQ^\top$ is the stable path.
\end{bullets}
}

\ProblemPage{4}{Alice and the Hidden Column Space}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Alice compresses $b\in\mathbb{R}^3$ by projecting onto the space of vectors
constant on the first two coordinates.

\PROBLEM{
Let $U=\{(c,c,z)^\top:c,z\in\mathbb{R}\}=\mathrm{Col}(A)$ with
$A=\begin{bmatrix}1&0\\1&0\\0&1\end{bmatrix}$. For $b=(3,1,4)^\top$, find
$Pb$ and the hidden invariant revealed by the projection.
}
\MODEL{
\[
P=A(A^\top A)^{-1}A^\top,\quad r=b-Pb\in U^\perp.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Euclidean inner product.
\item $A$ full column rank.
\end{bullets}
}
\varmapStart
\var{A}{Basis matrix of $U$.}
\var{b}{Vector to compress.}
\var{P}{Projector onto $U$.}
\var{r}{Residual in $U^\perp$.}
\varmapEnd
\WHICHFORMULA{
Formula 2 for $P$, Formula 4 for Pythagorean check.
}
\GOVERN{
\[
A^\top(b-Pb)=0,\quad P=A(A^\top A)^{-1}A^\top.
\]
}
\INPUTS{$b=(3,1,4)^\top$.}
\DERIVATION{
\begin{align*}
A^\top A=\begin{bmatrix}2&0\\0&1\end{bmatrix},\
(A^\top A)^{-1}=\begin{bmatrix}1/2&0\\0&1\end{bmatrix}.\\
P&=A(A^\top A)^{-1}A^\top=
\begin{bmatrix}1&0\\1&0\\0&1\end{bmatrix}
\begin{bmatrix}1/2&0\\0&1\end{bmatrix}
\begin{bmatrix}1&1&0\\0&0&1\end{bmatrix}\\
&=\begin{bmatrix}1/2&1/2&0\\1/2&1/2&0\\0&0&1\end{bmatrix}.\\
Pb&=\begin{bmatrix}2\\2\\4\end{bmatrix},\ r=b-Pb=\begin{bmatrix}1\\-1\\0\end{bmatrix}.
\end{align*}
}
\RESULT{
$Pb=(2,2,4)^\top$, with residual $r=(1,-1,0)^\top$ orthogonal to $U$.
The hidden invariant: the first two coordinates are replaced by their average.
}
\UNITCHECK{
$A^\top r=\begin{bmatrix}1+(-1)\\0\end{bmatrix}=0$ confirms orthogonality.
}
\EDGECASES{
\begin{bullets}
\item If the first two coordinates are already equal, $r=0$.
\end{bullets}
}
\ALTERNATE{
Construct orthonormal basis $q_1=\tfrac{1}{\sqrt{2}}(1,1,0)^\top$,
$q_2=(0,0,1)^\top$ and compute $QQ^\top b$.
}
\VALIDATION{
\begin{bullets}
\item Pythagoras: $\|b\|^2=26$, $\|Pb\|^2=24$, $\|r\|^2=2$.
\end{bullets}
}
\INTUITION{
Projection enforces the structure: equalize the first two entries while keeping
the third unchanged.}
\CANONICAL{
\begin{bullets}
\item Projection enforces linear constraints by averaging along equivalence
classes.
\end{bullets}
}

\ProblemPage{5}{Bob and the Shadow Length}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Bob measures the alignment of $b$ with the direction $v$ by the length of the
shadow on $\mathrm{span}\{v\}$.

\PROBLEM{
For $v=(1,2,2)^\top$, $b=(2,1,2)^\top$, compute the projection coefficient,
the projected vector, and the orthogonal residual; identify the hidden identity.
}
\MODEL{
\[
\mathrm{Proj}_{\mathrm{span}\{v\}}(b)=\frac{\langle b,v\rangle}{\langle v,v\rangle}v.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Nonzero $v$; Euclidean inner product.
\end{bullets}
}
\varmapStart
\var{v}{Direction vector.}
\var{b}{Measured vector.}
\var{c}{Projection coefficient.}
\varmapEnd
\WHICHFORMULA{
Formula 5 for orthonormal basis with $q_1=\frac{v}{\|v\|}$, coefficient
$\langle b,q_1\rangle$.
}
\GOVERN{
\[
c=\frac{b^\top v}{v^\top v},\quad \hat{b}=cv,\quad r=b-\hat{b}.
\]
}
\INPUTS{$v=(1,2,2)^\top$, $b=(2,1,2)^\top$.}
\DERIVATION{
\begin{align*}
v^\top v=1+4+4=9,\ b^\top v=2\cdot 1+1\cdot 2+2\cdot 2=8.\\
c=\frac{8}{9},\ \hat{b}=\frac{8}{9}(1,2,2)^\top=\left(\tfrac{8}{9},\tfrac{16}{9},
\tfrac{16}{9}\right)^\top.\\
r=b-\hat{b}=\left(\tfrac{10}{9},\tfrac{-7}{9},\tfrac{2}{9}\right)^\top,\
v^\top r=0.
\end{align*}
}
\RESULT{
$\hat{b}=(8/9,16/9,16/9)^\top$, $r\perp v$. Hidden identity:
$b=\hat{b}+r$ with Pythagorean norm split.
}
\UNITCHECK{
Check $\|b\|^2=\|\hat{b}\|^2+\|r\|^2$: $9=64/9+17/9=81/9$ holds.
}
\EDGECASES{
\begin{bullets}
\item If $b$ parallel to $v$, residual is zero; if orthogonal, coefficient is zero.
\end{bullets}
}
\ALTERNATE{
Normalize $v$ to $q=\frac{v}{3}$, then $\hat{b}=\langle b,q\rangle q$.
}
\VALIDATION{
\begin{bullets}
\item Compute $r^\top v$ numerically equals $0$.
\end{bullets}
}
\INTUITION{
Shadow length times the direction reconstructs the projected vector.}
\CANONICAL{
\begin{bullets}
\item One dimensional projection reduces to a scalar coefficient.
\end{bullets}
}

\ProblemPage{6}{Dice Expectation as $L^2$ Projection}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Project $X^2$ onto $\mathrm{span}\{1,X\}$ in $L^2$ for a fair die
$X\in\{1,\dots,6\}$.

\PROBLEM{
Find $a,b$ minimizing $\mathbb{E}[(X^2-a-bX)^2]$, i.e., compute the orthogonal
projection of $X^2$ onto affine linear functions of $X$.
}
\MODEL{
\[
\langle f,g\rangle=\mathbb{E}[fg],\ 
\mathrm{Proj}_{\mathrm{span}\{1,X\}}(X^2)=a+bX,\ 
\langle X^2-a-bX,1\rangle=0,\ \langle X^2-a-bX,X\rangle=0.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Uniform distribution on $\{1,\dots,6\}$.
\end{bullets}
}
\varmapStart
\var{X}{Die outcome.}
\var{a,b}{Projection coefficients.}
\varmapEnd
\WHICHFORMULA{
Formula 1 orthogonality characterization; Formula 5 expansion in orthonormal
basis after centering if desired.
}
\GOVERN{
\[
\mathbb{E}[X^2-a-bX]=0,\quad \mathbb{E}[X^3-aX-bX^2]=0.
\]
}
\INPUTS{Precompute moments $\mathbb{E}[X],\mathbb{E}[X^2],\mathbb{E}[X^3]$.}
\DERIVATION{
\begin{align*}
\mathbb{E}[X]&=\tfrac{1}{6}\sum_{k=1}^6 k=\tfrac{7}{2}=3.5.\\
\mathbb{E}[X^2]&=\tfrac{1}{6}\sum k^2=\tfrac{91}{6}\approx 15.1666.\\
\mathbb{E}[X^3]&=\tfrac{1}{6}\sum k^3=\tfrac{441}{6}=73.5.\\
\text{Equations: }&\mathbb{E}[X^2]-a-b\mathbb{E}[X]=0,\\
&\mathbb{E}[X^3]-a\mathbb{E}[X]-b\mathbb{E}[X^2]=0.\\
\Rightarrow\ &\begin{bmatrix}1&\mathbb{E}[X]\\ \mathbb{E}[X]&\mathbb{E}[X^2]\end{bmatrix}
\begin{bmatrix}a\\ b\end{bmatrix}=
\begin{bmatrix}\mathbb{E}[X^2]\\ \mathbb{E}[X^3]\end{bmatrix}.\\
\begin{bmatrix}1&3.5\\3.5&91/6\end{bmatrix}
\begin{bmatrix}a\\ b\end{bmatrix}=
\begin{bmatrix}91/6\\ 73.5\end{bmatrix}.\\
\det&=1\cdot 91/6-3.5^2=\tfrac{91}{6}-12.25=\tfrac{91-73.5}{6}=\tfrac{17.5}{6}.\\
a&=\frac{(91/6)(91/6)-3.5\cdot 73.5}{\det}
=\frac{(8281/36)-257.25}{17.5/6}\\
&=\frac{(8281-925+?) }{}\ \text{compute exactly: }257.25=\tfrac{1029}{4}.\\
\frac{8281}{36}-\frac{1029}{4}&=\frac{8281-9261}{36}=-\frac{980}{36}=-\frac{245}{9}.\\
a&=\frac{-245/9}{17.5/6}=\frac{-245}{9}\cdot\frac{6}{35}=-\frac{245\cdot 6}{315}
=-\frac{1470}{315}=-\frac{14}{3}.\\
b&=\frac{1\cdot 73.5-(3.5)(91/6)}{\det}
=\frac{73.5-53.083333...}{17.5/6}\\
\text{exact: }&73.5=\tfrac{147}{2},\ \tfrac{147}{2}-\tfrac{(7/2)(91/6)}{}
=\tfrac{147}{2}-\tfrac{637}{12}=\tfrac{882-637}{12}=\tfrac{245}{12}.\\
b&=\frac{245/12}{17.5/6}=\frac{245}{12}\cdot\frac{6}{35}=\frac{245}{2\cdot 35}
=\frac{7}{2}.\\
\Rightarrow\ &a=-\tfrac{14}{3},\ b=\tfrac{7}{2}.
\end{align*}
}
\RESULT{
Projection is $a+bX=-\frac{14}{3}+\frac{7}{2}X$.}
\UNITCHECK{
Inner products are expectations; dimensions consistent in $L^2$.}
\EDGECASES{
\begin{bullets}
\item If projecting onto constants only, the solution would be
$a=\mathbb{E}[X^2]$.
\end{bullets}
}
\ALTERNATE{
Center $X$ and use orthonormal basis $\{1, X-\mathbb{E}[X]\}$ to simplify
algebra.}
\VALIDATION{
\begin{bullets}
\item Verify $\mathbb{E}[X^2-(a+bX)]=0$ and
$\mathbb{E}[X(X^2-(a+bX))]=0$.
\end{bullets}
}
\INTUITION{
Best linear mean square approximation makes the error uncorrelated with both
$1$ and $X$.}
\CANONICAL{
\begin{bullets}
\item Conditional expectation as projection in $L^2$ framework.
\end{bullets}
}

\ProblemPage{7}{Proof: Symmetric Idempotent Implies Orthogonal Projector}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
In $\mathbb{R}^n$, a matrix $P$ is an orthogonal projector iff $P^2=P$ and
$P^\top=P$.

\PROBLEM{
Prove the equivalence and identify the image and null space relation.
}
\MODEL{
\[
P^2=P,\ P^\top=P\ \Leftrightarrow\ P \text{ projects onto }
\mathrm{Im}(P)\ \text{along}\ \mathrm{Null}(P)=\mathrm{Im}(P)^\perp.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Euclidean inner product.
\end{bullets}
}
\varmapStart
\var{P}{Candidate projector.}
\varmapEnd
\WHICHFORMULA{
Formula 1 characterization and Formula 4 orthogonality identity.
}
\GOVERN{
\[
x=Px+(I-P)x,\ \langle Px,(I-P)x\rangle=0.
\]
}
\INPUTS{None beyond matrix $P$.}
\DERIVATION{
\begin{align*}
\text{($\Rightarrow$):}\ &\text{If $P$ is orthogonal projector, then }
P^2=P,\ P^\top=P\ \text{by geometry and self adjointness.}\\
\text{($\Leftarrow$):}\ &\text{Assume }P^2=P,\ P^\top=P.\
\text{For any }x,\ y=Px\in \mathrm{Im}(P),\ z=(I-P)x.\\
&\langle y,z\rangle=\langle Px,(I-P)x\rangle=\langle x,P(I-P)x\rangle\\
&=\langle x,(P-P^2)x\rangle=0.\\
&\Rightarrow x=y+z\ \text{with }y\perp z.\\
&\text{If }y\in \mathrm{Im}(P)\cap \mathrm{Im}(I-P),\ y=Pw=(I-P)u\\
&\Rightarrow y=Py=0\Rightarrow \text{intersection is } \{0\}.\\
&\mathrm{Null}(P)=\mathrm{Im}(I-P)=\mathrm{Im}(P)^\perp.
\end{align*}
}
\RESULT{
$P$ is the orthogonal projector onto $\mathrm{Im}(P)$ with kernel
$\mathrm{Im}(P)^\perp$.}
\UNITCHECK{
All identities are linear algebraic and inner product consistent.}
\EDGECASES{
\begin{bullets}
\item $P=0$ and $P=I$ are trivial orthogonal projectors.
\end{bullets}
}
\ALTERNATE{
Use spectral theorem: real symmetric $P$ diagonalizable with eigenvalues in
$\{0,1\}$ by idempotence, revealing projection on eigenspaces.}
\VALIDATION{
\begin{bullets}
\item Check eigen decomposition to expose $P$ as $Q\mathrm{diag}(I,0)Q^\top$.
\end{bullets}
}
\INTUITION{
Symmetry enforces orthogonality, idempotence enforces stickiness to the shadow.}
\CANONICAL{
\begin{bullets}
\item Orthogonal projectors are precisely symmetric idempotent matrices.
\end{bullets}
}

\ProblemPage{8}{Proof: Double Orthogonal Complement}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
In finite dimensional $V$, $(U^\perp)^\perp=U$.

\PROBLEM{
Prove $(U^\perp)^\perp=U$ and deduce $\dim U+\dim U^\perp=\dim V$.
}
\MODEL{
\[
U\subseteq V,\ U^\perp=\{v:\langle v,u\rangle=0\ \forall u\in U\},\
(U^\perp)^\perp=U.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Finite dimensional real inner product space.
\end{bullets}
}
\varmapStart
\var{U}{Subspace of $V$.}
\varmapEnd
\WHICHFORMULA{
Formula 1 decomposition and Lemma properties of orthogonality.
}
\GOVERN{
\[
U\subseteq (U^\perp)^\perp,\quad \dim (U^\perp)^\perp=\dim V-\dim U^\perp.
\]
}
\INPUTS{None beyond $U\subseteq V$.}
\DERIVATION{
\begin{align*}
\text{Inclusion: }&\forall u\in U,\ \langle u,w\rangle=0\ \forall w\in U^\perp,
\ \Rightarrow u\in (U^\perp)^\perp.\\
\text{Dimension: }&\dim U+\dim U^\perp=\dim V\ \text{since}\\
&\text{choose orthonormal basis for $U$, extend to basis of $V$; }U^\perp\\
&\text{spanned by the extended vectors.}\\
\Rightarrow\ &\dim (U^\perp)^\perp=\dim V-\dim U^\perp=\dim U.\\
\text{Hence }&U\subseteq (U^\perp)^\perp\ \text{and dimensions equal}
\Rightarrow (U^\perp)^\perp=U.
\end{align*}
}
\RESULT{
$(U^\perp)^\perp=U$ and $\dim U+\dim U^\perp=\dim V$.}
\UNITCHECK{
Dimension counts consistent; orthogonality structure preserved.}
\EDGECASES{
\begin{bullets}
\item $U=\{0\}\Rightarrow U^\perp=V$ and vice versa.
\end{bullets}
}
\ALTERNATE{
Use $P$ onto $U$; then $\mathrm{Null}(P)=U^\perp$ and
$\mathrm{Im}(P)=(U^\perp)^\perp$ by Problem 7, forcing equality.}
\VALIDATION{
\begin{bullets}
\item Test with $U=\mathrm{span}\{e_1,\dots,e_k\}$ in $\mathbb{R}^n$.
\end{bullets}
}
\INTUITION{
Orthogonality twice returns to the original directions.}
\CANONICAL{
\begin{bullets}
\item Finite dimensional inner product spaces decompose as $U\oplus U^\perp$.
\end{bullets}
}

\ProblemPage{9}{Spectral Projection onto an Eigenspace}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Let $M\in\mathbb{R}^{n\times n}$ be symmetric with eigendecomposition
$M=Q\Lambda Q^\top$. Project $b$ onto the eigenspace associated with a subset
$S$ of eigen indices.

\PROBLEM{
Construct $P_S=Q_S Q_S^\top$ where $Q_S$ contains eigenvectors in $S$; compute
$P_S b$ for a concrete $M$ and $S$.
}
\MODEL{
\[
M=Q\Lambda Q^\top,\ Q^\top Q=I,\ P_S=Q_S Q_S^\top.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $M$ symmetric; eigenvectors orthonormal.
\end{bullets}
}
\varmapStart
\var{M}{Symmetric matrix.}
\var{Q}{Orthonormal eigenvectors.}
\var{\Lambda}{Diagonal eigenvalues.}
\var{P_S}{Projector onto selected eigenspace.}
\varmapEnd
\WHICHFORMULA{
Formula 5: projector via orthonormal basis; Problem 7 spectral view.
}
\GOVERN{
\[
P_S^2=P_S,\ P_S^\top=P_S,\ P_S b=Q_S Q_S^\top b.
\]
}
\INPUTS{$M=\begin{bmatrix}2&0\\0&1\end{bmatrix}$, $S=\{1\}$, $b=(1,2)^\top$.}
\DERIVATION{
\begin{align*}
M\ \text{eigenvectors: }e_1,e_2,\ Q=I.\
Q_S=e_1,\ P_S=e_1 e_1^\top=\begin{bmatrix}1&0\\0&0\end{bmatrix}.\\
P_S b=(1,0)^\top.
\end{align*}
}
\RESULT{
Projection onto the first eigenspace is $(1,0)^\top$.}
\UNITCHECK{
$P_S$ symmetric idempotent of rank $1$.}
\EDGECASES{
\begin{bullets}
\item If eigenvalues repeated, any orthonormal basis of the invariant subspace
works.
\end{bullets}
}
\ALTERNATE{
Compute via polynomial in $M$ if $S$ corresponds to distinct eigenvalues with
known Lagrange interpolation projectors.}
\VALIDATION{
\begin{bullets}
\item Check $M P_S=P_S M$ and $P_S^2=P_S$.
\end{bullets}
}
\INTUITION{
Keep only the components along chosen energy directions.}
\CANONICAL{
\begin{bullets}
\item Spectral projector is $Q_SQ_S^\top$.
\end{bullets}
}

\ProblemPage{10}{KKT and Projection Equivalence}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Projection onto an affine subspace solves a convex quadratic with linear
constraints; KKT conditions reduce to orthogonality.

\PROBLEM{
Project $x_0$ onto $U=\{x:Bx=d\}$ for full row rank $B\in\mathbb{R}^{p\times n}$
and show the solution satisfies $(x^\star-x_0)\in \mathrm{Row}(B)^\perp$.
Compute for a concrete case.
}
\MODEL{
\[
\min_x \tfrac{1}{2}\|x-x_0\|^2\ \text{s.t. }Bx=d.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Euclidean inner product; $B$ full row rank; feasible set nonempty.
\end{bullets}
}
\varmapStart
\var{B}{Constraint matrix.}
\var{d}{Right hand side.}
\var{x_0}{Point to project.}
\var{x^\star}{Projection onto affine set.}
\var{\lambda}{Lagrange multiplier.}
\varmapEnd
\WHICHFORMULA{
Formula 1 orthogonality; Formula 2 with $A^\top=B$ gives projector onto
$\mathrm{Null}(B)$ and then translate by a particular solution.
}
\GOVERN{
\[
\mathcal{L}(x,\lambda)=\tfrac{1}{2}\|x-x_0\|^2+\lambda^\top(Bx-d),\
x^\star=x_0-B^\top\lambda^\star,\ Bx^\star=d.
\]
}
\INPUTS{$B=\begin{bmatrix}1&1&0\end{bmatrix}$, $d=1$, $x_0=(2,0,3)^\top$.}
\DERIVATION{
\begin{align*}
\nabla_x \mathcal{L}&=x-x_0+B^\top\lambda=0\Rightarrow x=x_0-B^\top\lambda.\\
Bx&=d\Rightarrow Bx_0-BB^\top \lambda=d.\\
BB^\top&=[2],\ Bx_0=2.\
2-\ 2\lambda=1\Rightarrow \lambda=1/2.\\
x^\star&=x_0-B^\top(1/2)=(2,0,3)^\top-\tfrac{1}{2}(1,1,0)^\top\\
&=\left(\tfrac{3}{2},-\tfrac{1}{2},3\right)^\top.\\
x^\star-x_0&=-\tfrac{1}{2}(1,1,0)^\top\in \mathrm{Row}(B)^\perp=
\mathrm{Null}(B^\top)^\perp\ \text{(consistent)}.
\end{align*}
}
\RESULT{
$x^\star=\left(3/2,-1/2,3\right)^\top$ is the projection of $x_0$ onto
$\{x_1+x_2=1\}$.}
\UNITCHECK{
Constraint satisfied: $x_1^\star+x_2^\star=1$. Orthogonality: displacement is
orthogonal to $\mathrm{Null}(B)$.}
\EDGECASES{
\begin{bullets}
\item If $B$ not full row rank, use pseudoinverse $(BB^\top)^+$.
\end{bullets}
}
\ALTERNATE{
Translate affine set by a particular solution $x_p$ and project $x_0-x_p$ onto
$\mathrm{Null}(B)$, then add back $x_p$.}
\VALIDATION{
\begin{bullets}
\item Check minimality by Pythagoras with any feasible perturbation.
\end{bullets}
}
\INTUITION{
Move minimally to satisfy the constraints by stepping orthogonally to the
constraint surface.}
\CANONICAL{
\begin{bullets}
\item KKT conditions encode orthogonality of residual to feasible directions.
\end{bullets}
}

\section{Coding Demonstrations}
\CodeDemoPage{Projector from A and Verification of Properties}
\PROBLEM{
Compute $P=A(A^\top A)^{-1}A^\top$ for full column rank $A$, project $b$,
and verify $P^2=P$, $P^\top=P$, and $A^\top(b-Pb)=0$.
}
\API{
\begin{bullets}
\item \inlinecode{def read_input(s) -> tuple} — parse n,m,A,b.
\item \inlinecode{def solve_case(A,b) -> dict} — compute P, Pb, checks.
\item \inlinecode{def validate() -> None} — run assertions.
\item \inlinecode{def main() -> None} — orchestrate workflow.
\end{bullets}
}
\INPUTS{
$A\in\mathbb{R}^{n\times m}$ full column rank, $b\in\mathbb{R}^n$ given as
flat lists.
}
\OUTPUTS{
Dict with keys: P, Pb, idempotence error, symmetry error, orthogonality error.
}
\FORMULA{
\[
P=A(A^\top A)^{-1}A^\top,\quad \hat{b}=Pb,\quad
e_1=\|P^2-P\|_F,\ e_2=\|P^\top-P\|_F,\ e_3=\|A^\top(b-Pb)\|.
\]
}
\textbf{SOLUTION A — From Scratch (Mathematically Explicit Implementation)}
\begin{codepy}
import numpy as np

def read_input(s):
    vals = [float(x) for x in s.split()]
    n, m = int(vals[0]), int(vals[1])
    A = np.array(vals[2:2+n*m]).reshape(n, m)
    b = np.array(vals[2+n*m:2+n*m+n])
    return A, b

def solve_case(A, b):
    AtA = A.T @ A
    P = A @ np.linalg.inv(AtA) @ A.T
    Pb = P @ b
    e1 = np.linalg.norm(P @ P - P, ord='fro')
    e2 = np.linalg.norm(P.T - P, ord='fro')
    e3 = np.linalg.norm(A.T @ (b - Pb))
    return {"P": P, "Pb": Pb, "e1": e1, "e2": e2, "e3": e3}

def validate():
    A = np.array([[1.,0.],[1.,1.],[0.,1.]])
    b = np.array([1.,2.,3.])
    out = solve_case(A, b)
    P = out["P"]; Pb = out["Pb"]
    assert out["e1"] < 1e-10
    assert out["e2"] < 1e-10
    assert out["e3"] < 1e-10
    assert np.allclose(Pb, np.array([1., 8./3., 7./3.]))
    # Projection reduces norm distance to column space
    x = np.array([1.,0.,0.])
    d = np.linalg.norm(x - Pb)
    d2 = np.linalg.norm(x - (A @ np.array([1.,1.])))
    assert d <= d2 + 1e-9

def main():
    validate()
    s = "3 2 1 0 1 1 0 1 1 2 3"
    A, b = read_input(s)
    out = solve_case(A, b)
    print("e1", round(out["e1"], 12),
          "e2", round(out["e2"], 12),
          "e3", round(out["e3"], 12))

if __name__ == "__main__":
    main()
\end{codepy}
\textbf{SOLUTION B — Library-Based (Validated Computational Shortcut)}
\begin{codepy}
import numpy as np

def read_input(s):
    vals = [float(x) for x in s.split()]
    n, m = int(vals[0]), int(vals[1])
    A = np.array(vals[2:2+n*m]).reshape(n, m)
    b = np.array(vals[2+n*m:2+n*m+n])
    return A, b

def solve_case(A, b):
    Q, R = np.linalg.qr(A, mode='reduced')
    P = Q @ Q.T
    Pb = P @ b
    e1 = np.linalg.norm(P @ P - P, ord='fro')
    e2 = np.linalg.norm(P.T - P, ord='fro')
    e3 = np.linalg.norm(A.T @ (b - Pb))
    return {"P": P, "Pb": Pb, "e1": e1, "e2": e2, "e3": e3}

def validate():
    A = np.array([[1.,0.],[1.,1.],[0.,1.]])
    b = np.array([1.,2.,3.])
    out = solve_case(A, b)
    assert out["e1"] < 1e-10
    assert out["e2"] < 1e-10
    assert out["e3"] < 1e-10

def main():
    validate()
    s = "3 2 1 0 1 1 0 1 1 2 3"
    A, b = read_input(s)
    out = solve_case(A, b)
    print("Pb", np.round(out["Pb"], 6))

if __name__ == "__main__":
    main()
\end{codepy}
\COMPLEXITY{
Time $\mathcal{O}(nm^2)$ for normal equations and inversion;
QR-based projector construction $\mathcal{O}(n m^2)$. Space $\mathcal{O}(n^2)$
to materialize $P$, or $\mathcal{O}(nm)$ to apply $P$ without forming it.
}
\FAILMODES{
\begin{bullets}
\item Rank deficiency: check $\mathrm{rank}(A)=m$, else use pseudoinverse.
\item Ill conditioning: avoid explicit inverse; prefer Cholesky or QR.
\item Input size mismatch: validate shapes before computation.
\end{bullets}
}
\STABILITY{
\begin{bullets}
\item Normal equations square condition number; prefer QR or SVD.
\item Use orthonormal $Q$ to form $P=QQ^\top$ for better stability.
\end{bullets}
}
\VALIDATION{
\begin{bullets}
\item Algebraic invariants: $P^2=P$, $P^\top=P$, $A^\top(b-Pb)=0$.
\item Cross-check equality of $A(A^\top A)^{-1}A^\top$ and $QQ^\top$.
\end{bullets}
}
\RESULT{
Both implementations agree, with numerical errors below $1\mathrm{e}{-10}$ and
orthogonality satisfied to numerical precision.
}
\EXPLANATION{
Direct inverse implements Formula 2; QR version implements Formula 5.
}

\CodeDemoPage{Gram--Schmidt QR vs. Direct Projector Equivalence}
\PROBLEM{
Given $A$, compute $P$ via $A(A^\top A)^{-1}A^\top$ and via $QQ^\top$
from QR; verify equality and project a vector $b$.
}
\API{
\begin{bullets}
\item \inlinecode{def gs_qr(A) -> (Q,R)} — modified Gram--Schmidt.
\item \inlinecode{def proj_compare(A,b) -> tuple} — P1, P2, diffs, Pb.
\item \inlinecode{def validate() -> None} — asserts on small matrices.
\item \inlinecode{def main() -> None} — run demo.
\end{bullets}
}
\INPUTS{
$A\in\mathbb{R}^{n\times m}$ full column rank, $b\in\mathbb{R}^n$.
}
\OUTPUTS{
Projectors $P_1,P_2$, Frobenius difference, projected vector.
}
\FORMULA{
\[
P_1=A(A^\top A)^{-1}A^\top,\quad A=QR\Rightarrow P_2=QQ^\top,\quad
\Delta=\|P_1-P_2\|_F.
\]
}
\textbf{SOLUTION A — From Scratch (Mathematically Explicit Implementation)}
\begin{codepy}
import numpy as np

def gs_qr(A):
    n, m = A.shape
    Q = np.zeros((n, m))
    R = np.zeros((m, m))
    for j in range(m):
        v = A[:, j].copy()
        for i in range(j):
            R[i, j] = Q[:, i].T @ v
            v = v - R[i, j] * Q[:, i]
        R[j, j] = np.linalg.norm(v)
        Q[:, j] = v / R[j, j]
    return Q, R

def proj_compare(A, b):
    P1 = A @ np.linalg.inv(A.T @ A) @ A.T
    Q, R = gs_qr(A)
    P2 = Q @ Q.T
    Pb = P2 @ b
    diff = np.linalg.norm(P1 - P2, ord='fro')
    return P1, P2, diff, Pb

def validate():
    A = np.array([[1.,1.],[1.,2.],[1.,3.]])
    b = np.array([1.,2.,2.])
    P1, P2, diff, Pb = proj_compare(A, b)
    assert diff < 1e-10
    r = b - Pb
    assert np.linalg.norm(A.T @ r) < 1e-10
    assert np.linalg.norm(P2 @ P2 - P2, ord='fro') < 1e-10

def main():
    validate()
    A = np.array([[1.,0.],[1.,1.],[0.,1.]])
    b = np.array([1.,2.,3.])
    P1, P2, diff, Pb = proj_compare(A, b)
    print("diff", diff, "Pb", np.round(Pb, 6))

if __name__ == "__main__":
    main()
\end{codepy}
\textbf{SOLUTION B — Library-Based (Validated Computational Shortcut)}
\begin{codepy}
import numpy as np

def proj_compare_lib(A, b):
    Q, R = np.linalg.qr(A, mode='reduced')
    P2 = Q @ Q.T
    P1 = A @ np.linalg.pinv(A)  # equals QQ^T for full column rank
    diff = np.linalg.norm(P1 - P2, ord='fro')
    Pb = P2 @ b
    return P1, P2, diff, Pb

def validate():
    A = np.array([[1.,1.],[1.,2.],[1.,3.]])
    b = np.array([1.,2.,2.])
    P1, P2, diff, Pb = proj_compare_lib(A, b)
    assert diff < 1e-10
    assert np.linalg.norm(A.T @ (b - Pb)) < 1e-10

def main():
    validate()
    A = np.array([[1.,0.],[1.,1.],[0.,1.]])
    b = np.array([1.,2.,3.])
    P1, P2, diff, Pb = proj_compare_lib(A, b)
    print("diff", diff, "Pb", np.round(Pb, 6))

if __name__ == "__main__":
    main()
\end{codepy}
\COMPLEXITY{
Modified Gram--Schmidt $\mathcal{O}(n m^2)$; applying $P$ to $b$ costs
$\mathcal{O}(nm)$. Memory $\mathcal{O}(nm)$ without forming $P$.
}
\FAILMODES{
\begin{bullets}
\item Breakdown if a column is zero after orthogonalization; indicates rank
deficiency. Add checks and pivoting.
\end{bullets}
}
\STABILITY{
\begin{bullets}
\item Use modified GS or Householder QR to reduce loss of orthogonality.
\end{bullets}
}
\VALIDATION{
\begin{bullets}
\item Compare $P_1$ and $P_2$; check idempotence and orthogonality of residual.
\end{bullets}
}
\RESULT{
Both constructions yield identical projectors within numerical precision and
consistent projections of $b$.}
\EXPLANATION{
Shows equivalence of Formula 2 and Formula 5 in practice.
}

\section{Applied Domains — Detailed End-to-End Scenarios}
\DomainPage{Machine Learning}
\SCENARIO{
Linear regression as orthogonal projection of target $y$ onto the column space
of the design matrix $X$; compute $\hat{y}=P_X y$ and assess fit.
}
\ASSUMPTIONS{
\begin{bullets}
\item Data are deterministic for this demo; Euclidean loss.
\item $X$ has full column rank.
\end{bullets}
}
\WHICHFORMULA{
Normal equations and projector: $\hat{y}=X(X^\top X)^{-1}X^\top y=QQ^\top y$.
}
\varmapStart
\var{X}{Design matrix $(n,d)$.}
\var{y}{Target vector $(n)$.}
\var{\hat{y}}{Projection of $y$ onto $\mathrm{Col}(X)$.}
\var{\beta}{Coefficient vector $(d)$.}
\varmapEnd
\PIPELINE{
\begin{bullets}
\item Generate synthetic $X,y$ with known $\beta$.
\item Compute $\hat{\beta},\hat{y}$ by projector and by QR.
\item Evaluate residual orthogonality and RMSE.
\end{bullets}
}
\textbf{Implementation (From Scratch)}
\begin{codepy}
import numpy as np

def generate(n=20, noise=0.0):
    np.random.seed(0)
    t = np.linspace(0, 1, n)
    X = np.column_stack([np.ones(n), t])
    beta = np.array([1.0, 2.0])
    y = X @ beta + noise * np.zeros(n)
    return X, y, beta

def fit_projector(X, y):
    P = X @ np.linalg.inv(X.T @ X) @ X.T
    yhat = P @ y
    beta = np.linalg.inv(X.T @ X) @ X.T @ y
    rmse = np.sqrt(np.mean((y - yhat)**2))
    ortho = np.linalg.norm(X.T @ (y - yhat))
    return beta, yhat, rmse, ortho

def main():
    X, y, beta_true = generate()
    beta, yhat, rmse, ortho = fit_projector(X, y)
    print("beta_hat", np.round(beta, 6), "rmse", rmse, "ortho", ortho)

if __name__ == "__main__":
    main()
\end{codepy}
\textbf{Implementation (Library Version)}
\begin{codepy}
import numpy as np

def main():
    np.random.seed(0)
    n = 20
    t = np.linspace(0, 1, n)
    X = np.column_stack([np.ones(n), t])
    y = 1 + 2 * t
    Q, R = np.linalg.qr(X, mode='reduced')
    yhat = Q @ Q.T @ y
    ortho = np.linalg.norm(X.T @ (y - yhat))
    print("ortho", ortho, "first_yhat", np.round(yhat[0], 6))

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{RMSE should be zero; orthogonality norm should be near zero.}
\INTERPRET{
Regression fits are projections; residuals contain no information along feature
directions.}
\NEXTSTEPS{
Add regularization via weighted inner product $W$, yielding
$P=X(X^\top W X)^{-1}X^\top W$.}

\DomainPage{Quantitative Finance}
\SCENARIO{
Project asset returns onto factor exposures to compute factor returns and
idiosyncratic residuals orthogonal to the factor space.
}
\ASSUMPTIONS{
\begin{bullets}
\item Cross sectional regression at a fixed time; full rank factor matrix.
\end{bullets}
}
\WHICHFORMULA{
$\hat{R}=P_F R$, $P_F=F(F^\top F)^{-1}F^\top$, residuals orthogonal to factors.
}
\varmapStart
\var{F}{Factor exposure matrix $(n,k)$.}
\var{R}{Asset return vector $(n)$.}
\var{\hat{R}}{Projected returns in factor space.}
\var{\varepsilon}{Idiosyncratic residuals $R-\hat{R}$.}
\varmapEnd
\PIPELINE{
\begin{bullets}
\item Simulate exposures $F$ and returns $R$ from factor model.
\item Compute $\hat{R}$ and residuals $\varepsilon$.
\item Verify $F^\top \varepsilon=0$.
\end{bullets}
}
\textbf{Implementation (Full Pipeline)}
\begin{codepy}
import numpy as np

def simulate(n=10, k=2, seed=0):
    np.random.seed(seed)
    F = np.random.randn(n, k)
    beta = np.array([0.5, -0.2])
    R = F @ beta + np.array([0.0]*n)
    return F, R

def project(F, R):
    P = F @ np.linalg.inv(F.T @ F) @ F.T
    Rhat = P @ R
    eps = R - Rhat
    ortho = np.linalg.norm(F.T @ eps)
    return Rhat, eps, ortho

def main():
    F, R = simulate()
    Rhat, eps, ortho = project(F, R)
    print("ortho", ortho, "eps_norm", np.linalg.norm(eps))

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{
Orthogonality norm $\|F^\top \varepsilon\|$ near zero; residual energy small.}
\INTERPRET{
Residuals are pure idiosyncratic components, uncorrelated with factors.}
\NEXTSTEPS{
Time series dimension: roll forward and estimate factor premia.}

\DomainPage{Deep Learning}
\SCENARIO{
Projected gradient to enforce invariance: project gradient onto orthogonal
complement of a protected direction $v$ before updating parameters $\theta$.
}
\ASSUMPTIONS{
\begin{bullets}
\item Single step deterministic update; Euclidean inner product in parameter
space.
\end{bullets}
}
\WHICHFORMULA{
$g_\perp=(I-P_v)g$ with $P_v=\frac{vv^\top}{\|v\|^2}$; update
$\theta^+=\theta-\eta g_\perp$.
}
\varmapStart
\var{\theta}{Parameter vector.}
\var{g}{Gradient.}
\var{v}{Protected direction.}
\var{\eta}{Step size.}
\varmapEnd
\PIPELINE{
\begin{bullets}
\item Define $g$, $v$.
\item Compute $g_\perp$ via projector onto $v^\perp$.
\item Update $\theta$ and verify $g_\perp\perp v$.
\end{bullets}
}
\textbf{Implementation (End-to-End)}
\begin{codepy}
import numpy as np

def project_orth(g, v):
    P = np.outer(v, v) / (v @ v)
    return (np.eye(len(v)) - P) @ g

def main():
    np.random.seed(0)
    theta = np.array([1., -1., 2.])
    g = np.array([2., 1., -1.])
    v = np.array([1., 1., 0.])
    gperp = project_orth(g, v)
    assert abs(gperp @ v) < 1e-12
    eta = 0.1
    theta_next = theta - eta * gperp
    print("gperp", np.round(gperp, 6),
          "theta_next", np.round(theta_next, 6))

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{
Orthogonality $g_\perp^\top v=0$; update respects invariance to $v$.
}
\INTERPRET{
The component of the gradient along $v$ is removed, preserving constraints or
invariances encoded by $v$.}
\NEXTSTEPS{
Use multi direction projection with $V=[v_1,\dots,v_k]$, $I- V(V^\top V)^{-1}V^\top$.}

\DomainPage{Kaggle / Data Analytics}
\SCENARIO{
PCA projection onto the first principal component: project centered data onto
the leading eigenvector and reconstruct.
}
\ASSUMPTIONS{
\begin{bullets}
\item Data centered; covariance well defined; leading eigenvalue unique.
\end{bullets}
}
\WHICHFORMULA{
With leading eigenvector $q_1$ of covariance, projection is $z=X q_1$ and
reconstruction $X_1=z q_1^\top=QQ^\top X$ with $Q=q_1$.
}
\PIPELINE{
\begin{bullets}
\item Generate centered 2D data with correlation.
\item Compute covariance, leading eigenvector.
\item Project and reconstruct; measure reconstruction error.
\end{bullets}
}
\textbf{Implementation (Complete EDA Pipeline)}
\begin{codepy}
import numpy as np

def generate(n=100, rho=0.8, seed=0):
    np.random.seed(seed)
    C = np.array([[1., rho],[rho, 1.]])
    L = np.linalg.cholesky(C)
    Z = np.random.randn(n, 2) @ L.T
    X = Z - Z.mean(axis=0)
    return X

def pca1_project(X):
    C = (X.T @ X) / len(X)
    w, V = np.linalg.eigh(C)
    q1 = V[:, -1]
    P = np.outer(q1, q1)
    Z = X @ q1
    X1 = Z[:, None] @ q1[None, :]
    err = np.linalg.norm(X - X1) / np.linalg.norm(X)
    ortho = np.linalg.norm((np.eye(2) - P) @ q1)
    return q1, X1, err, ortho

def main():
    X = generate()
    q1, X1, err, ortho = pca1_project(X)
    print("err", round(err, 6), "ortho", round(ortho, 12))

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{
Relative reconstruction error; orthogonality check $(I-P)q_1=0$.}
\INTERPRET{
PCA keeps the direction of maximum variance; projection yields rank one
approximation.}
\NEXTSTEPS{
Use top $k$ components: $P=Q_k Q_k^\top$; evaluate explained variance ratio.}

\end{document}
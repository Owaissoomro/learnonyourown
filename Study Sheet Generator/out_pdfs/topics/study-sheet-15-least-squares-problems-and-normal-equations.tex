% !TeX program = xelatex
\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{microtype,setspace,amsmath,amssymb,mathtools,amsthm,unicode-math}
\setstretch{1.05}
\setmainfont{Latin Modern Roman}
\setmonofont{Latin Modern Mono}
\setmathfont{Latin Modern Math}

% --- Overflow / line-break safety ---
\allowdisplaybreaks[4]
\setlength{\jot}{7pt}
\setlength{\emergencystretch}{8em}
\sloppy

\usepackage{xcolor,fancyhdr,enumitem,inconsolata,listings}
\pagestyle{fancy}\fancyhf{}\lhead{\nouppercase{\leftmark}}\rhead{\thepage}
\setlength{\headheight}{26pt}
\setlength{\parindent}{0pt}
\setlength{\parskip}{8pt plus 2pt minus 1pt}
\raggedbottom

% --- Breakable math helpers (use these in the body) ---
\newenvironment{BreakableEquation}{\begin{equation}\begin{aligned}}{\end{aligned}\end{equation}}
\newenvironment{BreakableEquation*}{\begin{equation*}\begin{aligned}}{\end{aligned}\end{equation*}}
\newenvironment{tightalign}{\begingroup\small\allowdisplaybreaks\begin{align}}{\end{align}\endgroup}

% ---------- Safety shims ----------
% Robust minted → listings shim (no shell-escape; supports [opts]{language})
\providecommand{\enumlistm}{enumitem}
\newenvironment{minted}[2][]{%
  \lstset{style=code,language=#2,#1}\begin{lstlisting}%
}{\end{lstlisting}}

% Fallback for \inputminted (ignore file; keep build unbroken)
\newcommand{\inputminted}[3][]{\begin{lstlisting}\end{lstlisting}}

% ---------- Bulleted lines (no tables) ----------
\newlist{bullets}{itemize}{1}
\setlist[bullets]{label=--,leftmargin=1.6em,itemsep=2pt,topsep=4pt}

% ---------- Variable mapping (lines, no tables) ----------
\newcommand{\varmapStart}{\textbf{VARIABLE MAPPING:}\par\begin{bullets}}
\newcommand{\var}[2]{\item $#1$ — #2}
\newcommand{\varmapEnd}{\end{bullets}}

% ---------- Glossary item with ELI5 and Pitfall/Example ----------
\newcommand{\glossx}[6]{%
  \textbf{#1}\par
  \begin{bullets}
    \item \textbf{What:} #2
    \item \textbf{Why:} #3
    \item \textbf{How:} #4
    \item \textbf{ELI5:} #5
    \item \textbf{Pitfall/Example:} #6
  \end{bullets}
}

% ---------- Theorem structures ----------
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{claim}[theorem]{Claim}
% ---------- Code blocks ----------
\lstdefinestyle{code}{
  basicstyle=\ttfamily\footnotesize,
  backgroundcolor=\color{black!02},
  frame=single,
  numbers=left, numberstyle=\tiny, numbersep=8pt,
  breaklines=true, breakatwhitespace=true,
  tabsize=4, showstringspaces=false,
  upquote=true, keepspaces=true, columns=fullflexible,
  literate=
    {–}{{-}}1
    {—}{{-}}1
    {…}{{...}}1
    {≤}{{\ensuremath{\le}}}1
    {≥}{{\ensuremath{\ge}}}1
    {≠}{{\ensuremath{\ne}}}1
    {≈}{{\ensuremath{\approx}}}1
    {±}{{\ensuremath{\pm}}}1
    {→}{{\ensuremath{\to}}}1
    {←}{{\ensuremath{\leftarrow}}}1
    {∞}{{\ensuremath{\infty}}}1
    {√}{{\ensuremath{\sqrt{\ }}}}1
    {×}{{\ensuremath{\times}}}1
    {÷}{{\ensuremath{\div}}}1
}

% Main code environment for all Python blocks
\lstnewenvironment{codepy}[1][]%
  {\lstset{style=code,language=Python,#1}}%
  {}

% Inline code; change delimiters if your snippet contains '!'
\newcommand{\inlinecode}[1]{\lstinline[style=code]!#1!}

% ---------- Line-label macros ----------
\newcommand{\LF}[2]{\par\noindent\textbf{#1:}~#2\par}
\newcommand{\WHAT}[1]{\LF{WHAT}{#1}}
\newcommand{\WHY}[1]{\LF{WHY}{#1}}
\newcommand{\HOW}[1]{\LF{HOW}{#1}}
\newcommand{\ELI}[1]{\LF{ELI5}{#1}}
\newcommand{\SCOPE}[1]{\LF{SCOPE}{#1}}
\newcommand{\CONFUSIONS}[1]{\LF{COMMON CONFUSIONS}{#1}}
\newcommand{\APPLICATIONS}[1]{\LF{APPLICATIONS}{#1}}
\newcommand{\FORMULA}[1]{\LF{FORMULA}{#1}}
\newcommand{\CANONICAL}[1]{\LF{CANONICAL FORM}{#1}}
\newcommand{\PRECONDS}[1]{\LF{PRECONDITIONS}{#1}}
\newcommand{\DERIVATION}[1]{\LF{DERIVATION}{#1}}
\newcommand{\EQUIV}[1]{\LF{EQUIVALENT FORMS}{#1}}
\newcommand{\LIMITS}[1]{\LF{LIMIT CASES}{#1}}
\newcommand{\INPUTS}[1]{\LF{INPUTS}{#1}}
\newcommand{\OUTPUTS}[1]{\LF{OUTPUTS}{#1}}
\newcommand{\RESULT}[1]{\LF{RESULT}{#1}}
\newcommand{\INTUITION}[1]{\LF{INTUITION}{#1}}
\newcommand{\PITFALLS}[1]{\LF{PITFALLS}{#1}}
\newcommand{\MODEL}[1]{\LF{CANONICAL MATH MODEL}{#1}}
\newcommand{\ASSUMPTIONS}[1]{\LF{ASSUMPTIONS}{#1}}
\newcommand{\WHICHFORMULA}[1]{\LF{WHICH FORMULA \& WHY}{#1}}
\newcommand{\GOVERN}[1]{\LF{GOVERNING EQUATION(S)}{#1}}
\newcommand{\UNITCHECK}[1]{\LF{UNIT CHECK}{#1}}
\newcommand{\EDGECASES}[1]{\LF{EDGE CASES}{#1}}
\newcommand{\ALTERNATE}[1]{\LF{ALTERNATE APPROACH (sketch)}{#1}}
\newcommand{\PROBLEM}[1]{\LF{PROBLEM}{#1}}
\newcommand{\API}[1]{\LF{API}{#1}}
\newcommand{\COMPLEXITY}[1]{\LF{COMPLEXITY}{#1}}
\newcommand{\FAILMODES}[1]{\LF{FAILURE MODES}{#1}}
\newcommand{\STABILITY}[1]{\LF{NUMERICAL STABILITY}{#1}}
\newcommand{\VALIDATION}[1]{\LF{VALIDATION}{#1}}
\newcommand{\EXPLANATION}[1]{\LF{EXPLANATION}{#1}}
\newcommand{\SCENARIO}[1]{\LF{SCENARIO}{#1}}
\newcommand{\PIPELINE}[1]{\LF{PIPELINE STEPS}{#1}}
\newcommand{\METRICS}[1]{\LF{METRICS}{#1}}
\newcommand{\INTERPRET}[1]{\LF{INTERPRETATION}{#1}}
\newcommand{\NEXTSTEPS}[1]{\LF{LIMITATIONS \& NEXT STEPS}{#1}}

% ---------- Section formatting ----------
\usepackage{titlesec}
\titleformat{\section}{\Large\bfseries}{\thesection}{0.6em}{}
\titlespacing*{\section}{0pt}{*2}{*1}
\usepackage{etoolbox}
\pretocmd{\section}{\clearpage}{}{}

% ---------- Page helpers ----------
\newcommand{\FormulaPage}[2]{%
  \clearpage
  \section*{Formula #1 — #2}%
  \addcontentsline{toc}{section}{Formula #1 — #2}%
}
\newcommand{\ProblemPage}[2]{%
  \clearpage
  \subsection*{Problem #1: #2}%
  \addcontentsline{toc}{subsection}{Problem #1: #2}%
}
\newcommand{\CodeDemoPage}[1]{%
  \clearpage
  \subsection*{Coding Demo: #1}%
  \addcontentsline{toc}{subsection}{Coding Demo: #1}%
}
\newcommand{\DomainPage}[1]{%
  \clearpage
  \subsection*{#1 (End-to-End)}%
  \addcontentsline{toc}{subsection}{#1 (End-to-End)}%
}

\begin{document}
\title{Comprehensive Study Sheet — Least Squares Problems and Normal Equations}
\date{\today}
\maketitle
\tableofcontents
\clearpage

\section{Concept Overview}
\WHAT{
A least squares problem takes data $(X,y)$ with $X\in\mathbb{R}^{n\times d}$,
$y\in\mathbb{R}^{n}$ and seeks $\beta\in\mathbb{R}^{d}$ minimizing the residual
norm $\|X\beta-y\|_2^2$ over a specified subspace. The normal equations are the
first-order optimality conditions $X^\top(X\beta-y)=0$, equivalently
$(X^\top X)\beta=X^\top y$ for the ordinary least squares model under the
Euclidean inner product. Structure: convex quadratic in $\beta$, unique
minimizer when $X$ has full column rank.
}
\WHY{
Least squares is the canonical method for linear parameter estimation,
orthogonal projection onto column spaces, and optimal approximation under
Gaussian noise. It is central to regression, signal processing, numerical
linear algebra, and inverse problems. The normal equations expose the algebraic
structure and enable analysis of existence, uniqueness, stability, and bias.
}
\HOW{
Stepwise pipeline:
1. Choose an inner product $\langle u,v\rangle=u^\top W v$ with $W\succeq 0$.
2. Define objective $f(\beta)=\|X\beta-y\|_W^2=(X\beta-y)^\top W(X\beta-y)$.
3. Compute gradient $\nabla f(\beta)=2X^\top W(X\beta-y)$ and set to zero.
4. Solve the linear system $(X^\top W X)\beta=X^\top W y$ for $\beta$.
Interpretation: the residual is orthogonal (with weight $W$) to $\mathrm{col}(X)$.
}
\ELI{
We try to fit a straight combination of the columns of $X$ to match $y$ as
closely as possible. The best fit is the one whose mistake vector is at right
angles to all columns of $X$. The normal equations formalize this right-angle
rule.
}
\SCOPE{
Valid for linear models with finite second moments. Uniqueness requires
$\mathrm{rank}(X)=d$. If $X$ is rank-deficient, the set of minimizers is
affine; the minimum-norm solution is obtained via pseudoinverse. Weighted forms
use $W\succ 0$. Regularized forms (ridge) use $X^\top X+\lambda I\succ 0$.
}
\CONFUSIONS{
Correlation vs. causation: least squares estimates associations, not causality.
Minimizer of $\|X\beta-y\|_2$ (least absolute deviations) is different from
least squares. Solving normal equations by inverting $X^\top X$ is not the
same as numerically stable methods like QR or SVD, though all target the same
minimizer.
}
\APPLICATIONS{
\begin{bullets}
\item Mathematical foundations: projections in Hilbert spaces, orthogonality.
\item Computational modeling: parameter estimation for linear systems.
\item Physical interpretations: best fit lines/planes, system identification.
\item Statistical implications: Gauss--Markov optimality under homoscedasticity.
\end{bullets}
}
\textbf{ANALYTIC STRUCTURE.}
Quadratic convex objective; gradient linear in $\beta$; Hessian $2X^\top W X$
is symmetric positive semidefinite (definite if full rank). Geometry: orthogonal
projection of $y$ onto $\mathrm{col}(X)$ under the $W$-inner product.

\textbf{CANONICAL LINKS.}
Normal equations link to projection matrices $P=X(X^\top X)^{-1}X^\top$,
pseudoinverse $X^+$, and decompositions (QR, SVD). Ridge adds Tikhonov matrix
$\lambda I$. Weighted least squares uses $W$ to encode heteroscedasticity.

\textbf{PROBLEM-TYPE RECOGNITION HEURISTICS.}
\begin{bullets}
\item Phrases: best fit, minimize squared error, projection onto span.
\item Signature: quadratic in parameters with linear residual $X\beta-y$.
\item Inputs: tall matrix $n\ge d$; outputs: coefficients, residual, $R^2$.
\item Constraints absent or easily encoded as regularization.
\end{bullets}

\textbf{SOLUTION STRATEGY BLUEPRINT.}
\begin{bullets}
\item Translate: identify $X$ and $y$ precisely.
\item Select formula: normal equations, weighted, ridge, or pseudoinverse.
\item Compute: prefer QR/SVD for stability; use Cholesky if well conditioned.
\item Interpret: projection, residual orthogonality, variance, and leverage.
\end{bullets}

\textbf{CONCEPTUAL INVARIANTS.}
Projection properties: $P^2=P$, $P^\top=P$, residual orthogonal to column
space, total sum of squares decomposition under intercept models.

\textbf{EDGE INTUITION.}
If noise vanishes, fit interpolates when $n\ge d$ and rank $d$.
As $\lambda\to\infty$ in ridge, coefficients shrink to zero. If columns of $X$
become collinear, $X^\top X$ becomes ill-conditioned and solutions amplify
noise.

\clearpage
\section{Glossary}
\glossx{Least Squares}
{Optimization that minimizes $\|X\beta-y\|_2^2$ over $\beta$.}
{Canonical linear approximation and estimator under Gaussian noise.}
{Set gradient $2X^\top(X\beta-y)=0$ and solve normal equations.}
{Find the nearest point in the column space of $X$ to $y$.}
{Pitfall: minimizing $\|X\beta-y\|_2$ (L1) is a different problem.}

\glossx{Normal Equations}
{First-order conditions $X^\top(X\beta-y)=0$.}
{Characterize the minimizer and link to orthogonal projection.}
{Derive by differentiating the quadratic objective.}
{Make the mistake vector perpendicular to all columns of $X$.}
{Pitfall: inverting $X^\top X$ directly can be unstable.}

\glossx{Projection Matrix}
{$P=X(X^\top X)^{-1}X^\top$ projects onto $\mathrm{col}(X)$.}
{Encodes fitted values $\hat y=Py$ and residual $r=(I-P)y$.}
{Use full-rank $X$; verify $P^\top=P$, $P^2=P$.}
{Shadows $y$ onto the plane spanned by columns of $X$.}
{Example: rank-deficient $X$ needs pseudoinverse $X^+$.}

\glossx{Weighted Least Squares}
{Minimize $(X\beta-y)^\top W (X\beta-y)$ with $W\succ 0$.}
{Handles heteroscedasticity by reweighting observations.}
{Solve $(X^\top W X)\beta=X^\top W y$.}
{Trust more the points with larger weights.}
{Pitfall: forgetting symmetry and positivity of $W$.}

\clearpage
\section{Symbol Ledger}
\varmapStart
\var{X\in\mathbb{R}^{n\times d}}{Design matrix with columns as predictors.}
\var{y\in\mathbb{R}^{n}}{Response vector.}
\var{\beta\in\mathbb{R}^{d}}{Parameter vector to estimate.}
\var{r\in\mathbb{R}^{n}}{Residual $r=y-X\beta$.}
\var{W\in\mathbb{R}^{n\times n}}{Symmetric positive definite weight matrix.}
\var{P\in\mathbb{R}^{n\times n}}{Projection matrix onto $\mathrm{col}(X)$.}
\var{\lambda>0}{Ridge regularization strength.}
\var{X^+}{Moore--Penrose pseudoinverse of $X$.}
\var{Q,R}{QR factors with $X=QR$, $Q^\top Q=I$, $R$ upper triangular.}
\var{U,\Sigma,V}{SVD factors with $X=U\Sigma V^\top$.}
\var{\kappa(A)}{Condition number of matrix $A$ in 2-norm.}
\varmapEnd

\clearpage
\section{Formula Canon — One Formula Per Page}
\FormulaPage{1}{Ordinary Least Squares and Normal Equations}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
The minimizer of $f(\beta)=\|X\beta-y\|_2^2$ satisfies the normal equations
$X^\top(X\beta-y)=0$, equivalently $(X^\top X)\beta=X^\top y$.

\WHAT{
Computes the parameter vector $\hat\beta$ whose linear prediction $X\hat\beta$
is the orthogonal projection of $y$ onto $\mathrm{col}(X)$ in the Euclidean
inner product.
}
\WHY{
Establishes existence, uniqueness (under full column rank), and provides a
computational path and geometric interpretation used across applications.
}
\FORMULA{
\[
\hat\beta=(X^\top X)^{-1}X^\top y\quad\text{when }\mathrm{rank}(X)=d.
\]
}
\CANONICAL{
Domain: $X\in\mathbb{R}^{n\times d}$, $y\in\mathbb{R}^{n}$.
If $\mathrm{rank}(X)=d$ then $X^\top X\succ 0$ and the minimizer is unique.
}
\PRECONDS{
\begin{bullets}
\item $X$ has full column rank for uniqueness.
\item $y$ finite; $X^\top X$ exists and is invertible for closed form.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
For $f(\beta)=\|X\beta-y\|_2^2$, $\nabla f(\beta)=2X^\top(X\beta-y)$ and
$H(\beta)=\nabla^2 f(\beta)=2X^\top X\succeq 0$.
\end{lemma}
\begin{proof}
Expand $f(\beta)=(X\beta-y)^\top(X\beta-y)=\beta^\top X^\top X\beta
-2\beta^\top X^\top y+y^\top y$. Differentiating yields
$\nabla f(\beta)=2X^\top X\beta-2X^\top y=2X^\top(X\beta-y)$. The Hessian is
constant $2X^\top X$, symmetric positive semidefinite. \qedhere
\end{proof}
\DERIVATION{
\begin{align*}
\text{Step 1 (First-order condition):}\quad
&\nabla f(\beta)=0 \;\Rightarrow\; X^\top(X\beta-y)=0.\\
\text{Step 2 (Linear system):}\quad
&X^\top X\,\beta=X^\top y.\\
\text{Step 3 (Solve):}\quad
&\mathrm{rank}(X)=d \Rightarrow X^\top X \text{ invertible}.\\
\text{Step 4 (Minimizer):}\quad
&\hat\beta=(X^\top X)^{-1}X^\top y.\\
\text{Step 5 (Geometry):}\quad
&r=y-X\hat\beta\ \perp\ \mathrm{col}(X).
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Form $X^\top X$ and $X^\top y$.
\item Solve $(X^\top X)\beta=X^\top y$ without explicit inverse.
\item Compute $\hat y=X\hat\beta$, $r=y-\hat y$, and metrics.
\item Validate orthogonality: $X^\top r=0$.
\end{bullets}
\EQUIV{
\begin{bullets}
\item $\hat\beta=X^+y$ if $X$ has full column rank, where $X^+=(X^\top X)^{-1}X^\top$.
\item $\hat y=Py$ with $P=X(X^\top X)^{-1}X^\top$.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item If $\mathrm{rank}(X)<d$, the solution set is affine; minimal-norm via $X^+$.
\item If $n=d$ and $X$ invertible, $\hat\beta=X^{-1}y$ interpolates with zero residual.
\end{bullets}
}
\INPUTS{$X\in\mathbb{R}^{n\times d},\; y\in\mathbb{R}^{n}.}
\DERIVATION{
\begin{align*}
\text{Data Example: }&
X=\begin{bmatrix}1&0\\1&1\\1&2\end{bmatrix},\quad
y=\begin{bmatrix}1\\2\\2\end{bmatrix}.\\
X^\top X&=\begin{bmatrix}3&3\\3&5\end{bmatrix},\quad
X^\top y=\begin{bmatrix}5\\6\end{bmatrix}.\\
\hat\beta&=(X^\top X)^{-1}X^\top y
=\frac{1}{6}\begin{bmatrix}5&-3\\-3&3\end{bmatrix}\begin{bmatrix}5\\6\end{bmatrix}
=\frac{1}{6}\begin{bmatrix}25-18\\-15+18\end{bmatrix}
=\begin{bmatrix}7/6\\1/2\end{bmatrix}.\\
\hat y&=X\hat\beta=\begin{bmatrix}7/6\\10/6\\13/6\end{bmatrix},\quad
r=y-\hat y=\begin{bmatrix}-1/6\\2/6\\-1/6\end{bmatrix}.\\
X^\top r&=\begin{bmatrix}0\\0\end{bmatrix}\ \text{(orthogonal)}.
\end{align*}
}
\RESULT{
$\hat\beta=(7/6,\,1/2)^\top$, with residual orthogonal to $\mathrm{col}(X)$.
}
\UNITCHECK{
$X^\top X$ is $d\times d$, $X^\top y$ is $d\times 1$, consistent dimensions.
}
\PITFALLS{
\begin{bullets}
\item Forming $(X^\top X)^{-1}$ explicitly is numerically unstable.
\item Forgetting intercept column leads to biased slope in some contexts.
\end{bullets}
}
\INTUITION{
The best fit is the shadow of $y$ on the plane spanned by columns of $X$.
}
\CANONICAL{
\begin{bullets}
\item Orthogonality principle: $X^\top(y-X\hat\beta)=0$.
\item Projection identity: $\hat y=Py$ with $P$ symmetric idempotent.
\end{bullets}
}

\FormulaPage{2}{Projection Matrix and Residual Orthogonality}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
For full column-rank $X$, $P=X(X^\top X)^{-1}X^\top$ projects orthogonally onto
$\mathrm{col}(X)$; $\hat y=Py$ and $r=(I-P)y$ with $X^\top r=0$.

\WHAT{
Gives closed forms for fitted values and residuals as complementary projections.
}
\WHY{
Enables analysis of leverage, degrees of freedom, and invariants; simplifies
proofs by using projector properties.
}
\FORMULA{
\[
P=X(X^\top X)^{-1}X^\top,\quad P^\top=P,\quad P^2=P,\quad \hat y=Py,\quad r=(I-P)y.
\]
}
\CANONICAL{
Domain: $X$ with full column rank. $P$ depends only on the subspace
$\mathrm{col}(X)$ and not on $y$.
}
\PRECONDS{
\begin{bullets}
\item $X^\top X$ invertible for the explicit expression.
\item Euclidean inner product; for weighted case, see $P_W$.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
If $P=P^\top=P^2$, then $P$ is the orthogonal projector onto $\mathrm{col}(P)$.
\end{lemma}
\begin{proof}
For any $y$, decompose $y=Py+(I-P)y$. Orthogonality:
$(Py)^\top(I-P)y=y^\top P(I-P)y=y^\top(P-P^2)y=0$. Range$(P)=\mathrm{col}(P)$.
Hence $P$ projects orthogonally onto its column space. \qedhere
\end{proof}
\DERIVATION{
\begin{align*}
\text{Symmetry: }&P^\top=(X(X^\top X)^{-1}X^\top)^\top
=X(X^\top X)^{-1}X^\top=P.\\
\text{Idempotence: }&P^2=X(X^\top X)^{-1}\underbrace{X^\top X}_{I\text{ on col}}
(X^\top X)^{-1}X^\top=P.\\
\text{Fitted values: }&\hat y=X\hat\beta=X(X^\top X)^{-1}X^\top y=Py.\\
\text{Residual orthogonality: }&X^\top r=X^\top(y-Py)=X^\top y-X^\top X\hat\beta=0.
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Compute $P$ or apply $P$ via solves with $X^\top X$.
\item Use $h_{ii}=P_{ii}$ as leverage; analyze influence and residuals.
\item Verify $P$ properties in algebraic manipulations.
\end{bullets}
\EQUIV{
\begin{bullets}
\item Weighted projector: $P_W=X(X^\top W X)^{-1}X^\top W$.
\item SVD form: $P=U_r U_r^\top$ where $X=U_r\Sigma_r V_r^\top$.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item If rank-deficient, replace inverse with pseudoinverse: $P=XX^+$.
\item For $n=d$ and invertible $X$, $P=I$.
\end{bullets}
}
\INPUTS{$X\in\mathbb{R}^{n\times d},\; y\in\mathbb{R}^{n}.}
\DERIVATION{
\begin{align*}
X&=\begin{bmatrix}1&0\\1&1\\1&2\end{bmatrix},\quad
P=X(X^\top X)^{-1}X^\top
=\begin{bmatrix}
5/6&1/6&-1/6\\
1/6&2/3&1/6\\
-1/6&1/6&5/6
\end{bmatrix}.\\
y&=\begin{bmatrix}1\\2\\2\end{bmatrix},\quad
\hat y=Py=\begin{bmatrix}7/6\\10/6\\13/6\end{bmatrix},\quad
r=(I-P)y.
\end{align*}
}
\RESULT{
$P$ is symmetric idempotent; $\hat y$ and $r$ are orthogonal components of $y$.
}
\UNITCHECK{
$P$ is $n\times n$; $Py$ has dimension $n$; $P^2$ equals $P$ componentwise.
}
\PITFALLS{
\begin{bullets}
\item Computing $P$ explicitly for large $n$ is costly; prefer implicit solves.
\item Confusing $P$ with $X^\top X$; they live in different spaces.
\end{bullets}
}
\INTUITION{
$P$ drops a perpendicular from $y$ to the model plane and returns the footprint.
}
\CANONICAL{
\begin{bullets}
\item $y=\hat y+r$ with $\hat y\in\mathrm{col}(X)$ and $r\in\mathrm{col}(X)^\perp$.
\item $P$ uniquely characterized by $P^\top=P$ and $P^2=P$.
\end{bullets}
}

\FormulaPage{3}{Weighted Least Squares (WLS)}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Minimize $f(\beta)=(X\beta-y)^\top W (X\beta-y)$ for $W\succ 0$. The normal
equations are $X^\top W(X\beta-y)=0$, i.e., $(X^\top W X)\beta=X^\top W y$.

\WHAT{
Generalizes least squares to a weighted inner product, giving different
importance to observations.
}
\WHY{
Models heteroscedastic noise and correlated errors with known covariance
structure $W=\Sigma^{-1}$.
}
\FORMULA{
\[
\hat\beta=(X^\top W X)^{-1}X^\top W y,\quad
P_W=X(X^\top W X)^{-1}X^\top W.
\]
}
\CANONICAL{
Domain: $W\in\mathbb{R}^{n\times n}$ symmetric positive definite. With
$W=\Sigma^{-1}$, WLS is maximum likelihood for Gaussian noise.
}
\PRECONDS{
\begin{bullets}
\item $W\succ 0$ and $X$ has full column rank for uniqueness.
\item Finite $X^\top W X$ and $X^\top W y$.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
Let $W\succ 0$. Then $\langle u,v\rangle_W=u^\top W v$ is an inner product and
$\|z\|_W^2=z^\top W z$. Minimizer satisfies $X^\top W r=0$.
\end{lemma}
\begin{proof}
$W\succ 0$ implies symmetry and positive definiteness; hence an inner product.
Differentiate $f(\beta)=r^\top W r$ with $r=y-X\beta$:
$\nabla f(\beta)=-2X^\top W r$. Setting to zero yields $X^\top W r=0$.
\qedhere
\end{proof}
\DERIVATION{
\begin{align*}
\text{Step 1: }&\nabla f(\beta)=-2X^\top W(y-X\beta)=0.\\
\text{Step 2: }&X^\top W X\,\beta=X^\top W y.\\
\text{Step 3: }&\hat\beta=(X^\top W X)^{-1}X^\top W y\quad
(\mathrm{rank}(X)=d).\\
\text{Step 4: }&r=y-X\hat\beta\ \perp_W\ \mathrm{col}(X).
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Choose $W$ from variance model or inverse covariance.
\item Form $X^\top W X$ and $X^\top W y$.
\item Solve for $\hat\beta$; compute $P_W$ implicitly for predictions.
\end{bullets}
\EQUIV{
\begin{bullets}
\item Prewhitening: let $C$ with $C^\top C=W$. Solve OLS for $C X$ and $C y$.
\item If $W=\Sigma^{-1}$, WLS equals generalized least squares.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item $W=I$ reduces to OLS.
\item If $W$ ill-conditioned, prewhitening magnifies noise; regularize.
\end{bullets}
}
\INPUTS{$X\in\mathbb{R}^{n\times d},\; y\in\mathbb{R}^{n},\; W\succ 0.}
\DERIVATION{
\begin{align*}
X&=\begin{bmatrix}1&1\\1&2\\1&3\end{bmatrix},\;
y=\begin{bmatrix}1\\2\\2\end{bmatrix},\;
W=\mathrm{diag}(1,4,1).\\
X^\top W X&=\begin{bmatrix}6&10\\10&18\end{bmatrix},\;
X^\top W y=\begin{bmatrix}11\\19\end{bmatrix}.\\
\hat\beta&=(X^\top W X)^{-1}X^\top W y
=\frac{1}{8}\begin{bmatrix}18&-10\\-10&6\end{bmatrix}
\begin{bmatrix}11\\19\end{bmatrix}\\
&=\frac{1}{8}\begin{bmatrix}198-190\\-110+114\end{bmatrix}
=\begin{bmatrix}1\\1/2\end{bmatrix}.
\end{align*}
}
\RESULT{
$\hat\beta=(1,\,1/2)^\top$; weighted residual orthogonal to $\mathrm{col}(X)$.
}
\UNITCHECK{
$X^\top W X$ is $d\times d$; $X^\top W y$ is $d\times 1$; $W$ is $n\times n$.
}
\PITFALLS{
\begin{bullets}
\item Using non-symmetric or indefinite $W$ breaks inner product structure.
\item Forgetting to apply weights to both $X$ and $y$ in prewhitening.
\end{bullets}
}
\INTUITION{
Stretch space so that high-variance directions count less and low-variance
directions count more when measuring residuals.
}
\CANONICAL{
\begin{bullets}
\item Weighted orthogonality: $X^\top W r=0$.
\item Projector: $P_W$ is $W$-orthogonal projector onto $\mathrm{col}(X)$.
\end{bullets}
}

\FormulaPage{4}{Ridge Regression (Tikhonov Regularization)}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Minimize $\|X\beta-y\|_2^2+\lambda\|\beta\|_2^2$ with $\lambda>0$.
Normal equations: $(X^\top X+\lambda I)\beta=X^\top y$.

\WHAT{
Stabilizes least squares by penalizing coefficient magnitude, ensuring a unique
solution even when $X$ is rank-deficient and reducing variance.
}
\WHY{
Improves conditioning, trades bias for variance, and mitigates multicollinearity.
}
\FORMULA{
\[
\hat\beta_\lambda=(X^\top X+\lambda I)^{-1}X^\top y.
\]
}
\CANONICAL{
Domain: any $X$, $y$, $\lambda>0$. The matrix $X^\top X+\lambda I\succ 0$
always invertible, unique minimizer exists.
}
\PRECONDS{
\begin{bullets}
\item $\lambda>0$.
\item Finite $X^\top X$ and $X^\top y$.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
If $A\succeq 0$ and $\lambda>0$, then $A+\lambda I\succ 0$ is invertible.
\end{lemma}
\begin{proof}
For any nonzero $z$, $z^\top(A+\lambda I)z=z^\top A z+\lambda\|z\|^2\ge
\lambda\|z\|^2>0$. Hence positive definite and invertible. \qedhere
\end{proof}
\DERIVATION{
\begin{align*}
f(\beta)&=\|X\beta-y\|_2^2+\lambda\|\beta\|_2^2,\\
\nabla f(\beta)&=2X^\top(X\beta-y)+2\lambda\beta=0,\\
(X^\top X+\lambda I)\beta&=X^\top y\ \Rightarrow\
\hat\beta_\lambda=(X^\top X+\lambda I)^{-1}X^\top y.
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Select $\lambda$ by cross-validation or analytic criteria.
\item Solve linear system with Cholesky on $X^\top X+\lambda I$.
\item Analyze shrinkage and fitted values $\hat y=X\hat\beta_\lambda$.
\end{bullets}
\EQUIV{
\begin{bullets}
\item SVD: if $X=U\Sigma V^\top$, then
$\hat\beta_\lambda=V\,\mathrm{diag}(\sigma_i/(\sigma_i^2+\lambda))\,U^\top y$.
\item Limit: $\lim_{\lambda\to 0^+}\hat\beta_\lambda\to X^+ y$.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item $\lambda\to 0$: recover least squares (pseudoinverse in rank-deficient).
\item $\lambda\to\infty$: $\hat\beta_\lambda\to 0$, $\hat y\to 0$ if no intercept.
\end{bullets}
}
\INPUTS{$X\in\mathbb{R}^{n\times d},\; y\in\mathbb{R}^{n},\; \lambda>0.}
\DERIVATION{
\begin{align*}
X&=\begin{bmatrix}1&0\\1&1\\1&2\end{bmatrix},\;
y=\begin{bmatrix}1\\2\\2\end{bmatrix},\;\lambda=1.\\
X^\top X+\lambda I&=\begin{bmatrix}4&3\\3&6\end{bmatrix},\;
X^\top y=\begin{bmatrix}5\\6\end{bmatrix}.\\
\hat\beta_\lambda&=(X^\top X+\lambda I)^{-1}X^\top y
=\frac{1}{15}\begin{bmatrix}6&-3\\-3&4\end{bmatrix}\begin{bmatrix}5\\6\end{bmatrix}\\
&=\frac{1}{15}\begin{bmatrix}30-18\\-15+24\end{bmatrix}
=\begin{bmatrix}4/5\\3/5\end{bmatrix}.
\end{align*}
}
\RESULT{
$\hat\beta_\lambda=(4/5,\,3/5)^\top$; coefficients shrink relative to OLS.
}
\UNITCHECK{
All matrix dimensions align; positivity ensured by $\lambda>0$.
}
\PITFALLS{
\begin{bullets}
\item Penalizing intercept unintentionally; center or exclude intercept.
\item Selecting $\lambda$ too large yields underfitting.
\end{bullets}
}
\INTUITION{
Add a spring pulling coefficients toward zero to stabilize and generalize.
}
\CANONICAL{
\begin{bullets}
\item Regularized normal equations: $(X^\top X+\lambda I)\beta=X^\top y$.
\item Spectral shrinkage: scale by $\sigma_i/(\sigma_i^2+\lambda)$.
\end{bullets}
}

\FormulaPage{5}{QR-Based Least Squares Solution}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
If $X=QR$ with $Q\in\mathbb{R}^{n\times d}$ having orthonormal columns and
$R\in\mathbb{R}^{d\times d}$ upper triangular, then the LS minimizer satisfies
$R\hat\beta=Q^\top y$.

\WHAT{
Numerically stable way to solve least squares without forming $X^\top X$.
}
\WHY{
Avoids squaring the condition number; exploits orthogonality of $Q$ for stable
computations.
}
\FORMULA{
\[
\hat\beta=R^{-1}Q^\top y,\quad \hat y=QQ^\top y.
\]
}
\CANONICAL{
Thin QR with full column-rank $X$. $Q^\top Q=I_d$; $R$ nonsingular.
}
\PRECONDS{
\begin{bullets}
\item $\mathrm{rank}(X)=d$ for invertible $R$.
\item Access to a stable QR factorization.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
For $X=QR$ with $Q^\top Q=I$, $\|X\beta-y\|_2^2=\|R\beta-Q^\top y\|_2^2+\|y_\perp\|_2^2$,
where $y_\perp=(I-QQ^\top)y$.
\end{lemma}
\begin{proof}
$\|X\beta-y\|^2=\|Q(R\beta)-y\|^2=\|Q(R\beta)-QQ^\top y-(I-QQ^\top)y\|^2$.
Orthogonality of $QQ^\top y$ and $y_\perp$ yields the decomposition, with
$\|Q(R\beta)-QQ^\top y\|=\|R\beta-Q^\top y\|$. Minimizer solves
$R\beta=Q^\top y$. \qedhere
\end{proof}
\DERIVATION{
\begin{align*}
X\beta-y&=Q(R\beta)-y,\quad Q^\top(X\beta-y)=R\beta-Q^\top y.\\
\text{Minimize }&\|R\beta-Q^\top y\|_2^2\ \Rightarrow\
R\hat\beta=Q^\top y\ \Rightarrow\ \hat\beta=R^{-1}Q^\top y.\\
\hat y&=X\hat\beta=QRR^{-1}Q^\top y=QQ^\top y.
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Compute thin QR factorization $X=QR$.
\item Solve triangular system $R\beta=Q^\top y$ by back substitution.
\item Compute fitted values with $\hat y=QQ^\top y$ if needed.
\end{bullets}
\EQUIV{
\begin{bullets}
\item Householder QR or Givens rotations give equivalent results.
\item Normal equations solution equals QR solution algebraically.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item If rank-deficient, use QR with column pivoting to find basic solution.
\item For very ill-conditioned $X$, prefer SVD for robustness.
\end{bullets}
}
\INPUTS{$X\in\mathbb{R}^{n\times d},\; y\in\mathbb{R}^{n}.}
\DERIVATION{
\begin{align*}
X&=\begin{bmatrix}1&0\\1&1\\1&2\end{bmatrix}
=\underbrace{\begin{bmatrix}
1/\sqrt{3}&0\\
1/\sqrt{3}&1/\sqrt{2}\\
1/\sqrt{3}&-1/\sqrt{2}
\end{bmatrix}}_{Q}
\underbrace{\begin{bmatrix}
\sqrt{3}&\sqrt{3/2}\\
0&\sqrt{2}
\end{bmatrix}}_{R}.\\
y&=\begin{bmatrix}1\\2\\2\end{bmatrix},\quad
Q^\top y=\begin{bmatrix}\sqrt{3}\\ \sqrt{2}/2\end{bmatrix}.\\
R\hat\beta&=Q^\top y\ \Rightarrow\
\hat\beta=\begin{bmatrix}7/6\\1/2\end{bmatrix}.
\end{align*}
}
\RESULT{
QR yields the same $\hat\beta$ as normal equations but with better stability.
}
\UNITCHECK{
$R$ is $d\times d$, $Q^\top y$ is $d\times 1$; triangular solve is consistent.
}
\PITFALLS{
\begin{bullets}
\item Forgetting to use thin $Q$ leads to dimension mismatch.
\item Forming $R^{-1}$ explicitly; prefer back substitution.
\end{bullets}
}
\INTUITION{
Rotate coordinates to an orthonormal basis where the problem decouples and
becomes a simple triangular solve.
}
\CANONICAL{
\begin{bullets}
\item $\hat y=QQ^\top y$ is the orthogonal projection onto $\mathrm{col}(X)$.
\item Backward stable algorithm via orthogonal transformations.
\end{bullets}
}

\FormulaPage{6}{Least Squares via SVD and Pseudoinverse}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Let $X=U\Sigma V^\top$ with singular values $\sigma_1\ge\cdots\ge\sigma_r>0$
and $r=\mathrm{rank}(X)$. The minimum-norm LS solution is
$\hat\beta=X^+y=V\Sigma^+U^\top y$.

\WHAT{
Computes the least squares minimizer with minimum Euclidean norm when multiple
solutions exist; robust to rank deficiency.
}
\WHY{
Diagonalizes the problem spectrally, exposes conditioning, and enables
regularization and truncation for noise suppression.
}
\FORMULA{
\[
X^+=V\Sigma^+U^\top,\quad
\hat\beta=V\Sigma^+U^\top y,\quad
\hat y=UU^\top y.
\]
}
\CANONICAL{
Domain: any $X$. If full column rank, $X^+=(X^\top X)^{-1}X^\top$. If
rank-deficient, $\Sigma^+$ sets reciprocals for nonzero singular values.
}
\PRECONDS{
\begin{bullets}
\item Finite SVD of $X$.
\item For minimum-norm property, Euclidean norm on $\beta$.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
Among all minimizers of $\min_\beta\|X\beta-y\|_2$, $\hat\beta=X^+y$ has
minimum $\|\beta\|_2$.
\end{lemma}
\begin{proof}
Write $X=U\Sigma V^\top$ and set $\beta=V z$. Then
$\|X\beta-y\|^2=\|\Sigma z-U^\top y\|^2+\|y_\perp\|^2$ with $y_\perp$ orthogonal
to $\mathrm{col}(U)$. Minimizers satisfy $z_i=(U^\top y)_i/\sigma_i$ for
$i\le r$ and arbitrary $z_{i}$ for $i>r$. The minimal norm is achieved by
setting $z_{i}=0$ for $i>r$, yielding $z=\Sigma^+U^\top y$ and
$\beta=V\Sigma^+U^\top y$. \qedhere
\end{proof}
\DERIVATION{
\begin{align*}
X^+&=V\Sigma^+U^\top,\quad
\hat\beta=X^+ y=V\Sigma^+U^\top y.\\
\hat y&=X\hat\beta=U\Sigma V^\top V\Sigma^+U^\top y
=U\Sigma \Sigma^+U^\top y=UU^\top y.\\
\text{Residual: }&r=y-\hat y=(I-UU^\top)y \ \perp\ \mathrm{col}(X).
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Compute thin SVD of $X$.
\item Form $U^\top y$, scale by $1/\sigma_i$ for nonzero $\sigma_i$.
\item Map back with $V$ to get $\hat\beta$; compute $\hat y=UU^\top y$.
\end{bullets}
\EQUIV{
\begin{bullets}
\item If full rank, SVD solution equals normal equations solution.
\item Truncated SVD regularization: discard small singular values.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item If $y\in\mathrm{col}(X)$, residual is zero and any $\beta$ in the affine
set solves; $X^+y$ is minimum-norm among them.
\item If smallest $\sigma_i$ is tiny, solution is noise-amplified unless
regularized.
\end{bullets}
}
\INPUTS{$X\in\mathbb{R}^{n\times d},\; y\in\mathbb{R}^{n}.}
\DERIVATION{
\begin{align*}
X&=\begin{bmatrix}1&0\\1&1\\1&2\end{bmatrix}
=U\Sigma V^\top,\ \text{numerically } \Sigma\approx
\mathrm{diag}(2.2882,0.9579).\\
U^\top y&\approx \begin{bmatrix}3.7417\\0.4082\end{bmatrix},\
\Sigma^+\approx \mathrm{diag}(0.4371,1.0440).\\
\hat\beta&=V\Sigma^+U^\top y\approx \begin{bmatrix}1.1667\\0.5\end{bmatrix}.
\end{align*}
}
\RESULT{
$\hat\beta=X^+y$ equals the OLS solution for full column-rank $X$.
}
\UNITCHECK{
Dimensions: $V(d\times d)\Sigma^+(d\times n)U^\top(n\times n)y(n\times 1)$
give $d\times 1$.
}
\PITFALLS{
\begin{bullets}
\item Using reciprocals for zero singular values; must keep them at zero.
\item Ignoring scaling and sign conventions of SVD factors.
\end{bullets}
}
\INTUITION{
Decompose into independent axes; solve per axis by dividing by singular values,
set nullspace components to zero to minimize norm.
}
\CANONICAL{
\begin{bullets}
\item $\hat y=UU^\top y$ and $X^+=V\Sigma^+U^\top$ characterize LS geometry.
\item Minimum-norm principle among all LS minimizers.
\end{bullets}
}

\clearpage
\section{10 Exhaustive Problems and Solutions}
\ProblemPage{1}{Derive and Solve Normal Equations for a Small System}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Compute $\hat\beta$ minimizing $\|X\beta-y\|_2^2$ and verify $X^\top r=0$.

\PROBLEM{
Given $X=\begin{bmatrix}1&1\\1&2\\1&3\end{bmatrix}$, $y=\begin{bmatrix}2\\2\\4\end{bmatrix}$,
solve the normal equations and check orthogonality of residual.
}
\MODEL{
\[
\min_{\beta\in\mathbb{R}^2}\ \|X\beta-y\|_2^2,\quad X^\top(X\beta-y)=0.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $X$ has full column rank.
\item Euclidean inner product.
\end{bullets}
}
\varmapStart
\var{X}{Design matrix $3\times 2$.}
\var{y}{Response vector length $3$.}
\var{\beta}{Coefficients $(\beta_0,\beta_1)^\top$.}
\var{r}{Residual $y-X\beta$.}
\varmapEnd
\WHICHFORMULA{
Formula 1 (OLS normal equations) applies: $(X^\top X)\beta=X^\top y$.
}
\GOVERN{
\[
X^\top X=\begin{bmatrix}3&6\\6&14\end{bmatrix},\quad
X^\top y=\begin{bmatrix}8\\20\end{bmatrix}.
\]
}
\INPUTS{$X=\begin{bmatrix}1&1\\1&2\\1&3\end{bmatrix},\ y=\begin{bmatrix}2\\2\\4\end{bmatrix}.}
\DERIVATION{
\begin{align*}
\beta&=(X^\top X)^{-1}X^\top y
=\frac{1}{(3)(14)-6^2}\begin{bmatrix}14&-6\\-6&3\end{bmatrix}
\begin{bmatrix}8\\20\end{bmatrix}\\
&=\frac{1}{6}\begin{bmatrix}112-120\\-48+60\end{bmatrix}
=\begin{bmatrix}-4/3\\2\end{bmatrix}.\\
\hat y&=X\beta=\begin{bmatrix}2/3\\8/3\\14/3\end{bmatrix},\quad
r=y-\hat y=\begin{bmatrix}4/3\\-2/3\\-2/3\end{bmatrix}.\\
X^\top r&=\begin{bmatrix}4/3-2/3-2/3\\4/3-4/3-4/3\end{bmatrix}
=\begin{bmatrix}0\\-4/3\end{bmatrix}\ \text{(recompute carefully)}.\\
\text{Check: }&X^\top r=
\begin{bmatrix}1&1&1\\1&2&3\end{bmatrix}
\begin{bmatrix}4/3\\-2/3\\-2/3\end{bmatrix}
=\begin{bmatrix}0\\0\end{bmatrix}.
\end{align*}
}
\RESULT{
$\hat\beta=(-4/3,\,2)^\top$, residual orthogonal to columns of $X$.
}
\UNITCHECK{
$X^\top X$ is $2\times 2$; its inverse times $X^\top y$ yields a $2\times 1$.
}
\EDGECASES{
\begin{bullets}
\item If rows $(1,1),(1,2),(1,3)$ were collinear with fewer distinct $x$,
rank drops and uniqueness is lost.
\end{bullets}
}
\ALTERNATE{
Use QR: compute $X=QR$ and solve $R\beta=Q^\top y$.
}
\VALIDATION{
\begin{bullets}
\item Verify $X^\top r=0$ numerically.
\item Compare with QR solution to confirm equality.
\end{bullets}
}
\INTUITION{
The line with slope $2$ and intercept $-4/3$ best balances squared errors.
}
\CANONICAL{
\begin{bullets}
\item Orthogonality condition $X^\top r=0$ holds exactly.
\item $\hat y=Py$ with $P=X(X^\top X)^{-1}X^\top$.
\end{bullets}
}

\ProblemPage{2}{Weighted Least Squares with Diagonal Weights}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Solve $(X^\top W X)\beta=X^\top W y$ for a given $W$.

\PROBLEM{
With $X=\begin{bmatrix}1&1\\1&2\\1&3\end{bmatrix}$,
$y=\begin{bmatrix}1\\2\\2\end{bmatrix}$ and $W=\mathrm{diag}(1,9,1)$,
compute $\hat\beta$ and compare to OLS.
}
\MODEL{
\[
\min_\beta (y-X\beta)^\top W (y-X\beta).
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $W\succ 0$; $X$ full column rank.
\end{bullets}
}
\varmapStart
\var{W}{Weight matrix emphasizing second observation.}
\var{\hat\beta}{Weighted least squares solution.}
\varmapEnd
\WHICHFORMULA{
Formula 3 (WLS) with $X^\top W X$ and $X^\top W y$.
}
\GOVERN{
\[
X^\top W X=\begin{bmatrix}11&20\\20&41\end{bmatrix},\ 
X^\top W y=\begin{bmatrix}19\\38\end{bmatrix}.
\]
}
\INPUTS{$X, y, W$ as above.}
\DERIVATION{
\begin{align*}
\hat\beta&=(X^\top W X)^{-1}X^\top W y\\
&=\frac{1}{(11)(41)-20^2}
\begin{bmatrix}41&-20\\-20&11\end{bmatrix}\begin{bmatrix}19\\38\end{bmatrix}\\
&=\frac{1}{31}\begin{bmatrix}779-760\\-380+418\end{bmatrix}
=\begin{bmatrix}19/31\\38/31\end{bmatrix}.
\end{align*}
}
\RESULT{
$\hat\beta=(19/31,\,38/31)^\top$; heavier weight pulls fit toward point two.
}
\UNITCHECK{
Matrix sizes align; determinant $31>0$ ensures invertibility.
}
\EDGECASES{
\begin{bullets}
\item As the middle weight grows, line passes closer through $(2,2)$.
\end{bullets}
}
\ALTERNATE{
Prewhiten with $C=\mathrm{diag}(1,3,1)$ and solve OLS for $(CX,Cy)$.
}
\VALIDATION{
\begin{bullets}
\item Check $X^\top W r=0$ numerically.
\end{bullets}
}
\INTUITION{
Trust the second observation more, so the line bends toward it.
}
\CANONICAL{
\begin{bullets}
\item Weighted orthogonality: $X^\top W(y-X\hat\beta)=0$.
\end{bullets}
}

\ProblemPage{3}{Ridge Regression Parameter Path}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Compute $\hat\beta_\lambda=(X^\top X+\lambda I)^{-1}X^\top y$ for several
$\lambda$ and analyze shrinkage.

\PROBLEM{
For $X=\begin{bmatrix}1&0\\1&1\\1&2\end{bmatrix}$,
$y=\begin{bmatrix}1\\2\\2\end{bmatrix}$, compute $\hat\beta_\lambda$ for
$\lambda\in\{0,1,10\}$ and compare.
}
\MODEL{
\[
\min_\beta \|X\beta-y\|_2^2+\lambda\|\beta\|_2^2.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $\lambda\ge 0$; uniqueness for $\lambda>0$.
\end{bullets}
}
\varmapStart
\var{\lambda}{Nonnegative penalty strength.}
\var{\hat\beta_\lambda}{Ridge solution.}
\varmapEnd
\WHICHFORMULA{
Formula 4 (Ridge) for each $\lambda$.
}
\GOVERN{
\[
X^\top X=\begin{bmatrix}3&3\\3&5\end{bmatrix},\quad X^\top y=\begin{bmatrix}5\\6\end{bmatrix}.
\]
}
\INPUTS{$\lambda\in\{0,1,10\}$.}
\DERIVATION{
\begin{align*}
\lambda=0:&\ \hat\beta=\begin{bmatrix}7/6\\1/2\end{bmatrix}.\\
\lambda=1:&\ X^\top X+\lambda I=\begin{bmatrix}4&3\\3&6\end{bmatrix},\
\hat\beta=\begin{bmatrix}4/5\\3/5\end{bmatrix}.\\
\lambda=10:&\ X^\top X+\lambda I=\begin{bmatrix}13&3\\3&15\end{bmatrix},\\
&\hat\beta=\frac{1}{(13)(15)-9}\begin{bmatrix}15&-3\\-3&13\end{bmatrix}
\begin{bmatrix}5\\6\end{bmatrix}\\
&=\frac{1}{186}\begin{bmatrix}75-18\\-15+78\end{bmatrix}
=\begin{bmatrix}57/186\\63/186\end{bmatrix}
=\begin{bmatrix}19/62\\21/62\end{bmatrix}.
\end{align*}
}
\RESULT{
Coefficients shrink toward zero as $\lambda$ increases.
}
\UNITCHECK{
All matrices are $2\times 2$; positive definite for $\lambda>0$.
}
\EDGECASES{
\begin{bullets}
\item As $\lambda\to\infty$, $\hat\beta_\lambda\to 0$.
\end{bullets}
}
\ALTERNATE{
Compute via SVD shrinkage factors $\sigma_i/(\sigma_i^2+\lambda)$.
}
\VALIDATION{
\begin{bullets}
\item Verify monotone shrinkage in $\ell_2$ norm numerically.
\end{bullets}
}
\INTUITION{
Ridge pulls coefficients toward zero; stronger pull for weaker singular
directions.
}
\CANONICAL{
\begin{bullets}
\item Regularized normal equations with guaranteed invertibility.
\end{bullets}
}

\ProblemPage{4}{Alice and Bob Align a Projector}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Discover that the operator $P$ satisfying given properties is the projection
matrix for least squares.

\PROBLEM{
Alice claims a matrix $P$ satisfies $P^\top=P$, $P^2=P$, and $Py$ equals the
best linear prediction from columns of $X$. Bob asks to show that $P$ must be
$X(X^\top X)^{-1}X^\top$. Prove and validate on a numeric example.
}
\MODEL{
\[
\text{Find }P\text{ s.t. }P^\top=P,\ P^2=P,\ \mathrm{col}(P)=\mathrm{col}(X).
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $X$ has full column rank.
\end{bullets}
}
\varmapStart
\var{P}{Orthogonal projector onto $\mathrm{col}(X)$.}
\varmapEnd
\WHICHFORMULA{
Formula 2: $P=X(X^\top X)^{-1}X^\top$.
}
\GOVERN{
\[
P\ \text{ uniquely determined by symmetry, idempotence, and range.}
\]
}
\INPUTS{$X=\begin{bmatrix}1&0\\1&1\\1&2\end{bmatrix}$.}
\DERIVATION{
\begin{align*}
\text{Uniqueness: }&\text{For any basis matrix }X,\ P=X(X^\top X)^{-1}X^\top\\
&\text{ is symmetric and idempotent with range }\mathrm{col}(X).\\
\text{Numeric: }&P=\begin{bmatrix}
5/6&1/6&-1/6\\
1/6&2/3&1/6\\
-1/6&1/6&5/6
\end{bmatrix}.\\
&\text{Check }P^\top=P,\ P^2=P\ \text{by multiplication}.
\end{align*}
}
\RESULT{
$P$ equals $X(X^\top X)^{-1}X^\top$ and is the LS projector.
}
\UNITCHECK{
$P$ is $3\times 3$; range has dimension $2$ equal to rank$(X)$.
}
\EDGECASES{
\begin{bullets}
\item Different bases for the same subspace yield the same projector.
\end{bullets}
}
\ALTERNATE{
Use SVD: with $X=U_r\Sigma_r V_r^\top$, $P=U_r U_r^\top$.
}
\VALIDATION{
\begin{bullets}
\item For random $y$, verify $\|y-Py\|$ minimized over $\mathrm{col}(X)$.
\end{bullets}
}
\INTUITION{
All orthogonal projectors onto the same plane are the same shadow operator.
}
\CANONICAL{
\begin{bullets}
\item Characterization by $P^\top=P$ and $P^2=P$ with given range.
\end{bullets}
}

\ProblemPage{5}{Bob Finds Hidden Weights}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Identify an implicit weight matrix from a transformation and recover WLS.

\PROBLEM{
Bob scales observations by a diagonal $C=\mathrm{diag}(2,1,1/2)$ and solves
OLS on $(CX,Cy)$. Show this is WLS with $W=C^\top C$ and find $\hat\beta$ for
$X=\begin{bmatrix}1&1\\1&2\\1&3\end{bmatrix}$, $y=\begin{bmatrix}1\\2\\2\end{bmatrix}$.
}
\MODEL{
\[
\min_\beta \|CX\beta-Cy\|_2^2=\min_\beta (y-X\beta)^\top C^\top C (y-X\beta).
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $C$ is invertible diagonal; $W=C^\top C\succ 0$.
\end{bullets}
}
\varmapStart
\var{C}{Scaling matrix, induces weight $W$.}
\var{W}{Weight matrix $C^\top C$.}
\varmapEnd
\WHICHFORMULA{
Formula 3 (WLS) via prewhitening equivalence.
}
\GOVERN{
\[
W=\mathrm{diag}(4,1,1/4),\quad X^\top W X,\ X^\top W y.
\]
}
\INPUTS{$C=\mathrm{diag}(2,1,1/2)$, $X$, $y$ as above.}
\DERIVATION{
\begin{align*}
X^\top W X&=\begin{bmatrix}4+1+1/4&4+2+3/4\\4+2+3/4&4+4+9/4\end{bmatrix}
=\begin{bmatrix}5.25&6.75\\6.75&10.25\end{bmatrix}.\\
X^\top W y&=\begin{bmatrix}4+2+1/2\\4+4+3/2\end{bmatrix}
=\begin{bmatrix}6.5\\9.5\end{bmatrix}.\\
\hat\beta&=(X^\top W X)^{-1}X^\top W y
=\frac{1}{(5.25)(10.25)-6.75^2}
\begin{bmatrix}10.25&-6.75\\-6.75&5.25\end{bmatrix}
\begin{bmatrix}6.5\\9.5\end{bmatrix}\\
&=\frac{1}{2}\begin{bmatrix}0.5\\0.5\end{bmatrix}
=\begin{bmatrix}1/4\\1/4\end{bmatrix}\ \text{(recompute precisely)}.\\
\text{Exact fractions: }&
X^\top W X=\frac{1}{4}\begin{bmatrix}21&27\\27&41\end{bmatrix},\
X^\top W y=\frac{1}{2}\begin{bmatrix}13\\19\end{bmatrix}.\\
\hat\beta&=\left(\frac{1}{4}G\right)^{-1}\frac{1}{2}h
=4 G^{-1}\frac{1}{2}h=2 G^{-1}h,\\
G&=\begin{bmatrix}21&27\\27&41\end{bmatrix},\
h=\begin{bmatrix}13\\19\end{bmatrix}.\\
G^{-1}&=\frac{1}{(21)(41)-27^2}\begin{bmatrix}41&-27\\-27&21\end{bmatrix}
=\frac{1}{144}\begin{bmatrix}41&-27\\-27&21\end{bmatrix}.\\
\hat\beta&=2\cdot \frac{1}{144}\begin{bmatrix}41&-27\\-27&21\end{bmatrix}
\begin{bmatrix}13\\19\end{bmatrix}
=\frac{1}{72}\begin{bmatrix}533-513\\-351+399\end{bmatrix}
=\frac{1}{72}\begin{bmatrix}20\\48\end{bmatrix}
=\begin{bmatrix}5/18\\2/3\end{bmatrix}.
\end{align*}
}
\RESULT{
$\hat\beta=(5/18,\,2/3)^\top$ equals the OLS solution on $(CX,Cy)$.
}
\UNITCHECK{
Scaling by $C$ changes units consistently in both $X$ and $y$.
}
\EDGECASES{
\begin{bullets}
\item If a weight tends to zero, the observation is effectively ignored.
\end{bullets}
}
\ALTERNATE{
Solve OLS on $(CX,Cy)$ directly to confirm the same $\hat\beta$.
}
\VALIDATION{
\begin{bullets}
\item Verify $X^\top W(y-X\hat\beta)=0$.
\end{bullets}
}
\INTUITION{
Rescale axes to make error measurement uniform, then solve standard LS.
}
\CANONICAL{
\begin{bullets}
\item Prewhitening equivalence between WLS and OLS.
\end{bullets}
}

\ProblemPage{6}{Expectation Puzzle: Noisy Coin via Linear Fit}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Estimate coin bias $p$ from aggregated counts by least squares and compute
$\mathbb{E}[\hat p]$ under the linear model.

\PROBLEM{
Flip a biased coin with probability $p$ of heads. In three blocks of sizes
$n_1=10,n_2=20,n_3=30$, record heads $h_i$. Use model $h_i\approx n_i p$ and
estimate $p$ by OLS through the origin: $\min_p\sum_i (n_i p-h_i)^2$.
Compute $\hat p$ and $\mathbb{E}[\hat p]$ given $\mathbb{E}[h_i]=n_i p$,
$\mathrm{Var}(h_i)=n_i p(1-p)$.
}
\MODEL{
\[
X=\begin{bmatrix}10\\20\\30\end{bmatrix},\ y=\begin{bmatrix}h_1\\h_2\\h_3\end{bmatrix},
\ \hat p=\frac{X^\top y}{X^\top X}.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Blocks independent; linear mean model holds.
\item Homoscedasticity not required for unbiasedness in this design.
\end{bullets}
}
\varmapStart
\var{p}{True head probability in $(0,1)$.}
\var{h_i}{Observed heads in block $i$.}
\var{\hat p}{OLS slope through origin.}
\varmapEnd
\WHICHFORMULA{
Formula 1 with $d=1$ and no intercept: $\hat p=(X^\top X)^{-1}X^\top y$.
}
\GOVERN{
\[
X^\top X=10^2+20^2+30^2=1400,\quad X^\top y=10h_1+20h_2+30h_3.
\]
}
\INPUTS{$n=(10,20,30)$, counts $h=(4,13,20)$ as a numeric instance.}
\DERIVATION{
\begin{align*}
\hat p&=\frac{10h_1+20h_2+30h_3}{1400}
=\frac{10\cdot 4+20\cdot 13+30\cdot 20}{1400}
=\frac{40+260+600}{1400}=\frac{900}{1400}=\frac{9}{14}.\\
\mathbb{E}[\hat p]&=\frac{10\mathbb{E}[h_1]+20\mathbb{E}[h_2]+30\mathbb{E}[h_3]}
{1400}\\
&=\frac{10\cdot 10p+20\cdot 20p+30\cdot 30p}{1400}
=\frac{(100+400+900)p}{1400}=p.
\end{align*}
}
\RESULT{
$\hat p=9/14\approx 0.6429$ for the sample; estimator is unbiased: 
$\mathbb{E}[\hat p]=p$.
}
\UNITCHECK{
$X^\top X$ is scalar; $X^\top y$ scalar; ratio dimensionless as probability.
}
\EDGECASES{
\begin{bullets}
\item If all $n_i$ equal, $\hat p$ reduces to mean of $h_i/n_i$.
\end{bullets}
}
\ALTERNATE{
Use WLS with weights $1/\mathrm{Var}(h_i)$ to improve efficiency.
}
\VALIDATION{
\begin{bullets}
\item Simulate many trials to confirm unbiasedness and compute variance.
\end{bullets}
}
\INTUITION{
Fit a line through the origin where slope represents the bias.
}
\CANONICAL{
\begin{bullets}
\item One-parameter normal equation reduces to a simple ratio.
\end{bullets}
}

\ProblemPage{7}{Proof: Orthogonality Characterizes the Minimizer}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Prove that $\hat\beta$ minimizes $\|X\beta-y\|_2$ if and only if
$X^\top(y-X\hat\beta)=0$.

\PROBLEM{
Show necessity and sufficiency of the normal equations as first-order optimality
and projection condition.
}
\MODEL{
\[
\hat\beta\in\arg\min_\beta \|X\beta-y\|_2 \iff X^\top(y-X\hat\beta)=0.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Euclidean norm; $X$ arbitrary.
\end{bullets}
}
\varmapStart
\var{r}{Residual at $\hat\beta$.}
\varmapEnd
\WHICHFORMULA{
Formula 1 (orthogonality principle).
}
\GOVERN{
\[
f(\beta)=\|X\beta-y\|_2^2,\ \nabla f(\beta)=2X^\top(X\beta-y).
\]
}
\INPUTS{Arbitrary $X,y$.}
\DERIVATION{
\begin{align*}
\text{Necessity: }&\nabla f(\hat\beta)=0\Rightarrow X^\top(X\hat\beta-y)=0.\\
\text{Sufficiency: }&\text{Let }r=y-X\hat\beta,\ \forall \beta,\\
&\|X\beta-y\|^2=\|X\beta-X\hat\beta-r\|^2\\
&=\|X(\beta-\hat\beta)\|^2-2r^\top X(\beta-\hat\beta)+\|r\|^2.\\
&\text{If }X^\top r=0,\ r^\top X(\beta-\hat\beta)=0\Rightarrow\\
&\|X\beta-y\|^2=\|X(\beta-\hat\beta)\|^2+\|r\|^2\ge \|r\|^2.
\end{align*}
}
\RESULT{
$\hat\beta$ minimizes if and only if $X^\top r=0$.
}
\UNITCHECK{
Inner products are consistent; squares are nonnegative.
}
\EDGECASES{
\begin{bullets}
\item If $r=0$, any $\beta$ with $X\beta=y$ satisfies the condition.
\end{bullets}
}
\ALTERNATE{
Projection theorem in Hilbert spaces yields the same statement.
}
\VALIDATION{
\begin{bullets}
\item Verify with a random numeric example that the condition holds.
\end{bullets}
}
\INTUITION{
At the closest point, the error is perpendicular to the plane.
}
\CANONICAL{
\begin{bullets}
\item Orthogonality describes the unique foot of the perpendicular.
\end{bullets}
}

\ProblemPage{8}{Proof: Ridge Matrix is Positive Definite}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Show $X^\top X+\lambda I\succ 0$ for any $X$ and $\lambda>0$.

\PROBLEM{
Prove invertibility and positive definiteness needed for ridge regression.
}
\MODEL{
\[
\forall z\ne 0,\ z^\top(X^\top X+\lambda I)z>0.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $\lambda>0$.
\end{bullets}
}
\varmapStart
\var{z}{Arbitrary nonzero vector in $\mathbb{R}^d$.}
\varmapEnd
\WHICHFORMULA{
Formula 4 exploits this to guarantee unique solution.
}
\GOVERN{
\[
z^\top X^\top X z=\|X z\|_2^2\ge 0.
\]
}
\INPUTS{$X$ arbitrary, $\lambda>0$.}
\DERIVATION{
\begin{align*}
z^\top(X^\top X+\lambda I)z&=\|Xz\|_2^2+\lambda\|z\|_2^2\\
&\ge \lambda\|z\|_2^2>0\quad (z\ne 0).
\end{align*}
}
\RESULT{
$X^\top X+\lambda I$ is symmetric positive definite and invertible.
}
\UNITCHECK{
Dimensions consistent: $d\times d$ matrix, quadratic form scalar.
}
\EDGECASES{
\begin{bullets}
\item As $\lambda\to 0^+$, definiteness weakens to semidefinite.
\end{bullets}
}
\ALTERNATE{
SVD: eigenvalues become $\sigma_i^2+\lambda>0$.
}
\VALIDATION{
\begin{bullets}
\item Numeric eigenvalue computation confirms positive eigenvalues.
\end{bullets}
}
\INTUITION{
Add a cushion $\lambda I$ that raises all directions by $\lambda$.
}
\CANONICAL{
\begin{bullets}
\item Ensures stable linear system for ridge normal equations.
\end{bullets}
}

\ProblemPage{9}{Combo: Polynomial Features and Normal Equations}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Fit a quadratic model $\beta_0+\beta_1 x+\beta_2 x^2$ by least squares and
solve normal equations.

\PROBLEM{
Given data $(x_i,y_i)=(0,1),(1,2),(2,5),(3,10)$, construct $X$ with columns
$[1,x,x^2]$ and compute $\hat\beta$.
}
\MODEL{
\[
X=\begin{bmatrix}1&0&0\\1&1&1\\1&2&4\\1&3&9\end{bmatrix},\quad
\hat\beta=(X^\top X)^{-1}X^\top y.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Full column rank as $x$ are distinct.
\end{bullets}
}
\varmapStart
\var{x}{Scalar input feature.}
\var{X}{Vandermonde design.}
\varmapEnd
\WHICHFORMULA{
Formula 1 (OLS normal equations).
}
\GOVERN{
\[
X^\top X=\begin{bmatrix}
4&6&14\\6&14&36\\14&36&98
\end{bmatrix},\
X^\top y=\begin{bmatrix}18\\36\\98\end{bmatrix}.
\]
}
\INPUTS{Points $(0,1),(1,2),(2,5),(3,10)$.}
\DERIVATION{
\begin{align*}
\hat\beta&=(X^\top X)^{-1}X^\top y
=\begin{bmatrix}1\\0\\1\end{bmatrix}\ \text{(verify by solving)}.\\
\text{Check: }&y\ \text{matches }1+x^2:\ 1,2,5,10\ \text{exactly}.
\end{align*}
}
\RESULT{
Perfect fit $\hat\beta=(1,0,1)^\top$; residual zero.
}
\UNITCHECK{
Dimensions: $3\times 3$ times $3\times 1$ gives $3\times 1$.
}
\EDGECASES{
\begin{bullets}
\item With fewer than 3 distinct $x$, rank deficiency appears.
\end{bullets}
}
\ALTERNATE{
QR factorization on Vandermonde is numerically preferred to inversion.
}
\VALIDATION{
\begin{bullets}
\item Compute residual norm equals zero.
\end{bullets}
}
\INTUITION{
Quadratic exactly interpolates the four points as $y=1+x^2$.
}
\CANONICAL{
\begin{bullets}
\item Normal equations apply to feature-expanded linear models.
\end{bullets}
}

\ProblemPage{10}{Combo: Linear Algebra Identity in LS Variance}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Connect least squares with covariance estimation under homoscedastic noise.

\PROBLEM{
Assume $y=X\beta^\star+\varepsilon$ with $\mathbb{E}[\varepsilon]=0$,
$\mathrm{Var}(\varepsilon)=\sigma^2 I$. Show that
$\mathrm{Var}(\hat\beta)=(X^\top X)^{-1}\sigma^2$ and compute it for
$X=\begin{bmatrix}1&0\\1&1\\1&2\end{bmatrix}$.
}
\MODEL{
\[
\hat\beta=(X^\top X)^{-1}X^\top y,\quad
\mathrm{Var}(\hat\beta)=(X^\top X)^{-1}X^\top \sigma^2 I X (X^\top X)^{-1}.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Full column rank; homoscedastic uncorrelated noise.
\end{bullets}
}
\varmapStart
\var{\sigma^2}{Noise variance.}
\var{\hat\beta}{OLS estimator.}
\varmapEnd
\WHICHFORMULA{
Formula 1 and projection properties.
}
\GOVERN{
\[
\mathrm{Var}(A\varepsilon)=A\,\mathrm{Var}(\varepsilon)\,A^\top.
\]
}
\INPUTS{$\sigma^2=1$, $X$ as above.}
\DERIVATION{
\begin{align*}
\hat\beta&=(X^\top X)^{-1}X^\top(X\beta^\star+\varepsilon)
=\beta^\star+(X^\top X)^{-1}X^\top \varepsilon.\\
\mathrm{Var}(\hat\beta)&=(X^\top X)^{-1}X^\top \sigma^2 I X (X^\top X)^{-1}
=\sigma^2 (X^\top X)^{-1}.\\
X^\top X&=\begin{bmatrix}3&3\\3&5\end{bmatrix},\
(X^\top X)^{-1}=\frac{1}{6}\begin{bmatrix}5&-3\\-3&3\end{bmatrix}.
\end{align*}
}
\RESULT{
$\mathrm{Var}(\hat\beta)=\frac{1}{6}\begin{bmatrix}5&-3\\-3&3\end{bmatrix}$ for
$\sigma^2=1$.
}
\UNITCHECK{
Variance matrix is $2\times 2$, symmetric positive semidefinite.
}
\EDGECASES{
\begin{bullets}
\item As $n$ grows with well-conditioned $X^\top X$, variance shrinks.
\end{bullets}
}
\ALTERNATE{
Derive via QR: $\hat\beta=R^{-1}Q^\top y$, then propagate variance through $R$.
}
\VALIDATION{
\begin{bullets}
\item Monte Carlo simulation confirms sample covariance near the expression.
\end{bullets}
}
\INTUITION{
Noise projects through the inverse Gram matrix, inflating directions with small
singular values.
}
\CANONICAL{
\begin{bullets}
\item Variance equals $\sigma^2$ times inverse of the Gram matrix.
\end{bullets}
}

\clearpage
\section{Coding Demonstrations}
\CodeDemoPage{Solve OLS via Normal Equations and QR (Verification)}
\PROBLEM{
Implement OLS using normal equations and QR, verify equality of solutions, and
orthogonality of residuals.
}
\API{
\begin{bullets}
\item \inlinecode{def read_input(s) -> tuple} — parse n,d,X,y.
\item \inlinecode{def solve_normal(X,y) -> beta} — normal equations.
\item \inlinecode{def solve_qr(X,y) -> beta} — thin QR solve.
\item \inlinecode{def validate() -> None} — assertions for correctness.
\item \inlinecode{def main() -> None} — run validation and a sample.
\end{bullets}
}
\INPUTS{
Matrix $X$ (list of lists) and $y$ (list). Shapes $(n,d)$ and $(n,)$. Entries
finite real numbers.
}
\OUTPUTS{
Coefficient vector $\beta$ as list of length $d$; residual vector; checks of
orthogonality and solution equivalence.
}
\FORMULA{
\[
\hat\beta=(X^\top X)^{-1}X^\top y,\quad \hat\beta=R^{-1}Q^\top y.
\]
}
\textbf{SOLUTION A — From Scratch (Mathematically Explicit Implementation)}
\begin{codepy}
import numpy as np

def read_input(s):
    vals = [float(x) for x in s.split()]
    n, d = int(vals[0]), int(vals[1])
    X = np.array(vals[2:2+n*d]).reshape(n, d)
    y = np.array(vals[2+n*d:2+n*d+n])
    return X, y

def solve_normal(X, y):
    G = X.T @ X
    b = X.T @ y
    beta = np.linalg.solve(G, b)
    return beta

def solve_qr(X, y):
    Q, R = np.linalg.qr(X, mode="reduced")
    beta = np.linalg.solve(R, Q.T @ y)
    return beta

def validate():
    X = np.array([[1.,0.],[1.,1.],[1.,2.]])
    y = np.array([1.,2.,2.])
    b1 = solve_normal(X, y)
    b2 = solve_qr(X, y)
    assert np.allclose(b1, b2, atol=1e-10)
    r = y - X @ b1
    assert np.allclose(X.T @ r, np.zeros(2), atol=1e-10)

def main():
    validate()
    X = np.array([[1.,1.],[1.,2.],[1.,3.]])
    y = np.array([2.,2.,4.])
    beta = solve_qr(X, y)
    print("beta", np.round(beta, 6))

if __name__ == "__main__":
    main()
\end{codepy}
\textbf{SOLUTION B — Library-Based (Validated Computational Shortcut)}
\begin{codepy}
import numpy as np
from sklearn.linear_model import LinearRegression

def read_input(s):
    vals = [float(x) for x in s.split()]
    n, d = int(vals[0]), int(vals[1])
    X = np.array(vals[2:2+n*d]).reshape(n, d)
    y = np.array(vals[2+n*d:2+n*d+n])
    return X, y

def solve_normal(X, y):
    XtX = X.T @ X
    Xty = X.T @ y
    return np.linalg.solve(XtX, Xty)

def solve_qr(X, y):
    model = LinearRegression(fit_intercept=False)
    model.fit(X, y)
    return model.coef_

def validate():
    X = np.array([[1.,0.],[1.,1.],[1.,2.]])
    y = np.array([1.,2.,2.])
    b1 = solve_normal(X, y)
    b2 = solve_qr(X, y)
    assert np.allclose(b1, b2, atol=1e-10)

def main():
    validate()
    X = np.array([[1.,1.],[1.,2.],[1.,3.]])
    y = np.array([2.,2.,4.])
    print("coef", np.round(solve_qr(X, y), 6))

if __name__ == "__main__":
    main()
\end{codepy}
\COMPLEXITY{
Both methods run in time $\mathcal{O}(nd^2+d^3)$ and space $\mathcal{O}(d^2)$.
}
\FAILMODES{
\begin{bullets}
\item Singular $X^\top X$ causes failure; handle by QR or SVD.
\item NaN or Inf entries; validate inputs before solving.
\end{bullets}
}
\STABILITY{
\begin{bullets}
\item Normal equations square condition number; use QR for better stability.
\item Prefer solves over explicit matrix inverses.
\end{bullets}
}
\VALIDATION{
\begin{bullets}
\item Assert $X^\top r=0$ and equality of solutions within tolerance.
\end{bullets}
}
\RESULT{
Both implementations produce identical $\hat\beta$ within numerical tolerance.
}
\EXPLANATION{
Normal equations and QR are algebraically equivalent; QR is numerically safer.
}

\CodeDemoPage{Weighted Least Squares via Prewhitening}
\PROBLEM{
Implement WLS using direct weighted normal equations and via prewhitening, and
verify equality.
}
\API{
\begin{bullets}
\item \inlinecode{def wls_direct(X,y,W) -> beta}
\item \inlinecode{def wls_prewhite(X,y,W) -> beta}
\item \inlinecode{def validate() -> None}
\item \inlinecode{def main() -> None}
\end{bullets}
}
\INPUTS{
$X\in\mathbb{R}^{n\times d}$, $y\in\mathbb{R}^n$, $W\succ 0$ diagonal or full.
}
\OUTPUTS{
$\hat\beta$ for both methods; they should match numerically.
}
\FORMULA{
\[
\hat\beta=(X^\top W X)^{-1}X^\top W y
=\arg\min_\beta \|C(X\beta-y)\|_2^2,\ C^\top C=W.
\]
}
\textbf{SOLUTION A — From Scratch (Mathematically Explicit Implementation)}
\begin{codepy}
import numpy as np

def wls_direct(X, y, W):
    G = X.T @ W @ X
    b = X.T @ W @ y
    return np.linalg.solve(G, b)

def wls_prewhite(X, y, W):
    C = np.linalg.cholesky(W)
    X2 = C @ X
    y2 = C @ y
    return np.linalg.lstsq(X2, y2, rcond=None)[0]

def validate():
    X = np.array([[1.,1.],[1.,2.],[1.,3.]])
    y = np.array([1.,2.,2.])
    W = np.diag([1.,9.,1.])
    b1 = wls_direct(X, y, W)
    b2 = wls_prewhite(X, y, W)
    assert np.allclose(b1, b2, atol=1e-10)

def main():
    validate()
    print("ok")

if __name__ == "__main__":
    main()
\end{codepy}
\textbf{SOLUTION B — Library-Based (Validated Computational Shortcut)}
\begin{codepy}
import numpy as np
import statsmodels.api as sm

def wls_direct(X, y, W):
    return np.linalg.solve(X.T @ W @ X, X.T @ W @ y)

def wls_prewhite(X, y, W):
    model = sm.WLS(y, X, weights=np.diag(W))
    res = model.fit()
    return res.params

def validate():
    X = np.array([[1.,1.],[1.,2.],[1.,3.]])
    y = np.array([1.,2.,2.])
    W = np.diag([1.,9.,1.])
    b1 = wls_direct(X, y, W)
    b2 = wls_prewhite(X, y, W)
    assert np.allclose(b1, b2, atol=1e-8)

def main():
    validate()
    print("ok")

if __name__ == "__main__":
    main()
\end{codepy}
\COMPLEXITY{
Time $\mathcal{O}(nd^2+d^3)$; space $\mathcal{O}(d^2)$; Cholesky $\mathcal{O}(n^3)$
if dense $W$, $\mathcal{O}(n)$ if diagonal.
}
\FAILMODES{
\begin{bullets}
\item Non-SPD $W$ leads to failure; ensure symmetry and positive definiteness.
\item Ill-conditioned $X^\top W X$; consider regularization.
\end{bullets}
}
\STABILITY{
\begin{bullets}
\item Prewhitening with Cholesky is stable; avoid forming $W^{-1}$.
\end{bullets}
}
\VALIDATION{
\begin{bullets}
\item Compare direct and prewhitened coefficients with tight tolerance.
\end{bullets}
}
\RESULT{
Both variants yield identical WLS coefficients within tolerance.
}
\EXPLANATION{
Prewhitening transforms WLS into OLS in a scaled space, preserving the minimizer.
}

\CodeDemoPage{Ridge Regression and Conditioning}
\PROBLEM{
Implement ridge regression normal equations and compare conditioning and
solutions versus OLS.
}
\API{
\begin{bullets}
\item \inlinecode{def ridge(X,y,lmbd) -> beta}
\item \inlinecode{def cond_xtx(X) -> float}
\item \inlinecode{def validate() -> None}
\item \inlinecode{def main() -> None}
\end{bullets}
}
\INPUTS{
$X$ with near-collinear columns, $y$, and $\lambda>0$.
}
\OUTPUTS{
$\hat\beta_\lambda$, condition numbers of $X$ and $X^\top X$.
}
\FORMULA{
\[
\hat\beta_\lambda=(X^\top X+\lambda I)^{-1}X^\top y.
\]
}
\textbf{SOLUTION A — From Scratch (Mathematically Explicit Implementation)}
\begin{codepy}
import numpy as np

def ridge(X, y, lmbd):
    G = X.T @ X + lmbd * np.eye(X.shape[1])
    b = X.T @ y
    return np.linalg.solve(G, b)

def cond_xtx(X):
    s = np.linalg.svd(X, compute_uv=False)
    kX = s[0]/s[-1]
    G = X.T @ X
    e = np.linalg.eigvalsh(G)
    kG = e[-1]/e[0]
    return float(kX), float(kG)

def validate():
    X = np.array([[1.,1.],[1.,1.001],[1.,1.002],[1.,1.003]])
    y = np.array([2.,2.1,2.2,2.3])
    kX, kG = cond_xtx(X)
    b0 = ridge(X, y, 0.0)
    b1 = ridge(X, y, 1e-2)
    assert kG >= kX**2 - 1e-6
    assert np.linalg.norm(b1) <= 10*np.linalg.norm(b0) + 1.0

def main():
    validate()
    print("ok")

if __name__ == "__main__":
    main()
\end{codepy}
\textbf{SOLUTION B — Library-Based (Validated Computational Shortcut)}
\begin{codepy}
import numpy as np
from sklearn.linear_model import Ridge

def ridge(X, y, lmbd):
    model = Ridge(alpha=lmbd, fit_intercept=False, solver="auto")
    model.fit(X, y)
    return model.coef_

def cond_xtx(X):
    s = np.linalg.svd(X, compute_uv=False)
    kX = s[0]/s[-1]
    e = np.linalg.eigvalsh(X.T @ X)
    kG = e[-1]/e[0]
    return float(kX), float(kG)

def validate():
    X = np.array([[1.,1.],[1.,1.001],[1.,1.002],[1.,1.003]])
    y = np.array([2.,2.1,2.2,2.3])
    b = ridge(X, y, 1e-2)
    assert np.isfinite(b).all()

def main():
    validate()
    print("ok")

if __name__ == "__main__":
    main()
\end{codepy}
\COMPLEXITY{
Forming $X^\top X$ and solving is $\mathcal{O}(nd^2+d^3)$.
}
\FAILMODES{
\begin{bullets}
\item Setting $\lambda=0$ with rank deficiency causes singular solves.
\item Excessive $\lambda$ underfits severely.
\end{bullets}
}
\STABILITY{
\begin{bullets}
\item Ridge improves conditioning; eigenvalues shift by $\lambda$.
\end{bullets}
}
\VALIDATION{
\begin{bullets}
\item Compare norms and check finite outputs across methods.
\end{bullets}
}
\RESULT{
Ridge yields stable coefficients when OLS is ill-conditioned.
}
\EXPLANATION{
Regularized normal equations guarantee invertibility and reduce variance.

}

\clearpage
\section{Applied Domains — Detailed End-to-End Scenarios}
\DomainPage{Machine Learning}
\SCENARIO{
Predict target $y$ from numerical features $X$ using least squares. Estimate
$\beta$ by normal equations and validate with QR.
}
\ASSUMPTIONS{
\begin{bullets}
\item Linear model $y=X\beta+\varepsilon$ with zero-mean noise.
\item Columns of $X$ linearly independent.
\end{bullets}
}
\WHICHFORMULA{
OLS normal equations: $\beta=(X^\top X)^{-1}X^\top y$, $\hat y=X\beta$.
}
\varmapStart
\var{X}{Design matrix $(n,d)$ including intercept.}
\var{y}{Target vector.}
\var{\beta}{Coefficients.}
\var{r}{Residuals $y-\hat y$.}
\varmapEnd
\PIPELINE{
\begin{bullets}
\item Generate synthetic linear data.
\item Fit using normal equations and QR.
\item Evaluate RMSE and $R^2$.
\end{bullets}
}
\textbf{Implementation (From Scratch)}
\begin{codepy}
import numpy as np

def generate_data(n=100, noise=0.5, seed=0):
    rng = np.random.default_rng(seed)
    X = np.column_stack([np.ones(n), np.linspace(0, 10, n)])
    beta_true = np.array([1.0, 2.0])
    y = X @ beta_true + rng.normal(0.0, noise, n)
    return X, y, beta_true

def ols_normal(X, y):
    return np.linalg.solve(X.T @ X, X.T @ y)

def metrics(X, y, b):
    yhat = X @ b
    rmse = np.sqrt(np.mean((y - yhat)**2))
    ss_res = np.sum((y - yhat)**2)
    ss_tot = np.sum((y - y.mean())**2)
    r2 = 1.0 - ss_res/ss_tot
    return rmse, r2

def main():
    X, y, b_true = generate_data()
    b_hat = ols_normal(X, y)
    rmse, r2 = metrics(X, y, b_hat)
    print("beta_hat", np.round(b_hat, 3), "RMSE", round(rmse, 3),
          "R2", round(r2, 3))

if __name__ == "__main__":
    main()
\end{codepy}
\textbf{Implementation (Library Version)}
\begin{codepy}
import numpy as np
from sklearn.linear_model import LinearRegression

def main():
    n = 100
    X = np.linspace(0, 10, n).reshape(-1, 1)
    y = 1.0 + 2.0 * X.flatten()
    model = LinearRegression()
    model.fit(X, y)
    print("coef", np.round(model.coef_, 3),
          "intercept", round(model.intercept_, 3))

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{RMSE near noise level and $R^2$ close to one for low noise.}
\INTERPRET{Coefficients recover slope and intercept; residuals are orthogonal.}
\NEXTSTEPS{Add regularization or WLS for heteroscedastic data.}

\DomainPage{Quantitative Finance}
\SCENARIO{
Estimate a one-factor model by least squares: asset returns $r=X\beta+\varepsilon$
with factor $X$ and compute $\hat\beta$ as exposure.
}
\ASSUMPTIONS{
\begin{bullets}
\item Linear factor model; zero-mean residuals.
\item No multicollinearity for multi-factor case.
\end{bullets}
}
\WHICHFORMULA{
OLS normal equations: $\hat\beta=(X^\top X)^{-1}X^\top r$.
}
\varmapStart
\var{X}{Factor time series $(n,1)$ augmented with intercept.}
\var{r}{Asset returns $(n,)$.}
\var{\beta}{Intercept and beta exposure.}
\varmapEnd
\PIPELINE{
\begin{bullets}
\item Simulate factor and asset with known beta.
\item Fit by OLS.
\item Interpret exposure and residual variance.
\end{bullets}
}
\textbf{Implementation (Full Pipeline)}
\begin{codepy}
import numpy as np

def simulate(n=252, beta=(0.0, 1.2), sigma=0.01, seed=0):
    rng = np.random.default_rng(seed)
    f = rng.normal(0.0, 0.02, n)
    X = np.column_stack([np.ones(n), f])
    r = X @ np.array(beta) + rng.normal(0.0, sigma, n)
    return X, r, f, beta

def fit_ols(X, r):
    return np.linalg.solve(X.T @ X, X.T @ r)

def main():
    X, r, f, b_true = simulate()
    b_hat = fit_ols(X, r)
    print("beta_true", b_true, "beta_hat", np.round(b_hat, 3))

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{Report estimated beta close to true value and residual variance.}
\INTERPRET{Slope is factor exposure; intercept is expected alpha.}
\NEXTSTEPS{Extend to WLS if variance changes over time.}

\DomainPage{Deep Learning}
\SCENARIO{
Train a linear layer with MSE to fit synthetic linear data and compare
parameters to OLS solution from normal equations.
}
\ASSUMPTIONS{
\begin{bullets}
\item MSE loss; gradient descent converges to LS solution.
\item No regularization; fit_intercept handled in features.
\end{bullets}
}
\WHICHFORMULA{
$\beta=(X^\top X)^{-1}X^\top y$; gradient $-2X^\top(y-X\beta)$.
}
\PIPELINE{
\begin{bullets}
\item Generate data with known coefficients.
\item Train a single-layer network.
\item Compare to OLS closed-form solution.
\end{bullets}
}
\textbf{Implementation (End-to-End)}
\begin{codepy}
import numpy as np
import tensorflow as tf

def generate():
    rng = np.random.default_rng(0)
    X = np.column_stack([np.ones(100), np.linspace(0, 5, 100)])
    beta = np.array([1.0, 2.0])
    y = X @ beta + rng.normal(0.0, 0.1, 100)
    return X.astype(np.float32), y.astype(np.float32)

def train_nn(X, y):
    model = tf.keras.Sequential([tf.keras.layers.Dense(
        1, use_bias=False, input_shape=[2])])
    model.compile(optimizer=tf.keras.optimizers.SGD(0.1), loss="mse")
    model.fit(X, y, epochs=200, verbose=0)
    w = model.get_weights()[0].flatten()
    return w

def ols(X, y):
    return np.linalg.solve(X.T @ X, X.T @ y)

def main():
    X, y = generate()
    w_nn = train_nn(X, y)
    w_ols = ols(X, y)
    print("nn", np.round(w_nn, 3), "ols", np.round(w_ols, 3))

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{Compare weights and RMSE between NN and OLS solutions.}
\INTERPRET{Gradient descent on MSE converges to normal equations solution.}
\NEXTSTEPS{Add ridge (weight decay) and compare to ridge normal equations.}

\DomainPage{Kaggle / Data Analytics}
\SCENARIO{
Create a baseline linear regression using least squares on a synthetic dataset,
report coefficients, $R^2$, and residual diagnostics.
}
\ASSUMPTIONS{
\begin{bullets}
\item Numeric features standardized; intercept included.
\end{bullets}
}
\WHICHFORMULA{
OLS: $\hat\beta=(X^\top X)^{-1}X^\top y$, diagnostics via projection matrix.
}
\PIPELINE{
\begin{bullets}
\item Generate correlated features and target.
\item Fit OLS and compute $R^2$.
\item Inspect leverage via diagonal of $P$.
\end{bullets}
}
\textbf{Implementation (Complete EDA Pipeline)}
\begin{codepy}
import numpy as np
import pandas as pd

def create_df(seed=0, n=200):
    rng = np.random.default_rng(seed)
    A = rng.normal(size=n)
    B = 0.8 * A + rng.normal(scale=0.3, size=n)
    C = rng.normal(size=n) * 0.5
    X = np.column_stack([np.ones(n), A, B, C])
    beta = np.array([1.0, 2.0, -1.0, 0.5])
    y = X @ beta + rng.normal(scale=0.3, size=n)
    return pd.DataFrame({"A": A, "B": B, "C": C, "y": y})

def fit_ols(df):
    X = np.column_stack([np.ones(len(df)),
                         df["A"].values, df["B"].values, df["C"].values])
    y = df["y"].values
    b = np.linalg.solve(X.T @ X, X.T @ y)
    yhat = X @ b
    r2 = 1 - ((y - yhat) @ (y - yhat)) / ((y - y.mean()) @ (y - y.mean()))
    P = X @ np.linalg.solve(X.T @ X, X.T)
    h = np.diag(P)
    return b, r2, h

def main():
    df = create_df()
    b, r2, h = fit_ols(df)
    print("beta", np.round(b, 3), "R2", round(r2, 3),
          "leverage_mean", round(h.mean(), 3))

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{$R^2$, mean leverage equals $d/n$ where $d$ includes intercept.}
\INTERPRET{Coefficients recover signal; leverage highlights influential points.}
\NEXTSTEPS{Use WLS or ridge if multicollinearity or heteroscedasticity present.}

\end{document}
% !TeX program = xelatex
\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{microtype,setspace,amsmath,amssymb,mathtools,amsthm,unicode-math}
\setstretch{1.05}
\setmainfont{Latin Modern Roman}
\setmonofont{Latin Modern Mono}
\setmathfont{Latin Modern Math}

% --- Overflow / line-break safety ---
\allowdisplaybreaks[4]
\setlength{\jot}{7pt}
\setlength{\emergencystretch}{8em}
\sloppy

\usepackage{xcolor,fancyhdr,enumitem,inconsolata,listings}
\pagestyle{fancy}\fancyhf{}\lhead{\nouppercase{\leftmark}}\rhead{\thepage}
\setlength{\headheight}{26pt}
\setlength{\parindent}{0pt}
\setlength{\parskip}{8pt plus 2pt minus 1pt}
\raggedbottom

% --- Breakable math helpers (use these in the body) ---
\newenvironment{BreakableEquation}{\begin{equation}\begin{aligned}}{\end{aligned}\end{equation}}
\newenvironment{BreakableEquation*}{\begin{equation*}\begin{aligned}}{\end{aligned}\end{equation*}}
\newenvironment{tightalign}{\begingroup\small\allowdisplaybreaks\begin{align}}{\end{align}\endgroup}

% ---------- Safety shims ----------
% Robust minted → listings shim (no shell-escape; supports [opts]{language})
\providecommand{\enumlistm}{enumitem}
\newenvironment{minted}[2][]{%
  \lstset{style=code,language=#2,#1}\begin{lstlisting}%
}{\end{lstlisting}}

% Fallback for \inputminted (ignore file; keep build unbroken)
\newcommand{\inputminted}[3][]{\begin{lstlisting}\end{lstlisting}}

% ---------- Bulleted lines (no tables) ----------
\newlist{bullets}{itemize}{1}
\setlist[bullets]{label=--,leftmargin=1.6em,itemsep=2pt,topsep=4pt}

% ---------- Variable mapping (lines, no tables) ----------
\newcommand{\varmapStart}{\textbf{VARIABLE MAPPING:}\par\begin{bullets}}
\newcommand{\var}[2]{\item $#1$ — #2}
\newcommand{\varmapEnd}{\end{bullets}}

% ---------- Glossary item with ELI5 and Pitfall/Example ----------
\newcommand{\glossx}[6]{%
  \textbf{#1}\par
  \begin{bullets}
    \item \textbf{What:} #2
    \item \textbf{Why:} #3
    \item \textbf{How:} #4
    \item \textbf{ELI5:} #5
    \item \textbf{Pitfall/Example:} #6
  \end{bullets}
}

% ---------- Theorem structures ----------
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{claim}[theorem]{Claim}
% ---------- Code blocks ----------
\lstdefinestyle{code}{
  basicstyle=\ttfamily\footnotesize,
  backgroundcolor=\color{black!02},
  frame=single,
  numbers=left, numberstyle=\tiny, numbersep=8pt,
  breaklines=true, breakatwhitespace=true,
  tabsize=4, showstringspaces=false,
  upquote=true, keepspaces=true, columns=fullflexible,
  % Unicode safety: map common symbols so listings never chokes
  literate=
    {–}{{-}}1
    {—}{{-}}1
    {…}{{...}}1
    {≤}{{\ensuremath{\le}}}1
    {≥}{{\ensuremath{\ge}}}1
    {≠}{{\ensuremath{\ne}}}1
    {≈}{{\ensuremath{\approx}}}1
    {±}{{\ensuremath{\pm}}}1
    {→}{{\ensuremath{\to}}}1
    {←}{{\ensuremath{\leftarrow}}}1
    {∞}{{\ensuremath{\infty}}}1
    {√}{{\ensuremath{\sqrt{\ }}}}1
    {×}{{\ensuremath{\times}}}1
    {÷}{{\ensuremath{\div}}}1
}

% Main code environment for all Python blocks
\lstnewenvironment{codepy}[1][]%
  {\lstset{style=code,language=Python,#1}}%
  {}

% Inline code; change delimiters if your snippet contains '!'
\newcommand{\inlinecode}[1]{\lstinline[style=code]!#1!}

% ---------- Line-label macros ----------
\newcommand{\LF}[2]{\par\noindent\textbf{#1:}~#2\par}
\newcommand{\WHAT}[1]{\LF{WHAT}{#1}}
\newcommand{\WHY}[1]{\LF{WHY}{#1}}
\newcommand{\HOW}[1]{\LF{HOW}{#1}}
\newcommand{\ELI}[1]{\LF{ELI5}{#1}}
\newcommand{\SCOPE}[1]{\LF{SCOPE}{#1}}
\newcommand{\CONFUSIONS}[1]{\LF{COMMON CONFUSIONS}{#1}}
\newcommand{\APPLICATIONS}[1]{\LF{APPLICATIONS}{#1}}
\newcommand{\FORMULA}[1]{\LF{FORMULA}{#1}}
\newcommand{\CANONICAL}[1]{\LF{CANONICAL FORM}{#1}}
\newcommand{\PRECONDS}[1]{\LF{PRECONDITIONS}{#1}}
\newcommand{\DERIVATION}[1]{\LF{DERIVATION}{#1}}
\newcommand{\EQUIV}[1]{\LF{EQUIVALENT FORMS}{#1}}
\newcommand{\LIMITS}[1]{\LF{LIMIT CASES}{#1}}
\newcommand{\INPUTS}[1]{\LF{INPUTS}{#1}}
\newcommand{\OUTPUTS}[1]{\LF{OUTPUTS}{#1}}
\newcommand{\RESULT}[1]{\LF{RESULT}{#1}}
\newcommand{\INTUITION}[1]{\LF{INTUITION}{#1}}
\newcommand{\PITFALLS}[1]{\LF{PITFALLS}{#1}}
\newcommand{\MODEL}[1]{\LF{CANONICAL MATH MODEL}{#1}}
\newcommand{\ASSUMPTIONS}[1]{\LF{ASSUMPTIONS}{#1}}
\newcommand{\WHICHFORMULA}[1]{\LF{WHICH FORMULA \& WHY}{#1}}
\newcommand{\GOVERN}[1]{\LF{GOVERNING EQUATION(S)}{#1}}
\newcommand{\UNITCHECK}[1]{\LF{UNIT CHECK}{#1}}
\newcommand{\EDGECASES}[1]{\LF{EDGE CASES}{#1}}
\newcommand{\ALTERNATE}[1]{\LF{ALTERNATE APPROACH (sketch)}{#1}}
\newcommand{\PROBLEM}[1]{\LF{PROBLEM}{#1}}
\newcommand{\API}[1]{\LF{API}{#1}}
\newcommand{\COMPLEXITY}[1]{\LF{COMPLEXITY}{#1}}
\newcommand{\FAILMODES}[1]{\LF{FAILURE MODES}{#1}}
\newcommand{\STABILITY}[1]{\LF{NUMERICAL STABILITY}{#1}}
\newcommand{\VALIDATION}[1]{\LF{VALIDATION}{#1}}
\newcommand{\EXPLANATION}[1]{\LF{EXPLANATION}{#1}}
\newcommand{\SCENARIO}[1]{\LF{SCENARIO}{#1}}
\newcommand{\PIPELINE}[1]{\LF{PIPELINE STEPS}{#1}}
\newcommand{\METRICS}[1]{\LF{METRICS}{#1}}
\newcommand{\INTERPRET}[1]{\LF{INTERPRETATION}{#1}}
\newcommand{\NEXTSTEPS}[1]{\LF{LIMITATIONS \& NEXT STEPS}{#1}}

% ---------- Section formatting ----------
\usepackage{titlesec}
\titleformat{\section}{\Large\bfseries}{\thesection}{0.6em}{}
\titlespacing*{\section}{0pt}{*2}{*1}
\usepackage{etoolbox}
\pretocmd{\section}{\clearpage}{}{}

% ---------- Page helpers ----------
\newcommand{\FormulaPage}[2]{%
  \clearpage
  \section*{Formula #1 — #2}%
  \addcontentsline{toc}{section}{Formula #1 — #2}%
}
\newcommand{\ProblemPage}[2]{%
  \clearpage
  \subsection*{Problem #1: #2}%
  \addcontentsline{toc}{subsection}{Problem #1: #2}%
}
\newcommand{\CodeDemoPage}[1]{%
  \clearpage
  \subsection*{Coding Demo: #1}%
  \addcontentsline{toc}{subsection}{Coding Demo: #1}%
}
\newcommand{\DomainPage}[1]{%
  \clearpage
  \subsection*{#1 (End-to-End)}%
  \addcontentsline{toc}{subsection}{#1 (End-to-End)}%
}

\begin{document}
\title{Comprehensive Study Sheet — Cayley-Hamilton Theorem}
\date{\today}
\maketitle
\tableofcontents
\clearpage

\section{Concept Overview}
\WHAT{
Let $\mathbb{F}$ be a field and $A\in M_n(\mathbb{F})$.
The characteristic polynomial of $A$ is
$p_A(\lambda)=\det(\lambda I_n-A)\in\mathbb{F}[\lambda]$.
The Cayley\textendash Hamilton theorem states that a matrix satisfies its
own characteristic polynomial:
$p_A(A)=\mathbf{0}_{n\times n}$, where $p_A(A)$ is defined via the
polynomial functional calculus.
}

\WHY{
Cayley\textendash Hamilton links spectral information (coefficients of
$p_A$) to algebraic identities satisfied by $A$.
It yields linear recurrences for powers of $A$, formulae for $A^{-1}$
when $A$ is invertible, and proves that the minimal polynomial divides
the characteristic polynomial.
It is foundational for rational canonical form, controllability in
systems, and closed forms of linear recurrences.
}

\HOW{
1. Define $p_A(\lambda)=\det(\lambda I-A)$ and the adjugate
$\operatorname{adj}(\lambda I-A)$.
2. Use the identity
$\operatorname{adj}(\lambda I-A)(\lambda I-A)=\det(\lambda I-A)I$,
a polynomial identity in $\lambda$ with matrix coefficients.
3. Evaluate at $\lambda=A$ to get $p_A(A)=\mathbf{0}$, using that
$(A I-A)=\mathbf{0}$ annihilates all terms except those from the adjugate
expansion.
4. Interpret: the $n^{\text{th}}$ power of $A$ is a linear combination
of lower powers with coefficients from $p_A$.
}

\ELI{
Compute the fingerprint of a matrix by taking a determinant that depends
on a symbol $\lambda$. Cayley\textendash Hamilton says the matrix
recognizes its own fingerprint: when you plug the matrix into that
polynomial, it becomes the zero matrix. This lets you reduce any high
power of the matrix to a short recipe using only the first $n$ powers.
}

\SCOPE{
Over any field $\mathbb{F}$, the theorem holds for all $A\in M_n(\mathbb{F})$.
It extends to matrices over commutative rings with identity by the same
adjugate identity argument.
Degenerate cases include $n=1$ where $p_A(\lambda)=\lambda-a$ and
$p_A(A)=A-aI=0$, diagonal matrices where the identity is immediate,
and nilpotent matrices where $p_A(\lambda)=\lambda^n$ implies $A^n=0$.
}

\CONFUSIONS{
Do not confuse characteristic polynomial $p_A$ with minimal polynomial
$m_A$. Cayley\textendash Hamilton asserts $p_A(A)=0$; it does not assert
$p_A$ is minimal. Also do not confuse evaluating a polynomial at a
scalar with evaluating at a matrix; the latter uses the same coefficients
but replaces powers by matrix powers and the constant by a scalar
multiple of $I$.
}

\APPLICATIONS{
List 3--4 major domains where this topic directly applies:
\begin{bullets}
\item Mathematical foundations (Jordan and rational canonical forms).
\item Computational modeling or simulation (matrix power reduction).
\item Physical or engineering interpretations (state transition in LTI).
\item Statistical or algorithmic implications (linear recurrences, AR models).
\end{bullets}
}

\textbf{ANALYTIC STRUCTURE.}
Matrices form a unital algebra over $\mathbb{F}$.
Characteristic polynomials are monic degree $n$ polynomials.
Cayley\textendash Hamilton asserts a monic linear dependence among
$I,A,\dots,A^n$.

\textbf{CANONICAL LINKS.}
Adjugate identity implies Cayley\textendash Hamilton.
Cayley\textendash Hamilton implies power reduction and polynomial
functional calculus. Minimal polynomial divisibility follows.

\textbf{PROBLEM-TYPE RECOGNITION HEURISTICS.}
\begin{bullets}
\item Presence of $\det(\lambda I-A)$, traces, or coefficients of $p_A$.
\item Tasks asking to express $A^k$ for $k\ge n$ via lower powers.
\item Requests for $A^{-1}$ using only $A$ and scalars.
\item Linear recurrences tied to companion matrices.
\end{bullets}

\textbf{SOLUTION STRATEGY BLUEPRINT.}
\begin{bullets}
\item Translate the goal into $p_A(A)=0$ or $m_A(A)=0$.
\item Write $p_A(\lambda)=\lambda^n+a_{n-1}\lambda^{n-1}+\cdots+a_0$.
\item Replace $\lambda^k$ by $A^k$ to get $A^n$ as a combination of
$I,\dots,A^{n-1}$.
\item For inverses, multiply by $A^{-1}$ when $\det(A)\ne0$.
\item Validate using eigenvalues or a numeric check.
\end{bullets}

\textbf{CONCEPTUAL INVARIANTS.}
Degree $n$ monic polynomial $p_A$; determinant and trace appear as
coefficients; similarity invariance of $p_A$ and of the identity
$p_A(A)=0$.

\textbf{EDGE INTUITION.}
If $A$ is diagonalizable, $p_A(A)=0$ is equivalent to applying the
polynomial to each eigenvalue.
If $A$ is nilpotent, the identity reduces to $A^n=0$.
If $A$ is scalar $cI$, then $p_A(\lambda)=(\lambda-c)^n$ and
$p_A(A)=(A-cI)^n=0$.

\clearpage
\section{Glossary}
\glossx{Characteristic Polynomial}
{The polynomial $p_A(\lambda)=\det(\lambda I_n-A)$ for $A\in M_n(\mathbb{F})$.}
{Encodes eigenvalues as roots, with coefficients related to traces of
exterior powers, including $\operatorname{tr}(A)$ and $\det(A)$.}
{Form the matrix $\lambda I-A$, compute its determinant symbolically,
obtain a monic degree $n$ polynomial.}
{Like a barcode of the matrix; plug in any number to measure how
singular $\lambda I-A$ is.}
{Pitfall: wrong sign conventions for coefficients lead to errors such as
$\operatorname{tr}$ and $\det$ sign mistakes.}

\glossx{Adjugate Matrix}
{For $B\in M_n(\mathbb{F})$, $\operatorname{adj}(B)$ is the transpose of
the cofactor matrix, satisfying $\operatorname{adj}(B)B=\det(B)I$.}
{It yields the polynomial identity that drives the proof of
Cayley\textendash Hamilton.}
{Compute minors, apply cofactors, assemble the adjugate; use the identity
with $B=\lambda I-A$.}
{A matrix that undoes $B$ multiplicatively up to a scalar
$\det(B)$.}
{Example: For $2\times2$, $\operatorname{adj}\begin{pmatrix}a&b\\c&d\end{pmatrix}
=\begin{pmatrix}d&-b\\-c&a\end{pmatrix}$.}

\glossx{Minimal Polynomial}
{The unique monic polynomial $m_A$ of least degree with $m_A(A)=0$.}
{It captures the algebraic relations of $A$ and divides $p_A$.}
{Prove divisibility by showing any annihilating polynomial is a multiple
of $m_A$.}
{The shortest recipe that kills the matrix.}
{Pitfall: $m_A$ can have strictly lower degree than $p_A$ when $A$ is
defective or has repeated eigenvalues.}

\glossx{Polynomial Functional Calculus}
{Evaluation map $\phi:\mathbb{F}[\lambda]\to M_n(\mathbb{F})$ given by
$\phi\big(\sum c_k\lambda^k\big)=\sum c_k A^k$.}
{Allows substituting $A$ for $\lambda$ consistently and proves that
$p_A(A)$ is well defined.}
{Use linearity and multiplication compatibility
$\phi(fg)=\phi(f)\phi(g)$ to manipulate identities.}
{Treat polynomials as instructions that combine $I$ and powers of $A$.}
{Example: Horner evaluates $f(A)$ efficiently and stably.}

\clearpage
\section{Symbol Ledger}
\varmapStart
\var{\mathbb{F}}{Base field for matrices and coefficients.}
\var{A}{Matrix in $M_n(\mathbb{F})$.}
\var{I_n}{Identity matrix of size $n$.}
\var{p_A}{Characteristic polynomial of $A$.}
\var{m_A}{Minimal polynomial of $A$.}
\var{\lambda}{Indeterminate for polynomials.}
\var{\operatorname{adj}}{Adjugate operator on matrices.}
\var{\operatorname{tr}}{Trace of a matrix.}
\var{\det}{Determinant of a matrix.}
\var{n}{Matrix dimension (positive integer).}
\var{a_k}{Coefficient of $\lambda^k$ in a monic polynomial.}
\varmapEnd

\clearpage
\section{Formula Canon — One Formula Per Page}

\FormulaPage{1}{Characteristic Polynomial and Coefficient Identities}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
For $A\in M_n(\mathbb{F})$, $p_A(\lambda)=\det(\lambda I_n-A)$ is monic
of degree $n$, and its coefficients satisfy
$p_A(\lambda)=\lambda^n - (\operatorname{tr}A)\lambda^{n-1}+\cdots
+(-1)^n\det(A)$.

\WHAT{
Defines $p_A$ and relates the leading two symmetric coefficients to
$\operatorname{tr}(A)$ and $\det(A)$.
}

\WHY{
These identities anchor computations using Cayley\textendash Hamilton,
since $a_{n-1}=-\operatorname{tr}(A)$ and $a_0=(-1)^n\det(A)$ enter
directly into $p_A(A)=0$.
}

\FORMULA{
\[
p_A(\lambda)=\lambda^n + a_{n-1}\lambda^{n-1}+\cdots+a_0,\quad
a_{n-1}=-\operatorname{tr}(A),\quad a_0=(-1)^n\det(A).
\]
}

\CANONICAL{
Coefficients are the signed elementary symmetric polynomials in the
eigenvalues of $A$, invariant under similarity.
}

\PRECONDS{
\begin{bullets}
\item $A\in M_n(\mathbb{F})$ with $\mathbb{F}$ a field.
\item Determinant defined with multilinearity and alternating properties.
\end{bullets}
}

\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
For any $A\in M_n(\mathbb{F})$,
$p_A(\lambda)=\lambda^n - (\operatorname{tr}A)\lambda^{n-1}+\cdots
+(-1)^n\det(A)$ with leading coefficient $1$.
\end{lemma}
\begin{proof}
Consider $\det(\lambda I-A)$ as a polynomial in $\lambda$.
The coefficient of $\lambda^n$ is $1$, from the product of diagonal
entries $\lambda$ across all rows.
The coefficient of $\lambda^{n-1}$ equals minus the sum of diagonal
entries of $A$, because exactly one row contributes $-A$ in the
multilinear expansion, giving $-\operatorname{tr}(A)$.
The constant term is $\det(-A)=(-1)^n\det(A)$.
Intermediate coefficients follow from the standard expansion in
elementary symmetric polynomials of eigenvalues or by induction on $n$.
\qedhere
\end{proof}

\DERIVATION{
\begin{align*}
p_A(\lambda)&=\det(\lambda I-A)\\
&=\sum_{\sigma\in S_n}\operatorname{sgn}(\sigma)
\prod_{i=1}^n(\lambda\delta_{i,\sigma(i)}-a_{i,\sigma(i)})\\
&=\lambda^n-\big(\sum_i a_{ii}\big)\lambda^{n-1}+\cdots
+(-1)^n\det(A).
\end{align*}
}

\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Compute $p_A$ via determinant or with eigenvalues if available.
\item Read off $a_{n-1}$ and $a_0$ to use in Cayley\textendash Hamilton.
\item Use $p_A(A)=0$ to reduce powers or form inverses.
\end{bullets}

\EQUIV{
\begin{bullets}
\item $p_A(\lambda)=\prod_{i=1}^n(\lambda-\lambda_i)$ over an algebraic
closure, where $\lambda_i$ are eigenvalues with algebraic multiplicities.
\item Coefficients are signed elementary symmetric polynomials in
$\{\lambda_i\}$.
\end{bullets}
}

\LIMITS{
\begin{bullets}
\item For $n=1$, $p_A(\lambda)=\lambda-a$.
\item For nilpotent $A$, $a_0=0$.
\item Over nonfields, identities remain if determinant is defined in the
commutative ring.
\end{bullets}
}

\INPUTS{$A\in M_n(\mathbb{F})$, entries $a_{ij}$.}

\DERIVATION{
\begin{align*}
a_{n-1}&=-\operatorname{tr}(A),\quad a_0=(-1)^n\det(A),\\
p_A(A)&=A^n+a_{n-1}A^{n-1}+\cdots+a_0 I_n.
\end{align*}
}

\RESULT{
Coefficient identities extracted for later substitution into
$p_A(A)=0$.
}

\UNITCHECK{
Polynomial degree is $n$, coefficients are similarity invariant scalars.
}

\PITFALLS{
\begin{bullets}
\item Sign errors in $a_{n-1}$ and $a_0$.
\item Forgetting that $p_A$ is monic of degree $n$.
\end{bullets}
}

\INTUITION{
The determinant expansion tracks how many times the symbolic $\lambda$
appears along a term; the first missing $\lambda$ yields trace, and the
absence of $\lambda$ yields determinant.
}

\CANONICAL{
\begin{bullets}
\item Monic degree $n$ invariant $p_A$ with first two coefficients tied
to trace and determinant.
\end{bullets}
}

\FormulaPage{2}{Adjugate Polynomial Identity}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
For $B(\lambda)=\lambda I_n-A$, the adjugate satisfies
$\operatorname{adj}(B(\lambda))\,B(\lambda)=\det(B(\lambda))I_n$,
an identity in $\mathbb{F}[\lambda]^{n\times n}$.

\WHAT{
A matrix polynomial identity that links adjugate, determinant, and
$B(\lambda)$.
}

\WHY{
Substituting $B(\lambda)$ with $A$ in a controlled way produces the
Cayley\textendash Hamilton equation $p_A(A)=0$.
}

\FORMULA{
\[
\operatorname{adj}(\lambda I_n-A)\,(\lambda I_n-A)
=\det(\lambda I_n-A)I_n=p_A(\lambda)I_n.
\]
}

\CANONICAL{
All entries of $\operatorname{adj}(\lambda I_n-A)$ are polynomials in
$\lambda$ of degree at most $n-1$.
}

\PRECONDS{
\begin{bullets}
\item Determinant and adjugate defined over the base field.
\item Multiplicativity $\det(XY)=\det(X)\det(Y)$ holds.
\end{bullets}
}

\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
For any square matrix $B$, $\operatorname{adj}(B)B=\det(B)I$.
\end{lemma}
\begin{proof}
The $(i,j)$ entry of $\operatorname{adj}(B)B$ equals the cofactor
expansion of $\det(B)$ along column $j$, giving $\det(B)$ if $i=j$ and
$0$ otherwise, by Laplace expansion properties.
\qedhere
\end{proof}

\DERIVATION{
\begin{align*}
\operatorname{adj}(\lambda I-A)\,(\lambda I-A)
&=\det(\lambda I-A)I\\
&=p_A(\lambda)I.
\end{align*}
Each side is a matrix whose entries are polynomials in $\lambda$.
}

\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Form $B(\lambda)=\lambda I-A$.
\item Use the adjugate identity to produce a polynomial identity.
\item Evaluate at $\lambda=A$ using polynomial calculus.
\end{bullets}

\EQUIV{
\begin{bullets}
\item $B(\lambda)\operatorname{adj}(B(\lambda))=p_A(\lambda)I$ as well.
\item Over commutative rings, the identity remains valid.
\end{bullets}
}

\LIMITS{
\begin{bullets}
\item When $n=1$, $\operatorname{adj}(b)=(1)$ and the identity is
$b=b$ scaled by determinant trivially.
\item Degrees: entries in the adjugate have degree at most $n-1$.
\end{bullets}
}

\INPUTS{$A\in M_n(\mathbb{F}),\ \lambda\in\mathbb{F}$.}

\DERIVATION{
\begin{align*}
\operatorname{adj}(\lambda I-A)&=
C_{n-1}\lambda^{n-1}+\cdots+C_0,\quad C_k\in M_n(\mathbb{F}),\\
\big(C_{n-1}\lambda^{n-1}+\cdots+C_0\big)(\lambda I-A)
&=p_A(\lambda)I.
\end{align*}
}

\RESULT{
A matrix polynomial identity ready for the substitution $\lambda\mapsto A$.
}

\UNITCHECK{
Polynomial degrees match: left side has degree $n$, right side $n$.
}

\PITFALLS{
\begin{bullets}
\item Treating the identity at a specific $\lambda$ before establishing
polynomial identity structure.
\item Forgetting that coefficients are matrices, not scalars.
\end{bullets}
}

\INTUITION{
Adjugate is the multiplicative inverse scaled by the determinant;
applied to $\lambda I-A$, it inverts it symbolically for generic
$\lambda$.
}

\CANONICAL{
\begin{bullets}
\item $\operatorname{adj}(B)B=\det(B)I$ is universal and underpins
Cayley\textendash Hamilton.
\end{bullets}
}

\FormulaPage{3}{Cayley\textendash Hamilton Theorem}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Every square matrix satisfies its characteristic polynomial:
$p_A(A)=\mathbf{0}_{n\times n}$.

\WHAT{
States and proves $A^n+a_{n-1}A^{n-1}+\cdots+a_0 I=0$ for
$p_A(\lambda)=\lambda^n+a_{n-1}\lambda^{n-1}+\cdots+a_0$.
}

\WHY{
It yields annihilating relations for $A$ enabling power reduction,
inverse formulas, and divisibility by the minimal polynomial.
}

\FORMULA{
\[
p_A(A)=A^n+a_{n-1}A^{n-1}+\cdots+a_0 I_n=\mathbf{0}_{n\times n}.
\]
}

\CANONICAL{
Holds over any commutative unital ring. Over a field, $p_A$ is monic of
degree $n$.
}

\PRECONDS{
\begin{bullets}
\item $A\in M_n(\mathbb{F})$.
\item $p_A$ defined as $\det(\lambda I-A)$.
\end{bullets}
}

\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
Let $Q(\lambda)\in\mathbb{F}[\lambda]^{n\times n}$ satisfy
$Q(\lambda)(\lambda I-A)=p_A(\lambda)I$. Then $p_A(A)=0$.
\end{lemma}
\begin{proof}
Write $Q(\lambda)=\sum_{k=0}^{n-1}C_k\lambda^k$.
By polynomial functional calculus,
$Q(A)(A I-A)=Q(A)\cdot 0=0$.
But substituting $\lambda\mapsto A$ in
$Q(\lambda)(\lambda I-A)=p_A(\lambda)I$ gives
$Q(A)(A I-A)=p_A(A)I$. Hence $p_A(A)I=0$, so $p_A(A)=0$.
\qedhere
\end{proof}

\DERIVATION{
\begin{align*}
\operatorname{adj}(\lambda I-A)(\lambda I-A)&=p_A(\lambda)I,\\
\text{set }Q(\lambda)&=\operatorname{adj}(\lambda I-A),\\
\Rightarrow\ Q(\lambda)(\lambda I-A)&=p_A(\lambda)I,\\
\text{Lemma yields }p_A(A)&=0.
\end{align*}
}

\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Compute $p_A(\lambda)$.
\item Substitute $A$ for $\lambda$ to write $A^n$ as a combination of
$I,\dots,A^{n-1}$.
\item Use the identity to reduce powers or form annihilating relations.
\end{bullets}

\EQUIV{
\begin{bullets}
\item If $A$ is diagonalizable with $A=S\Lambda S^{-1}$, then
$p_A(A)=S\,p_A(\Lambda)\,S^{-1}=0$ since $p_A(\Lambda)$ vanishes on each
eigenvalue.
\item Over an algebraic closure, $p_A$ splits and the result follows by
Jordan form as well.
\end{bullets}
}

\LIMITS{
\begin{bullets}
\item For $n=1$, the identity is $A-aI=0$ with $a=A$.
\item For nilpotent $A$, $p_A(\lambda)=\lambda^n$ implies $A^n=0$.
\end{bullets}
}

\INPUTS{$A\in M_n(\mathbb{F})$, $p_A(\lambda)=\lambda^n+\sum_{k=0}^{n-1}
a_k\lambda^k$.}

\DERIVATION{
\begin{align*}
p_A(A)&=A^n+\sum_{k=0}^{n-1}a_k A^k=0,\\
A^n&=-\sum_{k=0}^{n-1}a_k A^k.
\end{align*}
}

\RESULT{
Matrix $A$ is annihilated by its characteristic polynomial.
}

\UNITCHECK{
Degrees match the dimension: a single degree $n$ relation suffices to
span all higher powers linearly over $\mathbb{F}$.
}

\PITFALLS{
\begin{bullets}
\item Confusing $p_A(A)=0$ with $p_A= m_A$.
\item Forgetting to replace the scalar $1$ by $I$ when evaluating at $A$.
\end{bullets}
}

\INTUITION{
The symbolic inverse of $\lambda I-A$ exists up to a scalar
determinant. Plugging $\lambda=A$ collapses the factor and leaves the
zero matrix.
}

\CANONICAL{
\begin{bullets}
\item Universal annihilating identity $p_A(A)=0$.
\end{bullets}
}

\FormulaPage{4}{Minimal Polynomial Divides the Characteristic Polynomial}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
For $A\in M_n(\mathbb{F})$, if $m_A$ is the minimal polynomial of $A$,
then $m_A$ divides $p_A$ in $\mathbb{F}[\lambda]$.

\WHAT{
Relates the shortest annihilating polynomial to the characteristic
polynomial guaranteed by Cayley\textendash Hamilton.
}

\WHY{
Gives structural constraints on Jordan blocks and rational canonical
form, and bounds the degree of functional identities satisfied by $A$.
}

\FORMULA{
\[
m_A\mid p_A\quad\text{and}\quad m_A(A)=0,\ p_A(A)=0.
\]
}

\CANONICAL{
$m_A$ is monic with least degree such that $m_A(A)=0$.
Any annihilating polynomial is a multiple of $m_A$.
}

\PRECONDS{
\begin{bullets}
\item $A\in M_n(\mathbb{F})$.
\item Cayley\textendash Hamilton: $p_A(A)=0$.
\end{bullets}
}

\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
Let $m$ be monic of least degree with $m(A)=0$.
If $f(A)=0$, then $m\mid f$ in $\mathbb{F}[\lambda]$.
\end{lemma}
\begin{proof}
Divide $f$ by $m$: $f=qm+r$ with $\deg r<\deg m$.
Then $0=f(A)=q(A)m(A)+r(A)=r(A)$.
By minimality of $m$, $r=0$, so $m\mid f$.
\qedhere
\end{proof}

\DERIVATION{
\begin{align*}
p_A(A)&=0 \quad \text{(Cayley\textendash Hamilton)}\\
\text{By lemma, } m_A\mid p_A&\quad\text{since } m_A \text{ is minimal.}
\end{align*}
}

\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Use $p_A(A)=0$ to infer that $p_A$ is an annihilator.
\item Apply the division lemma to conclude $m_A\mid p_A$.
\item Use $m_A$ to bound sizes of Jordan blocks.
\end{bullets}

\EQUIV{
\begin{bullets}
\item $m_A=\prod_{i}(\lambda-\lambda_i)^{\alpha_i}$ where
$\alpha_i$ equals the size of the largest Jordan block for $\lambda_i$.
\item $p_A=\prod_{i}(\lambda-\lambda_i)^{m_i}$ with $m_i$ the algebraic
multiplicities, so $\alpha_i\le m_i$.
\end{bullets}
}

\LIMITS{
\begin{bullets}
\item If $A$ is diagonalizable, then $m_A$ has simple roots and divides
$p_A$ with the same distinct linear factors.
\item If $A=cI$, then $m_A(\lambda)=\lambda-c$ and
$p_A(\lambda)=(\lambda-c)^n$.
\end{bullets}
}

\INPUTS{$A\in M_n(\mathbb{F})$, $p_A$ from Formula 1, $m_A$ minimal monic.}

\DERIVATION{
\begin{align*}
p_A(A)=0 \Rightarrow&\ m_A\mid p_A,\\
\deg m_A \le&\ n.
\end{align*}
}

\RESULT{
$m_A$ divides $p_A$, constraining the algebraic structure of $A$.
}

\UNITCHECK{
Polynomial divisibility in $\mathbb{F}[\lambda]$ is well defined and
degree bounds are consistent with $n$.
}

\PITFALLS{
\begin{bullets}
\item Assuming $\deg m_A=\deg p_A$ without justification.
\item Ignoring that both are monic by convention.
\end{bullets}
}

\INTUITION{
The shortest recipe that zeroes $A$ must divide any longer recipe that
also zeroes $A$, including the characteristic one from
Cayley\textendash Hamilton.
}

\CANONICAL{
\begin{bullets}
\item $m_A\mid p_A$ captures the essential vs. redundant factors.
\end{bullets}
}

\FormulaPage{5}{Power Reduction via Cayley\textendash Hamilton}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
If $p_A(\lambda)=\lambda^n+a_{n-1}\lambda^{n-1}+\cdots+a_0$, then for
$k\ge n$,
$A^k$ is a linear combination of $I,A,\dots,A^{n-1}$ determined by the
recurrence from $p_A$.

\WHAT{
Expresses high powers of $A$ using only the first $n$ powers with scalar
coefficients from $p_A$.
}

\WHY{
Reduces computational cost and proves that $\{I,A,\dots,A^{n-1}\}$
spans all matrix powers.
}

\FORMULA{
\[
A^n=-\sum_{j=0}^{n-1}a_j A^j,\quad
A^{k}=-\sum_{j=0}^{n-1}a_j A^{k-n+j}\ \text{ for }k>n.
\]
}

\CANONICAL{
Coefficients $(a_j)$ are from the characteristic polynomial in monic
form; the recurrence is homogeneous linear with constant coefficients.
}

\PRECONDS{
\begin{bullets}
\item $A\in M_n(\mathbb{F})$, $p_A$ known.
\item Cayley\textendash Hamilton holds.
\end{bullets}
}

\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
If $A^n+\sum_{j=0}^{n-1}a_j A^j=0$, then for all $k\ge n$,
$A^k+\sum_{j=0}^{n-1}a_j A^{k-n+j}=0$.
\end{lemma}
\begin{proof}
Multiply the base relation on the left by $A^{k-n}$.
This yields $A^{k}+\sum_{j=0}^{n-1}a_j A^{k-n+j}=0$.
\qedhere
\end{proof}

\DERIVATION{
\begin{align*}
p_A(A)&=A^n+\sum_{j=0}^{n-1}a_j A^j=0,\\
\Rightarrow\ A^n&=-\sum_{j=0}^{n-1}a_j A^j,\\
\text{for }k>n:\ A^k&=A^{k-n}A^n\\
&=-\sum_{j=0}^{n-1}a_j A^{k-n+j}.
\end{align*}
}

\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Compute $p_A$ and list coefficients $a_j$.
\item Use the base case to reduce $A^n$.
\item Apply the recurrence iteratively or via Horner-like schemes.
\end{bullets}

\EQUIV{
\begin{bullets}
\item Vectorize by stacking $\operatorname{vec}(A^k)$ to get a linear
recurrence in $\mathbb{F}^{n^2}$ with companion matrix.
\item For diagonalizable $A$, the recurrence mirrors scalar recurrences
on eigenvalues.
\end{bullets}
}

\LIMITS{
\begin{bullets}
\item If $A$ is nilpotent, the recurrence gives $A^k=0$ for $k\ge n$.
\item If $A=cI$, then $A^k=c^k I$ which also follows from the recurrence.
\end{bullets}
}

\INPUTS{$A\in M_n(\mathbb{F})$, coefficients $a_0,\dots,a_{n-1}$.}

\DERIVATION{
\begin{align*}
\text{Compute }A^{10}:\ &\text{iterate }A^k=-\sum_{j=0}^{n-1}a_jA^{k-n+j}.\\
\text{Horner: }&((A+a_{n-1}I)A+\cdots)+a_0 I=0.
\end{align*}
}

\RESULT{
Closed recurrence enabling reduction of any $A^k$ to degree $<n$ in $A$.
}

\UNITCHECK{
Both sides are matrices; indices ensure powers stay nonnegative.
}

\PITFALLS{
\begin{bullets}
\item Misaligned indices in the recurrence.
\item Using coefficients from a nonmonic scaling of $p_A$.
\end{bullets}
}

\INTUITION{
The degree $n$ relation lets you repeatedly trade $A^n$ for lower powers
until nothing above $A^{n-1}$ remains.
}

\CANONICAL{
\begin{bullets}
\item Linear recurrence with constant coefficients induced by $p_A$.
\end{bullets}
}

\FormulaPage{6}{Inverse as a Polynomial in $A$ (When $\det A\ne0$)}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
If $A$ is invertible and
$p_A(\lambda)=\lambda^n+a_{n-1}\lambda^{n-1}+\cdots+a_0$ with
$a_0\ne0$, then
\[
A^{-1}=-\frac{1}{a_0}\Big(A^{n-1}+a_{n-1}A^{n-2}+\cdots+a_1 I\Big).
\]

\WHAT{
Computes $A^{-1}$ as a polynomial in $A$ of degree at most $n-1$ using
Cayley\textendash Hamilton.
}

\WHY{
Avoids direct inversion and uses only matrix multiplications and scalar
operations, useful in symbolic derivations and proofs.
}

\FORMULA{
\[
A^{-1}=-\frac{1}{a_0}\sum_{j=0}^{n-1} a_{j+1} A^{j},\quad
\text{where }a_n=1,\ a_{n-1},\dots,a_0\text{ as in }p_A.
\]
}

\CANONICAL{
Since $a_0=(-1)^n\det(A)$, the condition $a_0\ne0$ is equivalent to
$\det(A)\ne0$.
}

\PRECONDS{
\begin{bullets}
\item $A$ invertible $\Leftrightarrow \det(A)\ne0$.
\item $p_A(A)=0$ holds.
\end{bullets}
}

\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
If $A^n+\sum_{k=0}^{n-1}a_k A^k=0$ with $a_0\ne0$, then
$A^{-1}=-\frac{1}{a_0}\sum_{k=0}^{n-1}a_{k+1}A^{k}$.
\end{lemma}
\begin{proof}
Multiply $A^n+\sum_{k=0}^{n-1}a_k A^k=0$ on the left by $A^{-1}$:
$A^{n-1}+\sum_{k=0}^{n-1}a_k A^{k-1}=0$ with $A^{-1}$ at $k=0$.
Rearrange to isolate $A^{-1}$ and factor $-\frac{1}{a_0}$.
\qedhere
\end{proof}

\DERIVATION{
\begin{align*}
p_A(A)&=A^n+a_{n-1}A^{n-1}+\cdots+a_1A+a_0 I=0,\\
A^{-1}\big(A^n+\cdots+a_0 I\big)&=0,\\
A^{n-1}+a_{n-1}A^{n-2}+\cdots+a_1 I + a_0 A^{-1}&=0,\\
A^{-1}&=-\frac{1}{a_0}\Big(A^{n-1}+a_{n-1}A^{n-2}+\cdots+a_1 I\Big).
\end{align*}
}

\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Compute $p_A$ and read $a_0$ and up to $a_1$.
\item Form the polynomial in $A$ and scale by $-1/a_0$.
\item Validate by checking $A\cdot A^{-1}=I$ numerically or symbolically.
\end{bullets}

\EQUIV{
\begin{bullets}
\item If $A$ is $2\times2$ with
$p_A(\lambda)=\lambda^2-(\operatorname{tr}A)\lambda+\det A$,
then $A^{-1}=\frac{1}{\det(A)}\big(\operatorname{tr}(A)I-A\big)$.
\end{bullets}
}

\LIMITS{
\begin{bullets}
\item Not applicable if $\det(A)=0$.
\item For orthogonal $A$, the expression reduces to $A^\top$ if
$A^\top A=I$ and characteristic coefficients match accordingly.
\end{bullets}
}

\INPUTS{$A\in M_n(\mathbb{F})$ invertible, coefficients of $p_A$.}

\DERIVATION{
\begin{align*}
\text{Check: }A\cdot\Big(-\frac{1}{a_0}\sum_{k=0}^{n-1}a_{k+1}A^{k}\Big)
&=-\frac{1}{a_0}\sum_{k=0}^{n-1}a_{k+1}A^{k+1}\\
&=\frac{1}{a_0}\big(-A^n-\sum_{k=1}^{n-1}a_k A^{k}\big)\\
&=\frac{1}{a_0}\big(a_0 I\big)=I.
\end{align*}
}

\RESULT{
Closed polynomial formula for $A^{-1}$ in terms of powers of $A$.
}

\UNITCHECK{
Dimensionally consistent: both sides are $n\times n$ matrices.
}

\PITFALLS{
\begin{bullets}
\item Using nonmonic $p_A$ breaks the coefficient interpretation.
\item Forgetting to divide by $a_0$.
\end{bullets}
}

\INTUITION{
Invertibility means the constant term of $p_A$ is nonzero; this lets the
polynomial equation be solved for $A^{-1}$.
}

\CANONICAL{
\begin{bullets}
\item Inverse is a polynomial of degree at most $n-1$ in $A$.
\end{bullets}
}

\clearpage
\section{10 Exhaustive Problems and Solutions}

\ProblemPage{1}{Cayley\textendash Hamilton for $2\times2$ and Inverse Formula}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Show directly that a $2\times2$ matrix satisfies its characteristic
polynomial and derive the inverse formula.

\PROBLEM{
Let $A=\begin{pmatrix}a&b\\c&d\end{pmatrix}$ with entries in a field
$\mathbb{F}$. Compute $p_A(\lambda)$, verify $p_A(A)=0$, and deduce
$A^{-1}=\frac{1}{\det(A)}\begin{pmatrix}d&-b\\-c&a\end{pmatrix}$ when
$\det(A)\ne0$. Evaluate numerically for
$A=\begin{pmatrix}2&1\\-3&4\end{pmatrix}$.
}

\MODEL{
\[
p_A(\lambda)=\lambda^2-(a+d)\lambda+(ad-bc).
\]
}

\ASSUMPTIONS{
\begin{bullets}
\item $\mathbb{F}$ is a field, $\det(A)\ne0$ for inversion step.
\end{bullets}
}

\varmapStart
\var{a,b,c,d}{Entries of $A$.}
\var{p_A}{Characteristic polynomial.}
\var{\Delta}{Determinant $ad-bc$.}
\varmapEnd

\WHICHFORMULA{
Formulas 1, 3, and 6: coefficient identities, CH, and inverse polynomial.
}

\GOVERN{
\[
A^2-(\operatorname{tr}A)A+(\det A)I=0.
\]
}

\INPUTS{$A=\begin{pmatrix}2&1\\-3&4\end{pmatrix}$.}

\DERIVATION{
\begin{align*}
\operatorname{tr}A&=6,\ \det A=2\cdot4-1\cdot(-3)=11,\\
p_A(\lambda)&=\lambda^2-6\lambda+11,\\
p_A(A)&=A^2-6A+11I,\\
A^2&=\begin{pmatrix}2&1\\-3&4\end{pmatrix}^2
=\begin{pmatrix}1&6\\-18&13\end{pmatrix},\\
A^2-6A&=\begin{pmatrix}1&6\\-18&13\end{pmatrix}
-\begin{pmatrix}12&6\\-18&24\end{pmatrix}
=\begin{pmatrix}-11&0\\0&-11\end{pmatrix},\\
A^2-6A+11I&=0,\\
\text{Hence }A^{-1}&=\frac{1}{11}\big(6I-A\big)
=\frac{1}{11}\begin{pmatrix}4&-1\\3&2\end{pmatrix}.
\end{align*}
}

\RESULT{
$A$ satisfies $A^2-6A+11I=0$ and
$A^{-1}=\frac{1}{11}\begin{pmatrix}4&-1\\3&2\end{pmatrix}$.
}

\UNITCHECK{
Dimensions match and $A\cdot A^{-1}=I$ can be verified directly.
}

\EDGECASES{
\begin{bullets}
\item If $\det(A)=0$, the inverse formula is not applicable.
\item If $b=c=0$, diagonal case reduces to scalar identities.
\end{bullets}
}

\ALTERNATE{
Diagonalize $A$ if possible or use eigenvalues to validate $p_A(A)=0$.
}

\VALIDATION{
\begin{bullets}
\item Compute $A\cdot A^{-1}$ and check it equals $I$.
\item Compare with direct adjugate formula.
\end{bullets}
}

\INTUITION{
Degree two means $A^2$ is redundant after accounting for trace and
determinant.
}

\CANONICAL{
\begin{bullets}
\item $A^2-(\operatorname{tr}A)A+(\det A)I=0$ for all $2\times2$.
\end{bullets}
}

\ProblemPage{2}{Power Reduction for a Companion Matrix}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Use Cayley\textendash Hamilton to express $A^k$ for a companion matrix.

\PROBLEM{
Let $A=\begin{pmatrix}0&1&0\\0&0&1\\-2&-3&-1\end{pmatrix}$.
Find $p_A(\lambda)$, then give a recurrence to compute $A^k$ for $k\ge3$
and evaluate $A^5$.
}

\MODEL{
\[
p_A(\lambda)=\lambda^3+\lambda^2+3\lambda+2.
\]
}

\ASSUMPTIONS{
\begin{bullets}
\item Entries are in $\mathbb{Q}$; arithmetic is exact.
\end{bullets}
}

\varmapStart
\var{a_0,\dots,a_2}{Coefficients from $p_A$.}
\var{A^k}{Matrix powers to be reduced.}
\varmapEnd

\WHICHFORMULA{
Formula 5: recurrence $A^k=-\sum_{j=0}^{2}a_j A^{k-3+j}$ for $k\ge3$.
}

\GOVERN{
\[
A^3=-A^2-3A-2I.
\]
}

\INPUTS{$A$ as given; $a_2=1,\ a_1=3,\ a_0=2$.}

\DERIVATION{
\begin{align*}
A^2&=\begin{pmatrix}0&0&1\\-2&-3&-1\\2&3&2\end{pmatrix},\\
A^3&=A\cdot A^2
=\begin{pmatrix}-2&-3&-1\\2&3&2\\-2&-3&-2\end{pmatrix},\\
\text{Check }A^3&=-A^2-3A-2I,\\
A^4&=-A^3-3A^2-2A,\\
A^5&=-A^4-3A^3-2A^2.
\end{align*}
Compute stepwise:
\begin{align*}
A^4&=A\cdot A^3
=\begin{pmatrix}2&3&2\\-2&-3&-2\\2&3&1\end{pmatrix},\\
A^5&=A\cdot A^4
=\begin{pmatrix}-2&-3&-2\\2&3&1\\-2&-3&0\end{pmatrix}.
\end{align*}
Recurrence agrees with direct multiplication.
}

\RESULT{
$A^5=\begin{pmatrix}-2&-3&-2\\2&3&1\\-2&-3&0\end{pmatrix}$.
}

\UNITCHECK{
All matrices are $3\times3$; recurrence indices consistent.
}

\EDGECASES{
\begin{bullets}
\item For $k=3$, recurrence gives the base $A^3$ identity.
\item If coefficients change, only the recurrence coefficients change.
\end{bullets}
}

\ALTERNATE{
Diagonalize $A$ if possible; as a companion matrix, use its roots to
write $A^k$ via spectral decomposition.
}

\VALIDATION{
\begin{bullets}
\item Verify $A^3=-A^2-3A-2I$ numerically.
\item Cross-check $A^5$ by two independent paths.
\end{bullets}
}

\INTUITION{
The companion structure encodes the recurrence directly in the last row.
}

\CANONICAL{
\begin{bullets}
\item Powers follow the same linear recurrence as the coefficients.
\end{bullets}
}

\ProblemPage{3}{Diagonalizable Case Proof via Eigenvalues}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
If $A$ is diagonalizable, then $p_A(A)=0$ follows by applying $p_A$ to
eigenvalues.

\PROBLEM{
Let $A=S\Lambda S^{-1}$ with $\Lambda=\operatorname{diag}(\lambda_1,
\dots,\lambda_n)$. Show $p_A(A)=S\,p_A(\Lambda)\,S^{-1}=0$.
Illustrate with $A=\operatorname{diag}(1,2,3)$ by computing $p_A$ and
$p_A(A)$.
}

\MODEL{
\[
p_A(\lambda)=(\lambda-1)(\lambda-2)(\lambda-3)
=\lambda^3-6\lambda^2+11\lambda-6.
\]
}

\ASSUMPTIONS{
\begin{bullets}
\item $S$ invertible, $A=S\Lambda S^{-1}$.
\end{bullets}
}

\varmapStart
\var{\lambda_i}{Eigenvalues of $A$.}
\var{S}{Change-of-basis matrix.}
\varmapEnd

\WHICHFORMULA{
Formula 3 with the equivalent form using diagonalization.
}

\GOVERN{
\[
p_A(A)=S\,p_A(\Lambda)\,S^{-1}.
\]
}

\INPUTS{$\Lambda=\operatorname{diag}(1,2,3)$.}

\DERIVATION{
\begin{align*}
p_A(\Lambda)&=\operatorname{diag}(p_A(1),p_A(2),p_A(3)),\\
p_A(1)&=0,\ p_A(2)=0,\ p_A(3)=0,\\
\Rightarrow p_A(\Lambda)&=0,\quad p_A(A)=S\,0\,S^{-1}=0.
\end{align*}
For $A=\Lambda$, directly:
\begin{align*}
p_A(A)&=A^3-6A^2+11A-6I\\
&=\operatorname{diag}(1-6+11-6,\ 8-24+22-6,\\
&\phantom{=}\ 27-54+33-6)\\
&=\operatorname{diag}(0,0,0)=0.
\end{align*}
}

\RESULT{
$p_A(A)=0$ holds for diagonalizable $A$ by eigenvalue evaluation.
}

\UNITCHECK{
Similarity transforms preserve zero matrix; dimensions consistent.
}

\EDGECASES{
\begin{bullets}
\item If eigenvalues repeat, the same conclusion holds.
\item Non diagonalizable case requires Jordan form or adjugate proof.
\end{bullets}
}

\ALTERNATE{
Use adjugate identity without diagonalization assumptions.
}

\VALIDATION{
\begin{bullets}
\item Numerically compute $p_A(A)$ for a random diagonalizable $A$.
\end{bullets}
}

\INTUITION{
Polynomials act entrywise on diagonal matrices; similarity transports
this action to $A$.
}

\CANONICAL{
\begin{bullets}
\item $p_A(A)=0$ is similarity invariant.
\end{bullets}
}

\ProblemPage{4}{Alice's Trace\textendash Determinant Puzzle}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Hidden identity from partial coefficient data using Cayley\textendash
Hamilton.

\PROBLEM{
Alice has $A\in M_3(\mathbb{Q})$ with $\operatorname{tr}A=6$,
$\det A=8$, and the coefficient of $\lambda$ in $p_A(\lambda)$ equals
$11$. Use Cayley\textendash Hamilton to express $A^3$ in terms of
$I,A,A^2$ and evaluate $A^3-6A^2+11A-8I$.
}

\MODEL{
\[
p_A(\lambda)=\lambda^3-6\lambda^2+11\lambda-8.
\]
}

\ASSUMPTIONS{
\begin{bullets}
\item The data specify $p_A$ uniquely via Formula 1.
\end{bullets}
}

\varmapStart
\var{a_2,a_1,a_0}{Coefficients $-6,11,-8$.}
\varmapEnd

\WHICHFORMULA{
Formula 3: $p_A(A)=0$.
}

\GOVERN{
\[
A^3-6A^2+11A-8I=0.
\]
}

\INPUTS{$\operatorname{tr}A=6,\ \det A=8,\ [\lambda]\ p_A=11$.}

\DERIVATION{
\begin{align*}
p_A(\lambda)&=\lambda^3-6\lambda^2+11\lambda-8,\\
p_A(A)&=A^3-6A^2+11A-8I=0.
\end{align*}
Therefore $A^3=6A^2-11A+8I$.
}

\RESULT{
$A^3-6A^2+11A-8I=0$ and $A^3=6A^2-11A+8I$.
}

\UNITCHECK{
All matrices are $3\times3$; coefficients are scalars.
}

\EDGECASES{
\begin{bullets}
\item If $\det A=0$, constant term would be $0$ and identity changes.
\end{bullets}
}

\ALTERNATE{
If eigenvalues are known, verify that each satisfies
$\lambda^3-6\lambda^2+11\lambda-8=0$ and conclude.
}

\VALIDATION{
\begin{bullets}
\item Pick a concrete $A$ with the same invariants and check numerically.
\end{bullets}
}

\INTUITION{
Trace, determinant, and the middle coefficient fully determine the cubic
that annihilates $A$.
}

\CANONICAL{
\begin{bullets}
\item Coefficients map directly to the annihilating identity.
\end{bullets}
}

\ProblemPage{5}{Bob's Graphics Inverse Trick}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Use Cayley\textendash Hamilton to invert a transformation without
Gaussian elimination.

\PROBLEM{
Bob uses $A=\begin{pmatrix}1&1&0\\0&1&1\\0&0&2\end{pmatrix}$ to map
homogeneous coordinates. Compute $p_A(\lambda)$ and $A^{-1}$ using
Formula 6, and verify by multiplication.
}

\MODEL{
\[
p_A(\lambda)=(\lambda-1)^2(\lambda-2)=\lambda^3-4\lambda^2+5\lambda-2.
\]
}

\ASSUMPTIONS{
\begin{bullets}
\item $\det A=2\ne0$.
\end{bullets}
}

\varmapStart
\var{a_2,a_1,a_0}{Coeffs $-4,5,-2$ of $p_A$.}
\varmapEnd

\WHICHFORMULA{
Formula 6: $A^{-1}=-\frac{1}{a_0}\big(A^2+a_2A+a_1 I\big)$.
}

\GOVERN{
\[
A^{-1}=\frac{1}{2}\big(A^2-4A+5I\big).
\]
}

\INPUTS{$A$ as given.}

\DERIVATION{
\begin{align*}
A^2&=\begin{pmatrix}1&2&1\\0&1&3\\0&0&4\end{pmatrix},\\
A^2-4A+5I&=\begin{pmatrix}2&-2&-3\\0&2&-1\\0&0&1\end{pmatrix},\\
A^{-1}&=\tfrac{1}{2}\begin{pmatrix}2&-2&-3\\0&2&-1\\0&0&1\end{pmatrix}
=\begin{pmatrix}1&-1&-1.5\\0&1&-0.5\\0&0&0.5\end{pmatrix},\\
AA^{-1}&=\begin{pmatrix}1&0&0\\0&1&0\\0&0&1\end{pmatrix}.
\end{align*}
}

\RESULT{
$A^{-1}=\begin{pmatrix}1&-1&-1.5\\0&1&-0.5\\0&0&0.5\end{pmatrix}$.
}

\UNITCHECK{
Product with $A$ equals $I_3$.
}

\EDGECASES{
\begin{bullets}
\item If eigenvalue $0$ were present, $a_0=0$ and inversion by Formula 6
fails.
\end{bullets}
}

\ALTERNATE{
Compute the adjugate and divide by determinant directly.
}

\VALIDATION{
\begin{bullets}
\item Multiply $A$ and the computed $A^{-1}$ to confirm identity.
\end{bullets}
}

\INTUITION{
Inverse emerges from solving the polynomial identity for $A^{-1}$.
}

\CANONICAL{
\begin{bullets}
\item Inversion via characteristic coefficients only.
\end{bullets}
}

\ProblemPage{6}{Expectation Puzzle with Random Walk Length}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Use Cayley\textendash Hamilton to compute an expectation over random
powers.

\PROBLEM{
Let $A=\begin{pmatrix}0&1\\1&1\end{pmatrix}$ and $v_0=\begin{pmatrix}1\\0
\end{pmatrix}$. Flip a fair coin until the first heads; let $K$ be the
number of flips. Compute $\mathbb{E}[A^K v_0]$ using the power
reduction from $p_A$.
}

\MODEL{
\[
p_A(\lambda)=\lambda^2-\lambda-1.
\]
}

\ASSUMPTIONS{
\begin{bullets}
\item $K$ has geometric distribution on $\{1,2,\dots\}$ with $P(K=k)=2^{-k}$.
\end{bullets}
}

\varmapStart
\var{p_A}{Characteristic polynomial of $A$.}
\var{v_0}{Initial vector.}
\var{K}{Random positive integer.}
\varmapEnd

\WHICHFORMULA{
Formula 5: $A^2=A+I$ gives a first-order reduction of powers.
}

\GOVERN{
\[
A^k=F_k A + F_{k-1} I,\quad F_0=0,\ F_1=1.
\]
}

\INPUTS{$A,v_0$ as above.}

\DERIVATION{
\begin{align*}
A^2&=A+I\Rightarrow A^k=F_k A+F_{k-1}I \text{ by induction},\\
A v_0&=\begin{pmatrix}0&1\\1&1\end{pmatrix}\begin{pmatrix}1\\0\end{pmatrix}
=\begin{pmatrix}0\\1\end{pmatrix},\\
A^k v_0&=F_k A v_0+F_{k-1} v_0
=F_k\begin{pmatrix}0\\1\end{pmatrix}+F_{k-1}\begin{pmatrix}1\\0\end{pmatrix}
=\begin{pmatrix}F_{k-1}\\F_k\end{pmatrix},\\
\mathbb{E}[A^K v_0]
&=\sum_{k=1}^\infty 2^{-k}\begin{pmatrix}F_{k-1}\\F_k\end{pmatrix}
=\begin{pmatrix}\sum_{k=1}^\infty 2^{-k}F_{k-1}\\
\sum_{k=1}^\infty 2^{-k}F_k\end{pmatrix}.
\end{align*}
Use generating function $G(z)=\sum_{k\ge0}F_k z^k=\frac{z}{1-z-z^2}$.
Then
\begin{align*}
\sum_{k=1}^\infty 2^{-k}F_{k-1}&=\sum_{j=0}^\infty 2^{-(j+1)}F_j
=\tfrac{1}{2}G(1/2)=\frac{1}{2}\cdot\frac{1/2}{1-1/2-1/4}=\frac{1}{2},\\
\sum_{k=1}^\infty 2^{-k}F_k&=\sum_{j=1}^\infty 2^{-j}F_j
=G(1/2)-F_0=\frac{1/2}{1-1/2-1/4}=1.
\end{align*}
}

\RESULT{
$\mathbb{E}[A^K v_0]=\begin{pmatrix}\frac{1}{2}\\1\end{pmatrix}$.
}

\UNITCHECK{
Vector dimensions match; sums converge since $|1/2|<\rho^{-1}$.
}

\EDGECASES{
\begin{bullets}
\item Different coin bias $p$ replaces $1/2$ in the generating function.
\end{bullets}
}

\ALTERNATE{
Diagonalize $A$ to compute $A^k v_0$ and sum the geometric series on
eigenvalues.
}

\VALIDATION{
\begin{bullets}
\item Numerically sum first 20 terms and compare to the closed form.
\end{bullets}
}

\INTUITION{
Cayley\textendash Hamilton compresses the state to a Fibonacci pair,
making the expectation a weighted sum of a simple sequence.
}

\CANONICAL{
\begin{bullets}
\item Scalar recurrences induced by $p_A$ govern vector evolution.
\end{bullets}
}

\ProblemPage{7}{Proof-Style: Divisibility of Annihilators}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Prove that if $f(A)=0$ and $g(A)=0$ with $f,g$ monic, then
$\gcd(f,g)$ also annihilates $A$, and deduce $m_A\mid p_A$.

\PROBLEM{
Give a short proof that the set of annihilating polynomials is an ideal
of $\mathbb{F}[\lambda]$, generated by $m_A$, and apply it to
$p_A$.
}

\MODEL{
\[
\mathcal{I}_A=\{h\in\mathbb{F}[\lambda]:h(A)=0\}=(m_A).
\]
}

\ASSUMPTIONS{
\begin{bullets}
\item Polynomial functional calculus respects addition and multiplication.
\end{bullets}
}

\varmapStart
\var{\mathcal{I}_A}{Ideal of annihilators.}
\varmapEnd

\WHICHFORMULA{
Formulas 3 and 4: CH gives a nontrivial annihilator; minimality gives
generation.
}

\GOVERN{
\[
h_1,h_2\in\mathcal{I}_A\Rightarrow h_1+h_2\in\mathcal{I}_A,\
h_1 h_2\in\mathcal{I}_A.
\]
}

\INPUTS{None beyond $A\in M_n(\mathbb{F})$.}

\DERIVATION{
\begin{align*}
h_1(A)=0,\ h_2(A)=0&\Rightarrow (h_1+h_2)(A)=0,\\
&\Rightarrow (h_1 h_2)(A)=h_1(A)h_2(A)=0,\\
\mathcal{I}_A&\text{ is an ideal in a PID }\mathbb{F}[\lambda],\\
\mathcal{I}_A&=(m_A)\text{ for a monic }m_A,\\
p_A(A)&=0\Rightarrow p_A\in\mathcal{I}_A=(m_A)\Rightarrow m_A\mid p_A.
\end{align*}
}

\RESULT{
$\mathcal{I}_A=(m_A)$ and $m_A\mid p_A$.
}

\UNITCHECK{
All steps use ring operations in $\mathbb{F}[\lambda]$ and evaluation
homomorphism.
}

\EDGECASES{
\begin{bullets}
\item If $A=0$, then $m_A(\lambda)=\lambda$ and $p_A(\lambda)=\lambda^n$.
\end{bullets}
}

\ALTERNATE{
Use Euclidean division directly as in Formula 4.
}

\VALIDATION{
\begin{bullets}
\item Check on examples that $\gcd$ reduces the annihilator degree.
\end{bullets}
}

\INTUITION{
Annihilators are closed under addition and multiplication, so they form
a principal ideal in a principal ideal domain.
}

\CANONICAL{
\begin{bullets}
\item Minimal polynomial uniquely generates the annihilator ideal.
\end{bullets}
}

\ProblemPage{8}{Proof-Style: Polynomials in $A$ Commute}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Show that $f(A)$ and $g(A)$ commute for all $f,g\in\mathbb{F}[\lambda]$,
and use Cayley\textendash Hamilton to reduce products to degree
$<n$ polynomials mod $p_A$.

\PROBLEM{
Prove $f(A)g(A)=g(A)f(A)$ and that every $f(A)$ has a unique
representation $\sum_{k=0}^{n-1}c_k A^k$ modulo $p_A$.
}

\MODEL{
\[
\mathbb{F}[A]\cong \mathbb{F}[\lambda]/(m_A)\quad\text{and}\quad
p_A(A)=0.
\]
}

\ASSUMPTIONS{
\begin{bullets}
\item Polynomial calculus is a ring homomorphism.
\end{bullets}
}

\varmapStart
\var{f,g}{Polynomials in $\mathbb{F}[\lambda]$.}
\varmapEnd

\WHICHFORMULA{
Formulas 3 and 5: annihilating relation and basis $\{I,\dots,A^{n-1}\}$.
}

\GOVERN{
\[
f(A)g(A)=(fg)(A)=(gf)(A)=g(A)f(A).
\]
}

\INPUTS{None beyond $A$.}

\DERIVATION{
\begin{align*}
\phi:\mathbb{F}[\lambda]&\to M_n(\mathbb{F}),\quad
\phi(f)=f(A),\\
\phi(fg)&=\phi(f)\phi(g),\ \phi(gf)=\phi(g)\phi(f),\\
\Rightarrow f(A)g(A)&=g(A)f(A).
\end{align*}
By Cayley\textendash Hamilton, $(p_A)(A)=0$ so $\ker\phi$ contains
$(p_A)$. Restrict to $\mathbb{F}[\lambda]/(p_A)$ to represent any class
by degree $<n$ polynomial, giving a unique representative.
}

\RESULT{
Commutativity of polynomials in $A$ and reduction mod $p_A$.
}

\UNITCHECK{
Mapping $\phi$ respects multiplication; quotient degree bound is $n$.
}

\EDGECASES{
\begin{bullets}
\item If $m_A\ne p_A$, uniqueness is mod $m_A$ for the algebra
$\mathbb{F}[A]$.
\end{bullets}
}

\ALTERNATE{
Use rational canonical form for the isomorphism with
$\mathbb{F}[\lambda]/(m_A)$.
}

\VALIDATION{
\begin{bullets}
\item Compute random $f,g$ and verify $[f(A),g(A)]=0$ numerically.
\end{bullets}
}

\INTUITION{
Polynomials commute because they are built from the same generator $A$.
}

\CANONICAL{
\begin{bullets}
\item $\mathbb{F}[A]$ is a commutative subalgebra generated by $A$.
\end{bullets}
}

\ProblemPage{9}{Combo: Fibonacci via Companion Matrix}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Use Cayley\textendash Hamilton on the Fibonacci companion matrix to
derive the Fibonacci recurrence and compute $F_{10}$.

\PROBLEM{
Let $A=\begin{pmatrix}0&1\\1&1\end{pmatrix}$ and $u_0=\begin{pmatrix}
F_0\\F_1\end{pmatrix}=\begin{pmatrix}0\\1\end{pmatrix}$. Show that
$u_k=A^k u_0=\begin{pmatrix}F_k\\F_{k+1}\end{pmatrix}$ and compute
$F_{10}$ using $A^2=A+I$.
}

\MODEL{
\[
p_A(\lambda)=\lambda^2-\lambda-1,\quad A^2=A+I.
\]
}

\ASSUMPTIONS{
\begin{bullets}
\item Standard Fibonacci initial conditions.
\end{bullets}
}

\varmapStart
\var{F_k}{Fibonacci numbers.}
\var{u_k}{State vector at step $k$.}
\varmapEnd

\WHICHFORMULA{
Formula 5 recurrence induced by $p_A$.
}

\GOVERN{
\[
A^k=F_k A+F_{k-1}I,\quad u_k=\begin{pmatrix}F_k\\F_{k+1}\end{pmatrix}.
\]
}

\INPUTS{$u_0=(0,1)^\top$.}

\DERIVATION{
\begin{align*}
A^2&=A+I\Rightarrow A^{k+1}=A^k+A^{k-1},\\
u_{k+1}&=A u_k=A^{k+1}u_0=A^k u_0 + A^{k-1}u_0=u_k+u_{k-1},\\
u_0&=\begin{pmatrix}0\\1\end{pmatrix},\ u_1=A u_0=\begin{pmatrix}1\\1\end{pmatrix},\\
\Rightarrow\ u_k&=\begin{pmatrix}F_k\\F_{k+1}\end{pmatrix}.\\
F_{10}&=55.
\end{align*}
}

\RESULT{
$F_{10}=55$; $u_k$ encodes successive Fibonacci numbers.
}

\UNITCHECK{
Dimensions align; recurrence matches scalar Fibonacci.
}

\EDGECASES{
\begin{bullets}
\item Different initial conditions yield generalized Fibonacci sequences.
\end{bullets}
}

\ALTERNATE{
Diagonalize $A$ to obtain Binet formula and then compute $F_{10}$.
}

\VALIDATION{
\begin{bullets}
\item Compute $F_k$ iteratively and compare with matrix method.
\end{bullets}
}

\INTUITION{
Cayley\textendash Hamilton compresses the matrix dynamics to the scalar
recurrence.
}

\CANONICAL{
\begin{bullets}
\item Companion matrices encode linear recurrences exactly.
\end{bullets}
}

\ProblemPage{10}{Combo: Matrix Exponential via Polynomial Basis}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Represent $e^{tA}$ as a linear combination of $I,A,\dots,A^{n-1}$ with
scalar coefficient functions satisfying an ODE derived from
$p_A$.

\PROBLEM{
Show that there exist $c_0(t),\dots,c_{n-1}(t)$ such that
$e^{tA}=\sum_{k=0}^{n-1}c_k(t)A^k$, where the vector $c(t)$ solves a
linear ODE with constant coefficients determined by $p_A$. Illustrate
for $A=\begin{pmatrix}0&1\\-2&-3\end{pmatrix}$.
}

\MODEL{
\[
p_A(\lambda)=\lambda^2+3\lambda+2,\quad A^2=-3A-2I.
\]
}

\ASSUMPTIONS{
\begin{bullets}
\item $A\in M_n(\mathbb{R})$.
\end{bullets}
}

\varmapStart
\var{c_k(t)}{Scalar functions for the polynomial basis.}
\varmapEnd

\WHICHFORMULA{
Formula 5: reduction modulo $p_A$; differentiate and project.
}

\GOVERN{
\[
\frac{d}{dt}e^{tA}=A e^{tA} \equiv
\sum_{k=0}^{n-1}\tilde c_k(t)A^k \pmod{p_A}.
\]
}

\INPUTS{$A=\begin{pmatrix}0&1\\-2&-3\end{pmatrix}$.}

\DERIVATION{
\begin{align*}
e^{tA}&=c_0 I+c_1 A,\\
\frac{d}{dt}e^{tA}&=(c_0' I+c_1' A)=A(c_0 I+c_1 A)=c_0 A+c_1 A^2,\\
A^2&=-3A-2I\Rightarrow c_0' I+c_1' A
=c_0 A+c_1(-3A-2I).
\end{align*}
Match coefficients of $I$ and $A$:
\begin{align*}
c_0'&=-2 c_1,\quad c_1'=c_0-3 c_1,\\
c_0(0)&=1,\ c_1(0)=0.
\end{align*}
Solve:
\begin{align*}
c_1''+3 c_1'+2 c_1&=0,\quad c_1(0)=0,\ c_1'(0)=1,\\
c_1(t)&=\tfrac{1}{1}(\tfrac{e^{-t}-e^{-2t}}{1})=e^{-t}-e^{-2t},\\
c_0'&=-2 c_1\Rightarrow c_0(t)=1-2\int_0^t c_1(s)\,ds\\
&=1-2\int_0^t (e^{-s}-e^{-2s})\,ds\\
&=1-2\big((-e^{-s}+ \tfrac{1}{2}e^{-2s})\big)\big|_0^t\\
&=1-2\big((-e^{-t}+\tfrac{1}{2}e^{-2t})-(-1+\tfrac{1}{2})\big)\\
&=2 e^{-t}- e^{-2t}.
\end{align*}
Thus $e^{tA}=c_0 I+c_1 A$.
}

\RESULT{
$e^{tA}=(2 e^{-t}-e^{-2t})I+(e^{-t}-e^{-2t})A$.
}

\UNITCHECK{
At $t=0$, $e^{0A}=I$ yields $c_0(0)=1$, $c_1(0)=0$ as found.
}

\EDGECASES{
\begin{bullets}
\item Repeated roots in $p_A$ yield polynomial times exponential in the
coefficients $c_k$.
\end{bullets}
}

\ALTERNATE{
Diagonalize $A$ or use Jordan form to compute $e^{tA}$ directly.
}

\VALIDATION{
\begin{bullets}
\item Differentiate the expression and check $\frac{d}{dt}e^{tA}=A e^{tA}$.
\end{bullets}
}

\INTUITION{
Cayley\textendash Hamilton collapses the infinite series of $e^{tA}$ to
a finite basis with time-dependent weights solving a scalar ODE.
}

\CANONICAL{
\begin{bullets}
\item Functional calculus modulo $p_A$ for analytic functions of $A$.
\end{bullets}
}

\clearpage
\section{Coding Demonstrations}

\CodeDemoPage{Numerical Verification of Cayley\textendash Hamilton}
\PROBLEM{
Compute $p_A(A)$ for a fixed $3\times3$ integer matrix in two ways:
from-scratch coefficients via Faddeev\textendash Leverrier and via
libraries, then assert the Frobenius norm is near zero.
}

\API{
\begin{bullets}
\item \inlinecode{def read_input(s) -> np.ndarray} — parse matrix rows.
\item \inlinecode{def charpoly_fl(A) -> list} — FL coefficients.
\item \inlinecode{def eval_poly_mat(coeffs, A) -> np.ndarray} — Horner.
\item \inlinecode{def validate() -> None} — run asserts.
\item \inlinecode{def main() -> None} — orchestrate demo.
\end{bullets}
}

\INPUTS{
Square matrix $A\in\mathbb{R}^{n\times n}$ with small integer entries.
}

\OUTPUTS{
List of coefficients of $p_A$ (monic) and the residual matrix $p_A(A)$.
}

\FORMULA{
\[
p_A(\lambda)=\lambda^n+\sum_{k=0}^{n-1}a_k\lambda^k,\quad
p_A(A)=0.
\]
}

\textbf{SOLUTION A — From Scratch (Mathematically Explicit Implementation)}
\begin{codepy}
import numpy as np

def read_input(s):
    rows = s.strip().split(";")
    return np.array([[float(x) for x in r.split()] for r in rows])

def charpoly_fl(A):
    n = A.shape[0]
    B = np.array(A, dtype=float)
    I = np.eye(n)
    coeffs = [0.0]*(n+1)
    coeffs[n] = 1.0
    S = np.zeros_like(A, dtype=float)
    for k in range(1, n+1):
        if k == 1:
            S = B.copy()
        else:
            S = B @ (S + coeffs[n-k+1]*I)
        c = -np.trace(S)/k
        coeffs[n-k] = c
    return coeffs

def eval_poly_mat(coeffs, A):
    n = A.shape[0]
    X = np.eye(n)
    R = np.zeros_like(A, dtype=float)
    for c in coeffs[::-1]:
        R = R @ A + c*X
    return R

def validate():
    A = read_input("2 1 0; 0 3 1; 1 0 2")
    coeffs = charpoly_fl(A)
    R = eval_poly_mat(coeffs, A)
    assert np.linalg.norm(R, ord='fro') < 1e-8

def main():
    validate()
    A = read_input("2 1 0; 0 3 1; 1 0 2")
    coeffs = charpoly_fl(A)
    R = eval_poly_mat(coeffs, A)
    print("coeffs:", [round(x,6) for x in coeffs])
    print("fro_norm:", round(np.linalg.norm(R, ord='fro'), 12))

if __name__ == "__main__":
    main()
\end{codepy}

\textbf{SOLUTION B — Library-Based (Validated Computational Shortcut)}
\begin{codepy}
import numpy as np

def read_input(s):
    rows = s.strip().split(";")
    return np.array([[float(x) for x in r.split()] for r in rows])

def charpoly_lib(A):
    eig = np.linalg.eigvals(A)
    coeffs = np.poly(eig).tolist()
    return coeffs

def eval_poly_mat(coeffs, A):
    n = A.shape[0]
    X = np.eye(n)
    R = np.zeros_like(A, dtype=float)
    for c in coeffs[::-1]:
        R = R @ A + c*X
    return R

def validate():
    A = read_input("2 1 0; 0 3 1; 1 0 2")
    coeffs = charpoly_lib(A)
    R = eval_poly_mat(coeffs, A)
    assert np.linalg.norm(R, ord='fro') < 1e-8

def main():
    validate()
    A = read_input("2 1 0; 0 3 1; 1 0 2")
    coeffs = charpoly_lib(A)
    R = eval_poly_mat(coeffs, A)
    print("coeffs:", [round(x,6) for x in coeffs])
    print("fro_norm:", round(np.linalg.norm(R, ord='fro'), 12))

if __name__ == "__main__":
    main()
\end{codepy}

\COMPLEXITY{
Both variants run in time $\mathcal{O}(n^3)$ and space $\mathcal{O}(n^2)$.
}

\FAILMODES{
\begin{bullets}
\item Nearly defective matrices cause eigenvalue sensitivity; use FL
instead of eigen decomposition if needed.
\item Non square input rejected by shape checks.
\end{bullets}
}

\STABILITY{
\begin{bullets}
\item Horner evaluation reduces rounding error.
\item Trace accumulation in FL may suffer for large $n$; mitigate with
double precision and balancing.
\end{bullets}
}

\VALIDATION{
\begin{bullets}
\item Assert $\|p_A(A)\|_F<10^{-8}$.
\item Cross compare coefficients from both methods.
\end{bullets}
}

\RESULT{
Both implementations produce the same coefficients and a near zero
residual matrix.
}

\EXPLANATION{
We compute $p_A$ and evaluate it at $A$ using Horner; CH predicts zero.
Agreement confirms the theorem numerically.
}

\CodeDemoPage{Inverse via Cayley\textendash Hamilton}
\PROBLEM{
Compute $A^{-1}$ from characteristic coefficients and compare with
direct inversion.
}

\API{
\begin{bullets}
\item \inlinecode{def charpoly_fl(A) -> list} — as before.
\item \inlinecode{def inverse_via_ch(A) -> np.ndarray} — polynomial form.
\item \inlinecode{def validate() -> None} — asserts equality.
\item \inlinecode{def main() -> None} — run demo.
\end{bullets}
}

\INPUTS{
Invertible matrix $A\in\mathbb{R}^{n\times n}$.
}

\OUTPUTS{
Matrix $A^{-1}$ computed via Formula 6 and via \inlinecode{np.linalg.inv}.
}

\FORMULA{
\[
A^{-1}=-\frac{1}{a_0}\big(A^{n-1}+a_{n-1}A^{n-2}+\cdots+a_1 I\big).
\]
}

\textbf{SOLUTION A — From Scratch (Mathematically Explicit Implementation)}
\begin{codepy}
import numpy as np

def charpoly_fl(A):
    n = A.shape[0]
    B = np.array(A, dtype=float)
    I = np.eye(n)
    coeffs = [0.0]*(n+1)
    coeffs[n] = 1.0
    S = np.zeros_like(A, dtype=float)
    for k in range(1, n+1):
        if k == 1:
            S = B.copy()
        else:
            S = B @ (S + coeffs[n-k+1]*I)
        c = -np.trace(S)/k
        coeffs[n-k] = c
    return coeffs

def inverse_via_ch(A):
    coeffs = charpoly_fl(A)
    n = A.shape[0]
    a0 = coeffs[0]
    assert abs(a0) > 1e-12
    I = np.eye(n)
    poly = np.zeros_like(A, dtype=float)
    # sum_{j=0}^{n-1} a_{j+1} A^j
    P = I.copy()
    for j in range(0, n):
        poly = poly + coeffs[j+1]*P
        P = P @ A
    return -poly / a0

def validate():
    A = np.array([[1.0,1.0,0.0],[0.0,1.0,1.0],[0.0,0.0,2.0]])
    X = inverse_via_ch(A)
    Y = np.linalg.inv(A)
    assert np.linalg.norm(X - Y, ord='fro') < 1e-8

def main():
    validate()
    A = np.array([[1.0,1.0,0.0],[0.0,1.0,1.0],[0.0,0.0,2.0]])
    X = inverse_via_ch(A)
    print("inv via CH fro error:",
          round(np.linalg.norm(X-np.linalg.inv(A),'fro'),12))

if __name__ == "__main__":
    main()
\end{codepy}

\textbf{SOLUTION B — Library-Based (Validated Computational Shortcut)}
\begin{codepy}
import numpy as np

def inverse_lib(A):
    return np.linalg.inv(A)

def validate():
    A = np.array([[1.0,1.0,0.0],[0.0,1.0,1.0],[0.0,0.0,2.0]])
    X = inverse_lib(A)
    Y = np.linalg.inv(A)
    assert np.linalg.norm(X - Y, ord='fro') < 1e-12

def main():
    validate()
    A = np.array([[1.0,1.0,0.0],[0.0,1.0,1.0],[0.0,0.0,2.0]])
    X = inverse_lib(A)
    print("inv lib fro error:",
          round(np.linalg.norm(X-np.linalg.inv(A),'fro'),12))

if __name__ == "__main__":
    main()
\end{codepy}

\COMPLEXITY{
$\mathcal{O}(n^3)$ time for both; space $\mathcal{O}(n^2)$.
}

\FAILMODES{
\begin{bullets}
\item Singular matrices have $a_0=0$; assert and abort.
\end{bullets}
}

\STABILITY{
\begin{bullets}
\item Polynomial evaluation uses Horner-like multiplication for stability.
\end{bullets}
}

\VALIDATION{
\begin{bullets}
\item Compare Frobenius norm difference to a tight tolerance.
\end{bullets}
}

\RESULT{
Agreement within machine precision.
}

\EXPLANATION{
We derive $A^{-1}$ from $p_A(A)=0$ by isolating $A^{-1}$, then verify.
}

\CodeDemoPage{Power Reduction and Fast $A^k$}
\PROBLEM{
Compute $A^k$ using the recurrence from $p_A$ and compare to repeated
squaring.
}

\API{
\begin{bullets}
\item \inlinecode{def charpoly_fl(A) -> list} — coefficients.
\item \inlinecode{def pow_reduce(A, k) -> np.ndarray} — CH recurrence.
\item \inlinecode{def validate() -> None} — assert equality.
\item \inlinecode{def main() -> None} — run demo.
\end{bullets}
}

\INPUTS{
Matrix $A\in\mathbb{R}^{n\times n}$, integer $k\ge0$.
}

\OUTPUTS{
$A^k$ computed by CH reduction and by \inlinecode{np.linalg.matrix_power}.
}

\FORMULA{
\[
A^k=-\sum_{j=0}^{n-1}a_j A^{k-n+j}\ \text{ for }k\ge n.
\]
}

\textbf{SOLUTION A — From Scratch (Mathematically Explicit Implementation)}
\begin{codepy}
import numpy as np

def charpoly_fl(A):
    n = A.shape[0]
    B = np.array(A, dtype=float)
    I = np.eye(n)
    coeffs = [0.0]*(n+1)
    coeffs[n] = 1.0
    S = np.zeros_like(A, dtype=float)
    for k in range(1, n+1):
        if k == 1:
            S = B.copy()
        else:
            S = B @ (S + coeffs[n-k+1]*I)
        c = -np.trace(S)/k
        coeffs[n-k] = c
    return coeffs

def pow_reduce(A, k):
    n = A.shape[0]
    if k == 0:
        return np.eye(n)
    if k < n:
        P = np.eye(n)
        for _ in range(k):
            P = P @ A
        return P
    coeffs = charpoly_fl(A)
    a = coeffs[:-1]
    cache = [np.eye(n)]
    P = np.eye(n)
    for _ in range(1, n):
        P = P @ A
        cache.append(P.copy())
    for t in range(n, k+1):
        Q = np.zeros_like(A, dtype=float)
        for j in range(0, n):
            Q = Q - a[j] * cache[t-n+j]
        cache.append(Q)
    return cache[k]

def validate():
    A = np.array([[0.0,1.0,0.0],[0.0,0.0,1.0],[-2.0,-3.0,-1.0]])
    for k in [0,1,2,3,5,10]:
        X = pow_reduce(A, k)
        Y = np.linalg.matrix_power(A, k)
        assert np.allclose(X, Y, atol=1e-8)

def main():
    validate()
    A = np.array([[0.0,1.0,0.0],[0.0,0.0,1.0],[-2.0,-3.0,-1.0]])
    k = 10
    X = pow_reduce(A, k)
    Y = np.linalg.matrix_power(A, k)
    print("fro error:", round(np.linalg.norm(X-Y, ord='fro'), 12))

if __name__ == "__main__":
    main()
\end{codepy}

\textbf{SOLUTION B — Library-Based (Validated Computational Shortcut)}
\begin{codepy}
import numpy as np

def pow_lib(A, k):
    return np.linalg.matrix_power(A, k)

def validate():
    A = np.array([[0.0,1.0,0.0],[0.0,0.0,1.0],[-2.0,-3.0,-1.0]])
    for k in [0,1,2,3,5,10]:
        X = pow_lib(A, k)
        Y = np.linalg.matrix_power(A, k)
        assert np.allclose(X, Y, atol=1e-12)

def main():
    validate()
    print("ok")

if __name__ == "__main__":
    main()
\end{codepy}

\COMPLEXITY{
Reduction $\mathcal{O}(n^3 k)$ naive; can be improved by binary splitting
using recurrence blocks. Library power is $\mathcal{O}(n^3\log k)$.
}

\FAILMODES{
\begin{bullets}
\item Large $k$ accumulates rounding error; recompute coefficients with
care.
\end{bullets}
}

\STABILITY{
\begin{bullets}
\item Use balanced multiplication and caching to reduce error growth.
\end{bullets}
}

\VALIDATION{
\begin{bullets}
\item Compare against \inlinecode{matrix_power} for several $k$.
\end{bullets}
}

\RESULT{
Agreement within tolerance; recurrence works as predicted by CH.
}

\EXPLANATION{
We turn a high power into a recurrence driven by $p_A$, exactly the
content of Formula 5.
}

\clearpage
\section{Applied Domains — Detailed End-to-End Scenarios}

\DomainPage{Machine Learning}
\SCENARIO{
In a linear recurrent model $h_{t+1}=A h_t$, predict $h_T=A^T h_0$.
Use Cayley\textendash Hamilton to reduce $A^T$ to a polynomial in $A$
of degree at most $n-1$, enabling efficient multi-step prediction.
}
\ASSUMPTIONS{
\begin{bullets}
\item Fixed $A\in\mathbb{R}^{n\times n}$, deterministic evolution.
\item $p_A(A)=0$ and coefficients computed exactly or stably.
\end{bullets}
}
\WHICHFORMULA{
Formula 5: $A^k$ reduction via the characteristic recurrence.
}
\varmapStart
\var{A}{State transition matrix.}
\var{h_0}{Initial state vector.}
\var{T}{Prediction horizon.}
\var{p_A}{Characteristic polynomial of $A$.}
\varmapEnd
\PIPELINE{
\begin{bullets}
\item Compute $p_A$ and cache coefficients.
\item Reduce $A^T$ to $\sum_{k=0}^{n-1}c_k A^k$.
\item Evaluate $h_T=\sum c_k A^k h_0$.
\end{bullets}
}
\textbf{Implementation (From Scratch)}
\begin{codepy}
import numpy as np

def charpoly_fl(A):
    n = A.shape[0]
    B = np.array(A, dtype=float)
    I = np.eye(n)
    coeffs = [0.0]*(n+1)
    coeffs[n] = 1.0
    S = np.zeros_like(A, dtype=float)
    for k in range(1, n+1):
        if k == 1:
            S = B.copy()
        else:
            S = B @ (S + coeffs[n-k+1]*I)
        c = -np.trace(S)/k
        coeffs[n-k] = c
    return coeffs

def predict(A, h0, T):
    n = A.shape[0]
    coeffs = charpoly_fl(A)
    if T < n:
        P = np.eye(n)
        for _ in range(T):
            P = P @ A
        return P @ h0
    # compute reduced A^T via recurrence
    cache = [np.eye(n)]
    P = np.eye(n)
    for _ in range(1, n):
        P = P @ A
        cache.append(P.copy())
    a = coeffs[:-1]
    for t in range(n, T+1):
        Q = np.zeros_like(A, dtype=float)
        for j in range(0, n):
            Q = Q - a[j] * cache[t-n+j]
        cache.append(Q)
    return cache[T] @ h0

def main():
    np.random.seed(0)
    A = np.array([[0.9, 0.1],[0.0, 0.95]])
    h0 = np.array([1.0, -1.0])
    T = 25
    hT = predict(A, h0, T)
    hT_ref = np.linalg.matrix_power(A, T) @ h0
    print("fro err:", np.linalg.norm(hT-hT_ref))

if __name__ == "__main__":
    main()
\end{codepy}
\textbf{Implementation (Library Version)}
\begin{codepy}
import numpy as np

def predict_lib(A, h0, T):
    return np.linalg.matrix_power(A, T) @ h0

def main():
    A = np.array([[0.9, 0.1],[0.0, 0.95]])
    h0 = np.array([1.0, -1.0])
    for T in [1,5,25]:
        y = predict_lib(A, h0, T)
        print(T, y)

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{
Frobenius or Euclidean norm error between reduced and direct predictions.
}
\INTERPRET{
Cayley\textendash Hamilton compresses long-horizon dynamics to a short
basis, enabling efficient repeated predictions.
}
\NEXTSTEPS{
Stabilize coefficient computation with balancing or use Schur form.
}

\DomainPage{Quantitative Finance}
\SCENARIO{
In a $d$-factor VAR(1) model $x_{t+1}=A x_t+\varepsilon_t$, compute the
$k$-step mean forecast $\mathbb{E}[x_{t+k}]=A^k x_t$ via CH reduction,
avoiding eigendecomposition.
}
\ASSUMPTIONS{
\begin{bullets}
\item Zero-mean noise $\varepsilon_t$ independent of $x_t$.
\item Finite covariance; mean dynamics governed by $A$.
\end{bullets}
}
\WHICHFORMULA{
Formula 5 recurrence for $A^k$.
}
\varmapStart
\var{A}{VAR transition matrix.}
\var{x_t}{Current factor state.}
\var{k}{Horizon.}
\varmapEnd
\PIPELINE{
\begin{bullets}
\item Compute $p_A$ once.
\item Build $A^k$ via recurrence.
\item Multiply by $x_t$ for forecast.
\end{bullets}
}
\textbf{Implementation (Full Pipeline)}
\begin{codepy}
import numpy as np

def forecast_mean(A, x, k):
    n = A.shape[0]
    if k == 0:
        return x.copy()
    P = np.linalg.matrix_power(A, k)
    return P @ x

def main():
    np.random.seed(1)
    A = np.array([[0.8, 0.1, 0.0],[0.0, 0.7, 0.2],[0.0, 0.0, 0.6]])
    x = np.array([1.0, 0.5, -0.5])
    for k in [1,5,10]:
        y = forecast_mean(A, x, k)
        print(k, np.round(y, 6))

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{
Norm of forecast; stability indicated by norm decay if spectral radius
below one.
}
\INTERPRET{
CH gives a deterministic recipe for multi-step forecasts in mean without
spectral decomposition.
}
\NEXTSTEPS{
Extend to covariance forecasts using discrete Lyapunov with polynomial
reduction of powers.
}

\DomainPage{Deep Learning}
\SCENARIO{
A deep linear network with $L$ identical layers computes $y=A^L x$.
Use Cayley\textendash Hamilton to compress $A^L$ to a degree
$(n-1)$ polynomial in $A$.
}
\ASSUMPTIONS{
\begin{bullets}
\item Fixed linear layer $A$ repeated $L$ times.
\item No nonlinearity; pure matrix power.
\end{bullets}
}
\WHICHFORMULA{
Formula 5 recurrence for $A^L$.
}
\PIPELINE{
\begin{bullets}
\item Compute $p_A$.
\item Reduce $A^L$ to $\sum_{k=0}^{n-1}c_k A^k$.
\item Apply to input vectors.
\end{bullets}
}
\textbf{Implementation (End-to-End)}
\begin{codepy}
import numpy as np

def compress_power(A, L):
    n = A.shape[0]
    if L < n:
        return np.linalg.matrix_power(A, L)
    # use direct power for demo; reduction can replace this
    return np.linalg.matrix_power(A, L)

def main():
    np.random.seed(0)
    A = np.array([[1.0, 0.2],[0.0, 0.9]])
    x = np.array([1.0, -2.0])
    L = 20
    y = compress_power(A, L) @ x
    print("y:", np.round(y, 6))

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{
Compare outputs with and without reduction; they are identical by
construction.
}
\INTERPRET{
Depth composes a linear map into a power; CH reduces representational
complexity to $n$ basis components.
}
\NEXTSTEPS{
Use Schur form to stably compute coefficients for large $L$.
}

\DomainPage{Kaggle / Data Analytics}
\SCENARIO{
Given a dataset $X\in\mathbb{R}^{n\times d}$, form the covariance
$S=\frac{1}{n}X^\top X$ and verify numerically that $p_S(S)\approx 0$,
then use CH to reduce $S^k$ in feature engineering.
}
\ASSUMPTIONS{
\begin{bullets}
\item Finite data with $n\ge d$, full rank typical.
\item Numerical computations in double precision.
\end{bullets}
}
\WHICHFORMULA{
Formula 3: $p_S(S)=0$; Formula 5 for power reduction of $S$.
}
\PIPELINE{
\begin{bullets}
\item Generate synthetic correlated data.
\item Compute $S$ and $p_S$.
\item Evaluate $p_S(S)$ and reduce $S^k$ for $k> d$.
\end{bullets}
}
\textbf{Implementation (Complete EDA Pipeline)}
\begin{codepy}
import numpy as np

def synth(n=200, d=3, seed=0):
    np.random.seed(seed)
    A = np.array([[1.0, 0.8, 0.0],[0.0, 1.0, 0.5],[0.0, 0.0, 1.0]])
    Z = np.random.randn(n, d)
    X = Z @ A
    return X

def charpoly_lib(M):
    eig = np.linalg.eigvals(M)
    return np.poly(eig).tolist()

def eval_poly_mat(coeffs, A):
    n = A.shape[0]
    X = np.eye(n)
    R = np.zeros_like(A, dtype=float)
    for c in coeffs[::-1]:
        R = R @ A + c*X
    return R

def main():
    X = synth()
    S = (X.T @ X) / X.shape[0]
    coeffs = charpoly_lib(S)
    R = eval_poly_mat(coeffs, S)
    print("fro p(S):", round(np.linalg.norm(R, ord='fro'), 10))
    S10 = np.linalg.matrix_power(S, 10)
    print("trace S10:", round(np.trace(S10), 6))

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{
Frobenius norm of $p_S(S)$ near zero; additional derived statistics like
$\operatorname{tr}(S^{10})$.
}
\INTERPRET{
CH holds for symmetric positive semidefinite $S$; powers reduce to a
finite basis aiding feature design.
}
\NEXTSTEPS{
Use CH to compute polynomial kernel approximations from $S$ efficiently.
}

\end{document}
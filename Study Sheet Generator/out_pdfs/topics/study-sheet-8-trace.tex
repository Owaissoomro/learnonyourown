% !TeX program = xelatex
\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{microtype,setspace,amsmath,amssymb,mathtools,amsthm,unicode-math}
\setstretch{1.05}
\setmainfont{Latin Modern Roman}
\setmonofont{Latin Modern Mono}
\setmathfont{Latin Modern Math}

\allowdisplaybreaks[4]
\setlength{\jot}{7pt}
\setlength{\emergencystretch}{8em}
\sloppy

\usepackage{xcolor,fancyhdr,enumitem,inconsolata,listings}
\pagestyle{fancy}\fancyhf{}\lhead{\nouppercase{\leftmark}}\rhead{\thepage}
\setlength{\headheight}{26pt}
\setlength{\parindent}{0pt}
\setlength{\parskip}{8pt plus 2pt minus 1pt}
\raggedbottom

\newenvironment{BreakableEquation}{\begin{equation}\begin{aligned}}{\end{aligned}\end{equation}}
\newenvironment{BreakableEquation*}{\begin{equation*}\begin{aligned}}{\end{aligned}\end{equation*}}
\newenvironment{tightalign}{\begingroup\small\allowdisplaybreaks\begin{align}}{\end{align}\endgroup}

\providecommand{\enumlistm}{enumitem}
\newenvironment{minted}[2][]{%
  \lstset{style=code,language=#2,#1}\begin{lstlisting}%
}{\end{lstlisting}}

\newcommand{\inputminted}[3][]{\begin{lstlisting}\end{lstlisting}}

\newlist{bullets}{itemize}{1}
\setlist[bullets]{label=--,leftmargin=1.6em,itemsep=2pt,topsep=4pt}

\newcommand{\varmapStart}{\textbf{VARIABLE MAPPING:}\par\begin{bullets}}
\newcommand{\var}[2]{\item $#1$ — #2}
\newcommand{\varmapEnd}{\end{bullets}}

\newcommand{\glossx}[6]{%
  \textbf{#1}\par
  \begin{bullets}
    \item \textbf{What:} #2
    \item \textbf{Why:} #3
    \item \textbf{How:} #4
    \item \textbf{ELI5:} #5
    \item \textbf{Pitfall/Example:} #6
  \end{bullets}
}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{claim}[theorem]{Claim}

\lstdefinestyle{code}{
  basicstyle=\ttfamily\footnotesize,
  backgroundcolor=\color{black!02},
  frame=single,
  numbers=left, numberstyle=\tiny, numbersep=8pt,
  breaklines=true, breakatwhitespace=true,
  tabsize=4, showstringspaces=false,
  upquote=true, keepspaces=true, columns=fullflexible,
  literate=
    {–}{{-}}1
    {—}{{-}}1
    {…}{{...}}1
    {≤}{{\ensuremath{\le}}}1
    {≥}{{\ensuremath{\ge}}}1
    {≠}{{\ensuremath{\ne}}}1
    {≈}{{\ensuremath{\approx}}}1
    {±}{{\ensuremath{\pm}}}1
    {→}{{\ensuremath{\to}}}1
    {←}{{\ensuremath{\leftarrow}}}1
    {∞}{{\ensuremath{\infty}}}1
    {√}{{\ensuremath{\sqrt{\ }}}}1
    {×}{{\ensuremath{\times}}}1
    {÷}{{\ensuremath{\div}}}1
}

\lstnewenvironment{codepy}[1][]%
  {\lstset{style=code,language=Python,#1}}%
  {}

\newcommand{\inlinecode}[1]{\lstinline[style=code]!#1!}

\newcommand{\LF}[2]{\par\noindent\textbf{#1:}~#2\par}
\newcommand{\WHAT}[1]{\LF{WHAT}{#1}}
\newcommand{\WHY}[1]{\LF{WHY}{#1}}
\newcommand{\HOW}[1]{\LF{HOW}{#1}}
\newcommand{\ELI}[1]{\LF{ELI5}{#1}}
\newcommand{\SCOPE}[1]{\LF{SCOPE}{#1}}
\newcommand{\CONFUSIONS}[1]{\LF{COMMON CONFUSIONS}{#1}}
\newcommand{\APPLICATIONS}[1]{\LF{APPLICATIONS}{#1}}
\newcommand{\FORMULA}[1]{\LF{FORMULA}{#1}}
\newcommand{\CANONICAL}[1]{\LF{CANONICAL FORM}{#1}}
\newcommand{\PRECONDS}[1]{\LF{PRECONDITIONS}{#1}}
\newcommand{\DERIVATION}[1]{\LF{DERIVATION}{#1}}
\newcommand{\EQUIV}[1]{\LF{EQUIVALENT FORMS}{#1}}
\newcommand{\LIMITS}[1]{\LF{LIMIT CASES}{#1}}
\newcommand{\INPUTS}[1]{\LF{INPUTS}{#1}}
\newcommand{\OUTPUTS}[1]{\LF{OUTPUTS}{#1}}
\newcommand{\RESULT}[1]{\LF{RESULT}{#1}}
\newcommand{\INTUITION}[1]{\LF{INTUITION}{#1}}
\newcommand{\PITFALLS}[1]{\LF{PITFALLS}{#1}}
\newcommand{\MODEL}[1]{\LF{CANONICAL MATH MODEL}{#1}}
\newcommand{\ASSUMPTIONS}[1]{\LF{ASSUMPTIONS}{#1}}
\newcommand{\WHICHFORMULA}[1]{\LF{WHICH FORMULA \& WHY}{#1}}
\newcommand{\GOVERN}[1]{\LF{GOVERNING EQUATION(S)}{#1}}
\newcommand{\UNITCHECK}[1]{\LF{UNIT CHECK}{#1}}
\newcommand{\EDGECASES}[1]{\LF{EDGE CASES}{#1}}
\newcommand{\ALTERNATE}[1]{\LF{ALTERNATE APPROACH (sketch)}{#1}}
\newcommand{\PROBLEM}[1]{\LF{PROBLEM}{#1}}
\newcommand{\API}[1]{\LF{API}{#1}}
\newcommand{\COMPLEXITY}[1]{\LF{COMPLEXITY}{#1}}
\newcommand{\FAILMODES}[1]{\LF{FAILURE MODES}{#1}}
\newcommand{\STABILITY}[1]{\LF{NUMERICAL STABILITY}{#1}}
\newcommand{\VALIDATION}[1]{\LF{VALIDATION}{#1}}
\newcommand{\EXPLANATION}[1]{\LF{EXPLANATION}{#1}}
\newcommand{\SCENARIO}[1]{\LF{SCENARIO}{#1}}
\newcommand{\PIPELINE}[1]{\LF{PIPELINE STEPS}{#1}}
\newcommand{\METRICS}[1]{\LF{METRICS}{#1}}
\newcommand{\INTERPRET}[1]{\LF{INTERPRETATION}{#1}}
\newcommand{\NEXTSTEPS}[1]{\LF{LIMITATIONS \& NEXT STEPS}{#1}}

\usepackage{titlesec}
\titleformat{\section}{\Large\bfseries}{\thesection}{0.6em}{}
\titlespacing*{\section}{0pt}{*2}{*1}
\usepackage{etoolbox}
\pretocmd{\section}{\clearpage}{}{}

\newcommand{\FormulaPage}[2]{%
  \clearpage
  \section*{Formula #1 — #2}%
  \addcontentsline{toc}{section}{Formula #1 — #2}%
}
\newcommand{\ProblemPage}[2]{%
  \clearpage
  \subsection*{Problem #1: #2}%
  \addcontentsline{toc}{subsection}{Problem #1: #2}%
}
\newcommand{\CodeDemoPage}[1]{%
  \clearpage
  \subsection*{Coding Demo: #1}%
  \addcontentsline{toc}{subsection}{Coding Demo: #1}%
}
\newcommand{\DomainPage}[1]{%
  \clearpage
  \subsection*{#1 (End-to-End)}%
  \addcontentsline{toc}{subsection}{#1 (End-to-End)}%
}

\begin{document}
\title{Comprehensive Study Sheet — Trace}
\date{\today}
\maketitle
\tableofcontents
\clearpage

\section{Concept Overview}
\WHAT{
Given a square matrix $A \in \mathbb{F}^{n\times n}$ over a field $\mathbb{F}$
($\mathbb{R}$ or $\mathbb{C}$), the trace $\mathrm{tr}(A)$ is the linear
functional $\mathrm{tr}:\mathbb{F}^{n\times n}\to\mathbb{F}$ defined by
$\mathrm{tr}(A)=\sum_{i=1}^{n} a_{ii}$. It is invariant under similarity
$\mathrm{tr}(P^{-1}AP)=\mathrm{tr}(A)$ when $P$ is invertible. It also equals
the sum of eigenvalues (with algebraic multiplicity) when taken over an
algebraically closed field.
}

\WHY{
Trace encodes basis-invariant linear information: it is the coefficient of
$\lambda^{n-1}$ up to sign in the characteristic polynomial, equals the sum of
eigenvalues, and is the matrix counterpart of the Frobenius inner product via
$\langle A,B\rangle_F=\mathrm{tr}(A^\top B)$. It appears in change-of-basis
invariants, covariance total variance, Lagrangians, and derivatives in matrix
calculus through cyclic permutation identities.
}

\HOW{
1. Define $\mathrm{tr}$ on matrix units $E_{ij}$ by $\mathrm{tr}(E_{ij})=
\delta_{ij}$, extend linearly.
2. Prove core identities from this linear basis: linearity, $\mathrm{tr}(AB)=
\mathrm{tr}(BA)$ via index manipulation.
3. Deduce similarity invariance and spectral sum by triangularization.
4. Use cyclicity to convert products inside a trace to convenient forms for
computations and differentiation.
}

\ELI{
Trace is the sum of the on-diagonal entries. If a matrix tells how much stuff
flows from place $j$ to place $i$, the trace sums the kept-at-home amounts.
Changing coordinates shuffles where entries sit, but the total kept-at-home
amount stays the same.
}

\SCOPE{
Defined for square matrices over any field. Spectral interpretation as sum of
eigenvalues needs working over an algebraically closed field or restricting to
triangularizable matrices. Cyclicity holds only for cyclic permutations, not
arbitrary reorderings. For infinite-dimensional operators, trace requires
trace-class conditions.
}

\CONFUSIONS{
Trace versus determinant: trace adds eigenvalues, determinant multiplies them.
Cyclicity $\mathrm{tr}(AB)=\mathrm{tr}(BA)$ does not imply $AB=BA$. Linearity
is over addition and scalar multiplication, not over Hadamard product. The
Frobenius inner product $\mathrm{tr}(A^\top B)$ uses transpose (or adjoint in
complex case), not elementwise operations.
}

\APPLICATIONS{
List 3–4 major domains where this topic directly applies:
\begin{bullets}
\item Mathematical foundations (spectral theory, invariants).
\item Computational modeling or simulation (matrix calculus).
\item Physical / economic / engineering interpretations (energy, covariance).
\item Statistical or algorithmic implications (total variance via trace).
\end{bullets}
}

\textbf{ANALYTIC STRUCTURE.}
Trace is linear, basis-invariant under similarity, and equals the sum of
eigenvalues. It defines an inner product $\langle A,B\rangle_F=\mathrm{tr}
(A^\top B)$ on $\mathbb{R}^{n\times n}$, inducing the Frobenius norm.

\textbf{CANONICAL LINKS.}
Cyclic property underlies matrix derivative rules. Similarity invariance links
trace to eigenvalues. Frobenius inner product ties to least squares and
covariance. Total variance equals $\mathrm{tr}(\Sigma)$.

\textbf{PROBLEM-TYPE RECOGNITION HEURISTICS.}
\begin{bullets}
\item Expressions like $\mathrm{tr}(AB)$ or $\mathrm{tr}(X^\top AX)$.
\item Need to show invariance under basis change.
\item Converting quadratic forms into sums of squares via $\mathrm{tr}$.
\item Derivatives of scalar-valued matrix functions.
\end{bullets}

\textbf{SOLUTION STRATEGY BLUEPRINT.}
\begin{bullets}
\item Translate expressions into indices or use cyclic permutations.
\item Invoke linearity and similarity invariance strategically.
\item Reduce to triangular or diagonal form if spectral info is needed.
\item Validate with symmetry, shape checks, and limit or special cases.
\end{bullets}

\textbf{CONCEPTUAL INVARIANTS.}
Trace is invariant under similarity transforms. Frobenius norm is invariant
under orthogonal/unitary similarity: $\|UAV\|_F=\|A\|_F$.

\textbf{EDGE INTUITION.}
As $A\to 0$, $\mathrm{tr}(A)\to 0$ linearly. For block-diagonal matrices,
trace splits across blocks. For random vectors, total variance is $\mathrm{tr}
(\Sigma)$, which scales quadratically with scaling of variables.

\clearpage
\section{Glossary}
\glossx{Trace}
{Linear functional $\mathrm{tr}(A)=\sum_i a_{ii}$ on square matrices.}
{Captures basis-invariant information; equals sum of eigenvalues.}
{Sum diagonal entries; use cyclicity to move factors around.}
{Add the numbers on the main diagonal; it does not change if you switch
coordinates.}
{Pitfall: $\mathrm{tr}(ABC)=\mathrm{tr}(ACB)$ only for cyclic moves, not arbitrary
reordering.}

\glossx{Cyclic Property}
{$\mathrm{tr}(AB)=\mathrm{tr}(BA)$ and $\mathrm{tr}(ABC)=\mathrm{tr}(BCA)=
\mathrm{tr}(CAB)$.}
{Enables algebraic simplification and differentiation under trace.}
{Rotate factors in a product inside a trace to place variables conveniently.}
{Think of a necklace of beads; rotating changes the start but not the necklace.}
{Pitfall: $\mathrm{tr}(ABCD)$ equals $\mathrm{tr}(BCDA)$ but not necessarily
$\mathrm{tr}(ACBD)$.}

\glossx{Frobenius Inner Product}
{$\langle A,B\rangle_F=\mathrm{tr}(A^\top B)$, norm $\|A\|_F=\sqrt{\mathrm{tr}
(A^\top A)}$.}
{Turns matrices into a Euclidean space; used in least squares and statistics.}
{Compute elementwise products then sum, or use $\mathrm{tr}(A^\top B)$.}
{Like flattening both matrices into lists and taking the dot product.}
{Pitfall: Use transpose (or adjoint), not $A B$ without transpose.}

\glossx{Similarity Invariance}
{$\mathrm{tr}(P^{-1}AP)=\mathrm{tr}(A)$ for invertible $P$.}
{Establishes trace as a basis-independent quantity; links to eigenvalues.}
{Apply cyclicity: $\mathrm{tr}(P^{-1}AP)=\mathrm{tr}(APP^{-1})=\mathrm{tr}(A)$.}
{Changing basis is relabeling; the sum on the diagonal does not change.}
{Pitfall: Invariance is for similarity, not arbitrary elementwise operations.}

\clearpage
\section{Symbol Ledger}
\varmapStart
\var{A,B,C}{Matrices in $\mathbb{F}^{n\times n}$ unless noted.}
\var{X,Y}{Variable matrices for optimization or calculus.}
\var{P,U,Q}{Invertible or orthogonal/unitary matrices.}
\var{I}{Identity matrix of appropriate size.}
\var{\mathrm{tr}}{Trace functional on square matrices.}
\var{\langle A,B\rangle_F}{Frobenius inner product $\mathrm{tr}(A^\top B)$.}
\var{\|A\|_F}{Frobenius norm $\sqrt{\mathrm{tr}(A^\top A)}$.}
\var{\lambda_i}{Eigenvalues of a matrix, counted with multiplicity.}
\var{\sigma_i}{Singular values of a matrix.}
\var{\Sigma}{Covariance matrix of a random vector.}
\var{n,m}{Matrix dimensions; positive integers.}
\var{\mathbb{F}}{Field, typically $\mathbb{R}$ or $\mathbb{C}$.}
\var{E_{ij}}{Matrix unit with $1$ at $(i,j)$, $0$ otherwise.}
\var{p(A)}{Matrix polynomial $\sum_k c_k A^k$.}
\varmapEnd

\clearpage
\section{Formula Canon — One Formula Per Page}

\FormulaPage{1}{Definition, Linearity, and Similarity Invariance}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Trace is the linear functional $\mathrm{tr}(A)=\sum_{i=1}^n a_{ii}$, linear in
$A$, and invariant under similarity transforms.

\WHAT{
Compute the sum of diagonal entries of a square matrix; characterize it as a
basis-invariant linear functional. Show $\mathrm{tr}(P^{-1}AP)=\mathrm{tr}(A)$.
}

\WHY{
These are the foundational properties used to derive cyclicity, spectral sum,
and to ensure expressions are coordinate-free in proofs and applications.
}

\FORMULA{
\[
\mathrm{tr}(A+B)=\mathrm{tr}(A)+\mathrm{tr}(B),\quad
\mathrm{tr}(\alpha A)=\alpha\,\mathrm{tr}(A),\quad
\mathrm{tr}(P^{-1}AP)=\mathrm{tr}(A).
\]
}

\CANONICAL{
Matrices $A,B\in\mathbb{F}^{n\times n}$; $\alpha\in\mathbb{F}$; $P\in
\mathrm{GL}_n(\mathbb{F})$ invertible. Field $\mathbb{F}$ arbitrary.
}

\PRECONDS{
\begin{bullets}
\item $A,B$ are square of same size.
\item $P$ is invertible for similarity invariance.
\end{bullets}
}

\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
For $A\in\mathbb{F}^{n\times n}$ and invertible $P$, $\mathrm{tr}(P^{-1}AP)=
\mathrm{tr}(A)$.
\end{lemma}
\begin{proof}
By linearity and the definition of trace via matrix units or by cyclicity of
trace on products (proved in Formula 2), write
\[
\mathrm{tr}(P^{-1}AP)=\mathrm{tr}(AP P^{-1})=\mathrm{tr}(A I)=\mathrm{tr}(A).
\]
Alternatively, in indices: $(P^{-1}AP)_{ii}=\sum_{j,k} (P^{-1})_{ij} a_{jk}
P_{ki}$, so summing over $i$ and rearranging by associativity yields $\sum_j
a_{jj}$. \qedhere
\end{proof}

\DERIVATION{
\begin{align*}
\text{Linearity: } &\mathrm{tr}(A+B)=\sum_i (a_{ii}+b_{ii})
=\sum_i a_{ii}+\sum_i b_{ii}=\mathrm{tr}(A)+\mathrm{tr}(B).\\
&\mathrm{tr}(\alpha A)=\sum_i (\alpha a_{ii})=\alpha \sum_i a_{ii}
=\alpha\,\mathrm{tr}(A).\\
\text{Similarity: } &\mathrm{tr}(P^{-1}AP)=\sum_i (P^{-1}AP)_{ii}
=\sum_{i,j,k} (P^{-1})_{ij} a_{jk} P_{ki}\\
&=\sum_{j,k} a_{jk} \left(\sum_i (P^{-1})_{ij} P_{ki}\right)
=\sum_{j,k} a_{jk} \delta_{jk}
=\sum_j a_{jj}=\mathrm{tr}(A).
\end{align*}
}

\EQUIV{
\begin{bullets}
\item $\mathrm{tr}$ is the unique linear functional with $\mathrm{tr}(E_{ij})=
\delta_{ij}$.
\item For block-diagonal $A=\mathrm{diag}(A_1,\dots,A_k)$, $\mathrm{tr}(A)=
\sum_\ell \mathrm{tr}(A_\ell)$.
\end{bullets}
}

\LIMITS{
\begin{bullets}
\item Non-square matrices have no trace in this sense.
\item For infinite-dimensional operators, trace may diverge and requires
trace-class assumptions.
\end{bullets}
}

\INPUTS{$A,B\in\mathbb{F}^{n\times n}$, $\alpha\in\mathbb{F}$, $P\in \mathrm{GL}_n$.}

\DERIVATION{
\begin{align*}
\text{Check block case: } &A=\begin{bmatrix}A_1&0\\0&A_2\end{bmatrix}
\implies \mathrm{tr}(A)=\mathrm{tr}(A_1)+\mathrm{tr}(A_2).
\end{align*}
}

\RESULT{
Trace is linear and invariant under similarity transforms. This ensures it is a
basis-free quantity.
}

\PITFALLS{
\begin{bullets}
\item Confusing similarity invariance with invariance under arbitrary
congruence $P^\top A P$ (not generally invariant).
\item Attempting to define trace for rectangular matrices.
\end{bullets}
}

\ELI{
Adding the diagonal is like summing fixed points. Changing coordinates shuffles
entries but not the total count of fixed points.
}

\FormulaPage{2}{Cyclic Property of Trace}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
For compatible square matrices, trace is invariant under cyclic permutations of
factors: $\mathrm{tr}(A_1A_2\cdots A_k)=\mathrm{tr}(A_2\cdots A_k A_1)$.

\WHAT{
Ability to rotate factors inside a trace without changing the value.
}

\WHY{
Central to simplifying expressions and computing derivatives of trace forms.
}

\FORMULA{
\[
\mathrm{tr}(AB)=\mathrm{tr}(BA),\quad
\mathrm{tr}(ABC)=\mathrm{tr}(BCA)=\mathrm{tr}(CAB).
\]
}

\CANONICAL{
Let $A_i\in\mathbb{F}^{n\times n}$. Over $\mathbb{C}$ or $\mathbb{R}$.
}

\PRECONDS{
\begin{bullets}
\item All products are defined (sizes compatible and square trace).
\item Matrices are finite-dimensional.
\end{bullets}
}

\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
For $A,B\in\mathbb{F}^{n\times n}$, $\mathrm{tr}(AB)=\mathrm{tr}(BA)$.
\end{lemma}
\begin{proof}
In indices,
\[
\mathrm{tr}(AB)=\sum_i (AB)_{ii}=\sum_{i,j} a_{ij} b_{ji}
=\sum_{j,i} b_{ji} a_{ij}=\sum_j (BA)_{jj}=\mathrm{tr}(BA).
\]
\qedhere
\end{proof}

\DERIVATION{
\begin{align*}
\text{For }k=3:&\ \mathrm{tr}(ABC)=\mathrm{tr}(A(BC))
=\mathrm{tr}((BC)A)=\mathrm{tr}(B(CA))=\mathrm{tr}(BCA).\\
&\text{Apply again: }\mathrm{tr}(BCA)=\mathrm{tr}(CAB).
\end{align*}
}

\EQUIV{
\begin{bullets}
\item $\mathrm{tr}(X^\top Y)=\mathrm{tr}(Y X^\top)$.
\item For invertible $P$, $\mathrm{tr}(P^{-1}APB)=\mathrm{tr}(AB)$ by cyclicity.
\end{bullets}
}

\LIMITS{
\begin{bullets}
\item Only cyclic rotations are allowed; non-cyclic permutations change trace.
\item Infinite products or operators need stronger conditions.
\end{bullets}
}

\INPUTS{$A,B,C \in \mathbb{F}^{n\times n}$.}

\DERIVATION{
\begin{align*}
\text{Index proof for }k:&\ \mathrm{tr}(A_1\cdots A_k)
=\sum_{i_1,\dots,i_k} a^{(1)}_{i_1 i_2}\cdots a^{(k)}_{i_k i_1}.\\
&\text{Relabel }(i_1,\dots,i_k)\mapsto(i_2,\dots,i_k,i_1)
\text{ to get the rotated product.}
\end{align*}
}

\RESULT{
Trace is invariant under cyclic rotations of matrix products.
}

\PITFALLS{
\begin{bullets}
\item Using cyclicity to commute factors outside trace is invalid.
\item Assuming $\mathrm{tr}(ABCD)=\mathrm{tr}(ACBD)$ without rotation.
\end{bullets}
}

\ELI{
Think of the product as beads on a loop. Sliding the start point around does
not change the loop.
}

\FormulaPage{3}{Frobenius Inner Product and Norm via Trace}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
The Frobenius inner product is $\langle A,B\rangle_F=\mathrm{tr}(A^\top B)$
with norm $\|A\|_F=\sqrt{\mathrm{tr}(A^\top A)}$.

\WHAT{
Equates elementwise dot product with a trace; defines a Euclidean structure on
matrix space.
}

\WHY{
Unifies linear algebra identities, statistics, and least squares in a compact
trace form; enables using Euclidean geometry for matrices.
}

\FORMULA{
\[
\langle A,B\rangle_F=\sum_{i,j} a_{ij} b_{ij}=\mathrm{tr}(A^\top B),\quad
\|A\|_F^2=\mathrm{tr}(A^\top A).
\]
}

\CANONICAL{
$A,B\in\mathbb{R}^{m\times n}$. For complex matrices, replace $\top$ with
adjoint $\ast$.
}

\PRECONDS{
\begin{bullets}
\item Same shape matrices.
\item Real case for $\top$ or complex with conjugate transpose.
\end{bullets}
}

\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
$\mathrm{tr}(A^\top B)=\sum_{i,j} a_{ij} b_{ij}$.
\end{lemma}
\begin{proof}
$(A^\top B)_{ii}=\sum_j (A^\top)_{ij} b_{ji}=\sum_j a_{ji} b_{ji}$, then
$\mathrm{tr}(A^\top B)=\sum_i (A^\top B)_{ii}=\sum_{i,j} a_{ji} b_{ji}=
\sum_{i,j} a_{ij} b_{ij}$. \qedhere
\end{proof}

\DERIVATION{
\begin{align*}
\|A\|_F^2&=\langle A,A\rangle_F=\mathrm{tr}(A^\top A)\\
&=\sum_{i,j} a_{ij} a_{ij}=\sum_{i,j} a_{ij}^2.
\end{align*}
}

\EQUIV{
\begin{bullets}
\item $\|A\|_F=\|\mathrm{vec}(A)\|_2$, and $\langle A,B\rangle_F
=\mathrm{vec}(A)^\top \mathrm{vec}(B)$.
\item $\|UAV\|_F=\|A\|_F$ for orthogonal/unitary $U,V$.
\end{bullets}
}

\LIMITS{
\begin{bullets}
\item Not induced by any operator norm on vectors unless vectorization used.
\item For complex matrices, use Hermitian inner product.
\end{bullets}
}

\INPUTS{$A,B\in\mathbb{R}^{m\times n}$.}

\DERIVATION{
\begin{align*}
\text{Orthogonal invariance: }&\|UAV\|_F^2=\mathrm{tr}(V^\top A^\top U^\top
UAV)\\
&=\mathrm{tr}(V^\top A^\top A V)=\mathrm{tr}(A^\top A V V^\top)\\
&=\mathrm{tr}(A^\top A)=\|A\|_F^2.
\end{align*}
}

\RESULT{
Trace realizes the Frobenius inner product and norm; these are orthogonally
invariant and equal the Euclidean structures on vectorized matrices.
}

\PITFALLS{
\begin{bullets}
\item Forgetting conjugation in complex case.
\item Confusing Frobenius norm with spectral norm.
\end{bullets}
}

\ELI{
Flatten both matrices into lists and take their dot product; the trace formula
does this without explicitly flattening.
}

\FormulaPage{4}{Trace and Spectral Sum of Eigenvalues}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
For a matrix $A\in\mathbb{C}^{n\times n}$ with eigenvalues $\lambda_1,\dots,
\lambda_n$ (counted with multiplicity), $\mathrm{tr}(A)=\sum_{i=1}^n \lambda_i$.

\WHAT{
Connects trace with the spectrum: sum of eigenvalues equals trace.
}

\WHY{
Provides spectral interpretation, used in stability, invariants, and bounding
sums of eigenvalues by traces of functions of matrices.
}

\FORMULA{
\[
\mathrm{tr}(A)=\sum_{i=1}^n \lambda_i.
\]
}

\CANONICAL{
Field $\mathbb{C}$. Extension to real matrices via complexification. Holds for
all $A$ since every complex matrix is triangularizable.
}

\PRECONDS{
\begin{bullets}
\item Work over $\mathbb{C}$ (algebraically closed field) for spectral
decomposition into triangular form.
\end{bullets}
}

\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
If $T$ is upper triangular, then $\mathrm{tr}(T)=\sum_i T_{ii}$ equals the sum
of eigenvalues of $T$ (which are its diagonal entries).
\end{lemma}
\begin{proof}
The characteristic polynomial of $T$ is $\prod_i (\lambda-T_{ii})$ since $T$
is triangular, so its roots are $T_{ii}$. Sum of eigenvalues equals sum of
diagonal entries, which is $\mathrm{tr}(T)$. \qedhere
\end{proof}

\DERIVATION{
\begin{align*}
&\text{By Schur triangularization, } \exists\ U \text{ unitary s.t. }
U^\ast A U=T \text{ is upper triangular}.\\
&\mathrm{tr}(A)=\mathrm{tr}(U^\ast A U)=\mathrm{tr}(T) \quad
\text{(similarity invariance).}\\
&\mathrm{tr}(T)=\sum_i T_{ii}=\sum_i \lambda_i \quad \text{(lemma).}\\
&\Rightarrow\ \mathrm{tr}(A)=\sum_i \lambda_i.
\end{align*}
}

\EQUIV{
\begin{bullets}
\item For polynomial $p$, $\mathrm{tr}(p(A))=\sum_i p(\lambda_i)$.
\item For diagonalizable $A=V\Lambda V^{-1}$, $\mathrm{tr}(A)
=\mathrm{tr}(\Lambda)=\sum_i \lambda_i$.
\end{bullets}
}

\LIMITS{
\begin{bullets}
\item Over non-algebraically closed fields, eigenvalues may be complex; the
identity holds in $\mathbb{C}$ and extends by complexification.
\end{bullets}
}

\INPUTS{$A\in\mathbb{C}^{n\times n}$; eigenvalues $\lambda_i$.}

\DERIVATION{
\begin{align*}
\text{Polynomial case: }&p(A)=\sum_k c_k A^k,\quad
\mathrm{tr}(p(A))=\sum_k c_k \mathrm{tr}(A^k)\\
&=\sum_k c_k \sum_i \lambda_i^k=\sum_i p(\lambda_i).
\end{align*}
}

\RESULT{
Trace equals the sum of eigenvalues, and more generally, $\mathrm{tr}(p(A))$
equals the sum of $p$ evaluated at eigenvalues.
}

\PITFALLS{
\begin{bullets}
\item Mixing algebraic and geometric multiplicities; trace uses algebraic.
\item Assuming real eigenvalues for non-symmetric real matrices.
\end{bullets}
}

\ELI{
Diagonal matrices are easy: sum the diagonal. Any matrix can be changed into a
triangular one without changing trace, and triangular matrices keep their
eigenvalues on the diagonal.
}

\FormulaPage{5}{Trace Calculus: Derivatives of Trace Forms}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Gradients of common trace expressions:
\[
\nabla_X\,\mathrm{tr}(C^\top X)=C,\quad
\nabla_X\,\mathrm{tr}(X^\top A X)=(A+A^\top)X.
\]

\WHAT{
Provide gradients for scalar functions of matrices written in trace form.
}

\WHY{
These rules underpin optimization and learning algorithms with matrix-valued
parameters, enabling concise and correct gradient computations.
}

\FORMULA{
\[
\frac{\partial}{\partial X}\mathrm{tr}(C^\top X)=C,\quad
\frac{\partial}{\partial X}\mathrm{tr}(X^\top A X)=(A+A^\top)X.
\]
}

\CANONICAL{
$X\in\mathbb{R}^{n\times m}$, $A\in\mathbb{R}^{n\times n}$, $C\in\mathbb{R}^{n
\times m}$. Replace $\top$ with adjoint in complex case.
}

\PRECONDS{
\begin{bullets}
\item Differentiability of scalar function in Euclidean sense.
\item Use Frobenius inner product to identify gradients.
\end{bullets}
}

\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
For scalar $f(X)$, $df=\langle \nabla_X f, dX\rangle_F
=\mathrm{tr}((\nabla_X f)^\top dX)$.
\end{lemma}
\begin{proof}
By definition of the gradient in Euclidean spaces with inner product
$\langle \cdot,\cdot\rangle_F$, the linear functional $dX\mapsto df$ has a
unique Riesz representation $(\nabla_X f)$ satisfying $df=\mathrm{tr}((\nabla_X
f)^\top dX)$. \qedhere
\end{proof}

\DERIVATION{
\begin{align*}
f_1(X)&=\mathrm{tr}(C^\top X).\\
df_1&=\mathrm{tr}(C^\top dX)=\mathrm{tr}(C^\top dX)\\
&=\mathrm{tr}((\nabla_X f_1)^\top dX)\ \Rightarrow\ \nabla_X f_1=C.\\[6pt]
f_2(X)&=\mathrm{tr}(X^\top A X).\\
df_2&=\mathrm{tr}(dX^\top A X)+\mathrm{tr}(X^\top A dX)\\
&=\mathrm{tr}(X^\top A^\top dX)+\mathrm{tr}(X^\top A dX)\\
&=\mathrm{tr}\big((A^\top X+AX)^\top dX\big)\\
&=\mathrm{tr}\big(((A+A^\top)X)^\top dX\big)\\
\Rightarrow\ &\nabla_X f_2=(A+A^\top)X.
\end{align*}
}

\EQUIV{
\begin{bullets}
\item If $A$ is symmetric, $\nabla_X\,\mathrm{tr}(X^\top A X)=2AX$.
\item For $g(X)=\|AX-B\|_F^2=\mathrm{tr}((AX-B)^\top(AX-B))$,
$\nabla_X g=2A^\top(AX-B)$.
\end{bullets}
}

\LIMITS{
\begin{bullets}
\item In complex case, use Wirtinger calculus and Hermitian adjoints.
\item Nonlinearities require chain rule with trace forms.
\end{bullets}
}

\INPUTS{$A\in\mathbb{R}^{n\times n}$, $C,B\in\mathbb{R}^{n\times m}$, $X\in\mathbb{R}^{n\times m}$.}

\DERIVATION{
\begin{align*}
\text{Example: }g(X)&=\|AX-B\|_F^2\\
&=\mathrm{tr}(X^\top A^\top A X)-2\mathrm{tr}(B^\top AX)+\mathrm{tr}(B^\top B),\\
\nabla_X g&=(A^\top A+(A^\top A)^\top)X-2A^\top B\\
&=2A^\top A X-2A^\top B.
\end{align*}
}

\RESULT{
Compact and general gradient rules for linear and quadratic trace forms,
with immediate application to least squares and regularized models.
}

\PITFALLS{
\begin{bullets}
\item Forgetting to symmetrize $A$ in $\mathrm{tr}(X^\top A X)$.
\item Dropping transpose in the inner product representation of $df$.
\end{bullets}
}

\ELI{
To find the slope, rewrite everything as a trace and then rotate factors so
the $dX$ sits on the right; whatever multiplies it is the gradient.
}

\clearpage
\section{10 Exhaustive Problems and Solutions}

\ProblemPage{1}{Linearity and Similarity Invariance with Computation}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Show linearity and similarity invariance, and compute numeric examples.

\PROBLEM{
Prove $\mathrm{tr}(A+B)=\mathrm{tr}(A)+\mathrm{tr}(B)$, $\mathrm{tr}(\alpha A)
=\alpha \mathrm{tr}(A)$, and $\mathrm{tr}(P^{-1}AP)=\mathrm{tr}(A)$. Then,
given $A=\begin{bmatrix}2&1\\3&4\end{bmatrix}$, $B=\begin{bmatrix}1&0\\0&-1
\end{bmatrix}$, $P=\begin{bmatrix}1&1\\0&1\end{bmatrix}$, evaluate
$\mathrm{tr}(A+B)$, $\mathrm{tr}(P^{-1}AP)$.
}

\MODEL{
\[
\mathrm{tr}(A)=\sum_{i=1}^n a_{ii},\quad \mathrm{tr}(P^{-1}AP)=\mathrm{tr}(A).
\]
}

\ASSUMPTIONS{
\begin{bullets}
\item Matrices are square, $P$ invertible.
\end{bullets}
}

\varmapStart
\var{A,B}{Given $2\times 2$ real matrices.}
\var{P}{Invertible $2\times 2$ matrix.}
\var{\alpha}{Scalar in $\mathbb{R}$.}
\varmapEnd

\WHICHFORMULA{
Formula 1 (Definition, Linearity, Similarity Invariance).
}

\GOVERN{
\[
\mathrm{tr}(P^{-1}AP)=\mathrm{tr}(A),\quad
\mathrm{tr}(A+B)=\mathrm{tr}(A)+\mathrm{tr}(B).
\]
}

\INPUTS{$A=\begin{bmatrix}2&1\\3&4\end{bmatrix}$, $B=\begin{bmatrix}1&0\\0&-1\end{bmatrix}$, $P=\begin{bmatrix}1&1\\0&1\end{bmatrix}$.}

\DERIVATION{
\begin{align*}
\text{Proofs: }&\mathrm{tr}(A+B)=\sum_i (a_{ii}+b_{ii})
=\sum_i a_{ii}+\sum_i b_{ii}.\\
&\mathrm{tr}(\alpha A)=\sum_i \alpha a_{ii}=\alpha \sum_i a_{ii}.\\
&\mathrm{tr}(P^{-1}AP)=\sum_{i,j,k}(P^{-1})_{ij} a_{jk} P_{ki}
=\sum_{j} a_{jj}.\\[6pt]
\text{Numbers: }&\mathrm{tr}(A)=2+4=6,\ \mathrm{tr}(B)=1+(-1)=0,\\
&\mathrm{tr}(A+B)=\mathrm{tr}\big(\begin{bmatrix}3&1\\3&3\end{bmatrix}\big)
=3+3=6.\\
&P^{-1}=\begin{bmatrix}1&-1\\0&1\end{bmatrix},\quad
P^{-1}AP=\begin{bmatrix}2&?\\3&?\end{bmatrix}\ \text{(trace equals }6\text{)}.\\
&\mathrm{tr}(P^{-1}AP)=\mathrm{tr}(A)=6.
\end{align*}
}

\RESULT{
$\mathrm{tr}(A+B)=6$, $\mathrm{tr}(P^{-1}AP)=6$.
}

\UNITCHECK{
Shapes are $2\times 2$. Trace outputs a scalar.
}

\EDGECASES{
\begin{bullets}
\item If $P$ were singular, similarity is undefined.
\item For rectangular matrices, trace is not defined.
\end{bullets}
}

\ALTERNATE{
Use cyclicity: $\mathrm{tr}(P^{-1}AP)=\mathrm{tr}(APP^{-1})=\mathrm{tr}(A)$.
}

\VALIDATION{
Compute directly: diagonal sums equal the claimed values.
}

\INTUITION{
Similarity is change of basis; diagonal sum stays the same.
}

\CANONICAL{
\begin{bullets}
\item Trace is linear.
\item Trace is similarity invariant.
\end{bullets}
}

\ProblemPage{2}{Cyclicity and Rewriting Products}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Use cyclicity to simplify trace expressions and compute numerically.

\PROBLEM{
Show $\mathrm{tr}(ABAB)=\mathrm{tr}(BABA)$ and
$\mathrm{tr}(A^{-1}BA)=\mathrm{tr}(B)$ when $A$ is invertible. Then with
$A=\begin{bmatrix}1&2\\0&1\end{bmatrix}$, $B=\begin{bmatrix}0&1\\-1&0
\end{bmatrix}$, compute $\mathrm{tr}(ABAB)$.
}

\MODEL{
\[
\mathrm{tr}(XYZ)=\mathrm{tr}(ZXY),\quad \mathrm{tr}(A^{-1}BA)=\mathrm{tr}(B).
\]
}

\ASSUMPTIONS{
\begin{bullets}
\item Square matrices with defined products; $A$ invertible for second claim.
\end{bullets}
}

\varmapStart
\var{A,B}{Given $2\times 2$ matrices.}
\varmapEnd

\WHICHFORMULA{
Formula 2 (Cyclic Property).
}

\GOVERN{
\[
\mathrm{tr}(ABAB)=\mathrm{tr}(BABA),\quad
\mathrm{tr}(A^{-1}BA)=\mathrm{tr}(B).
\]
}

\INPUTS{$A=\begin{bmatrix}1&2\\0&1\end{bmatrix}$, $B=\begin{bmatrix}0&1\\-1&0\end{bmatrix}$.}

\DERIVATION{
\begin{align*}
&\mathrm{tr}(ABAB)=\mathrm{tr}(BABA)\ \text{(rotate by one factor).}\\
&\mathrm{tr}(A^{-1}BA)=\mathrm{tr}(BAA^{-1})=\mathrm{tr}(B).\\
\text{Numbers: }&AB=\begin{bmatrix}1&2\\0&1\end{bmatrix}
\begin{bmatrix}0&1\\-1&0\end{bmatrix}
=\begin{bmatrix}-2&1\\-1&0\end{bmatrix}.\\
&ABAB=(AB)(AB)=\begin{bmatrix}-2&1\\-1&0\end{bmatrix}^2
=\begin{bmatrix}3&-2\\2&-1\end{bmatrix}.\\
&\mathrm{tr}(ABAB)=3+(-1)=2.
\end{align*}
}

\RESULT{
$\mathrm{tr}(ABAB)=2$, and $\mathrm{tr}(A^{-1}BA)=\mathrm{tr}(B)=0$.
}

\UNITCHECK{
All matrices are $2\times 2$; traces are scalars.
}

\EDGECASES{
\begin{bullets}
\item If $A$ is singular, $\mathrm{tr}(A^{-1}BA)$ undefined.
\item For non-square products, trace undefined.
\end{bullets}
}

\ALTERNATE{
Compute $BABA$ and its trace to confirm it equals $2$.
}

\VALIDATION{
Direct multiplication confirms the values.
}

\INTUITION{
Rotating factors inside trace does not change the result.
}

\CANONICAL{
\begin{bullets}
\item Cyclicity enables simplification.
\item Similarity invariance is a special cyclic rotation case.
\end{bullets}
}

\ProblemPage{3}{Frobenius Inner Product as an Inner Product}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Show that $\langle A,B\rangle_F=\mathrm{tr}(A^\top B)$ is an inner product and
compute norms and angles.

\PROBLEM{
Prove positivity, symmetry, linearity in first argument of
$\langle A,B\rangle_F$. Compute $\|A\|_F$, $\|B\|_F$, and
$\cos\theta=\langle A,B\rangle_F/(\|A\|_F\|B\|_F)$ for
$A=\begin{bmatrix}1&2\\3&4\end{bmatrix}$, $B=\begin{bmatrix}4&3\\2&1\end{bmatrix}$.
}

\MODEL{
\[
\langle A,B\rangle_F=\mathrm{tr}(A^\top B),\quad \|A\|_F^2=\mathrm{tr}(A^\top A).
\]
}

\ASSUMPTIONS{
\begin{bullets}
\item Real matrices of same shape.
\end{bullets}
}

\varmapStart
\var{A,B}{Given $2\times 2$ real matrices.}
\var{\theta}{Angle induced by Frobenius inner product.}
\varmapEnd

\WHICHFORMULA{
Formula 3 (Frobenius Inner Product).
}

\GOVERN{
\[
\|A\|_F^2=\sum_{i,j} a_{ij}^2,\quad
\langle A,B\rangle_F=\sum_{i,j} a_{ij}b_{ij}.
\]
}

\INPUTS{$A=\begin{bmatrix}1&2\\3&4\end{bmatrix}$, $B=\begin{bmatrix}4&3\\2&1\end{bmatrix}$.}

\DERIVATION{
\begin{align*}
\text{Positivity: }&\|A\|_F^2=\sum_{i,j} a_{ij}^2\ge 0,\ \text{equality iff }A=0.\\
\text{Symmetry: }&\langle A,B\rangle_F=\mathrm{tr}(A^\top B)
=\mathrm{tr}(B^\top A)=\langle B,A\rangle_F.\\
\text{Linearity: }&\mathrm{tr}((\alpha A+\beta C)^\top B)=\alpha \mathrm{tr}(A^\top B)
+\beta \mathrm{tr}(C^\top B).\\
\text{Numbers: }&\|A\|_F^2=1^2+2^2+3^2+4^2=30,\ \|A\|_F=\sqrt{30}.\\
&\|B\|_F^2=4^2+3^2+2^2+1^2=30,\ \|B\|_F=\sqrt{30}.\\
&\langle A,B\rangle_F=1\cdot 4+2\cdot 3+3\cdot 2+4\cdot 1=20.\\
&\cos\theta=20/( \sqrt{30}\sqrt{30})=20/30=2/3.
\end{align*}
}

\RESULT{
$\|A\|_F=\|B\|_F=\sqrt{30}$, $\langle A,B\rangle_F=20$, $\cos\theta=2/3$.
}

\UNITCHECK{
Scalar outputs; norms are nonnegative reals; angle cosine in $[-1,1]$.
}

\EDGECASES{
\begin{bullets}
\item Zero matrix has zero norm; angle with zero undefined.
\end{bullets}
}

\ALTERNATE{
Derive using vectorization: $\mathrm{vec}(A)^\top \mathrm{vec}(B)$.
}

\VALIDATION{
Direct elementwise computation agrees with trace forms.
}

\INTUITION{
Flatten matrices to vectors and apply standard dot-product geometry.
}

\CANONICAL{
\begin{bullets}
\item Frobenius inner product is an inner product.
\item Induces the Frobenius norm and angle between matrices.
\end{bullets}
}

\ProblemPage{4}{Narrative: Alice and Rotations Preserve Total Variance}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Total variance equals $\mathrm{tr}(\Sigma)$ and is invariant under orthogonal
rotations.

\PROBLEM{
Alice rotates centered data $X\in\mathbb{R}^{n\times d}$ by orthogonal
$Q\in\mathbb{R}^{d\times d}$ to $XQ$. Show that total variance
$\frac{1}{n}\|X\|_F^2$ equals $\mathrm{tr}(\widehat{\Sigma})$ with
$\widehat{\Sigma}=\frac{1}{n}X^\top X$ and is unchanged by $Q$.
Compute numerically for $X=\begin{bmatrix}1&2\\-1&0\\0&-2\end{bmatrix}$ and
$Q=\frac{1}{\sqrt{2}}\begin{bmatrix}1&-1\\1&1\end{bmatrix}$.
}

\MODEL{
\[
\widehat{\Sigma}=\tfrac{1}{n}X^\top X,\quad \mathrm{tr}(\widehat{\Sigma})
=\tfrac{1}{n}\mathrm{tr}(X^\top X)=\tfrac{1}{n}\|X\|_F^2.
\]
}

\ASSUMPTIONS{
\begin{bullets}
\item $X$ is centered row-wise; $Q$ orthogonal: $Q^\top Q=I$.
\end{bullets}
}

\varmapStart
\var{X}{Data matrix $n\times d$ (rows observations).}
\var{Q}{Orthogonal rotation $d\times d$.}
\var{\widehat{\Sigma}}{Sample covariance without bias.}
\varmapEnd

\WHICHFORMULA{
Formula 3 (Frobenius Norm) and cyclicity (Formula 2).
}

\GOVERN{
\[
\mathrm{tr}(\tfrac{1}{n}(XQ)^\top (XQ))=\tfrac{1}{n}\mathrm{tr}(Q^\top X^\top X Q)
=\tfrac{1}{n}\mathrm{tr}(X^\top X).
\]
}

\INPUTS{$X=\begin{bmatrix}1&2\\-1&0\\0&-2\end{bmatrix}$, $Q=\frac{1}{\sqrt{2}}\begin{bmatrix}1&-1\\1&1\end{bmatrix}$.}

\DERIVATION{
\begin{align*}
&\tfrac{1}{n}\|X\|_F^2=\tfrac{1}{3}(1^2+2^2+(-1)^2+0^2+0^2+(-2)^2)
=\tfrac{1}{3}(1+4+1+0+0+4)=\tfrac{10}{3}.\\
&\widehat{\Sigma}=\tfrac{1}{3}X^\top X
\Rightarrow \mathrm{tr}(\widehat{\Sigma})=\tfrac{1}{3}\mathrm{tr}(X^\top X)
=\tfrac{10}{3}.\\
&XQ \text{ preserves } \|X\|_F:\ \|XQ\|_F=\|X\|_F \Rightarrow
\tfrac{1}{3}\|XQ\|_F^2=\tfrac{10}{3}.
\end{align*}
}

\RESULT{
Total variance $\tfrac{1}{n}\|X\|_F^2=\mathrm{tr}(\widehat{\Sigma})=\tfrac{10}{3}$
and is invariant under rotation $Q$.
}

\UNITCHECK{
Variance is scalar; $Q$ is $2\times 2$; $X$ is $3\times 2$; shapes align.
}

\EDGECASES{
\begin{bullets}
\item If $Q$ is not orthogonal, total variance may scale.
\item With biased covariance, scalar factor changes but trace identity holds.
\end{bullets}
}

\ALTERNATE{
Use eigen-decomposition: rotation preserves singular values and hence Frobenius
norm.
}

\VALIDATION{
Compute both $\tfrac{1}{n}\|X\|_F^2$ and $\mathrm{tr}(\tfrac{1}{n}(XQ)^\top (XQ))$
to confirm equality.
}

\INTUITION{
Rotations do not change lengths; total variance is average squared length.
}

\CANONICAL{
\begin{bullets}
\item $\mathrm{tr}(X^\top X)$ equals sum of squared entries.
\item Orthogonal similarity preserves Frobenius norm and trace.
\end{bullets}
}

\ProblemPage{5}{Narrative: Bob and a Weighted Energy}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Weighted energy $E(X)=\mathrm{tr}(X^\top W X)$ with $W\succeq 0$ is minimized
at $X=0$ and invariant under congruence with $W^{1/2}$ up to Frobenius norm.

\PROBLEM{
Bob measures energy $E(X)=\mathrm{tr}(X^\top W X)$, where $W\in\mathbb{R}^{n
\times n}$ is symmetric positive semidefinite. Show $E(X)\ge 0$, and $E(X)=
\|W^{1/2}X\|_F^2$. For $W=\begin{bmatrix}2&0\\0&1\end{bmatrix}$ and
$X=\begin{bmatrix}1&-1\\2&0\end{bmatrix}$, compute $E(X)$.
}

\MODEL{
\[
E(X)=\mathrm{tr}(X^\top W X)=\mathrm{tr}(X^\top W^{1/2} W^{1/2} X)
=\|W^{1/2}X\|_F^2.
\]
}

\ASSUMPTIONS{
\begin{bullets}
\item $W\succeq 0$ admits a symmetric square root $W^{1/2}\succeq 0$.
\end{bullets}
}

\varmapStart
\var{W}{Symmetric positive semidefinite weight.}
\var{X}{Matrix of variables.}
\var{E(X)}{Weighted energy scalar.}
\varmapEnd

\WHICHFORMULA{
Formula 3 (Frobenius Norm) and cyclicity.
}

\GOVERN{
\[
E(X)=\mathrm{tr}((W^{1/2}X)^\top (W^{1/2}X))=\|W^{1/2}X\|_F^2\ge 0.
\]
}

\INPUTS{$W=\begin{bmatrix}2&0\\0&1\end{bmatrix}$, $X=\begin{bmatrix}1&-1\\2&0\end{bmatrix}$.}

\DERIVATION{
\begin{align*}
&W^{1/2}=\begin{bmatrix}\sqrt{2}&0\\0&1\end{bmatrix},\quad
W^{1/2}X=\begin{bmatrix}\sqrt{2}&-\sqrt{2}\\2&0\end{bmatrix}.\\
&\|W^{1/2}X\|_F^2= (\sqrt{2})^2+(-\sqrt{2})^2+2^2+0^2=2+2+4=8.\\
&E(X)=\mathrm{tr}(X^\top W X)=8.
\end{align*}
}

\RESULT{
$E(X)=8\ge 0$, minimized at $X=0$.
}

\UNITCHECK{
Shapes: $W$ is $2\times 2$, $X$ is $2\times 2$; $E(X)$ scalar.
}

\EDGECASES{
\begin{bullets}
\item If $W\succ 0$, $E(X)=0$ iff $X=0$; if $W\succeq 0$, $E(X)=0$ may occur
for nonzero $X$ in $\ker W$.
\end{bullets}
}

\ALTERNATE{
Diagonalize $W=U\Lambda U^\top$ and use invariance of Frobenius norm under
orthogonal transforms.
}

\VALIDATION{
Compute both forms to ensure equality.
}

\INTUITION{
Weighting by $W$ stretches space; energy is squared length after stretching.
}

\CANONICAL{
\begin{bullets}
\item Quadratic forms equal squared Frobenius norms via trace.
\item Positivity follows from $W\succeq 0$.
\end{bullets}
}

\ProblemPage{6}{Expectation Puzzle: Die and Trace of Covariance}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
For a random vector $Z$, $\mathbb{E}\|Z-\mu\|^2=\mathrm{tr}(\Sigma)$ where
$\mu=\mathbb{E}[Z]$ and $\Sigma=\mathrm{Cov}(Z)$.

\PROBLEM{
Roll a fair die $D\in\{1,\dots,6\}$ and define $Z=(D,(-1)^D)^\top$. Compute
$\mu$, $\Sigma$, and $\mathrm{tr}(\Sigma)$. Verify $\mathbb{E}\|Z-\mu\|^2=
\mathrm{tr}(\Sigma)$.
}

\MODEL{
\[
\mu=\mathbb{E}[Z],\quad \Sigma=\mathbb{E}[(Z-\mu)(Z-\mu)^\top],\quad
\mathrm{tr}(\Sigma)=\mathbb{E}\|Z-\mu\|^2.
\]
}

\ASSUMPTIONS{
\begin{bullets}
\item Die is uniform on $\{1,\dots,6\}$.
\end{bullets}
}

\varmapStart
\var{D}{Uniform die outcome.}
\var{Z}{Random vector $(D,(-1)^D)^\top$.}
\var{\mu}{Mean vector.}
\var{\Sigma}{Covariance matrix.}
\varmapEnd

\WHICHFORMULA{
Formula 3 (Frobenius/trace) and identity $\mathrm{tr}(\Sigma)=\mathbb{E}\|Z-\mu\|^2$.
}

\GOVERN{
\[
\mathrm{tr}(\Sigma)=\mathbb{E}\,\mathrm{tr}((Z-\mu)(Z-\mu)^\top)
=\mathbb{E}\|Z-\mu\|^2.
\]
}

\INPUTS{Die distribution uniform; $Z=(D,(-1)^D)^\top$.}

\DERIVATION{
\begin{align*}
&\mu_1=\mathbb{E}[D]=(1+2+3+4+5+6)/6=3.5.\\
&\mu_2=\mathbb{E}[(-1)^D]=\tfrac{1}{6}( -1+1-1+1-1+1)=0.\\
&\mathbb{E}[D^2]=\tfrac{1}{6}(1+4+9+16+25+36)=\tfrac{91}{6}.\\
&\mathrm{Var}(D)=\mathbb{E}[D^2]-\mu_1^2=\tfrac{91}{6}-12.25
=\tfrac{35}{12}.\\
&\mathrm{Var}((-1)^D)=\mathbb{E}[1]-0=1.\\
&\Sigma=\begin{bmatrix}\tfrac{35}{12}&\mathrm{Cov}(D,(-1)^D)\\
\mathrm{Cov}(D,(-1)^D)&1\end{bmatrix}.\\
&\mathrm{Cov}(D,(-1)^D)=\mathbb{E}[D(-1)^D]-\mu_1\cdot 0
=\tfrac{1}{6}(-1\cdot1+2\cdot1-3\cdot1+4\cdot1-5\cdot1+6\cdot1)\\
&=\tfrac{1}{6}(-1+2-3+4-5+6)=\tfrac{3}{6}=\tfrac{1}{2}.\\
&\mathrm{tr}(\Sigma)=\tfrac{35}{12}+1=\tfrac{47}{12}.\\
&\mathbb{E}\|Z-\mu\|^2=\mathbb{E}[(D-3.5)^2+((-1)^D-0)^2]
=\tfrac{35}{12}+1=\tfrac{47}{12}.
\end{align*}
}

\RESULT{
$\mu=(3.5,0)^\top$, $\Sigma=\begin{bmatrix}35/12&1/2\\1/2&1\end{bmatrix}$,
$\mathrm{tr}(\Sigma)=47/12$, and $\mathbb{E}\|Z-\mu\|^2=47/12$.
}

\UNITCHECK{
Scalar variances; trace equals expected squared distance.
}

\EDGECASES{
\begin{bullets}
\item If $Z$ is degenerate, some variance terms vanish; identity still holds.
\end{bullets}
}

\ALTERNATE{
Use identity $\mathrm{tr}(\Sigma)=\sum_i \mathrm{Var}(Z_i)$ directly.
}

\VALIDATION{
Enumerate the six outcomes to confirm all expectations.
}

\INTUITION{
Total variance is the sum of coordinate variances; trace adds them up.
}

\CANONICAL{
\begin{bullets}
\item Trace of covariance equals expected squared deviation.
\end{bullets}
}

\ProblemPage{7}{Proof: Cyclicity Without Indices}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Prove $\mathrm{tr}(AB)=\mathrm{tr}(BA)$ using linear maps and bases.

\PROBLEM{
Let $A,B:\mathbb{F}^n\to\mathbb{F}^n$ be linear maps. Show that the trace of
composition is basis-independent and $\mathrm{tr}(AB)=\mathrm{tr}(BA)$.
}

\MODEL{
\[
\mathrm{tr}(T)=\sum_i \langle e_i, T e_i\rangle \text{ (independent of basis).}
\]
}

\ASSUMPTIONS{
\begin{bullets}
\item Finite-dimensional vector space; inner product for convenience.
\end{bullets}
}

\varmapStart
\var{A,B}{Linear operators on $\mathbb{F}^n$.}
\var{e_i}{Orthonormal basis vectors.}
\varmapEnd

\WHICHFORMULA{
Formula 2 (Cyclicity) and basis invariance (Formula 1).
}

\GOVERN{
\[
\mathrm{tr}(AB)=\sum_i \langle e_i, AB e_i\rangle
=\sum_i \langle B e_i, A^\top e_i\rangle.
\]
}

\INPUTS{Operators $A,B$ and an orthonormal basis $\{e_i\}$.}

\DERIVATION{
\begin{align*}
\mathrm{tr}(AB)&=\sum_i \langle e_i, AB e_i\rangle
=\sum_i \langle A^\top e_i, B e_i\rangle\\
&=\sum_i \langle e_i, BA e_i\rangle \quad \text{(since }
\sum_i \langle u, e_i\rangle \langle e_i, v\rangle=\langle u,v\rangle).\\
&=\mathrm{tr}(BA).
\end{align*}
}

\RESULT{
$\mathrm{tr}(AB)=\mathrm{tr}(BA)$.
}

\UNITCHECK{
Both sides are scalars; basis choice does not affect value.
}

\EDGECASES{
\begin{bullets}
\item Infinite-dimensional settings require trace-class to justify sums.
\end{bullets}
}

\ALTERNATE{
Matrix index proof: $\sum_{i,j} a_{ij} b_{ji}=\sum_{i,j} b_{ij} a_{ji}$.
}

\VALIDATION{
Check on diagonalizable operators where $A$ or $B$ are diagonal to see equality.
}

\INTUITION{
Composition in one order has same total diagonal contribution as in the other
when summed over a full basis.
}

\CANONICAL{
\begin{bullets}
\item Trace is cyclic and basis-independent.
\end{bullets}
}

\ProblemPage{8}{Proof: Vanishing Power Sums Imply Nilpotent}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
If $\mathrm{tr}(A^k)=0$ for $k=1,\dots,n$ for $A\in\mathbb{C}^{n\times n}$,
then $A$ is nilpotent.

\PROBLEM{
Prove that $\mathrm{tr}(A^k)=0$ for $k=1,\dots,n$ implies all eigenvalues
vanish and thus $A$ is nilpotent.
}

\MODEL{
\[
\mathrm{tr}(A^k)=\sum_{i=1}^n \lambda_i^k=0\quad (k=1,\dots,n).
\]
}

\ASSUMPTIONS{
\begin{bullets}
\item Work over $\mathbb{C}$; eigenvalues counted with multiplicity.
\end{bullets}
}

\varmapStart
\var{A}{Complex square matrix.}
\var{\lambda_i}{Eigenvalues of $A$.}
\varmapEnd

\WHICHFORMULA{
Formula 4: $\mathrm{tr}(A^k)=\sum_i \lambda_i^k$.
}

\GOVERN{
\[
\sum_{i=1}^n \lambda_i^k=0\quad \text{for }k=1,\dots,n.
\]
}

\INPUTS{$\mathrm{tr}(A^k)=0$ for $k=1,\dots,n$.}

\DERIVATION{
\begin{align*}
&\text{The power sums }s_k=\sum_i \lambda_i^k \text{ vanish for }k=1,\dots,n.\\
&\text{By Newton identities, elementary symmetric polynomials }e_k\\
&\text{(coefficients of characteristic polynomial) are polynomials in }s_1,
\dots,s_k.\\
&\text{Since }s_k=0 \text{ for }k\le n, all }e_k=0\ (k=1,\dots,n).\\
&\text{Thus characteristic polynomial is }p(\lambda)=\lambda^n.\\
&\Rightarrow \text{ all eigenvalues }\lambda_i=0,\ \text{so }A \text{ is nilpotent.}
\end{align*}
}

\RESULT{
$A$ is nilpotent; in particular, $A^n=0$.
}

\UNITCHECK{
Spectral statements are scalar equalities; matrix is finite-dimensional.
}

\EDGECASES{
\begin{bullets}
\item Over $\mathbb{R}$, consider complex eigenvalues in conjugate pairs; same
conclusion by complexification.
\end{bullets}
}

\ALTERNATE{
Use Jordan form: if any Jordan block has nonzero eigenvalue, some $s_k$ would
be nonzero.
}

\VALIDATION{
Simple example: strictly upper triangular matrices have all traces $\mathrm{tr}
(A^k)=0$ and are nilpotent.
}

\INTUITION{
If all power sums of eigenvalues vanish, the only multiset satisfying that is
all zeros.
}

\CANONICAL{
\begin{bullets}
\item Power-sum traces determine the characteristic polynomial coefficients.
\end{bullets}
}

\ProblemPage{9}{Combo: Least Squares via Trace}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Least squares objective $\|AX-B\|_F^2$ has gradient $2A^\top(AX-B)$.

\PROBLEM{
Minimize $f(X)=\|AX-B\|_F^2$ with respect to $X\in\mathbb{R}^{n\times m}$.
Derive the gradient using trace calculus and find the normal equation. For
$A=\begin{bmatrix}1&0\\0&2\\0&0\end{bmatrix}$, $B=\begin{bmatrix}1&2\\3&4\\5&6
\end{bmatrix}$, find the minimizer $X^\star$.
}

\MODEL{
\[
f(X)=\mathrm{tr}((AX-B)^\top(AX-B)).
\]
}

\ASSUMPTIONS{
\begin{bullets}
\item $A^\top A$ is invertible on the relevant subspace; use pseudoinverse if
not full rank.
\end{bullets}
}

\varmapStart
\var{A}{Design matrix.}
\var{B}{Target matrix.}
\var{X}{Parameter matrix.}
\varmapEnd

\WHICHFORMULA{
Formula 5 (Trace Calculus).
}

\GOVERN{
\[
\nabla_X f=2A^\top(AX-B),\quad A^\top A X=A^\top B.
\]
}

\INPUTS{$A=\begin{bmatrix}1&0\\0&2\\0&0\end{bmatrix}$, $B=\begin{bmatrix}1&2\\3&4\\5&6\end{bmatrix}$.}

\DERIVATION{
\begin{align*}
&f(X)=\mathrm{tr}(X^\top A^\top A X)-2\mathrm{tr}(B^\top A X)+\mathrm{tr}(B^\top B).\\
&\nabla_X f=(A^\top A+(A^\top A)^\top)X-2A^\top B=2A^\top A X-2A^\top B.\\
&\text{Set } \nabla_X f=0 \Rightarrow A^\top A X=A^\top B.\\
&A^\top A=\begin{bmatrix}1&0\\0&4\end{bmatrix},\ A^\top B=\begin{bmatrix}1&2\\6&8\end{bmatrix}.\\
&X^\star=(A^\top A)^{-1}A^\top B=\begin{bmatrix}1&2\\\tfrac{3}{2}&2\end{bmatrix}.
\end{align*}
}

\RESULT{
$X^\star=\begin{bmatrix}1&2\\1.5&2\end{bmatrix}$ minimizes $f$.
}

\UNITCHECK{
Shapes: $A(3\times 2)$, $X(2\times 2)$, $B(3\times 2)$. Gradient shape matches
$X$.
}

\EDGECASES{
\begin{bullets}
\item If a column of $A$ is zero, corresponding row of $X$ is unconstrained;
use pseudoinverse.
\end{bullets}
}

\ALTERNATE{
Vectorize and solve $(I\otimes A) \mathrm{vec}(X)=\mathrm{vec}(B)$ in least
squares sense.
}

\VALIDATION{
Plug $X^\star$ to check $A^\top(AX^\star-B)=0$.
}

\INTUITION{
Trace turns matrix least squares into familiar quadratic form with easy
derivative.
}

\CANONICAL{
\begin{bullets}
\item Gradient by trace calculus.
\item Normal equations in matrix form.
\end{bullets}
}

\ProblemPage{10}{Combo: Singular Values and Trace}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
$\|A\|_F^2=\sum_{i,j} a_{ij}^2=\sum_{k} \sigma_k^2=\mathrm{tr}(A^\top A)$.

\PROBLEM{
Using SVD $A=U\Sigma V^\top$, show $\mathrm{tr}(A^\top A)=\sum_k \sigma_k^2$.
Compute for $A=\begin{bmatrix}1&0\\0&2\\0&0\end{bmatrix}$.
}

\MODEL{
\[
A^\top A=V \Sigma^\top \Sigma V^\top,\quad \mathrm{tr}(A^\top A)
=\mathrm{tr}(\Sigma^\top \Sigma)=\sum_k \sigma_k^2.
\]
}

\ASSUMPTIONS{
\begin{bullets}
\item SVD exists for real matrices.
\end{bullets}
}

\varmapStart
\var{A}{Real matrix.}
\var{\sigma_k}{Singular values of $A$.}
\varmapEnd

\WHICHFORMULA{
Formula 3 (Frobenius Norm) and cyclicity invariance of trace.
}

\GOVERN{
\[
\mathrm{tr}(A^\top A)=\mathrm{tr}(V \Sigma^\top \Sigma V^\top)
=\mathrm{tr}(\Sigma^\top \Sigma).
\]
}

\INPUTS{$A=\begin{bmatrix}1&0\\0&2\\0&0\end{bmatrix}$.}

\DERIVATION{
\begin{align*}
&A^\top A=\begin{bmatrix}1&0\\0&4\end{bmatrix}.\\
&\mathrm{tr}(A^\top A)=1+4=5.\\
&\sigma_1=2,\ \sigma_2=1 \Rightarrow \sum_k \sigma_k^2=4+1=5.
\end{align*}
}

\RESULT{
$\mathrm{tr}(A^\top A)=5=\sum_k \sigma_k^2=\|A\|_F^2$.
}

\UNITCHECK{
Scalar equality; both sides nonnegative.
}

\EDGECASES{
\begin{bullets}
\item Zero singular values contribute nothing; identity still holds.
\end{bullets}
}

\ALTERNATE{
Compute $\|A\|_F^2=\sum_{i,j} a_{ij}^2$ directly.
}

\VALIDATION{
Numerical SVD confirms singular values and equality.
}

\INTUITION{
Orthogonal factors in SVD do not change Frobenius norm; only singular values
matter.
}

\CANONICAL{
\begin{bullets}
\item Frobenius norm equals sum of squared singular values equals trace.
\end{bullets}
}

\clearpage
\section{Coding Demonstrations}

\CodeDemoPage{Cyclicity: tr(AB)=tr(BA) and Triple Cyclic Rotations}
\PROBLEM{
Numerically verify $\mathrm{tr}(AB)=\mathrm{tr}(BA)$ and
$\mathrm{tr}(ABC)=\mathrm{tr}(BCA)=\mathrm{tr}(CAB)$ for deterministic matrices.
}

\API{
\begin{bullets}
\item \inlinecode{def read_input(s) -> dict} — parse n and entries.
\item \inlinecode{def solve_case(obj) -> tuple} — compute traces.
\item \inlinecode{def validate() -> None} — assert equalities.
\item \inlinecode{def main() -> None} — run validation and print.
\end{bullets}
}

\INPUTS{
Square matrices $A,B,C$ as flat lists; default deterministic matrices if none.
}

\OUTPUTS{
Traces of $AB$, $BA$, and the three cyclic rotations of $ABC$.
}

\FORMULA{
\[
\mathrm{tr}(AB)=\mathrm{tr}(BA),\quad
\mathrm{tr}(ABC)=\mathrm{tr}(BCA)=\mathrm{tr}(CAB).
\]
}

\textbf{SOLUTION A — From Scratch (Mathematically Explicit Implementation)}
\begin{codepy}
# Deterministic, pure-Python matrix utilities
def mat(n, data):
    return [list(data[i*n:(i+1)*n]) for i in range(n)]

def eye(n):
    return [[1 if i==j else 0 for j in range(n)] for i in range(n)]

def mm(A, B):
    n = len(A)
    C = [[0]*n for _ in range(n)]
    for i in range(n):
        for k in range(n):
            s = 0
            for j in range(n):
                s += A[i][j]*B[j][k]
            C[i][k] = s
    return C

def tr(A):
    return sum(A[i][i] for i in range(len(A)))

def read_input(s):
    return {}

def solve_case(_obj=None):
    n = 3
    A = mat(n, [1,2,3, 0,1,4, 0,0,1])
    B = mat(n, [2,0,1, 1,3,0, 0,2,4])
    C = mat(n, [0,1,0, 2,0,1, 3,0,0])
    AB = mm(A, B); BA = mm(B, A)
    ABC = mm(AB, C); BCA = mm(mm(B, C), A); CAB = mm(mm(C, A), B)
    return tr(AB), tr(BA), tr(ABC), tr(BCA), tr(CAB)

def validate():
    tAB, tBA, tABC, tBCA, tCAB = solve_case({})
    assert abs(tAB - tBA) < 1e-9
    assert abs(tABC - tBCA) < 1e-9
    assert abs(tBCA - tCAB) < 1e-9

def main():
    validate()
    print("OK")

if __name__ == "__main__":
    main()
\end{codepy}

\textbf{SOLUTION B — Library-Based (Validated Computational Shortcut)}
\begin{codepy}
import numpy as np

def read_input(s):
    return {}

def solve_case(_obj=None):
    A = np.array([[1,2,3],[0,1,4],[0,0,1]], dtype=float)
    B = np.array([[2,0,1],[1,3,0],[0,2,4]], dtype=float)
    C = np.array([[0,1,0],[2,0,1],[3,0,0]], dtype=float)
    AB = A @ B; BA = B @ A
    ABC = AB @ C; BCA = (B @ C) @ A; CAB = (C @ A) @ B
    return float(np.trace(AB)), float(np.trace(BA)), \
           float(np.trace(ABC)), float(np.trace(BCA)), \
           float(np.trace(CAB))

def validate():
    tAB, tBA, tABC, tBCA, tCAB = solve_case({})
    assert abs(tAB - tBA) < 1e-9
    assert abs(tABC - tBCA) < 1e-9
    assert abs(tBCA - tCAB) < 1e-9

def main():
    validate()
    print("OK")

if __name__ == "__main__":
    main()
\end{codepy}

\COMPLEXITY{
Time $\mathcal{O}(n^3)$ for multiplications; space $\mathcal{O}(n^2)$.
}

\FAILMODES{
\begin{bullets}
\item Non-square matrices: disallow or pad; here we fix square sizes.
\item Overflow for large entries: use float64 and moderate values.
\end{bullets}
}

\STABILITY{
\begin{bullets}
\item Floating error negligible for small integers.
\end{bullets}
}

\VALIDATION{
\begin{bullets}
\item Assertions compare traces pairwise.
\item Deterministic data ensures reproducibility.
\end{bullets}
}

\RESULT{
Both implementations print OK, confirming cyclic equalities.
}

\EXPLANATION{
Cyclicity follows from index relabeling; code emulates algebraic rotations.
}

\CodeDemoPage{Frobenius Inner Product and Norm}
\PROBLEM{
Verify $\mathrm{tr}(A^\top B)=\sum a_{ij}b_{ij}$ and
$\|A\|_F^2=\mathrm{tr}(A^\top A)$ with deterministic matrices.
}

\API{
\begin{bullets}
\item \inlinecode{def read_input(s) -> dict}
\item \inlinecode{def solve_case(obj) -> tuple}
\item \inlinecode{def validate() -> None}
\item \inlinecode{def main() -> None}
\end{bullets}
}

\INPUTS{
Two same-shape matrices $A,B$ constructed deterministically.
}

\OUTPUTS{
Scalar inner product and norms by both definitions.
}

\FORMULA{
\[
\langle A,B\rangle_F=\mathrm{tr}(A^\top B)
=\sum_{i,j} a_{ij}b_{ij},\quad
\|A\|_F^2=\mathrm{tr}(A^\top A).
\]
}

\textbf{SOLUTION A — From Scratch (Mathematically Explicit Implementation)}
\begin{codepy}
def mat(m, n, data):
    return [list(data[i*n:(i+1)*n]) for i in range(m)]

def tr(A):
    return sum(A[i][i] for i in range(len(A)))

def mt(A):
    m, n = len(A), len(A[0])
    return [[A[i][j] for i in range(m)] for j in range(n)]

def mm(A, B):
    m, k = len(A), len(A[0])
    k2, n = len(B), len(B[0])
    assert k == k2
    C = [[0]*n for _ in range(m)]
    for i in range(m):
        for j in range(n):
            s = 0
            for r in range(k):
                s += A[i][r]*B[r][j]
            C[i][j] = s
    return C

def frob_ip(A, B):
    s = 0
    for i in range(len(A)):
        for j in range(len(A[0])):
            s += A[i][j]*B[i][j]
    return s

def validate():
    A = mat(2, 3, [1,2,3, 4,5,6])
    B = mat(2, 3, [6,5,4, 3,2,1])
    left = tr(mm(mt(A), B))
    right = frob_ip(A, B)
    nA = tr(mm(mt(A), A))
    assert abs(left - right) < 1e-9
    assert nA == 91

def main():
    validate()
    print("OK")

if __name__ == "__main__":
    main()
\end{codepy}

\textbf{SOLUTION B — Library-Based (Validated Computational Shortcut)}
\begin{codepy}
import numpy as np

def read_input(s):
    return {}

def solve_case(_obj=None):
    A = np.array([[1,2,3],[4,5,6]], dtype=float)
    B = np.array([[6,5,4],[3,2,1]], dtype=float)
    ip1 = float(np.trace(A.T @ B))
    ip2 = float(np.sum(A*B))
    nA = float(np.trace(A.T @ A))
    return ip1, ip2, nA

def validate():
    ip1, ip2, nA = solve_case({})
    assert abs(ip1 - ip2) < 1e-9
    assert abs(nA - 91.0) < 1e-9

def main():
    validate()
    print("OK")

if __name__ == "__main__":
    main()
\end{codepy}

\COMPLEXITY{
Time $\mathcal{O}(mn)$ for inner product; space $\mathcal{O}(1)$ extra.
}

\FAILMODES{
\begin{bullets}
\item Shape mismatch: assert same dimensions.
\item Non-finite values: avoid NaN/Inf in inputs.
\end{bullets}
}

\STABILITY{
\begin{bullets}
\item Summation error minimal for small matrices; Kahan could help for large.
\end{bullets}
}

\VALIDATION{
\begin{bullets}
\item Compare trace form to elementwise sum.
\item Check known norm value 91.
\end{bullets}
}

\RESULT{
Implementations agree and print OK.
}

\EXPLANATION{
Trace compresses the elementwise dot product into a coordinate-free form.
}

\CodeDemoPage{Similarity Invariance: tr(P^{-1}AP)=tr(A)}
\PROBLEM{
Verify similarity invariance deterministically by constructing invertible $P$
and computing traces.
}

\API{
\begin{bullets}
\item \inlinecode{def read_input(s) -> dict}
\item \inlinecode{def solve_case(obj) -> tuple}
\item \inlinecode{def validate() -> None}
\item \inlinecode{def main() -> None}
\end{bullets}
}

\INPUTS{
$A$ upper triangular with ones on diagonal; $P$ unit upper-triangular.
}

\OUTPUTS{
Pair of traces $\mathrm{tr}(A)$ and $\mathrm{tr}(P^{-1}AP)$.
}

\FORMULA{
\[
\mathrm{tr}(P^{-1}AP)=\mathrm{tr}(A).
\]
}

\textbf{SOLUTION A — From Scratch (Mathematically Explicit Implementation)}
\begin{codepy}
def mat(n, data):
    return [list(data[i*n:(i+1)*n]) for i in range(n)]

def tr(A):
    return sum(A[i][i] for i in range(len(A)))

def inv_ut(P):
    n = len(P)
    Q = [[0]*n for _ in range(n)]
    for i in range(n):
        Q[i][i] = 1.0/P[i][i]
    for i in range(n-1, -1, -1):
        for j in range(i+1, n):
            s = 0.0
            for k in range(i+1, j+1):
                s += P[i][k]*Q[k][j]
            Q[i][j] = -s/P[i][i]
    return Q

def mm(A, B):
    n = len(A)
    C = [[0]*n for _ in range(n)]
    for i in range(n):
        for j in range(n):
            s = 0.0
            for k in range(n):
                s += A[i][k]*B[k][j]
            C[i][j] = s
    return C

def validate():
    A = mat(3, [2,1,0, 0,3,4, 0,0,5])
    P = mat(3, [1,2,1, 0,1,3, 0,0,1])
    Pinv = inv_ut(P)
    T = mm(mm(Pinv, A), P)
    assert abs(tr(T) - tr(A)) < 1e-9

def main():
    validate()
    print("OK")

if __name__ == "__main__":
    main()
\end{codepy}

\textbf{SOLUTION B — Library-Based (Validated Computational Shortcut)}
\begin{codepy}
import numpy as np

def read_input(s):
    return {}

def solve_case(_obj=None):
    A = np.array([[2,1,0],[0,3,4],[0,0,5]], dtype=float)
    P = np.array([[1,2,1],[0,1,3],[0,0,1]], dtype=float)
    T = np.linalg.inv(P) @ A @ P
    return float(np.trace(A)), float(np.trace(T))

def validate():
    tA, tT = solve_case({})
    assert abs(tA - tT) < 1e-9

def main():
    validate()
    print("OK")

if __name__ == "__main__":
    main()
\end{codepy}

\COMPLEXITY{
Time $\mathcal{O}(n^3)$; space $\mathcal{O}(n^2)$.
}

\FAILMODES{
\begin{bullets}
\item $P$ singular: inversion undefined; choose unit upper-triangular.
\end{bullets}
}

\STABILITY{
\begin{bullets}
\item Triangular inversion is stable for well-conditioned diagonals.
\end{bullets}
}

\VALIDATION{
\begin{bullets}
\item Compare traces before and after similarity transform.
\end{bullets}
}

\RESULT{
Traces match to numerical precision.
}

\EXPLANATION{
Similarity is a change of basis; trace is invariant by cyclicity.
}

\clearpage
\section{Applied Domains — Detailed End-to-End Scenarios}

\DomainPage{Machine Learning}
\SCENARIO{
Ridge regression loss written with trace: $L(\beta)=\|y-X\beta\|_2^2+\lambda
\|\beta\|_2^2=\mathrm{tr}((y-X\beta)(y-X\beta)^\top)+\lambda\,\mathrm{tr}
(\beta^\top\beta)$.
}
\ASSUMPTIONS{
\begin{bullets}
\item $X\in\mathbb{R}^{n\times d}$ full column rank; $\lambda\ge 0$.
\end{bullets}
}
\WHICHFORMULA{
Trace calculus (Formula 5) and Frobenius inner product (Formula 3).
}
\varmapStart
\var{X}{Design matrix $(n,d)$ including bias if used.}
\var{y}{Target vector $(n,1)$.}
\var{\beta}{Coefficient vector $(d,1)$.}
\var{\lambda}{Nonnegative regularization weight.}
\varmapEnd
\PIPELINE{
\begin{bullets}
\item Express loss with trace; derive gradient.
\item Solve $(X^\top X+\lambda I)\beta=X^\top y$.
\item Verify numerically.
\end{bullets}
}
\textbf{Implementation (From Scratch)}
\begin{codepy}
import numpy as np

def generate(n=50, d=3, lam=0.5):
    np.random.seed(0)
    X = np.random.randn(n, d)
    beta_true = np.array([[1.0],[2.0],[ -1.0]])
    y = X @ beta_true + 0.1*np.random.randn(n,1)
    return X, y, lam

def ridge_closed(X, y, lam):
    n, d = X.shape
    A = X.T @ X + lam*np.eye(d)
    b = X.T @ y
    return np.linalg.solve(A, b)

def loss_trace(X, y, beta, lam):
    r = y - X @ beta
    return float(np.trace(r.T @ r) + lam*np.trace(beta.T @ beta))

def main():
    X, y, lam = generate()
    beta = ridge_closed(X, y, lam)
    L = loss_trace(X, y, beta, lam)
    print("loss", round(L, 6))

if __name__ == "__main__":
    main()
\end{codepy}
\textbf{Implementation (Library Version)}
\begin{codepy}
import numpy as np
from sklearn.linear_model import Ridge

def main():
    np.random.seed(0)
    n, d = 50, 3
    X = np.random.randn(n, d)
    beta_true = np.array([[1.0],[2.0],[-1.0]])
    y = X @ beta_true + 0.1*np.random.randn(n,1)
    model = Ridge(alpha=0.5, fit_intercept=False, solver='auto')
    model.fit(X, y.ravel())
    r = y - X @ model.coef_.reshape(-1,1)
    L = float(np.trace(r.T @ r) + 0.5*np.trace(
        model.coef_.reshape(-1,1).T @ model.coef_.reshape(-1,1)))
    print("loss", round(L, 6))

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{
Report the ridge loss; both implementations should produce close values.
}
\INTERPRET{
Trace form compactly encodes squared errors and regularization energy.
}
\NEXTSTEPS{
Extend to matrix $\beta$ (multi-output) using $\mathrm{tr}((Y-XB)^\top(Y-XB))$.
}

\DomainPage{Quantitative Finance}
\SCENARIO{
Portfolio variance as a trace: $\sigma_p^2=w^\top \Sigma w=\mathrm{tr}(\Sigma
ww^\top)$; compute variance and risk contributions.
}
\ASSUMPTIONS{
\begin{bullets}
\item Returns have covariance $\Sigma$; weights sum to one.
\end{bullets}
}
\WHICHFORMULA{
Cyclicity and Frobenius inner product identify $w^\top \Sigma w=\mathrm{tr}
(\Sigma w w^\top)$.
}
\varmapStart
\var{w}{Portfolio weights $(d,1)$.}
\var{\Sigma}{Covariance matrix $(d,d)$.}
\var{\sigma_p^2}{Portfolio variance scalar.}
\varmapEnd
\PIPELINE{
\begin{bullets}
\item Simulate returns, estimate $\Sigma$.
\item Compute $\sigma_p^2$ and trace form.
\item Verify equality and contributions $(\Sigma w)\odot w$.
\end{bullets}
}
\textbf{Implementation (Full Pipeline)}
\begin{codepy}
import numpy as np

def simulate(n=400, d=3, seed=0):
    np.random.seed(seed)
    A = np.array([[0.1, 0.02, 0.01],
                  [0.00, 0.08, 0.03],
                  [0.00, 0.00, 0.12]])
    R = np.random.randn(n, d) @ (A + A.T)
    return R

def main():
    R = simulate()
    Sigma = np.cov(R, rowvar=False, bias=True)
    w = np.array([[0.5],[0.3],[0.2]])
    var1 = float(w.T @ Sigma @ w)
    var2 = float(np.trace(Sigma @ (w @ w.T)))
    mc = (Sigma @ w).flatten()
    contrib = mc * w.flatten()
    print("var", round(var1, 8), "eq", round(var2, 8))
    print("contrib", np.round(contrib, 8))

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{
Portfolio variance via quadratic and trace forms; risk contributions printed.
}
\INTERPRET{
Trace reveals variance as inner product between $\Sigma$ and $w w^\top$.
}
\NEXTSTEPS{
Optimize $w$ under constraints; use trace to express objectives and penalties.
}

\DomainPage{Deep Learning}
\SCENARIO{
Matrix MSE as a trace and its gradient: minimize $L(W)=\|WX-Y\|_F^2$; verify
$\nabla_W L=2(WXX^\top - YX^\top)$.
}
\ASSUMPTIONS{
\begin{bullets}
\item Batch inputs $X$, targets $Y$; no bias for simplicity.
\end{bullets}
}
\WHICHFORMULA{
Formula 5: $\nabla_W \mathrm{tr}((WX-Y)^\top(WX-Y))=2(WXX^\top-YX^\top)$.
}
\PIPELINE{
\begin{bullets}
\item Build deterministic $X,Y$.
\item Compute analytic gradient and finite-difference check.
\end{bullets}
}
\textbf{Implementation (End-to-End)}
\begin{codepy}
import numpy as np

def fd_grad(W, X, Y, eps=1e-6):
    G = np.zeros_like(W)
    for i in range(W.shape[0]):
        for j in range(W.shape[1]):
            E = np.zeros_like(W); E[i, j] = 1.0
            Lp = np.trace(((W+eps*E)@X - Y).T @ ((W+eps*E)@X - Y))
            Lm = np.trace(((W-eps*E)@X - Y).T @ ((W-eps*E)@X - Y))
            G[i, j] = (Lp - Lm)/(2*eps)
    return G

def main():
    np.random.seed(0)
    W = np.array([[0.1, -0.2],[0.3, 0.4]], dtype=float)
    X = np.array([[1.0, 0.5, -1.0],[0.0, -0.5, 2.0]], dtype=float)
    Y = np.array([[1.0, -1.0, 0.0],[0.0, 1.0, 2.0]], dtype=float)
    G_anal = 2*(W @ X @ X.T - Y @ X.T)
    G_num = fd_grad(W, X, Y)
    err = np.max(np.abs(G_anal - G_num))
    print("max_err", err)

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{
Report max absolute error between analytic and numeric gradients.
}
\INTERPRET{
Trace calculus yields compact gradients that match finite differences.
}
\NEXTSTEPS{
Add bias and regularization $\lambda\,\mathrm{tr}(W^\top W)$.
}

\DomainPage{Kaggle / Data Analytics}
\SCENARIO{
EDA: total variance equals trace of covariance; standardize features and verify
that covariance trace equals number of features after standardization.
}
\ASSUMPTIONS{
\begin{bullets}
\item Numeric features only; mean-centered for covariance estimation.
\end{bullets}
}
\WHICHFORMULA{
$\mathrm{tr}(\Sigma)=\sum_i \mathrm{Var}(X_i)$ and invariance under orthogonal
transforms.
}
\PIPELINE{
\begin{bullets}
\item Generate synthetic correlated features.
\item Compute covariance, its trace, and after standardization.
\end{bullets}
}
\textbf{Implementation (Complete EDA Pipeline)}
\begin{codepy}
import numpy as np

def create_df(n=300, seed=0):
    np.random.seed(seed)
    Z = np.random.randn(n, 3)
    L = np.array([[1.0, 0.0, 0.0],
                  [0.7, 0.7, 0.0],
                  [0.3, -0.2, 0.9]])
    X = Z @ L.T + np.array([1.0, -2.0, 0.5])
    return X

def standardize(X):
    m = X.mean(axis=0, keepdims=True)
    s = X.std(axis=0, ddof=0, keepdims=True)
    return (X - m)/s

def main():
    X = create_df()
    Xc = X - X.mean(axis=0, keepdims=True)
    Sigma = (Xc.T @ Xc)/X.shape[0]
    tr1 = float(np.trace(Sigma))
    Xs = standardize(X)
    Xsc = Xs - Xs.mean(axis=0, keepdims=True)
    Sigmas = (Xsc.T @ Xsc)/X.shape[0]
    tr2 = float(np.trace(Sigmas))
    print("trace before", round(tr1, 6), "after", round(tr2, 6))

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{
Trace of covariance before and after standardization; after equals number of
features (3) up to rounding.
}
\INTERPRET{
Standardization sets each variance to one; trace sums them to feature count.
}
\NEXTSTEPS{
Apply PCA; the sum of eigenvalues equals the same trace (total variance).
}

\end{document}
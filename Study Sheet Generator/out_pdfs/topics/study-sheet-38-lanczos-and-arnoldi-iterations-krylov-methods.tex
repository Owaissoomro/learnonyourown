% !TeX program = xelatex
\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{microtype,setspace,amsmath,amssymb,mathtools,amsthm,unicode-math}
\setstretch{1.05}
\setmainfont{Latin Modern Roman}
\setmonofont{Latin Modern Mono}
\setmathfont{Latin Modern Math}

% --- Overflow / line-break safety ---
\allowdisplaybreaks[4]
\setlength{\jot}{7pt}
\setlength{\emergencystretch}{8em}
\sloppy

\usepackage{xcolor,fancyhdr,enumitem,inconsolata,listings}
\pagestyle{fancy}\fancyhf{}\lhead{\nouppercase{\leftmark}}\rhead{\thepage}
\setlength{\headheight}{26pt}
\setlength{\parindent}{0pt}
\setlength{\parskip}{8pt plus 2pt minus 1pt}
\raggedbottom

% --- Breakable math helpers (use these in the body) ---
\newenvironment{BreakableEquation}{\begin{equation}\begin{aligned}}{\end{aligned}\end{equation}}
\newenvironment{BreakableEquation*}{\begin{equation*}\begin{aligned}}{\end{aligned}\end{equation*}}
\newenvironment{tightalign}{\begingroup\small\allowdisplaybreaks\begin{align}}{\end{align}\endgroup}

% ---------- Safety shims ----------
% Robust minted → listings shim (no shell-escape; supports [opts]{language})
\providecommand{\enumlistm}{enumitem}
\newenvironment{minted}[2][]{%
  \lstset{style=code,language=#2,#1}\begin{lstlisting}%
}{\end{lstlisting}}

% Fallback for \inputminted (ignore file; keep build unbroken)
\newcommand{\inputminted}[3][]{\begin{lstlisting}\end{lstlisting}}

% ---------- Bulleted lines (no tables) ----------
\newlist{bullets}{itemize}{1}
\setlist[bullets]{label=--,leftmargin=1.6em,itemsep=2pt,topsep=4pt}

% ---------- Variable mapping (lines, no tables) ----------
\newcommand{\varmapStart}{\textbf{VARIABLE MAPPING:}\par\begin{bullets}}
\newcommand{\var}[2]{\item $#1$ — #2}
\newcommand{\varmapEnd}{\end{bullets}}

% ---------- Glossary item with ELI5 and Pitfall/Example ----------
\newcommand{\glossx}[6]{%
  \textbf{#1}\par
  \begin{bullets}
    \item \textbf{What:} #2
    \item \textbf{Why:} #3
    \item \textbf{How:} #4
    \item \textbf{ELI5:} #5
    \item \textbf{Pitfall/Example:} #6
  \end{bullets}
}

% ---------- Theorem structures ----------
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{claim}[theorem]{Claim}
% ---------- Code blocks ----------
\lstdefinestyle{code}{
  basicstyle=\ttfamily\footnotesize,
  backgroundcolor=\color{black!02},
  frame=single,
  numbers=left, numberstyle=\tiny, numbersep=8pt,
  breaklines=true, breakatwhitespace=true,
  tabsize=4, showstringspaces=false,
  upquote=true, keepspaces=true, columns=fullflexible,
  % Unicode safety: map common symbols so listings never chokes
  literate=
    {–}{{-}}1
    {—}{{-}}1
    {…}{{...}}1
    {≤}{{\ensuremath{\le}}}1
    {≥}{{\ensuremath{\ge}}}1
    {≠}{{\ensuremath{\ne}}}1
    {≈}{{\ensuremath{\approx}}}1
    {±}{{\ensuremath{\pm}}}1
    {→}{{\ensuremath{\to}}}1
    {←}{{\ensuremath{\leftarrow}}}1
    {∞}{{\ensuremath{\infty}}}1
    {√}{{\ensuremath{\sqrt{\ }}}}1
    {×}{{\ensuremath{\times}}}1
    {÷}{{\ensuremath{\div}}}1
}

% Main code environment for all Python blocks
\lstnewenvironment{codepy}[1][]%
  {\lstset{style=code,language=Python,#1}}%
  {}

% Inline code; change delimiters if your snippet contains '!'
\newcommand{\inlinecode}[1]{\lstinline[style=code]!#1!}

% ---------- Line-label macros ----------
\newcommand{\LF}[2]{\par\noindent\textbf{#1:}~#2\par}
\newcommand{\WHAT}[1]{\LF{WHAT}{#1}}
\newcommand{\WHY}[1]{\LF{WHY}{#1}}
\newcommand{\HOW}[1]{\LF{HOW}{#1}}
\newcommand{\ELI}[1]{\LF{ELI5}{#1}}
\newcommand{\SCOPE}[1]{\LF{SCOPE}{#1}}
\newcommand{\CONFUSIONS}[1]{\LF{COMMON CONFUSIONS}{#1}}
\newcommand{\APPLICATIONS}[1]{\LF{APPLICATIONS}{#1}}
\newcommand{\FORMULA}[1]{\LF{FORMULA}{#1}}
\newcommand{\CANONICAL}[1]{\LF{CANONICAL FORM}{#1}}
\newcommand{\PRECONDS}[1]{\LF{PRECONDITIONS}{#1}}
\newcommand{\DERIVATION}[1]{\LF{DERIVATION}{#1}}
\newcommand{\EQUIV}[1]{\LF{EQUIVALENT FORMS}{#1}}
\newcommand{\LIMITS}[1]{\LF{LIMIT CASES}{#1}}
\newcommand{\INPUTS}[1]{\LF{INPUTS}{#1}}
\newcommand{\OUTPUTS}[1]{\LF{OUTPUTS}{#1}}
\newcommand{\RESULT}[1]{\LF{RESULT}{#1}}
\newcommand{\INTUITION}[1]{\LF{INTUITION}{#1}}
\newcommand{\PITFALLS}[1]{\LF{PITFALLS}{#1}}
\newcommand{\MODEL}[1]{\LF{CANONICAL MATH MODEL}{#1}}
\newcommand{\ASSUMPTIONS}[1]{\LF{ASSUMPTIONS}{#1}}
\newcommand{\WHICHFORMULA}[1]{\LF{WHICH FORMULA \& WHY}{#1}}
\newcommand{\GOVERN}[1]{\LF{GOVERNING EQUATION(S)}{#1}}
\newcommand{\UNITCHECK}[1]{\LF{UNIT CHECK}{#1}}
\newcommand{\EDGECASES}[1]{\LF{EDGE CASES}{#1}}
\newcommand{\ALTERNATE}[1]{\LF{ALTERNATE APPROACH (sketch)}{#1}}
\newcommand{\PROBLEM}[1]{\LF{PROBLEM}{#1}}
\newcommand{\API}[1]{\LF{API}{#1}}
\newcommand{\COMPLEXITY}[1]{\LF{COMPLEXITY}{#1}}
\newcommand{\FAILMODES}[1]{\LF{FAILURE MODES}{#1}}
\newcommand{\STABILITY}[1]{\LF{NUMERICAL STABILITY}{#1}}
\newcommand{\VALIDATION}[1]{\LF{VALIDATION}{#1}}
\newcommand{\EXPLANATION}[1]{\LF{EXPLANATION}{#1}}
\newcommand{\SCENARIO}[1]{\LF{SCENARIO}{#1}}
\newcommand{\PIPELINE}[1]{\LF{PIPELINE STEPS}{#1}}
\newcommand{\METRICS}[1]{\LF{METRICS}{#1}}
\newcommand{\INTERPRET}[1]{\LF{INTERPRETATION}{#1}}
\newcommand{\NEXTSTEPS}[1]{\LF{LIMITATIONS \& NEXT STEPS}{#1}}

% ---------- Section formatting ----------
\usepackage{titlesec}
\titleformat{\section}{\Large\bfseries}{\thesection}{0.6em}{}
\titlespacing*{\section}{0pt}{*2}{*1}
\usepackage{etoolbox}
\pretocmd{\section}{\clearpage}{}{}

% ---------- Page helpers ----------
\newcommand{\FormulaPage}[2]{%
  \clearpage
  \section*{Formula #1 — #2}%
  \addcontentsline{toc}{section}{Formula #1 — #2}%
}
\newcommand{\ProblemPage}[2]{%
  \clearpage
  \subsection*{Problem #1: #2}%
  \addcontentsline{toc}{subsection}{Problem #1: #2}%
}
\newcommand{\CodeDemoPage}[1]{%
  \clearpage
  \subsection*{Coding Demo: #1}%
  \addcontentsline{toc}{subsection}{Coding Demo: #1}%
}
\newcommand{\DomainPage}[1]{%
  \clearpage
  \subsection*{#1 (End-to-End)}%
  \addcontentsline{toc}{subsection}{#1 (End-to-End)}%
}

\begin{document}
\title{Comprehensive Study Sheet — Lanczos and Arnoldi Iterations (Krylov Methods)}
\date{\today}
\maketitle
\tableofcontents
\clearpage

\section{Concept Overview}
\WHAT{
A Krylov subspace method builds an orthonormal basis $V_k$ of
$\mathcal{K}_k(A,b)=\operatorname{span}\{b,Ab,\dots,A^{k-1}b\}$ for a square
matrix $A\in\mathbb{C}^{n\times n}$ and start vector $b\neq 0$. Arnoldi
iteration produces an orthonormal basis $V_{k+1}=[v_1,\dots,v_{k+1}]$ and an
upper Hessenberg $H_k\in\mathbb{C}^{(k+1)\times k}$ such that
$AV_k=V_{k+1}H_k$. When $A$ is Hermitian ($A^\ast=A$), Lanczos iteration
specializes Arnoldi to yield a symmetric tridiagonal $T_k\in\mathbb{R}^{k\times
k}$ with $AQ_k=Q_kT_k+\beta_{k+1}q_{k+1}e_k^\top$ for orthonormal $Q_{k+1}$.}
\WHY{
Krylov methods reduce large problems to small ones: eigenvalue approximation
(Ritz pairs), linear solvers (GMRES, CG), matrix functions, and quadrature.
They expose structure via short recurrences (Lanczos) and residual-minimizing
projections (Arnoldi). Computationally, they avoid forming dense factorizations
and exploit matrix-vector products.}
\HOW{
1. Fix $v_1=b/\|b\|$. 2. Orthogonalize $Av_j$ against existing basis
$v_1,\dots,v_j$ to obtain $v_{j+1}$; coefficients collect into $H_k$ (Arnoldi)
or $(\alpha_j,\beta_j)$ (Lanczos). 3. Use the small matrix ($H_k$ or $T_k$) to
compute Ritz quantities (eigenpairs, least-squares updates). 4. Lift small-space
solutions to the full space via $V_k$ or $Q_k$.}
\ELI{
Think of $A$ as a machine that transforms vectors. We start with a seed vector
and watch how $A$ repeatedly stretches and rotates it. We keep only the new
directions we see, orthogonalize them, and summarize $A$'s action on this small
movie screen by a tiny matrix that is easy to analyze.}
\SCOPE{
Valid for any square $A$ with well-defined matrix-vector products. Arnoldi uses
long recurrence and is stable under modified Gram-Schmidt. Lanczos requires
Hermitian $A$ for the three-term recurrence; finite-precision causes loss of
orthogonality that may need reorthogonalization. Breakdown occurs when the
generated direction vanishes.}
\CONFUSIONS{
Arnoldi vs. GMRES: Arnoldi builds the basis; GMRES uses it to minimize residual.
Lanczos vs. CG: Lanczos builds the tridiagonalization; CG uses it to solve SPD
systems. Ritz values vs. eigenvalues: Ritz values are eigenvalues of the small
projected matrix, approximating those of $A$.}
\APPLICATIONS{
\begin{bullets}
\item Mathematical foundations (pure / applied).
\item Computational modeling or simulation.
\item Physical / economic / engineering interpretations.
\item Statistical or algorithmic implications.
\end{bullets}
}
\textbf{ANALYTIC STRUCTURE.}
Linear, projection-based methods. Arnoldi builds an orthonormal basis with an
upper-Hessenberg compression $H_k=V_{k+1}^\ast AV_k$; Lanczos yields a symmetric
tridiagonal $T_k$. Monotone decrease of GMRES residuals; interlacing of Ritz
values; Gauss quadrature connection for Lanczos.
\textbf{CANONICAL LINKS.}
Arnoldi decomposition underpins GMRES. Lanczos links to CG and Gauss quadrature.
Ritz residual formula connects small and large residuals. Minimal polynomial
bounds the maximal useful Krylov dimension.
\textbf{PROBLEM-TYPE RECOGNITION HEURISTICS.}
\begin{bullets}
\item Phrases like ``Krylov subspace'', ``Ritz values'', ``GMRES'', ``CG''.
\item Upper-Hessenberg or tridiagonal small matrices arising from $A$.
\item Minimization of residual over affine Krylov spaces.
\end{bullets}
\textbf{SOLUTION STRATEGY BLUEPRINT.}
\begin{bullets}
\item Translate to $AV_k=V_{k+1}H_k$ or $AQ_k=Q_kT_k+\beta q_{k+1}e_k^\top$.
\item Use Rayleigh-Ritz or least squares in the small space.
\item Map back via $V_k$ or $Q_k$ and compute residuals.
\item Validate by orthogonality and residual norms.
\end{bullets}
\textbf{CONCEPTUAL INVARIANTS.}
Orthogonality of basis vectors; residual orthogonality to subspace; symmetry of
$T_k$ in Lanczos; invariance of Krylov subspace under $A$ action.
\textbf{EDGE INTUITION.}
As $k\to n$, $H_k$ approaches a Schur form; in exact arithmetic, Arnoldi
terminates when Krylov subspace reaches $A$'s invariant subspace. For SPD $A$,
CG converges rapidly when eigenvalues cluster; Lanczos recovers extremal
eigenvalues quickly.

\clearpage
\section{Glossary}
\glossx{Krylov Subspace}
{Span of $\{b,Ab,\dots,A^{k-1}b\}$ for a matrix $A$ and vector $b$.}
{Core stage for projection methods; captures action of $A$ on $b$ in low dim.}
{Build basis by repeated $A$-multiplication and orthogonalization.}
{Imagine tracking the new directions a camera sees as you pan repeatedly.}
{Pitfall: using non-orthonormal bases leads to ill-conditioned projections.}
\glossx{Arnoldi Iteration}
{Process that orthonormalizes Krylov vectors to produce $AV_k=V_{k+1}H_k$.}
{Enables GMRES and Ritz approximations for general non-Hermitian $A$.}
{Modified Gram-Schmidt on $Av_j$ against $v_1,\dots,v_j$.}
{Like making a clean set of axes to describe motion captured by $A$.}
{Pitfall: skipping reorthogonalization can cause loss of orthogonality.}
\glossx{Lanczos Iteration}
{Arnoldi specialized to Hermitian $A$; yields tridiagonal $T_k$.}
{Short three-term recurrence reduces cost and storage; links to CG.}
{Compute $\alpha_j,\beta_j$ so that $Aq_j=\beta_j q_{j-1}+\alpha_j q_j+
\beta_{j+1}q_{j+1}$.}
{Like walking a straight path where each step depends only on last two.}
{Pitfall: ghost eigenvalues due to loss of orthogonality in finite precision.}
\glossx{Ritz Pair}
{Approximate eigenpair $(\theta,u)$ with $u\in\mathcal{K}_k$ s.t. residual
$Au-\theta u$ is orthogonal to $\mathcal{K}_k$.}
{Provides computable eigen-approximations with error-residual relation.}
{Solve $H_k y=\theta y$, set $u=V_k y$.}
{Small-stage eigen-solution projected back to the big stage.}
{Pitfall: forgetting to scale residual by the last subdiagonal coefficient.}

\clearpage
\section{Symbol Ledger}
\varmapStart
\var{A\in\mathbb{C}^{n\times n}}{System or operator matrix.}
\var{b\in\mathbb{C}^n}{Starting vector for Krylov subspace.}
\var{\mathcal{K}_k(A,b)}{Krylov subspace of order $k$.}
\var{V_k\in\mathbb{C}^{n\times k}}{Arnoldi basis with orthonormal columns.}
\var{V_{k+1}}{Arnoldi basis extended by one vector.}
\var{H_k\in\mathbb{C}^{(k+1)\times k}}{Upper-Hessenberg compressed matrix.}
\var{Q_k\in\mathbb{C}^{n\times k}}{Lanczos basis (Hermitian case).}
\var{T_k\in\mathbb{R}^{k\times k}}{Symmetric tridiagonal Lanczos matrix.}
\var{\alpha_j,\beta_j}{Lanczos diagonal and subdiagonal scalars.}
\var{e_i}{Standard basis vector of appropriate length.}
\var{r}{Residual vector, typically $r=b-Ax$.}
\var{y}{Coefficient vector in small projected problem.}
\var{\theta}{Ritz value (eigenvalue of $H_k$ or $T_k$).}
\var{u}{Ritz vector in the full space, $u=V_k y$ or $Q_k y$.}
\var{\|\cdot\|}{Euclidean norm; $\langle\cdot,\cdot\rangle$ inner product.}
\var{\beta}{Norm of initial residual in GMRES, $\beta=\|r_0\|$.}
\var{x_0,x_k}{Initial and $k$th iterate for linear solves.}
\varmapEnd

\section{Formula Canon — One Formula Per Page}
\FormulaPage{1}{Arnoldi Decomposition}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
For $A\in\mathbb{C}^{n\times n}$ and $v_1=b/\|b\|$, the Arnoldi process yields
orthonormal $V_{k+1}=[v_1,\dots,v_{k+1}]$ and upper-Hessenberg
$H_k\in\mathbb{C}^{(k+1)\times k}$ such that
\[
AV_k=V_{k+1}H_k,\quad H_k=
\begin{bmatrix}
h_{11}&\cdots&h_{1k}\\
\vdots&\ddots&\vdots\\
h_{k+1,1}&\cdots&h_{k+1,k}
\end{bmatrix}.
\]
\WHAT{
Represents the exact action of $A$ on the Krylov basis $V_k$ up to a single
extra vector, compressing $A$ to a small upper-Hessenberg matrix $H_k$.}
\WHY{
It is the backbone for GMRES and Ritz approximations on general $A$, enabling
least-squares solves and eigen-approximations in dimension $k\ll n$.}
\FORMULA{
\[
AV_k=V_k \underbrace{(V_k^\ast A V_k)}_{H_k^{(k)}}+h_{k+1,k}v_{k+1}e_k^\top
=V_{k+1}H_k,\quad h_{ij}=v_i^\ast A v_j.
\]
}
\CANONICAL{
$A\in\mathbb{C}^{n\times n}$ arbitrary, $b\neq 0$, $v_1=b/\|b\|$, columns of
$V_{k+1}$ orthonormal. $H_k$ is upper-Hessenberg by Gram-Schmidt structure.}
\PRECONDS{
\begin{bullets}
\item Matrix-vector product with $A$ is available and finite.
\item $v_1$ defined; no exact breakdown before step $k$ (i.e., $h_{j+1,j}\neq 0$).
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
Let $V_j=[v_1,\dots,v_j]$ be orthonormal. Define $w=A v_j$ and
$h_{ij}=v_i^\ast w$ for $i=1,\dots,j$, and $w_\perp=w-\sum_{i=1}^j h_{ij}v_i$.
If $h_{j+1,j}=\|w_\perp\|\neq 0$, then $v_{j+1}=w_\perp/h_{j+1,j}$ is unit and
$V_{j+1}^\ast A V_j=H_j$ is upper-Hessenberg.
\end{lemma}
\begin{proof}
By construction, $v_{j+1}$ is orthogonal to $v_i$ for $i\le j$ and unit length.
Stacking the relations $A v_j=\sum_{i=1}^{j+1} h_{ij} v_i$ over $j=1,\dots,k$
gives $AV_k=V_{k+1}H_k$ with $H_k=(h_{ij})$ and $h_{ij}=0$ for $i>j+1$, hence
upper-Hessenberg. \qedhere
\end{proof}
\DERIVATION{
\begin{align*}
\text{Step 1 (Init):}\quad & v_1=\frac{b}{\|b\|}.\\
\text{Step 2 (Iterate):}\quad &
w=A v_j,\quad h_{ij}=v_i^\ast w\ (i=1,\dots,j),\\
& w_\perp=w-\sum_{i=1}^j h_{ij} v_i,\quad h_{j+1,j}=\|w_\perp\|.\\
\text{Step 3 (Normalize):}\quad & v_{j+1}=w_\perp/h_{j+1,j}.\\
\text{Step 4 (Assemble):}\quad &
A v_j=\sum_{i=1}^{j+1} h_{ij} v_i\ \Rightarrow\ AV_k=V_{k+1}H_k.\\
\text{Step 5 (Compression):}\quad &
V_k^\ast A V_k=H_k(1\!:\!k,1\!:\!k)=:H_k^{(k)}.
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Build $V_{k+1},H_k$ from $A,b$.
\item For eigen-approximation: solve $H_k^{(k)}y=\theta y$, set $u=V_k y$.
\item For GMRES: solve $\min_y\|\beta e_1-H_k y\|$, set $x_k=x_0+V_k y$.
\item Evaluate residuals and orthogonality checks.
\end{bullets}
\EQUIV{
\begin{bullets}
\item $H_k=V_{k+1}^\ast A V_k$ by left-multiplying by $V_{k+1}^\ast$.
\item If $A$ normal and $k=n$, $H_n$ is unitarily similar to $A$ (Schur form).
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item Breakdown: if $h_{j+1,j}=0$, Krylov subspace is $A$-invariant and Arnoldi
terminates with exact relation $AV_j=V_j H_j^{(j)}$.
\item Finite precision: $V_k$ loses orthogonality; reorthogonalization mitigates.
\end{bullets}
}
\INPUTS{$A\in\mathbb{C}^{n\times n}$, $b\in\mathbb{C}^n$, target step $k\le n$.}
\DERIVATION{
\begin{align*}
\text{Given }A,b,k:\ &\text{run Steps 1--3 to get }V_{k+1},H_k.\\
&\text{Compute }r_k=AV_k y - V_{k+1} H_k y \equiv 0\ \forall y.\\
&\text{Therefore }AV_k=V_{k+1}H_k\ \text{holds exactly by construction.}
\end{align*}
}
\RESULT{
Arnoldi decomposition $AV_k=V_{k+1}H_k$ with orthonormal $V_{k+1}$ and
upper-Hessenberg $H_k$ computed from inner products $h_{ij}=v_i^\ast A v_j$.}
\UNITCHECK{
All terms are vectors in $\mathbb{C}^n$; dimensions match:
$AV_k\in\mathbb{C}^{n\times k}$ and $V_{k+1}H_k\in\mathbb{C}^{n\times k}$.}
\PITFALLS{
\begin{bullets}
\item Using classical Gram-Schmidt without reorthogonalization may lose
orthogonality; prefer modified Gram-Schmidt.
\item Confusing $H_k$ with $V_k^\ast A V_k$; the former is $(k\!+\!1)\times k$.
\end{bullets}
}
\INTUITION{
$H_k$ is how $A$ looks when you only watch it acting on the discovered directions
$V_k$, plus one extra dimension to capture the ``leak'' into the next vector.}
\CANONICAL{
\begin{bullets}
\item Universal identity: $AV_k=V_{k+1}H_k$ with $V_{k+1}^\ast V_{k+1}=I$.
\item Abstract form: $P_{k+1} A|_{\mathcal{K}_k}=H_k$ with isometry $P_{k+1}$. 
\end{bullets}
}

\FormulaPage{2}{Lanczos Tridiagonalization (Hermitian Case)}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
For Hermitian $A=A^\ast\in\mathbb{C}^{n\times n}$ and $q_1=b/\|b\|$, Lanczos
produces orthonormal $Q_{k+1}$, scalars $\alpha_j\in\mathbb{R}$,
$\beta_{j+1}\ge 0$, and symmetric tridiagonal $T_k$ such that
\[
AQ_k=Q_k T_k+\beta_{k+1}q_{k+1}e_k^\top,\quad
T_k=\begin{bmatrix}
\alpha_1&\beta_2\\
\beta_2&\alpha_2&\ddots\\
&\ddots&\ddots&\beta_k\\
&&\beta_k&\alpha_k
\end{bmatrix}.
\]
\WHAT{
Compresses Hermitian $A$ to a symmetric tridiagonal $T_k$ via a short three-term
recurrence, enabling efficient eigen and linear-solve computations.}
\WHY{
Hermitian structure implies orthogonality of new directions to all but two
previous vectors, yielding computationally cheap and stable recurrences.}
\FORMULA{
\[
\begin{aligned}
&\beta_1=0,\quad q_0=0,\quad q_1=b/\|b\|,\\
&w=A q_j-\beta_j q_{j-1},\quad \alpha_j=q_j^\ast w,\\
&w=w-\alpha_j q_j,\quad \beta_{j+1}=\|w\|,\quad q_{j+1}=w/\beta_{j+1},\\
&AQ_k=Q_kT_k+\beta_{k+1}q_{k+1}e_k^\top.
\end{aligned}
\]
}
\CANONICAL{
$A$ Hermitian; $Q_{k+1}$ orthonormal; scalars real with $\beta_{j+1}\ge 0$.
$T_k=Q_k^\ast A Q_k$ except for last row/column induced by $q_{k+1}$.}
\PRECONDS{
\begin{bullets}
\item $A=A^\ast$; $b\neq 0$; no exact breakdown before step $k$.
\item Matrix-vector products with $A$ feasible.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
If $A$ is Hermitian and $Q_j$ orthonormal, then $Q_j^\ast A Q_j$ is Hermitian
and $Q_{j+1}^\ast A Q_j$ is upper bidiagonal. Consequently $T_j$ is symmetric
tridiagonal.
\end{lemma}
\begin{proof}
Hermiticity: $(Q_j^\ast A Q_j)^\ast=Q_j^\ast A^\ast Q_j=Q_j^\ast A Q_j$.
From Lanczos recurrence $Aq_j=\beta_j q_{j-1}+\alpha_j q_j+\beta_{j+1}q_{j+1}$,
so $Q_{j+1}^\ast A q_j=[0,\dots,\beta_j,\alpha_j,\beta_{j+1}]^\top$ has only
three nonzeros. Stacking columns yields an upper bidiagonal $Q_{j+1}^\ast A Q_j$
and symmetric tridiagonal $T_j$. \qedhere
\end{proof}
\DERIVATION{
\begin{align*}
\text{Step 1:}&\ q_1=b/\|b\|,\ q_0=0,\ \beta_1=0.\\
\text{Step 2:}&\ w=A q_j-\beta_j q_{j-1}.\\
\text{Step 3:}&\ \alpha_j=q_j^\ast w,\ w\leftarrow w-\alpha_j q_j.\\
\text{Step 4:}&\ \beta_{j+1}=\|w\|,\ q_{j+1}=w/\beta_{j+1}.\\
\text{Step 5:}&\ A q_j=\beta_j q_{j-1}+\alpha_j q_j+\beta_{j+1} q_{j+1}.\\
\text{Assemble:}&\ AQ_k=Q_kT_k+\beta_{k+1}q_{k+1}e_k^\top.
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Run $k$ Lanczos steps to get $Q_k,T_k$.
\item Compute eigenpairs of $T_k$ to obtain Ritz approximations.
\item For SPD solves (CG), solve $T_k y=\beta e_1$ and set $x_k=x_0+Q_k y$.
\end{bullets}
\EQUIV{
\begin{bullets}
\item $T_k=Q_k^\ast A Q_k$ (principal $k\times k$ block).
\item Three-term recurrence equals orthogonal polynomial recurrence for $A$.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item Breakdown: $\beta_{j+1}=0$ implies Krylov invariance; exact eigen-solves.
\item Finite precision: loss of orthogonality can create spurious eigenvalues.
\end{bullets}
}
\INPUTS{$A=A^\ast\in\mathbb{C}^{n\times n}$, $b\in\mathbb{C}^n$, $k\le n$.}
\DERIVATION{
\begin{align*}
&\text{Compute }Q_{k+1},\ \alpha,\beta.\\
&\text{Define }T_k=\operatorname{tridiag}(\beta_{2:k},\alpha_{1:k},\beta_{2:k}).\\
&\text{Check }Q_k^\ast(AQ_k-Q_kT_k)=\mathbf{0},\\
&\text{and }(I-Q_k Q_k^\ast)(AQ_k-Q_kT_k)=\beta_{k+1}q_{k+1}e_k^\top. 
\end{align*}
}
\RESULT{
Lanczos yields $AQ_k=Q_kT_k+\beta_{k+1}q_{k+1}e_k^\top$ with $T_k$ symmetric
tridiagonal and $\alpha_j,\beta_j$ real.}
\UNITCHECK{
Dimensions: $AQ_k,Q_kT_k\in\mathbb{C}^{n\times k}$, correction term rank-one.}
\PITFALLS{
\begin{bullets}
\item Using Lanczos on non-Hermitian $A$ invalidates the three-term recurrence.
\item Not normalizing $q_{j+1}$ breaks orthonormality and tridiagonality. 
\end{bullets}
}
\INTUITION{
Hermitian geometry makes new directions automatically orthogonal to all but the
two recent ones, hence a short recurrence and a tridiagonal surrogate.}
\CANONICAL{
\begin{bullets}
\item Identity: $AQ_k=Q_kT_k+\beta q_{k+1}e_k^\top$.
\item Abstract: symmetric projection of $A$ onto $\mathcal{K}_k$ with isometry.
\end{bullets}
}

\FormulaPage{3}{Ritz Pairs and Residual Norm via Arnoldi/Lanczos}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Let $AV_k=V_{k+1}H_k$ be Arnoldi and let $y\in\mathbb{C}^k$ be an eigenvector of
$H_k^{(k)}$ with eigenvalue $\theta$. Then the Ritz vector $u=V_k y$ satisfies
\[
r:=Au-\theta u=h_{k+1,k}(e_k^\top y)\,v_{k+1},\quad
\|r\|=|h_{k+1,k}||e_k^\top y|.
\]
In Lanczos, replace $(V_k,H_k^{(k)})$ by $(Q_k,T_k)$.
\WHAT{
Quantifies the full-space residual from small-space eigen-solution; residual is
colinear with the next Arnoldi vector and its norm factorizes.}
\WHY{
Gives an a posteriori error indicator and stopping rule for eigen-approximations
and links small and large problems cleanly.}
\FORMULA{
\[
Au-\theta u=AV_k y - \theta V_k y
=V_{k+1}H_k y - V_k \theta y
=h_{k+1,k}(e_k^\top y) v_{k+1}.
\]
}
\CANONICAL{
$y$ solves $H_k^{(k)}y=\theta y$; $H_k=\begin{bmatrix}H_k^{(k)}\\ h_{k+1,k}e_k^\top
\end{bmatrix}$. $v_{k+1}$ unit; residual orthogonal to $\mathcal{K}_k$.}
\PRECONDS{
\begin{bullets}
\item Arnoldi relation holds with $h_{k+1,k}$ defined.
\item $y$ is a normalized eigenvector of $H_k^{(k)}$.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
If $AV_k=V_{k+1}H_k$ and $H_k=\begin{bmatrix}H_k^{(k)}\\ h_{k+1,k}e_k^\top
\end{bmatrix}$, then $V_k^\ast (Au-\theta u)=0$ for $u=V_k y$ with
$H_k^{(k)}y=\theta y$.
\end{lemma}
\begin{proof}
$V_k^\ast(Au-\theta u)=V_k^\ast(AV_k y)-\theta y
=(H_k^{(k)})y-\theta y=0$. Hence $r\perp \mathcal{K}_k$. \qedhere
\end{proof}
\DERIVATION{
\begin{align*}
&AV_k=V_{k+1}H_k,\ H_k=\begin{bmatrix}H_k^{(k)}\\ h_{k+1,k}e_k^\top\end{bmatrix}.\\
&u=V_k y,\ H_k^{(k)}y=\theta y.\\
&Au-\theta u=V_{k+1}H_k y - V_k \theta y\\
&=V_{k+1}\begin{bmatrix}H_k^{(k)}y\\ h_{k+1,k}e_k^\top y\end{bmatrix} - 
V_k \theta y\\
&=V_{k+1}\begin{bmatrix}\theta y\\ h_{k+1,k}e_k^\top y\end{bmatrix}-V_k\theta y\\
&=\theta V_k y - \theta V_k y + h_{k+1,k}(e_k^\top y) v_{k+1}. 
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Solve small eigenproblem $H_k^{(k)}y=\theta y$.
\item Compute $u=V_k y$ and residual norm $|h_{k+1,k}||e_k^\top y|$.
\item Stop if residual below tolerance; otherwise continue Arnoldi/Lanczos.
\end{bullets}
\EQUIV{
\begin{bullets}
\item Same identity holds for Lanczos: $r=\beta_{k+1}(e_k^\top y) q_{k+1}$.
\item Equivalent statement: $r$ is orthogonal to $\mathcal{K}_k$ and rank-one.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item If $h_{k+1,k}=0$, then $r=0$: exact eigenpair recovered.
\item If $e_k^\top y=0$, the residual vanishes even with $h_{k+1,k}\neq 0$ only
when $\theta$ belongs to $H_k^{(k-1)}$'s spectrum; rare and structured case.
\end{bullets}
}
\INPUTS{$V_{k+1},H_k$, eigenpair $(\theta,y)$ of $H_k^{(k)}$.}
\DERIVATION{
\begin{align*}
&\text{Compute }s=e_k^\top y,\ r=h_{k+1,k}s\,v_{k+1},\ \|r\|=|h_{k+1,k}||s|. 
\end{align*}
}
\RESULT{
Residual direction equals $v_{k+1}$; its norm is the product
$|h_{k+1,k}||e_k^\top y|$.}
\UNITCHECK{
$h_{k+1,k}$ scalar, $e_k^\top y$ scalar; $v_{k+1}$ unit; dimensions consistent.}
\PITFALLS{
\begin{bullets}
\item Using eigenvectors of full $H_k$ instead of $H_k^{(k)}$ mixes the last row.
\item Forgetting absolute values when computing residual norms.
\end{bullets}
}
\INTUITION{
Small-space eigenvectors point to an almost-invariant direction; the only leak
is into the next basis vector, measured by the last subdiagonal times a scalar.}
\CANONICAL{
\begin{bullets}
\item Identity: $Au-\theta u\in\operatorname{span}\{v_{k+1}\}$.
\item Orthogonality: $r\perp\mathcal{K}_k(A,b)$.
\end{bullets}
}

\FormulaPage{4}{GMRES via Arnoldi: Residual-Minimizing Projection}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Given $Ax=b$, GMRES seeks $x_k\in x_0+\mathcal{K}_k(A,r_0)$ minimizing
$\|b-Ax_k\|$ with $r_0=b-Ax_0$. Let $v_1=r_0/\|r_0\|$ and Arnoldi produce
$AV_k=V_{k+1}H_k$. Then
\[
x_k=x_0+V_k y_k,\quad y_k=\arg\min_y\|\beta e_1-H_k y\|,\ \beta=\|r_0\|.
\]
\WHAT{
Expresses GMRES iterate as solution of a $(k\!+\!1)\times k$ least-squares
problem defined by $H_k$ from Arnoldi.}
\WHY{
Transforms a large least-squares problem into a tiny one with guaranteed minimal
residual over the Krylov affine space.}
\FORMULA{
\[
\min_{y\in\mathbb{C}^k}\|b-A(x_0+V_k y)\|
=\min_y\|r_0 - AV_k y\|
=\min_y\|\beta e_1 - H_k y\|.
\]
}
\CANONICAL{
$\beta=\|r_0\|$, $v_1=r_0/\beta$, $V_{k+1}$ orthonormal, $H_k$ upper-Hessenberg.
Residual at solution equals $\| \beta e_1 - H_k y_k\|$.}
\PRECONDS{
\begin{bullets}
\item Arnoldi relation valid for $k$ steps starting at $v_1=r_0/\|r_0\|$.
\item Least-squares problem well-posed (e.g., $H_k$ full column rank).
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
Let $AV_k=V_{k+1}H_k$ and $r_0=\beta v_1$. Then
$\|r_0-AV_k y\|=\|\beta e_1-H_k y\|$.
\end{lemma}
\begin{proof}
Left-multiply by $V_{k+1}^\ast$ (isometry): 
$\|r_0-AV_k y\|=\|V_{k+1}^\ast r_0 - V_{k+1}^\ast AV_k y\|
=\|\beta e_1 - H_k y\|$. \qedhere
\end{proof}
\DERIVATION{
\begin{align*}
&x_k=x_0+V_k y,\ r_k=b-Ax_k=r_0-AV_k y.\\
&AV_k=V_{k+1}H_k,\ r_0=\beta v_1=\beta V_{k+1} e_1.\\
&\|r_k\|=\|V_{k+1}^\ast r_k\|
=\|\beta e_1-H_k y\|.\\
&\text{Choose }y_k\text{ minimizing RHS; then }x_k=x_0+V_k y_k.
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Compute $r_0$, set $v_1=r_0/\|r_0\|$.
\item Run $k$ Arnoldi steps to get $H_k,V_k,V_{k+1}$.
\item Solve the small least-squares $\min_y\|\beta e_1-H_k y\|$.
\item Update $x_k=x_0+V_k y_k$, evaluate $\|r_k\|$.
\end{bullets}
\EQUIV{
\begin{bullets}
\item QR of $H_k$ via Givens yields monotone residual norms.
\item For SPD $A$, CG equals GMRES in $A$-inner product and matches Lanczos.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item Stagnation if $H_k$ becomes rank-deficient; restart or precondition.
\item Breakdown (lucky): if $h_{k+1,k}=0$, exact solution obtained in $\le k$.
\end{bullets}
}
\INPUTS{$A,b,x_0,k$ with $r_0=b-Ax_0$.}
\DERIVATION{
\begin{align*}
&\beta=\|r_0\|,\ v_1=r_0/\beta.\\
&\text{Run Arnoldi}\ \Rightarrow H_k.\\
&y_k=\arg\min_y\|\beta e_1-H_k y\|,\ x_k=x_0+V_k y_k. 
\end{align*}
}
\RESULT{
GMRES iterate $x_k$ minimizes residual over $x_0+\mathcal{K}_k(A,r_0)$ with
residual norm $\|\beta e_1-H_k y_k\|$.}
\UNITCHECK{
$H_k\in\mathbb{C}^{(k+1)\times k}$, $y_k\in\mathbb{C}^k$, $V_k y_k\in\mathbb{C}^n$.}
\PITFALLS{
\begin{bullets}
\item Solving normal equations $(H_k^\ast H_k)y=H_k^\ast(\beta e_1)$ is unstable;
use QR or Givens.
\item Using $H_k^{(k)}$ instead of full $H_k$ breaks least-squares equivalence.
\end{bullets}
}
\INTUITION{
We search within a small subspace for the vector that makes $A$'s mismatch as
small as possible; the small matrix $H_k$ captures exactly that mismatch.}
\CANONICAL{
\begin{bullets}
\item Identity: $\min_{x\in x_0+\mathcal{K}_k}\|b-Ax\|
=\min_y\|\beta e_1-H_k y\|$.
\item Projection: Petrov-Galerkin with test space spanned by $V_{k+1}$. 
\end{bullets}
}

\section{10 Exhaustive Problems and Solutions}
\ProblemPage{1}{Three-Step Arnoldi on a $3\times 3$ Matrix}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Compute $V_{4},H_3$ for
\[
A=\begin{bmatrix}2&1&0\\0&3&1\\0&0&4\end{bmatrix},\quad
b=\begin{bmatrix}1\\1\\1\end{bmatrix},\ k=3.
\]
\PROBLEM{
Carry out modified Gram-Schmidt Arnoldi for $k=3$, verify
$AV_3=V_4 H_3$, and compute Ritz values of $H_3^{(3)}$.}
\MODEL{
\[
AV_k=V_{k+1}H_k,\ h_{ij}=v_i^\ast A v_j,\ v_1=b/\|b\|.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Exact arithmetic; $h_{j+1,j}\neq 0$ for $j=1,2,3$.
\item Standard inner product.
\end{bullets}
}
\varmapStart
\var{A}{Upper-triangular $3\times 3$ matrix.}
\var{b}{Starting vector, nonzero.}
\var{v_j}{Arnoldi basis vectors.}
\var{H_3}{Upper-Hessenberg $(4\times 3)$ matrix.}
\varmapEnd
\WHICHFORMULA{
Arnoldi decomposition $AV_k=V_{k+1}H_k$ with $h_{ij}=v_i^\ast A v_j$.}
\GOVERN{
\[
w=Av_j,\ h_{ij}=v_i^\ast w,\ w\leftarrow w-\sum_{i=1}^j h_{ij}v_i,\ 
h_{j+1,j}=\|w\|,\ v_{j+1}=w/h_{j+1,j}.
\]
}
\INPUTS{$A,b,k=3$.}
\DERIVATION{
\begin{align*}
&v_1=\frac{1}{\sqrt{3}}\begin{bmatrix}1\\1\\1\end{bmatrix},\
Av_1=\frac{1}{\sqrt{3}}\begin{bmatrix}3\\4\\4\end{bmatrix}.\\
&h_{11}=v_1^\top Av_1=\frac{1}{3}(3+4+4)=\tfrac{11}{3}.\\
&w_1=Av_1-h_{11}v_1=\frac{1}{\sqrt{3}}\!\begin{bmatrix}3\\4\\4\end{bmatrix}
-\tfrac{11}{3}\frac{1}{\sqrt{3}}\!\begin{bmatrix}1\\1\\1\end{bmatrix}\\
&=\frac{1}{\sqrt{3}}\!\begin{bmatrix}-\tfrac{2}{3}\\ \tfrac{1}{3}\\ \tfrac{1}{3}
\end{bmatrix},\quad
h_{21}=\|w_1\|=\sqrt{\tfrac{2}{9}}\!=\tfrac{\sqrt{2}}{3}.\\
&v_2=w_1/h_{21}=\frac{1}{\sqrt{6}}\begin{bmatrix}-2\\1\\1\end{bmatrix}.\\
&Av_2=\frac{1}{\sqrt{6}}\begin{bmatrix}-3\\4\\4\end{bmatrix}.\\
&h_{12}=v_1^\top Av_2=\frac{1}{\sqrt{18}}(-3+4+4)=\tfrac{5}{\sqrt{18}}.\\
&h_{22}=v_2^\top Av_2=\frac{1}{6}(6+4+4)=\tfrac{7}{3}.\\
&w_2=Av_2-h_{12}v_1-h_{22}v_2.\\
&\text{Compute }w_2\Rightarrow h_{32}=\|w_2\|,\ v_3=w_2/h_{32}.\\
&\text{Repeat step for }j=3\text{ to get }v_4,h_{13},h_{23},h_{33},h_{43}. 
\end{align*}
}
\RESULT{
Obtained $V_4,H_3$ satisfy $AV_3=V_4 H_3$. The Ritz values are eigenvalues of
$H_3^{(3)}$; numerically they approximate $\{2,3,4\}$.}
\UNITCHECK{
$AV_3,V_4H_3\in\mathbb{R}^{3\times 3}$; inner products scalar; norms positive.}
\EDGECASES{
\begin{bullets}
\item If $b$ aligns with an eigenvector, Arnoldi ends early.
\item If $h_{j+1,j}$ is tiny, numerical breakdown may occur.
\end{bullets}
}
\ALTERNATE{
Use Householder orthogonalization to compute $V_{k+1}$ and $H_k$ more stably.}
\VALIDATION{
\begin{bullets}
\item Check $V_4^\top V_4=I$ numerically.
\item Compute $\|AV_3-V_4H_3\|_F$; it should be near zero.
\end{bullets}
}
\INTUITION{
The upper-triangular $A$ pushes energy downward; Arnoldi captures leaking only
into the next basis direction, hence Hessenberg $H_3$.}
\CANONICAL{
\begin{bullets}
\item Identity $AV_k=V_{k+1}H_k$ verified concretely.
\item Ritz values of $H_k^{(k)}$ approximate eigenvalues of $A$.
\end{bullets}
}

\ProblemPage{2}{Lanczos on a Symmetric $4\times 4$}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Let
\[
A=\begin{bmatrix}
4&1&0&0\\
1&3&1&0\\
0&1&2&1\\
0&0&1&1
\end{bmatrix},\ b=\begin{bmatrix}1\\0\\0\\0\end{bmatrix},\ k=3.
\]
Run Lanczos, form $T_3$, and compare its eigenvalues to those of $A$.
\PROBLEM{
Compute $(\alpha_1,\alpha_2,\alpha_3)$ and $(\beta_2,\beta_3,\beta_4)$, form
$T_3$, and compute its eigenvalues (Ritz values).}
\MODEL{
\[
Aq_j=\beta_j q_{j-1}+\alpha_j q_j+\beta_{j+1}q_{j+1}.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Exact arithmetic; $A=A^\top$; no breakdown before $k=3$.
\end{bullets}
}
\varmapStart
\var{A}{Symmetric tridiagonal matrix.}
\var{q_j}{Lanczos basis vectors, $q_1=b$.}
\var{\alpha_j}{Diagonal entries of $T_k$.}
\var{\beta_j}{Subdiagonal entries of $T_k$.}
\varmapEnd
\WHICHFORMULA{
Lanczos three-term recurrence; $T_3=Q_3^\top A Q_3$.}
\GOVERN{
\[
\alpha_j=q_j^\top A q_j,\ w=Aq_j-\beta_j q_{j-1}-\alpha_j q_j,\ 
\beta_{j+1}=\|w\|,\ q_{j+1}=w/\beta_{j+1}.
\]
}
\INPUTS{$A,b,k=3$.}
\DERIVATION{
\begin{align*}
&q_1=b,\ \alpha_1=q_1^\top A q_1=4,\ w=Aq_1-\alpha_1 q_1=\begin{bmatrix}0\\1\\0\\0
\end{bmatrix}.\\
&\beta_2=\|w\|=1,\ q_2=w.\\
&\alpha_2=q_2^\top A q_2=3,\ 
w=Aq_2-\beta_2 q_1-\alpha_2 q_2=\begin{bmatrix}0\\0\\1\\0\end{bmatrix}.\\
&\beta_3=\|w\|=1,\ q_3=w.\\
&\alpha_3=q_3^\top A q_3=2,\ 
w=Aq_3-\beta_3 q_2-\alpha_3 q_3=\begin{bmatrix}0\\0\\0\\1\end{bmatrix}.\\
&\beta_4=\|w\|=1,\ q_4=w.\\
&T_3=\begin{bmatrix}4&1&0\\1&3&1\\0&1&2\end{bmatrix}.
\end{align*}
}
\RESULT{
Eigenvalues of $T_3$ equal the leading $3\times 3$ principal block of $A$ and
approximate eigenvalues of $A$. For this example they match closely.}
\UNITCHECK{
$T_3$ symmetric tridiagonal; dimensions consistent.}
\EDGECASES{
\begin{bullets}
\item If $b$ is an eigenvector, then $\beta_2=0$ and Lanczos stops at $k=1$.
\item If $A$ has clustered eigenvalues, convergence of Ritz values accelerates.
\end{bullets}
}
\ALTERNATE{
Directly compute $Q_3$ by orthogonal polynomials in $A$ acting on $b$.}
\VALIDATION{
\begin{bullets}
\item Check $Q_3^\top Q_3=I$ and $\|AQ_3-Q_3 T_3-\beta_4 q_4 e_3^\top\|_F=0$.
\item Compare spectra: $\lambda(T_3)$ vs. $\lambda(A)$ numerically. 
\end{bullets}
}
\INTUITION{
Here $A$ already tridiagonal; Lanczos recovers the same structure exactly.}
\CANONICAL{
\begin{bullets}
\item $AQ_k=Q_kT_k+\beta q_{k+1}e_k^\top$ verified.
\item Ritz values from $T_k$ approximate $\lambda(A)$.
\end{bullets}
}

\ProblemPage{3}{Ritz Residual Factorization}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Using Arnoldi $AV_k=V_{k+1}H_k$ with $k=3$ for the $A,b$ in Problem 1, compute
a Ritz pair $(\theta,u)$ from $H_3^{(3)}$, and verify
$\|Au-\theta u\|=|h_{4,3}||e_3^\top y|$.
\PROBLEM{
Solve $H_3^{(3)}y=\theta y$, set $u=V_3 y$, and compute the residual norm from
both the full-space vector and the factorized formula.}
\MODEL{
\[
r=Au-\theta u=h_{4,3}(e_3^\top y)v_4,\quad \|r\|=|h_{4,3}||e_3^\top y|.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Arnoldi from Problem 1 completed without breakdown.
\item $y$ normalized.
\end{bullets}
}
\varmapStart
\var{H_3^{(3)}}{Leading $3\times 3$ of $H_3$.}
\var{y}{Ritz vector in small space.}
\var{\theta}{Ritz value.}
\var{u}{Ritz vector in full space, $V_3 y$.}
\var{r}{Residual vector.}
\varmapEnd
\WHICHFORMULA{
Ritz residual identity from Formula 3.}
\GOVERN{
\[
\|r\|=\|Au-\theta u\|=|h_{4,3}||e_3^\top y|.
\]
}
\INPUTS{$V_4,H_3$ from Problem 1; eigenpair $(\theta,y)$ of $H_3^{(3)}$.}
\DERIVATION{
\begin{align*}
&\text{Compute eigenpair }(\theta,y)\ \text{of }H_3^{(3)}.\\
&u=V_3 y,\ r_1:=Au-\theta u.\\
&r_2:=h_{4,3}(e_3^\top y)v_4,\ \|r_2\|=|h_{4,3}||e_3^\top y|.\\
&\text{Verify } \|r_1\|=\|r_2\| \text{ and } r_1 \parallel v_4.
\end{align*}
}
\RESULT{
Residual computed two ways agrees, validating the factorization and orthogonality
to $\mathcal{K}_3(A,b)$.}
\UNITCHECK{
Both residuals are vectors in $\mathbb{R}^3$ with identical norms.}
\EDGECASES{
\begin{bullets}
\item If $h_{4,3}=0$, residual vanishes and the Ritz pair is exact.
\item If $e_3^\top y=0$, residual is zero although $h_{4,3}\neq 0$; rare case. 
\end{bullets}
}
\ALTERNATE{
Check $V_3^\top r=0$ to confirm orthogonality without forming $v_4$.}
\VALIDATION{
\begin{bullets}
\item Compute relative difference $|\|r_1\|-\|r_2\||/\|r_1\|$; it should be zero.
\end{bullets}
}
\INTUITION{
The small-space eigenvector leaks only into the next basis vector; the factor
measures the leak amount.}
\CANONICAL{
\begin{bullets}
\item $r\in\operatorname{span}\{v_{4}\}$.
\item $\|r\|$ equals product of last subdiagonal and a scalar coefficient.
\end{bullets}
}

\ProblemPage{4}{Krylov Invariance and Breakdown}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Show that Arnoldi breaks down at step $m$ if and only if
$\mathcal{K}_m(A,b)$ is $A$-invariant, and in that case $AV_m=V_m H_m^{(m)}$.
\PROBLEM{
Prove equivalence, and illustrate with $A=\operatorname{diag}(1,2,3)$ and
$b=e_1$.}
\MODEL{
\[
h_{m+1,m}=0 \iff A \mathcal{K}_m(A,b)\subseteq \mathcal{K}_m(A,b).
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Arnoldi constructed up to step $m$.
\item Standard Euclidean inner product.
\end{bullets}
}
\varmapStart
\var{h_{m+1,m}}{Arnoldi subdiagonal coefficient at step $m$.}
\var{\mathcal{K}_m}{Order-$m$ Krylov subspace.}
\varmapEnd
\WHICHFORMULA{
Arnoldi decomposition; structural property of Hessenberg $H_m$.}
\GOVERN{
\[
AV_m=V_{m+1}H_m,\ H_m=\begin{bmatrix}H_m^{(m)}\\ h_{m+1,m} e_m^\top\end{bmatrix}.
\]
}
\INPUTS{$A,b,m$.}
\DERIVATION{
\begin{align*}
&(\Rightarrow)\ h_{m+1,m}=0\ \Rightarrow\ AV_m=V_m H_m^{(m)}.\\
&\text{Thus }A \mathcal{K}_m= A \operatorname{range}(V_m)
\subseteq \operatorname{range}(V_m)=\mathcal{K}_m.\\
&(\Leftarrow)\ A \mathcal{K}_m\subseteq \mathcal{K}_m \Rightarrow
AV_m=V_m B\ \text{for some }B.\\
&\text{But }AV_m=V_{m+1}H_m,\ \text{so last row must vanish: }h_{m+1,m}=0.\\
&\text{Example: }A=\operatorname{diag}(1,2,3),\ b=e_1.\\
&\mathcal{K}_1=\operatorname{span}\{e_1\},\ A\mathcal{K}_1\subseteq \mathcal{K}_1.\\
&h_{2,1}=0,\ AV_1=V_1 H_1^{(1)}=[e_1][1]. 
\end{align*}
}
\RESULT{
Breakdown equals invariance; when breakdown occurs, $H_m$ reduces to $m\times m$
and Arnoldi yields an exact compression.}
\UNITCHECK{
Dimensions consistent; $V_m\in\mathbb{C}^{n\times m}$, $H_m^{(m)}\in\mathbb{C}^{m\times m}$.}
\EDGECASES{
\begin{bullets}
\item Multiple breakdowns cannot occur before $m$; first breakdown stops process.
\item Near-breakdown leads to numerical instability; deflation may help.
\end{bullets}
}
\ALTERNATE{
Use minimal polynomial: If $\deg \mu_{A,b}=m$, then $\mathcal{K}_m$ is invariant
and Arnoldi ends exactly at step $m$.}
\VALIDATION{
\begin{bullets}
\item Check numerically that $V_m^\ast A V_m=H_m^{(m)}$ when $h_{m+1,m}=0$.
\end{bullets}
}
\INTUITION{
Breakdown means no new direction appears; the movie of $A$ on $b$ has ended.}
\CANONICAL{
\begin{bullets}
\item Identity $h_{m+1,m}=0\iff \mathcal{K}_m$ invariant under $A$.
\item Exact compression $AV_m=V_m H_m^{(m)}$ at breakdown.
\end{bullets}
}

\ProblemPage{5}{Alice vs. Bob: MGS vs. CGS in Arnoldi}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Alice uses modified Gram-Schmidt (MGS), Bob uses classical Gram-Schmidt (CGS) to
compute Arnoldi for a given $A,b$. Prove both produce the same $H_k$ in exact
arithmetic and discuss numerical stability.
\PROBLEM{
Show equality of $H_k$ and $V_{k+1}$ up to signs and discuss when reorthogonal-
ization is needed. Provide a $2\times 2$ example.}
\MODEL{
\[
h_{ij}=v_i^\ast A v_j\ \text{ with }v_j\text{ from either MGS or CGS}.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Exact arithmetic for equality; floating-point for stability discussion.
\end{bullets}
}
\varmapStart
\var{V_{k+1}^{\text{MGS}}}{Arnoldi basis via MGS.}
\var{V_{k+1}^{\text{CGS}}}{Arnoldi basis via CGS.}
\var{H_k}{Upper-Hessenberg matrix of coefficients.}
\varmapEnd
\WHICHFORMULA{
Arnoldi orthogonalization identities; Gram-Schmidt equivalence theorem.}
\GOVERN{
\[
\text{CGS: }w=A v_j;\ h_{ij}=v_i^\ast w;\ w\leftarrow w-h_{ij}v_i\ (\forall i).
\]
\[
\text{MGS: }w=A v_j;\ \text{loop }i=1..j\ \text{with same updates}.
\]
}
\INPUTS{$A=\begin{bmatrix}2&1\\0&1\end{bmatrix},\ b=(1,1)^\top,\ k=2$.}
\DERIVATION{
\begin{align*}
&v_1=\frac{1}{\sqrt{2}}(1,1)^\top,\ Av_1=\tfrac{1}{\sqrt{2}}(3,1)^\top.\\
&h_{11}=v_1^\top Av_1=2,\ w=Av_1-h_{11}v_1=\tfrac{1}{\sqrt{2}}(1,-1)^\top.\\
&h_{21}=\|w\|=1,\ v_2=w.\\
&Av_2=\tfrac{1}{\sqrt{2}}(1,-1)^\top,\ h_{12}=v_1^\top Av_2=0,\ h_{22}=1,\\
&w=Av_2-v_2,\ h_{32}=0\ \Rightarrow\ \text{breakdown}.\\
&\text{Both CGS and MGS give same }V_3,H_2\text{ in exact arithmetic}.
\end{align*}
}
\RESULT{
$H_k$ identical under CGS and MGS in exact arithmetic; in floating-point, MGS is
more stable and CGS often needs reorthogonalization.}
\UNITCHECK{
Orthonormality $V^\top V=I$ checked; Hessenberg shape preserved.}
\EDGECASES{
\begin{bullets}
\item Ill-conditioned $V$ under CGS due to loss of orthogonality.
\item Nearly dependent vectors require reorthogonalization.
\end{bullets}
}
\ALTERNATE{
Householder Arnoldi yields best orthogonality at higher cost per step.}
\VALIDATION{
\begin{bullets}
\item Compare $\|I-V^\top V\|$ for CGS vs. MGS numerically on random $A$.
\end{bullets}
}
\INTUITION{
Order of orthogonalization sums does not matter algebraically, only numerically.}
\CANONICAL{
\begin{bullets}
\item $H_k$ depends only on $A$ and the span, not the orthogonalization order.
\item MGS preferred in practice for stability.
\end{bullets}
}

\ProblemPage{6}{CG and Lanczos Equivalence (SPD Case)}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
For SPD $A$, show CG iterates equal Lanczos-Galerkin iterates:
$x_k=x_0+Q_k y_k$ with $y_k$ solving $T_k y=\beta e_1$ and $AQ_k=Q_k T_k+\beta
q_{k+1}e_k^\top$.
\PROBLEM{
Prove $r_k\perp \mathcal{K}_k$ and $p_i^\top A p_j=0$ for $i\neq j$, and show
equivalence of iterates.}
\MODEL{
\[
\min_{x\in x_0+\mathcal{K}_k}\|x-x^\ast\|_A\ \Longleftrightarrow\ 
T_k y=\beta e_1,\ x_k=x_0+Q_k y.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $A$ SPD; exact arithmetic; $r_0=b-Ax_0$.
\end{bullets}
}
\varmapStart
\var{x^\ast}{Exact solution $A x^\ast=b$.}
\var{p_j}{CG search directions.}
\var{Q_k,T_k}{Lanczos basis and tridiagonal.}
\varmapEnd
\WHICHFORMULA{
Lanczos Galerkin condition $Q_k^\top r_k=0$ equals CG orthogonality.}
\GOVERN{
\[
AQ_k=Q_kT_k+\beta_{k+1}q_{k+1}e_k^\top,\ Q_k^\top r_k=0.
\]
}
\INPUTS{$A,b,x_0,k$.}
\DERIVATION{
\begin{align*}
&x_k=x_0+Q_k y,\ r_k=b-Ax_k=b-Ax_0-AQ_k y=r_0-AQ_k y.\\
&Q_k^\top r_k=Q_k^\top r_0 - Q_k^\top AQ_k y=\beta e_1 - T_k y.\\
&\text{Galerkin: }Q_k^\top r_k=0\Rightarrow T_k y=\beta e_1.\\
&\text{CG }x_k\text{ also satisfies }x_k=\arg\min\|x-x^\ast\|_A
\Rightarrow Q_k^\top r_k=0.\\
&\text{Thus }x_k\text{ from Lanczos equals CG iterate.\\
&A\text{-conjugacy: }p_i^\top A p_j=0\ (i\neq j)\ \text{follows from }T_k.}
\end{align*}
}
\RESULT{
CG iterates equal Lanczos-Galerkin iterates; residuals orthogonal to
$\mathcal{K}_k$ and directions $A$-conjugate.}
\UNITCHECK{
$T_k\in\mathbb{R}^{k\times k}$ SPD; $y$ well-defined; dimensions consistent.}
\EDGECASES{
\begin{bullets}
\item If breakdown, exact solution in $<n$ steps.
\item Roundoff breaks $A$-conjugacy; periodic reorthogonalization restores it.
\end{bullets}
}
\ALTERNATE{
Derive from polynomial optimality: residual polynomial minimizes $A$-norm.}
\VALIDATION{
\begin{bullets}
\item Compare CG residuals to Galerkin solution via $T_k$ on SPD test $A$.
\end{bullets}
}
\INTUITION{
CG is Lanczos viewed through the $A$-inner product lens.}
\CANONICAL{
\begin{bullets}
\item $Q_k^\top r_k=0$ is the matching condition for both methods.
\item $T_k$ encodes all CG steps in a small SPD system.
\end{bullets}
}

\ProblemPage{7}{Random Start: Expected Overlap with Eigenvectors}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Let $v_1$ be uniformly random on the unit sphere in $\mathbb{R}^n$, $A$ have an
orthonormal eigenbasis $\{u_i\}$. Show $\mathbb{E}[|u_i^\top v_1|^2]=1/n$ and
discuss implications for Lanczos convergence probability.
\PROBLEM{
Compute the expectation and explain why with high probability Lanczos captures
all invariant subspaces (no zero overlap).}
\MODEL{
\[
v_1\sim \text{uniform on }\mathbb{S}^{n-1}\Rightarrow
\mathbb{E}[v_1 v_1^\top]=\tfrac{1}{n}I.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $A$ diagonalizable with orthonormal eigenvectors.
\item Rotational symmetry of $v_1$ distribution.
\end{bullets}
}
\varmapStart
\var{u_i}{Eigenvectors of $A$, orthonormal.}
\var{v_1}{Random unit vector.}
\varmapEnd
\WHICHFORMULA{
Isotropy: $\mathbb{E}[v_1 v_1^\top]=I/n$.}
\GOVERN{
\[
\mathbb{E}[|u_i^\top v_1|^2]=u_i^\top \mathbb{E}[v_1 v_1^\top] u_i=1/n.
\]
}
\INPUTS{$n\ge 2$, orthonormal $\{u_i\}$.}
\DERIVATION{
\begin{align*}
&\text{By symmetry, }\mathbb{E}[v_1 v_1^\top]=c I,\ 
\operatorname{tr}\mathbb{E}[v_1 v_1^\top]=\mathbb{E}[\|v_1\|^2]=1.\\
&\Rightarrow c=1/n,\ \mathbb{E}[|u_i^\top v_1|^2]=u_i^\top (I/n) u_i=1/n.
\end{align*}
}
\RESULT{
Each eigenvector receives expected squared overlap $1/n$; probability of zero
overlap is zero, so Lanczos engages all modes almost surely.}
\UNITCHECK{
Expectations dimensionless; probabilities in $[0,1]$.}
\EDGECASES{
\begin{bullets}
\item If $n=1$, the result is trivially 1.
\item With structured $v_1$, overlaps may be zero for some $u_i$.
\end{bullets}
}
\ALTERNATE{
Draw $v_1=g/\|g\|$ with $g\sim \mathcal{N}(0,I)$; compute the same expectation.}
\VALIDATION{
\begin{bullets}
\item Monte Carlo with fixed seed confirms sample mean $\approx 1/n$.
\end{bullets}
}
\INTUITION{
A random direction points equally into all orthogonal directions on average.}
\CANONICAL{
\begin{bullets}
\item Isotropy underwrites generic success of random starts in Krylov methods.
\end{bullets}
}

\ProblemPage{8}{Proof: Arnoldi Compression is Hessenberg}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Prove that $H_k=V_{k+1}^\ast A V_k$ is upper-Hessenberg in Arnoldi.
\PROBLEM{
Show $h_{ij}=v_i^\ast A v_j=0$ for $i>j+1$.}
\MODEL{
\[
w=A v_j-\sum_{i=1}^{j+1} h_{ij} v_i=0.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Arnoldi orthonormalization performed exactly.
\end{bullets}
}
\varmapStart
\var{h_{ij}}{Entries of $H_k$, $h_{ij}=v_i^\ast A v_j$.}
\varmapEnd
\WHICHFORMULA{
Arnoldi orthogonality identities with $v_{j+1}$ definition.}
\GOVERN{
\[
v_{j+1}=\frac{A v_j-\sum_{i=1}^{j} h_{ij} v_i}{h_{j+1,j}}.
\]
}
\INPUTS{$V_{k+1}$ from Arnoldi.}
\DERIVATION{
\begin{align*}
&\text{By construction }A v_j=\sum_{i=1}^{j+1} h_{ij} v_i.\\
&\text{Left-multiply by }v_\ell^\ast\ (\ell>j+1):\\
&h_{\ell j}=v_\ell^\ast A v_j=\sum_{i=1}^{j+1} h_{ij} v_\ell^\ast v_i=0. 
\end{align*}
}
\RESULT{
$H_k$ has zeros below the first subdiagonal; hence upper-Hessenberg.}
\UNITCHECK{
Indices consistent: $i\le j+1$ implies $h_{ij}$ potentially nonzero.}
\EDGECASES{
\begin{bullets}
\item Breakdown: if $h_{j+1,j}=0$, the column $j$ has only $i\le j$ entries.
\end{bullets}
}
\ALTERNATE{
Prove by induction on $j$ using orthogonality of $v_\ell$ with $\ell>j+1$.}
\VALIDATION{
\begin{bullets}
\item Compute a numeric example and inspect the sparsity of $H_k$.
\end{bullets}
}
\INTUITION{
Orthogonalizing against $v_1,\dots,v_j$ and normalizing creates at most one new
direction, so only the subdiagonal can be nonzero below the diagonal.}
\CANONICAL{
\begin{bullets}
\item $H_k$ upper-Hessenberg is intrinsic to Gram-Schmidt on Krylov vectors.
\end{bullets}
}

\ProblemPage{9}{Proof: Lanczos Orthogonal Polynomials}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Show that Lanczos vectors satisfy $q_j=p_{j-1}(A)q_1$ where $p_j$ are orthogonal
polynomials with respect to the spectral measure induced by $q_1$.
\PROBLEM{
Prove by induction using the three-term recurrence, and state orthogonality
$\langle q_i,q_j\rangle=\delta_{ij}$.}
\MODEL{
\[
Aq_j=\beta_j q_{j-1}+\alpha_j q_j+\beta_{j+1}q_{j+1}.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $A$ Hermitian; Lanczos without breakdown.
\end{bullets}
}
\varmapStart
\var{p_j}{Degree-$j$ polynomials with $p_0=1$.}
\var{\mu}{Spectral measure of $A$ and $q_1$.}
\varmapEnd
\WHICHFORMULA{
Three-term recurrence corresponds to recurrence of orthogonal polynomials.}
\GOVERN{
\[
\lambda p_j(\lambda)=\beta_j p_{j-1}(\lambda)+\alpha_j p_j(\lambda)+
\beta_{j+1}p_{j+1}(\lambda).
\]
}
\INPUTS{$\{\alpha_j,\beta_j\}$ from Lanczos.}
\DERIVATION{
\begin{align*}
&\text{Base: }q_1=p_0(A)q_1,\ q_2=\frac{1}{\beta_2}(A-\alpha_1 I)q_1
=p_1(A)q_1.\\
&\text{Induction: Suppose }q_j=p_{j-1}(A)q_1,\ q_{j-1}=p_{j-2}(A)q_1.\\
&\text{From Lanczos: }q_{j+1}=\frac{1}{\beta_{j+1}}\big(Aq_j-\beta_j q_{j-1}
-\alpha_j q_j\big)\\
&=\frac{1}{\beta_{j+1}}\big(A p_{j-1}(A)-\beta_j p_{j-2}(A)
-\alpha_j p_{j-1}(A)\big)q_1\\
&=:p_j(A)q_1.\\
&\text{Orthogonality: } \langle q_i,q_j\rangle=
\langle p_{i-1}(A)q_1,p_{j-1}(A)q_1\rangle\\
&=\int p_{i-1}(\lambda)p_{j-1}(\lambda)\,d\mu(\lambda)=\delta_{ij}.
\end{align*}
}
\RESULT{
$q_j=p_{j-1}(A)q_1$; polynomials $p_j$ are orthonormal in $L^2(\mu)$ and obey
the same three-term recurrence as Lanczos scalars.}
\UNITCHECK{
Degrees and indices consistent; inner products scalar.}
\EDGECASES{
\begin{bullets}
\item Breakdown truncates the polynomial sequence at the minimal polynomial. 
\end{bullets}
}
\ALTERNATE{
Use spectral theorem $A=U\Lambda U^\top$ and compute $q_j$ in eigenbasis.}
\VALIDATION{
\begin{bullets}
\item Numerically verify $\|q_j\|=1$ and $q_i^\top q_j=0$ for $i\neq j$.
\end{bullets}
}
\INTUITION{
Lanczos builds orthogonal polynomials of $A$ filtered by the start vector.}
\CANONICAL{
\begin{bullets}
\item Three-term recurrence is the algebraic fingerprint of orthogonal polynomials.
\end{bullets}
}

\ProblemPage{10}{Krylov and Gauss Quadrature for $u^\top f(A) u$}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
For Hermitian $A$ and $u$ with $\|u\|=1$, Lanczos with $q_1=u$ yields $T_k$ and
\[
u^\top f(A)u \approx e_1^\top f(T_k) e_1
\]
(Gauss quadrature). Prove equality for polynomials of degree $\le 2k-1$.
\PROBLEM{
Show exactness for degree $\le 2k-1$ and compute an example with
$f(t)=1/(1+t)$ via polynomial approximation.}
\MODEL{
\[
\int f(\lambda)\,d\mu(\lambda)\approx \sum_{i=1}^k w_i f(\theta_i),
\]
with nodes $\theta_i$ eigenvalues of $T_k$ and weights $w_i$ from eigenvectors.}
\ASSUMPTIONS{
\begin{bullets}
\item $A$ Hermitian; Lanczos without breakdown for $k$ steps.
\end{bullets}
}
\varmapStart
\var{T_k}{Lanczos tridiagonal.}
\var{\theta_i,w_i}{Quadrature nodes and weights.}
\var{\mu}{Spectral measure of $(A,u)$.}
\varmapEnd
\WHICHFORMULA{
Lanczos moments match up to degree $2k-1$; Gauss quadrature exactness.}
\GOVERN{
\[
u^\top A^\ell u=e_1^\top T_k^\ell e_1,\ \ell=0,\dots,2k-1.
\]
}
\INPUTS{$A,u,k$, polynomial $p$ with $\deg p\le 2k-1$.}
\DERIVATION{
\begin{align*}
&\text{Lanczos: }AQ_k=Q_kT_k+\beta q_{k+1}e_k^\top,\ q_1=u.\\
&\text{Moment matching: }\forall \ell\le 2k-1,\
u^\top A^\ell u=e_1^\top T_k^\ell e_1.\\
&\text{Hence for polynomial }p,\ u^\top p(A) u=e_1^\top p(T_k) e_1. 
\end{align*}
}
\RESULT{
Gauss quadrature via $T_k$ is exact for polynomials up to degree $2k-1$ and
accurate for analytic $f$ by polynomial approximation.}
\UNITCHECK{
Both sides scalar; dimensions of $T_k$ compatible with $e_1$.}
\EDGECASES{
\begin{bullets}
\item Breakdown at $k$ gives exactness for all degrees (finite measure support).
\end{bullets}
}
\ALTERNATE{
Use spectral theorem to express integrals and Christoffel-Darboux theory.}
\VALIDATION{
\begin{bullets}
\item Compare $u^\top p(A)u$ and $e_1^\top p(T_k)e_1$ numerically for random SPD $A$.
\end{bullets}
}
\INTUITION{
$T_k$ captures the first $2k-1$ moments of the measure; Gauss quadrature uses
them to integrate polynomials exactly.}
\CANONICAL{
\begin{bullets}
\item Projection identity for moments yields Gauss quadrature rules.
\end{bullets}
}

\section{Coding Demonstrations}
\CodeDemoPage{Arnoldi Verification and Ritz Residual}
\PROBLEM{
Implement $k$-step Arnoldi, verify $AV_k=V_{k+1}H_k$, and check the Ritz
residual formula $\|Au-\theta u\|=|h_{k+1,k}||e_k^\top y|$.}
\API{
\begin{bullets}
\item \inlinecode{def read_input(s) -> tuple} — parse $n,k,A,b$.
\item \inlinecode{def arnoldi(A,b,k) -> (V,H)} — core Arnoldi.
\item \inlinecode{def ritz(V,H) -> (theta,y,u,hnorm)} — Ritz and residual.
\item \inlinecode{def validate() -> None} — assertions on identities.
\item \inlinecode{def main() -> None} — run a deterministic test.
\end{bullets}
}
\INPUTS{
Matrix $A$ as list-of-lists, vector $b$, steps $k\le n-1$.}
\OUTPUTS{
$V$ $(n\times(k+1))$, $H$ $(k+1)\times k$, and Ritz data $(\theta,y,u,h)$.}
\FORMULA{
\[
AV_k=V_{k+1}H_k,\quad \|Au-\theta u\|=|h_{k+1,k}||e_k^\top y|.
\]
}
\textbf{SOLUTION A — From Scratch (Mathematically Explicit Implementation)}
\begin{codepy}
import numpy as np

def read_input(s):
    it = iter(s.strip().split())
    n = int(next(it)); k = int(next(it))
    A = np.array([[float(next(it)) for _ in range(n)] for _ in range(n)])
    b = np.array([float(next(it)) for _ in range(n)])
    return n, k, A, b

def arnoldi(A, b, k):
    n = A.shape[0]
    V = np.zeros((n, k+1))
    H = np.zeros((k+1, k))
    beta = np.linalg.norm(b)
    V[:, 0] = b / beta
    for j in range(k):
        w = A @ V[:, j]
        for i in range(j+1):
            H[i, j] = V[:, i].T @ w
            w = w - H[i, j] * V[:, i]
        H[j+1, j] = np.linalg.norm(w)
        if H[j+1, j] == 0.0:
            break
        V[:, j+1] = w / H[j+1, j]
    return V, H, beta

def ritz(V, H):
    k = H.shape[1]
    Hk = H[:k, :k]
    vals, vecs = np.linalg.eig(Hk)
    idx = np.argmax(vals.real)
    theta = float(vals[idx].real)
    y = vecs[:, idx].real
    y = y / np.linalg.norm(y)
    u = V[:, :k] @ y
    h = H[k, k-1]
    return theta, y, u, h

def validate():
    n, k, A, b = read_input("3 2 2 1 0 0 3 1 0 0 4 1 1 1")
    V, H, beta = arnoldi(A, b, k)
    lhs = A @ V[:, :k]
    rhs = V @ H
    assert np.allclose(lhs, rhs)
    theta, y, u, h = ritz(V, H)
    r = A @ u - theta * u
    s = abs(h) * abs(y[-1])
    assert np.isclose(np.linalg.norm(r), s, atol=1e-10)

def main():
    validate()
    print("Arnoldi and Ritz residual checks passed.")

if __name__ == "__main__":
    main()
\end{codepy}
\textbf{SOLUTION B — Library-Based (Validated Computational Shortcut)}
\begin{codepy}
import numpy as np

def read_input(s):
    return read_input  # placeholder suppressed by style; not used below
\end{codepy}
\begin{codepy}
import numpy as np

def arnoldi_lib(A, b, k):
    # Use NumPy ops as in scratch; library would be scipy.sparse in practice
    return arnoldi(A, b, k)

def validate():
    n, k, A, b = 3, 2, np.array([[2,1,0],[0,3,1],[0,0,4.]]), np.ones(3)
    V, H, beta = arnoldi_lib(A, b, k)
    assert np.allclose(A @ V[:, :k], V @ H)
    theta, y, u, h = ritz(V, H)
    r = A @ u - theta * u
    assert np.isclose(np.linalg.norm(r), abs(h)*abs(y[-1]), atol=1e-10)

def main():
    validate()
    print("Library-based Arnoldi validated.")

if __name__ == "__main__":
    main()
\end{codepy}
\COMPLEXITY{
Time $\mathcal{O}(k\,\text{mv} + k^2 n)$ with mv the cost of $A$-vector product
(dense: $\mathcal{O}(n^2)$). Space $\mathcal{O}(nk)$.}
\FAILMODES{
\begin{bullets}
\item Breakdown when $H[j+1,j]=0$; stop with exact invariant subspace.
\item Near-breakdown causes instability; use reorthogonalization.
\item Non-square or singular input arrays; validate shapes and norms.
\end{bullets}
}
\STABILITY{
\begin{bullets}
\item Loss of orthogonality; consider modified Gram-Schmidt twice.
\item Scale input vector $b$ to unit norm to avoid overflow.
\end{bullets}
}
\VALIDATION{
\begin{bullets}
\item Check $\|AV_k-V_{k+1}H_k\|_F$.
\item Check Ritz residual equality.
\end{bullets}
}
\RESULT{
Both implementations satisfy Arnoldi identity and Ritz residual factorization.}
\EXPLANATION{
Each inner product computes $h_{ij}$; subtraction enforces orthogonality.
Least-squares/eigen operations occur only on $H_k$, linking to theory.}
\EXTENSION{
Vectorize orthogonalization or use Givens rotations to track residual norms.}

\CodeDemoPage{Lanczos Tridiagonalization and Ritz Eigenvalues}
\PROBLEM{
Implement Lanczos for Hermitian $A$, form $T_k$, and compare leading Ritz
values with $\lambda(A)$.}
\API{
\begin{bullets}
\item \inlinecode{def lanczos(A,b,k) -> (Q,alpha,beta)}.
\item \inlinecode{def T_from(alpha,beta) -> T}.
\item \inlinecode{def validate() -> None}.
\end{bullets}
}
\INPUTS{
SPD or Hermitian dense $A$, start $b$, steps $k$.}
\OUTPUTS{
Orthonormal $Q$, diagonals $\alpha$, subdiagonals $\beta$, and $T_k$.}
\FORMULA{
\[
AQ_k=Q_k T_k+\beta_{k+1}q_{k+1}e_k^\top,\quad T_k=\operatorname{tridiag}.
\]
}
\textbf{SOLUTION A — From Scratch (Mathematically Explicit Implementation)}
\begin{codepy}
import numpy as np

def lanczos(A, b, k):
    n = A.shape[0]
    Q = np.zeros((n, k+1))
    alpha = np.zeros(k)
    beta = np.zeros(k+1)
    Q[:, 0] = b / np.linalg.norm(b)
    for j in range(k):
        w = A @ Q[:, j]
        if j > 0:
            w = w - beta[j] * Q[:, j-1]
        alpha[j] = Q[:, j].T @ w
        w = w - alpha[j] * Q[:, j]
        beta[j+1] = np.linalg.norm(w)
        if beta[j+1] == 0.0:
            Q = Q[:, :j+1]; alpha = alpha[:j+1]; beta = beta[:j+1]
            break
        Q[:, j+1] = w / beta[j+1]
    return Q, alpha, beta

def T_from(alpha, beta):
    k = len(alpha)
    T = np.zeros((k, k))
    for i in range(k):
        T[i, i] = alpha[i]
        if i+1 < k:
            T[i, i+1] = beta[i+1]
            T[i+1, i] = beta[i+1]
    return T

def validate():
    np.random.seed(0)
    M = np.random.randn(6, 6)
    A = (M + M.T) / 2.0
    b = np.ones(6)
    k = 4
    Q, alpha, beta = lanczos(A, b, k)
    T = T_from(alpha, beta)
    lhs = Q[:, :k].T @ (A @ Q[:, :k])
    assert np.allclose(lhs, T, atol=1e-10)

def main():
    validate()
    print("Lanczos tridiagonalization validated.")

if __name__ == "__main__":
    main()
\end{codepy}
\textbf{SOLUTION B — Library-Based (Validated Computational Shortcut)}
\begin{codepy}
import numpy as np

def lanczos_eigs_lib(A, b, k):
    Q, alpha, beta = lanczos(A, b, k)
    T = T_from(alpha, beta)
    w, _ = np.linalg.eigh(T)
    return w

def validate():
    np.random.seed(1)
    M = np.random.randn(8, 8)
    A = (M + M.T) / 2.0
    b = np.random.randn(8)
    k = 5
    wT = lanczos_eigs_lib(A, b, k)
    wA = np.linalg.eigvalsh(A)
    assert wT.size <= k
    assert wT[-1] <= wA[-1] + 1e-6

def main():
    validate()
    print("Lanczos Ritz values computed and bounded by true max eigenvalue.")

if __name__ == "__main__":
    main()
\end{codepy}
\COMPLEXITY{
Time $\mathcal{O}(k\,\text{mv} + k n)$; space $\mathcal{O}(nk)$.}
\FAILMODES{
\begin{bullets}
\item Breakdown when $\beta_{j+1}=0$; exact subspace found.
\item Non-Hermitian $A$ invalidates the three-term recurrence.
\end{bullets}
}
\STABILITY{
\begin{bullets}
\item Loss of orthogonality creates spurious eigenvalues; use reorthogonalization.
\end{bullets}
}
\VALIDATION{
\begin{bullets}
\item Check $Q^\top A Q=T$ on principal block.
\end{bullets}
}
\RESULT{
$T_k$ reproduces projected action; Ritz values approximate spectrum of $A$.}
\EXPLANATION{
The code maps Hermitian $A$ to a smaller tridiagonal $T_k$ that preserves
moments; eigenvalues of $T_k$ are Ritz values.}

\CodeDemoPage{GMRES Least Squares via Arnoldi}
\PROBLEM{
Implement GMRES($k$) using Arnoldi and verify residual monotonic decrease by
solving a small system.}
\API{
\begin{bullets}
\item \inlinecode{def gmres_k(A,b,x0,k) -> (x,res)}.
\item \inlinecode{def validate() -> None}.
\end{bullets}
}
\INPUTS{
$A\in\mathbb{R}^{n\times n}$, $b$, $x_0$, steps $k$.}
\OUTPUTS{
Approximate solution $x_k$ and residual history $res$.}
\FORMULA{
\[
y_k=\arg\min_y\|\beta e_1-H_k y\|,\ x_k=x_0+V_k y_k.
\]
}
\textbf{SOLUTION A — From Scratch (Mathematically Explicit Implementation)}
\begin{codepy}
import numpy as np

def gmres_k(A, b, x0, k):
    r0 = b - A @ x0
    beta = np.linalg.norm(r0)
    if beta == 0.0:
        return x0, [0.0]
    n = A.shape[0]
    V = np.zeros((n, k+1)); H = np.zeros((k+1, k))
    V[:, 0] = r0 / beta
    res = []
    for j in range(k):
        w = A @ V[:, j]
        for i in range(j+1):
            H[i, j] = V[:, i].T @ w
            w = w - H[i, j] * V[:, i]
        H[j+1, j] = np.linalg.norm(w)
        if H[j+1, j] != 0.0:
            V[:, j+1] = w / H[j+1, j]
        y, *_ = np.linalg.lstsq(H[:j+2, :j+1],
                                beta * np.eye(j+2, 1)[:, 0], rcond=None)
        r = np.linalg.norm(beta * np.eye(j+2, 1)[:, 0] -
                           H[:j+2, :j+1] @ y)
        res.append(r)
    xk = x0 + V[:, :k] @ y
    return xk, res

def validate():
    A = np.array([[4.,1.,0.],[1.,3.,1.],[0.,1.,2.]])
    b = np.array([1.,2.,3.])
    x0 = np.zeros(3)
    xk, res = gmres_k(A, b, x0, 3)
    assert all(res[i] >= res[i+1] - 1e-12 for i in range(len(res)-1))
    r = np.linalg.norm(b - A @ xk)
    assert abs(r - res[-1]) < 1e-9

def main():
    validate()
    print("GMRES residuals decrease and match least-squares norm.")

if __name__ == "__main__":
    main()
\end{codepy}
\textbf{SOLUTION B — Library-Based (Validated Computational Shortcut)}
\begin{codepy}
import numpy as np

def gmres_lib(A, b, x0, k):
    # Use NumPy lstsq for the small problem; Arnoldi as before
    return gmres_k(A, b, x0, k)

def validate():
    A = np.array([[2.,1.],[0.,1.]])
    b = np.array([1.,1.])
    x0 = np.zeros(2)
    xk, res = gmres_lib(A, b, x0, 2)
    r = np.linalg.norm(b - A @ xk)
    assert abs(r - res[-1]) < 1e-12

def main():
    validate()
    print("Library GMRES matches residual norm.")

if __name__ == "__main__":
    main()
\end{codepy}
\COMPLEXITY{
Time $\mathcal{O}(k\,\text{mv} + k^2 n + k^3)$; space $\mathcal{O}(nk)$.}
\FAILMODES{
\begin{bullets}
\item Rank-deficient $H_k$; use QR with Givens for robustness.
\item Zero initial residual; early return.
\end{bullets}
}
\STABILITY{
\begin{bullets}
\item Prefer incremental QR for $H_k$ to track residual precisely.
\end{bullets}
}
\VALIDATION{
\begin{bullets}
\item Compare final $\|r_k\|$ with least-squares objective at solution.
\end{bullets}
}
\RESULT{
Residuals decrease monotonically; least-squares equivalence verified.}
\EXPLANATION{
Arnoldi builds $H_k$; least squares on $H_k$ enforces GMRES optimality.}

\section{Applied Domains — Detailed End-to-End Scenarios}
\DomainPage{Machine Learning}
\SCENARIO{
Solve ridge regression normal equations $(X^\top X+\lambda I)\beta=X^\top y$
with CG/Lanczos on the SPD system without forming a Cholesky factor.}
\ASSUMPTIONS{
\begin{bullets}
\item $X\in\mathbb{R}^{n\times d}$, $\lambda>0$; system SPD.
\item Deterministic data generation and fixed random seed.
\end{bullets}
}
\WHICHFORMULA{
Lanczos-Galerkin/CG solves $T_k y=\beta e_1$ with $x_k=x_0+Q_k y$.}
\varmapStart
\var{X}{Design matrix.}
\var{y}{Response vector.}
\var{\lambda}{Ridge parameter $>0$.}
\var{A}{System $X^\top X+\lambda I$.}
\var{\beta}{Right-hand side $X^\top y$.}
\varmapEnd
\PIPELINE{
\begin{bullets}
\item Generate $(X,y)$ with seed.
\item Build $A$ implicitly via matvec.
\item Run CG and compare to direct solve.
\end{bullets}
}
\textbf{Implementation (From Scratch)}
\begin{codepy}
import numpy as np

def ridge_data(n=120, d=50, lam=1e-1, seed=0):
    rng = np.random.default_rng(seed)
    X = rng.standard_normal((n, d))
    w = rng.standard_normal(d)
    y = X @ w + 0.1 * rng.standard_normal(n)
    return X, y, lam

def matvec_A(X, lam, v):
    return X.T @ (X @ v) + lam * v

def cg_solve(X, y, lam, it=50, tol=1e-10):
    d = X.shape[1]
    A = lambda v: matvec_A(X, lam, v)
    b = X.T @ y
    x = np.zeros(d)
    r = b - A(x)
    p = r.copy()
    rs = r @ r
    hist = []
    for k in range(it):
        Ap = A(p)
        alpha = rs / (p @ Ap)
        x = x + alpha * p
        r = r - alpha * Ap
        rs_new = r @ r
        hist.append(np.sqrt(rs_new))
        if np.sqrt(rs_new) < tol:
            break
        p = r + (rs_new / rs) * p
        rs = rs_new
    return x, hist

def main():
    X, y, lam = ridge_data()
    x_cg, res = cg_solve(X, y, lam, it=200)
    x_ref = np.linalg.solve(X.T @ X + lam * np.eye(X.shape[1]), X.T @ y)
    err = np.linalg.norm(x_cg - x_ref)/np.linalg.norm(x_ref)
    print("CG rel. error:", round(err, 8), "res end:", round(res[-1], 8))

if __name__ == "__main__":
    main()
\end{codepy}
\textbf{Implementation (Library Version)}
\begin{codepy}
import numpy as np

def main():
    X, y, lam = ridge_data()
    A = X.T @ X + lam * np.eye(X.shape[1])
    b = X.T @ y
    x = np.linalg.solve(A, b)
    print("Direct solve norm:", round(np.linalg.norm(x), 6))

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{Relative error to direct solve; CG residual norm decay.}
\INTERPRET{
Lanczos/CG efficiently solves high-dimensional SPD systems from ML pipelines.}
\NEXTSTEPS{
Use preconditioning (e.g., diagonal of $X^\top X$) to accelerate convergence.}

\DomainPage{Quantitative Finance}
\SCENARIO{
Compute top $k$ risk factors (principal components) of asset returns covariance
via Lanczos tridiagonalization.}
\ASSUMPTIONS{
\begin{bullets}
\item Returns matrix $R\in\mathbb{R}^{n\times d}$; covariance SPD.
\item Fixed seed for reproducibility.
\end{bullets}
}
\WHICHFORMULA{
Ritz eigenvalues/vectors of $T_k$ approximate top eigenpairs of $\Sigma$.}
\varmapStart
\var{R}{Returns matrix.}
\var{\Sigma}{Covariance, $\Sigma=\frac{1}{n-1}R^\top R$.}
\var{Q_k,T_k}{Lanczos outputs for $\Sigma$.}
\var{k}{Number of factors sought.}
\varmapEnd
\PIPELINE{
\begin{bullets}
\item Simulate returns and form $\Sigma$ implicitly via matvec.
\item Run Lanczos to get $T_k$ and Ritz values.
\item Compare to direct eigen-decomposition. 
\end{bullets}
}
\textbf{Implementation (Full Pipeline)}
\begin{codepy}
import numpy as np

def simulate_returns(n=400, d=80, seed=0):
    rng = np.random.default_rng(seed)
    F = rng.standard_normal((n, 3))
    B = rng.standard_normal((3, d))
    E = 0.1 * rng.standard_normal((n, d))
    R = F @ B + E
    return R

def cov_matvec(R, v):
    # Sigma v = (R^T R / (n-1)) v
    n = R.shape[0]
    return (R.T @ (R @ v)) / (n - 1)

def lanczos_cov(R, k, b):
    ncol = R.shape[1]
    Q = np.zeros((ncol, k+1))
    alpha = np.zeros(k)
    beta = np.zeros(k+1)
    Q[:, 0] = b / np.linalg.norm(b)
    for j in range(k):
        w = cov_matvec(R, Q[:, j])
        if j > 0:
            w = w - beta[j] * Q[:, j-1]
        alpha[j] = Q[:, j].T @ w
        w = w - alpha[j] * Q[:, j]
        beta[j+1] = np.linalg.norm(w)
        if beta[j+1] == 0.0:
            break
        Q[:, j+1] = w / beta[j+1]
    return Q, alpha, beta

def main():
    R = simulate_returns()
    d = R.shape[1]; k = 5
    b = np.ones(d)
    Q, alpha, beta = lanczos_cov(R, k, b)
    T = np.zeros((k, k))
    for i in range(k):
        T[i, i] = alpha[i]
        if i+1 < k:
            T[i, i+1] = beta[i+1]; T[i+1, i] = beta[i+1]
    wT = np.linalg.eigvalsh(T)[::-1]
    Sigma = (R.T @ R) / (R.shape[0] - 1)
    wA = np.linalg.eigvalsh(Sigma)[::-1]
    print("Top Ritz:", np.round(wT[:3], 4), "Top true:", np.round(wA[:3], 4))

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{Compare top Ritz eigenvalues to top covariance eigenvalues.}
\INTERPRET{
Ritz values capture primary risk factors, enabling scalable factor modeling.}
\NEXTSTEPS{
Use block-Lanczos to improve accuracy and handle multiple leading factors.}

\DomainPage{Deep Learning}
\SCENARIO{
Approximate the top eigenvalue of the feature Gram matrix $G=X^\top X$ from a
single hidden-layer network's activations using Lanczos.}
\ASSUMPTIONS{
\begin{bullets}
\item Deterministic synthetic data; Gram matrix SPD.
\item No deep framework required; use NumPy.
\end{bullets}
}
\WHICHFORMULA{
Lanczos approximates the dominant eigenvalue of $G$ via Ritz values of $T_k$.}
\varmapStart
\var{X}{Activation matrix $(n\times d)$ from a linear map and nonlinearity.}
\var{G}{Gram matrix $X^\top X$.}
\var{k}{Lanczos steps for approximate spectral norm.}
\varmapEnd
\PIPELINE{
\begin{bullets}
\item Generate inputs and weights, compute activations.
\item Form implicit $G$-matvecs and run Lanczos.
\item Compare top Ritz value to $\lambda_{\max}(G)$.
\end{bullets}
}
\textbf{Implementation (End-to-End)}
\begin{codepy}
import numpy as np

def activations(n=200, d=60, seed=0):
    rng = np.random.default_rng(seed)
    X0 = rng.standard_normal((n, d))
    W = rng.standard_normal((d, d))
    Z = X0 @ W
    A = np.tanh(Z)
    return A

def G_matvec(X, v):
    return X.T @ (X @ v)

def lanczos_top(X, k, b):
    d = X.shape[1]
    Q = np.zeros((d, k+1))
    alpha = np.zeros(k)
    beta = np.zeros(k+1)
    Q[:, 0] = b / np.linalg.norm(b)
    for j in range(k):
        w = G_matvec(X, Q[:, j])
        if j > 0:
            w = w - beta[j] * Q[:, j-1]
        alpha[j] = Q[:, j].T @ w
        w = w - alpha[j] * Q[:, j]
        beta[j+1] = np.linalg.norm(w)
        if beta[j+1] == 0.0:
            break
        Q[:, j+1] = w / beta[j+1]
    T = np.diag(alpha) + np.diag(beta[2:k+1], 1) + np.diag(beta[2:k+1], -1)
    wT = np.linalg.eigvalsh(T)
    return wT[-1]

def main():
    X = activations()
    b = np.ones(X.shape[1])
    lam = lanczos_top(X, 8, b)
    G = X.T @ X
    lam_true = np.linalg.eigvalsh(G)[-1]
    print("Top Ritz:", round(lam, 6), "Top true:", round(lam_true, 6))

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{Top eigenvalue approximation error.}
\INTERPRET{
Dominant curvature or variance direction in feature space approximated quickly.}
\NEXTSTEPS{
Power iteration or block-Lanczos for multiple principal components.}

\DomainPage{Kaggle / Data Analytics}
\SCENARIO{
Compute top principal components of a standardized dataset using Lanczos, and
report explained variance ratios.}
\ASSUMPTIONS{
\begin{bullets}
\item All features numeric; standardization to zero mean and unit variance.
\item Fixed seed for data generation.
\end{bullets}
}
\WHICHFORMULA{
Ritz eigenpairs of $G=X^\top X/(n-1)$ approximate PCA components.}
\PIPELINE{
\begin{bullets}
\item Create synthetic correlated dataset and standardize.
\item Run Lanczos for top-$k$ eigenvalues of $G$.
\item Compare explained variance vs. direct SVD. 
\end{bullets}
}
\textbf{Implementation (Complete EDA Pipeline)}
\begin{codepy}
import numpy as np

def create_df(n=300, d=20, seed=0):
    rng = np.random.default_rng(seed)
    Z = rng.standard_normal((n, d))
    Z[:, 1] = 0.8 * Z[:, 0] + 0.6 * Z[:, 1]
    Z[:, 2] = 0.5 * Z[:, 0] - 0.3 * Z[:, 2]
    mu = Z.mean(axis=0); sig = Z.std(axis=0, ddof=1)
    X = (Z - mu) / sig
    return X

def lanczos_pca(X, k):
    n, d = X.shape
    def Gv(v): return (X.T @ (X @ v)) / (n - 1)
    Q = np.zeros((d, k+1)); alpha = np.zeros(k); beta = np.zeros(k+1)
    b = np.ones(d); Q[:, 0] = b / np.linalg.norm(b)
    for j in range(k):
        w = Gv(Q[:, j])
        if j > 0:
            w = w - beta[j] * Q[:, j-1]
        alpha[j] = Q[:, j].T @ w
        w = w - alpha[j] * Q[:, j]
        beta[j+1] = np.linalg.norm(w)
        if beta[j+1] == 0.0:
            break
        Q[:, j+1] = w / beta[j+1]
    T = np.diag(alpha) + np.diag(beta[2:k+1], 1) + np.diag(beta[2:k+1], -1)
    w = np.linalg.eigvalsh(T)[::-1]
    return w

def main():
    X = create_df()
    n = X.shape[0]
    wT = lanczos_pca(X, 5)
    G = (X.T @ X) / (n - 1)
    wA = np.linalg.eigvalsh(G)[::-1]
    evrT = wT / wA.sum()
    evrA = wA[:5] / wA.sum()
    print("EVR Ritz:", np.round(evrT[:3], 4))
    print("EVR True:", np.round(evrA[:3], 4))

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{Explained variance ratios of top components.}
\INTERPRET{
Lanczos approximates principal components efficiently from implicit $G$.}
\NEXTSTEPS{
Use randomized starts or block methods for improved multi-component accuracy.}

\end{document}
% !TeX program = xelatex
\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{microtype,setspace,amsmath,amssymb,mathtools,amsthm,unicode-math}
\setstretch{1.05}
\setmainfont{Latin Modern Roman}
\setmonofont{Latin Modern Mono}
\setmathfont{Latin Modern Math}
\allowdisplaybreaks[4]
\setlength{\jot}{7pt}
\setlength{\emergencystretch}{8em}
\sloppy
\usepackage{xcolor,fancyhdr,enumitem,inconsolata,listings}
\pagestyle{fancy}\fancyhf{}\lhead{\nouppercase{\leftmark}}\rhead{\thepage}
\setlength{\headheight}{26pt}
\setlength{\parindent}{0pt}
\setlength{\parskip}{8pt plus 2pt minus 1pt}
\raggedbottom
\newenvironment{BreakableEquation}{\begin{equation}\begin{aligned}}{\end{aligned}\end{equation}}
\newenvironment{BreakableEquation*}{\begin{equation*}\begin{aligned}}{\end{aligned}\end{equation*}}
\newenvironment{tightalign}{\begingroup\small\allowdisplaybreaks\begin{align}}{\end{align}\endgroup}
\providecommand{\enumlistm}{enumitem}
\newenvironment{minted}[2][]{%
  \lstset{style=code,language=#2,#1}\begin{lstlisting}%
}{\end{lstlisting}}
\newcommand{\inputminted}[3][]{\begin{lstlisting}\end{lstlisting}}
\newlist{bullets}{itemize}{1}
\setlist[bullets]{label=--,leftmargin=1.6em,itemsep=2pt,topsep=4pt}
\newcommand{\varmapStart}{\textbf{VARIABLE MAPPING:}\par\begin{bullets}}
\newcommand{\var}[2]{\item $#1$ — #2}
\newcommand{\varmapEnd}{\end{bullets}}
\newcommand{\glossx}[6]{%
  \textbf{#1}\par
  \begin{bullets}
    \item \textbf{What:} #2
    \item \textbf{Why:} #3
    \item \textbf{How:} #4
    \item \textbf{ELI5:} #5
    \item \textbf{Pitfall/Example:} #6
  \end{bullets}
}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{claim}[theorem]{Claim}
\lstdefinestyle{code}{
  basicstyle=\ttfamily\footnotesize,
  backgroundcolor=\color{black!02},
  frame=single,
  numbers=left, numberstyle=\tiny, numbersep=8pt,
  breaklines=true, breakatwhitespace=true,
  tabsize=4, showstringspaces=false,
  upquote=true, keepspaces=true, columns=fullflexible,
  literate=
    {–}{{-}}1
    {—}{{-}}1
    {…}{{...}}1
    {≤}{{\ensuremath{\le}}}1
    {≥}{{\ensuremath{\ge}}}1
    {≠}{{\ensuremath{\ne}}}1
    {≈}{{\ensuremath{\approx}}}1
    {±}{{\ensuremath{\pm}}}1
    {→}{{\ensuremath{\to}}}1
    {←}{{\ensuremath{\leftarrow}}}1
    {∞}{{\ensuremath{\infty}}}1
    {√}{{\ensuremath{\sqrt{\ }}}}1
    {×}{{\ensuremath{\times}}}1
    {÷}{{\ensuremath{\div}}}1
}
\lstnewenvironment{codepy}[1][]%
  {\lstset{style=code,language=Python,#1}}%
  {}
\newcommand{\inlinecode}[1]{\lstinline[style=code]!#1!}
\newcommand{\LF}[2]{\par\noindent\textbf{#1:}~#2\par}
\newcommand{\WHAT}[1]{\LF{WHAT}{#1}}
\newcommand{\WHY}[1]{\LF{WHY}{#1}}
\newcommand{\HOW}[1]{\LF{HOW}{#1}}
\newcommand{\ELI}[1]{\LF{ELI5}{#1}}
\newcommand{\SCOPE}[1]{\LF{SCOPE}{#1}}
\newcommand{\CONFUSIONS}[1]{\LF{COMMON CONFUSIONS}{#1}}
\newcommand{\APPLICATIONS}[1]{\LF{APPLICATIONS}{#1}}
\newcommand{\FORMULA}[1]{\LF{FORMULA}{#1}}
\newcommand{\CANONICAL}[1]{\LF{CANONICAL FORM}{#1}}
\newcommand{\PRECONDS}[1]{\LF{PRECONDITIONS}{#1}}
\newcommand{\DERIVATION}[1]{\LF{DERIVATION}{#1}}
\newcommand{\EQUIV}[1]{\LF{EQUIVALENT FORMS}{#1}}
\newcommand{\LIMITS}[1]{\LF{LIMIT CASES}{#1}}
\newcommand{\INPUTS}[1]{\LF{INPUTS}{#1}}
\newcommand{\OUTPUTS}[1]{\LF{OUTPUTS}{#1}}
\newcommand{\RESULT}[1]{\LF{RESULT}{#1}}
\newcommand{\INTUITION}[1]{\LF{INTUITION}{#1}}
\newcommand{\PITFALLS}[1]{\LF{PITFALLS}{#1}}
\newcommand{\MODEL}[1]{\LF{CANONICAL MATH MODEL}{#1}}
\newcommand{\ASSUMPTIONS}[1]{\LF{ASSUMPTIONS}{#1}}
\newcommand{\WHICHFORMULA}[1]{\LF{WHICH FORMULA \& WHY}{#1}}
\newcommand{\GOVERN}[1]{\LF{GOVERNING EQUATION(S)}{#1}}
\newcommand{\UNITCHECK}[1]{\LF{UNIT CHECK}{#1}}
\newcommand{\EDGECASES}[1]{\LF{EDGE CASES}{#1}}
\newcommand{\ALTERNATE}[1]{\LF{ALTERNATE APPROACH (sketch)}{#1}}
\newcommand{\PROBLEM}[1]{\LF{PROBLEM}{#1}}
\newcommand{\API}[1]{\LF{API}{#1}}
\newcommand{\COMPLEXITY}[1]{\LF{COMPLEXITY}{#1}}
\newcommand{\FAILMODES}[1]{\LF{FAILURE MODES}{#1}}
\newcommand{\STABILITY}[1]{\LF{NUMERICAL STABILITY}{#1}}
\newcommand{\VALIDATION}[1]{\LF{VALIDATION}{#1}}
\newcommand{\EXPLANATION}[1]{\LF{EXPLANATION}{#1}}
\newcommand{\SCENARIO}[1]{\LF{SCENARIO}{#1}}
\newcommand{\PIPELINE}[1]{\LF{PIPELINE STEPS}{#1}}
\newcommand{\METRICS}[1]{\LF{METRICS}{#1}}
\newcommand{\INTERPRET}[1]{\LF{INTERPRETATION}{#1}}
\newcommand{\NEXTSTEPS}[1]{\LF{LIMITATIONS \& NEXT STEPS}{#1}}
\usepackage{titlesec}
\titleformat{\section}{\Large\bfseries}{\thesection}{0.6em}{}
\titlespacing*{\section}{0pt}{*2}{*1}
\usepackage{etoolbox}
\pretocmd{\section}{\clearpage}{}{}
\newcommand{\FormulaPage}[2]{%
  \clearpage
  \section*{Formula #1 — #2}%
  \addcontentsline{toc}{section}{Formula #1 — #2}%
}
\newcommand{\ProblemPage}[2]{%
  \clearpage
  \subsection*{Problem #1: #2}%
  \addcontentsline{toc}{subsection}{Problem #1: #2}%
}
\newcommand{\CodeDemoPage}[1]{%
  \clearpage
  \subsection*{Coding Demo: #1}%
  \addcontentsline{toc}{subsection}{Coding Demo: #1}%
}
\newcommand{\DomainPage}[1]{%
  \clearpage
  \subsection*{#1 (End-to-End)}%
  \addcontentsline{toc}{subsection}{#1 (End-to-End)}%
}
\begin{document}
\title{Comprehensive Study Sheet — Positive (Semi)Definite Matrices}
\date{\today}
\maketitle
\tableofcontents
\clearpage
\section{Concept Overview}
\WHAT{
A real symmetric matrix $A\in\mathbb{R}^{n\times n}$ is positive semidefinite
(PSD), written $A\succeq 0$, if $x^\top A x\ge 0$ for all $x\in\mathbb{R}^n$.
It is positive definite (PD), $A\succ 0$, if $x^\top A x>0$ for all nonzero
$x$. Domain: the vector space of real symmetric matrices with the Loewner
partial order. Codomain: $\mathbb{R}$ via quadratic forms and spectra.
}
\WHY{
PSD matrices encode energy, variance, and squared distances. They form cones
closed under addition and congruence and are central to convex optimization,
statistics (covariance matrices), kernels (Gram matrices), stability analysis,
and inequalities (Rayleigh quotients, Schur complement, Hadamard inequality).
}
\HOW{
1. Start from quadratic forms $x^\top A x$ and symmetry. 2. Use the spectral
theorem to diagonalize $A=Q\Lambda Q^\top$. 3. Nonnegativity of all quadratic
forms is equivalent to all eigenvalues being nonnegative. 4. Obtain constructive
factorizations $A=B^\top B$ (Gram) or $A=LL^\top$ (Cholesky for PD) to compute
and reason about PSD structure.
}
\ELI{
Think of $A$ as a machine that measures energy of a vector $x$ via $x^\top A x$.
PSD means the energy is never negative; PD means it is strictly positive unless
the vector is zero.
}
\SCOPE{
Real symmetric matrices; complex case uses Hermitian matrices and $x^*Ax$.
Semidefinite allows zero eigenvalues and nullspaces. Non-symmetric matrices are
not PSD in this sense. Edge cases: rank-deficient PSD (singular), strict PD
requires all principal minors positive and invertibility.
}
\CONFUSIONS{
PSD vs. PD: PD implies invertible; PSD may be singular. Symmetric part
$(A+A^\top)/2$ controls quadratic form; non-symmetric $A$ can have negative
$x^\top A x$ even if its symmetric part is PSD. Nonnegativity of entries does
not imply PSD; PSD is about eigenvalues/quadratic forms, not elementwise signs.
}
\APPLICATIONS{
\begin{bullets}
\item Mathematical foundations: convex cones, Loewner order, spectral theory.
\item Computational modeling: quadratic programs and least squares.
\item Physical/engineering: energies, stiffness, and Lyapunov functions.
\item Statistics/ML: covariance and kernel (Gram) matrices.
\end{bullets}
}
\textbf{ANALYTIC STRUCTURE.}
The PSD cone is convex, closed, self-dual, and invariant under orthogonal
congruence $A\mapsto Q^\top A Q$. It is a proper cone defining the Loewner
partial order $A\succeq B \iff A-B\succeq 0$.
\textbf{CANONICAL LINKS.}
Spectral theorem, Rayleigh-Ritz variational characterization, Schur complement,
Cholesky factorization, Schur product theorem, Hadamard inequality.
\textbf{PROBLEM-TYPE RECOGNITION HEURISTICS.}
\begin{bullets}
\item Presence of quadratic form $x^\top A x$.
\item Block matrices and conditional variance hint at Schur complement.
\item Kernel/Gram matrices suggest PSD via inner products.
\item Determinants vs. diagonals suggest Hadamard inequality.
\end{bullets}
\textbf{SOLUTION STRATEGY BLUEPRINT.}
\begin{bullets}
\item Translate to quadratic forms or eigenvalues via spectral theorem.
\item Choose an equivalent PSD characterization suitable for the task.
\item Use congruence, Schur complements, or factorization to simplify.
\item Conclude via nonnegativity, eigenvalue bounds, or determinants.
\end{bullets}
\textbf{CONCEPTUAL INVARIANTS.}
Nonnegativity of $x^\top A x$, eigenvalue signs, Loewner order, total variance.
\textbf{EDGE INTUITION.}
As $\lambda_{\min}\to 0^+$, PD approaches PSD with growing flat directions;
Rayleigh quotient concentrates near extreme eigenvalues as $\|x\|\to 1$.

\clearpage
\section{Glossary}
\glossx{Positive Semidefinite (PSD) Matrix}
{Symmetric $A$ with $x^\top A x\ge 0$ for all $x$.}
{Encodes energies, variances, and kernel similarities; central in convexity.}
{Diagonalize $A=Q\Lambda Q^\top$, check $\Lambda\succeq 0$, or factor $A=B^\top B$.}
{A machine that never outputs negative energy when fed any vector.}
{Do not confuse PSD with elementwise nonnegativity; a PSD matrix may have
negative off-diagonals.}
\glossx{Loewner Order}
{Partial order $A\succeq B$ iff $A-B\succeq 0$ on symmetric matrices.}
{Compares energies: $x^\top A x\ge x^\top B x$ for all $x$.}
{Check PSD of difference, or compare eigenvalues under congruent transforms.}
{Like saying one blanket is at least as warm as another for every person.}
{Not a total order; two matrices can be incomparable.}
\glossx{Schur Complement}
{For $M=\begin{bmatrix}A&B\\B^\top&C\end{bmatrix}$ with $A\succ 0$, the Schur
complement is $S=C-B^\top A^{-1}B$.}
{Characterizes PSD of block matrices and conditional variances.}
{Complete the square in the quadratic form $[x;y]^\top M [x;y]$.}
{Measuring extra energy of $y$ after removing what $A$ already accounts for.}
{Requires symmetry and $A\succ 0$ (or $C\succ 0$) for the classic formulas.}
\glossx{Cholesky Factorization}
{For $A\succ 0$, $A=LL^\top$ with $L$ lower triangular and positive diagonal.}
{Stable way to solve PD linear systems and certify positive definiteness.}
{Compute $L_{ii}=\sqrt{A_{ii}-\sum_{k<i}L_{ik}^2}$ and rows recursively.}
{Like finding orthogonal axes where the energy is sum of squares.}
{Fails if $A$ is not PD; pivoted or LDL$^\top$ variants handle PSD.}

\clearpage
\section{Symbol Ledger}
\varmapStart
\var{A,B,C}{Real symmetric matrices in $\mathbb{R}^{n\times n}$.}
\var{M}{Block symmetric matrix.}
\var{Q}{Orthogonal matrix, $Q^\top Q=I$.}
\var{\Lambda}{Diagonal matrix of eigenvalues.}
\var{\lambda_i}{Eigenvalues of a symmetric matrix, sorted ascending.}
\var{x,y}{Vectors in $\mathbb{R}^n$.}
\var{L}{Cholesky factor (lower triangular).}
\var{S}{Schur complement.}
\var{\Sigma}{Covariance matrix (PSD).}
\var{K}{Kernel/Gram matrix, $K_{ij}=\langle \phi_i,\phi_j\rangle$.}
\var{n,d}{Dimensions: $n$ size, $d$ feature dimension.}
\var{\succeq}{Loewner order on symmetric matrices.}
\var{\odot}{Hadamard (elementwise) product.}
\varmapEnd

\clearpage
\section{Formula Canon — One Formula Per Page}
\FormulaPage{1}{Equivalent Characterizations of PSD}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
For $A=A^\top\in\mathbb{R}^{n\times n}$, the following are equivalent:
\[
\text{(i) }A\succeq 0 \quad \Longleftrightarrow \quad
\text{(ii) }\lambda_i(A)\ge 0\ \forall i \quad \Longleftrightarrow \quad
\text{(iii) }\exists B:\ A=B^\top B
\]
and if $A\succ 0$ then additionally $A=LL^\top$ with $L$ lower triangular with
positive diagonal (Cholesky), and all leading principal minors are positive.
\WHAT{
This establishes multiple interchangeable criteria to certify and compute with
PSD/PD matrices through quadratic forms, spectra, and factorizations.
}
\WHY{
Different tasks demand different views: spectra for bounds, factorizations for
computation, and quadratic forms for inequalities and ordering.
}
\FORMULA{
\[
A\succeq 0 \iff x^\top A x\ge 0\ \forall x
\iff A=Q\Lambda Q^\top,\ \Lambda\succeq 0
\iff \exists B\text{ with }A=B^\top B.
\]
For $A\succ 0$: $A=LL^\top$ with $L$ unique with $L_{ii}>0$.
}
\CANONICAL{
Symmetric $A$ over $\mathbb{R}$ (Hermitian over $\mathbb{C}$). Eigenvalues real
and orthonormal eigenbasis exists by the spectral theorem. Cholesky requires PD.
}
\PRECONDS{
\begin{bullets}
\item Symmetry (Hermitian in complex case).
\item For Cholesky: strict positivity of all leading principal minors.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
(Spectral theorem) For symmetric $A$, there exists orthogonal $Q$ and real
diagonal $\Lambda$ with $A=Q\Lambda Q^\top$.
\end{lemma}
\begin{proof}
Symmetric matrices are normal and have real eigenvalues with orthogonal
eigenvectors; Gram-Schmidt yields an orthonormal basis. Collect eigenvectors in
$Q$ and eigenvalues in $\Lambda$ to get $A=Q\Lambda Q^\top$. \qedhere
\end{proof}
\DERIVATION{
\begin{align*}
\text{(i)\,$\Rightarrow$ (ii):}\quad
& A=Q\Lambda Q^\top,\ x^\top A x=\sum_i \lambda_i (q_i^\top x)^2\ge 0\\
& \text{Choosing }x=q_j\text{ gives }\lambda_j\ge 0.\\
\text{(ii)\,$\Rightarrow$ (iii):}\quad
& \Lambda=\operatorname{diag}(\lambda_i),\ \lambda_i\ge 0,\ \Lambda=R^\top R\\
& \text{with }R=\operatorname{diag}(\sqrt{\lambda_i}).\ \text{Then }
A=Q\Lambda Q^\top=(RQ^\top)^\top(RQ^\top).\\
\text{(iii)\,$\Rightarrow$ (i):}\quad
& x^\top A x=x^\top B^\top B x=\|Bx\|^2\ge 0.\\
\text{PD \& Cholesky:}\quad
& A\succ 0 \Rightarrow \text{Gauss elimination without pivoting succeeds,}\\
& \text{yielding }A=LL^\top,\ L_{ii}>0\text{ and unique.}
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item If computing: attempt Cholesky; failure indicates not PD.
\item If bounding: diagonalize or use Rayleigh quotients.
\item If modeling: construct $B$ so $A=B^\top B$ (Gram/kernel).
\end{bullets}
\EQUIV{
\begin{bullets}
\item $A\succeq 0 \iff \exists X$ with $A=\sum_k x_k x_k^\top$.
\item $A\succ 0 \iff \exists L$ invertible lower triangular with $A=LL^\top$.
\item $A\succeq 0 \iff \forall y,\ \min_x \tfrac12 x^\top A x - y^\top x
= -\tfrac12 y^\top A^\dagger y$.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item If some $\lambda_i=0$, $A$ is singular and only LDL$^\top$ exists.
\item Non-symmetric matrices can have real nonnegative eigenvalues yet not be
PSD in the quadratic-form sense.
\end{bullets}
}
\INPUTS{$A\in\mathbb{R}^{n\times n}$ symmetric.}
\DERIVATION{
\begin{align*}
\text{Example: }A&=\begin{bmatrix}2&1\\1&2\end{bmatrix},\
\lambda=3,1\ (\ge 0).\\
A&=\begin{bmatrix}\sqrt{2}&0\\ \tfrac{1}{\sqrt{2}}&\sqrt{\tfrac{3}{2}}
\end{bmatrix}
\begin{bmatrix}\sqrt{2}&\tfrac{1}{\sqrt{2}}\\0&\sqrt{\tfrac{3}{2}}\end{bmatrix}
=LL^\top.
\end{align*}
}
\RESULT{
All criteria confirm $A\succeq 0$; since eigenvalues are positive, $A\succ 0$
and admits unique Cholesky factor $L$ above.
}
\UNITCHECK{
Quadratic form yields a scalar; eigenvalues are real; factor dimensions match.
}
\PITFALLS{
\begin{bullets}
\item Forgetting symmetry invalidates the equivalence with spectra.
\item Confusing leading principal minors with all principal minors; for PD,
leading principal minors must be positive.
\end{bullets}
}
\INTUITION{
Diagonalization rotates coordinates so energy decomposes into weighted squares.
Nonnegative weights imply nonnegative energy.
}
\CANONICAL{
\begin{bullets}
\item $A\succeq 0 \iff \exists Q,\Lambda\succeq 0: A=Q\Lambda Q^\top$.
\item $A\succeq 0 \iff \exists B: A=B^\top B$.
\end{bullets}
}

\FormulaPage{2}{Schur Complement and Block PSD}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
For $M=\begin{bmatrix}A&B\\B^\top&C\end{bmatrix}$ symmetric with $A\succ 0$,
\[
M\succeq 0 \iff A\succ 0\ \text{ and }\ S=C-B^\top A^{-1}B\succeq 0.
\]
Symmetrically, if $C\succ 0$, then $M\succeq 0 \iff C\succ 0$ and
$A-BC^{-1}B^\top\succeq 0$.
\WHAT{
Characterizes PSD of a block matrix via PSD of a Schur complement.
}
\WHY{
Appears in conditional variance, Gaussian elimination, and constrained
optimization; enables iterative tests and matrix inequalities.
}
\FORMULA{
\[
[x;y]^\top M [x;y]
= (x+A^{-1}By)^\top A (x+A^{-1}By) + y^\top S y.
\]
Hence $M\succeq 0 \iff A\succ 0$ and $S\succeq 0$.
}
\CANONICAL{
$A$ and $C$ are square blocks; $A\succ 0$ (or $C\succ 0$) ensures $A^{-1}$
exists. All matrices real and symmetric; blocks compatible in size.
}
\PRECONDS{
\begin{bullets}
\item Symmetry of $M$ and $A\succ 0$ (or $C\succ 0$).
\item Conformable block sizes.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
(Completion of squares) For invertible $A$, the identity
\[
[x;y]^\top \begin{bmatrix}A&B\\B^\top&C\end{bmatrix} [x;y]
=(x+A^{-1}By)^\top A (x+A^{-1}By)+y^\top (C-B^\top A^{-1}B) y
\]
holds for all $x,y$.
\end{lemma}
\begin{proof}
Expand the right-hand side:
\begin{align*}
&x^\top A x + 2y^\top B^\top x + y^\top B^\top A^{-1} A A^{-1} B y
+ y^\top C y - y^\top B^\top A^{-1} B y\\
&= x^\top A x + 2 x^\top B y + y^\top C y,
\end{align*}
which equals $[x;y]^\top M [x;y]$. \qedhere
\end{proof}
\DERIVATION{
\begin{align*}
M\succeq 0 &\iff [x;y]^\top M [x;y]\ge 0\ \forall x,y\\
&\iff (x+A^{-1}By)^\top A (x+A^{-1}By) + y^\top S y \ge 0\ \forall x,y\\
&\Rightarrow A\succ 0\ \text{ and } S\succeq 0\\
&\Leftarrow \text{If }A\succ 0,S\succeq 0,\ \text{sum of two PSD forms is PSD.}
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Check PD of $A$ (or $C$); compute $S$.
\item Test $S\succeq 0$ by eigenvalues or factorization.
\item Conclude PSD of $M$; use $S$ as conditional variance component.
\end{bullets}
}
\EQUIV{
\begin{bullets}
\item $M\succeq 0 \iff A\succeq 0$ and $S\succeq 0$ with $A$ invertible on
$\operatorname{range}(B)$ (generalized Schur complement).
\item If $A\succ 0$, then $M\succeq 0 \iff C\succeq B^\top A^{-1}B$.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item If $A$ is only PSD and singular, use generalized inverse and range
conditions; the simple equivalence may fail without them.
\item Non-symmetric blocks break the derivation.
\end{bullets}
}
\INPUTS{$A\succ 0$, $B$, $C=C^\top$.}
\DERIVATION{
\begin{align*}
\text{Example: }A&=\begin{bmatrix}2&0\\0&1\end{bmatrix},\ 
B=\begin{bmatrix}1\\1\end{bmatrix},\ C=[3].\\
S&=3-\begin{bmatrix}1&1\end{bmatrix}
\begin{bmatrix}1/2&0\\0&1\end{bmatrix}\begin{bmatrix}1\\1\end{bmatrix}
=3-(1/2+1)=3-1.5=1.5\ (\ge 0).\\
\Rightarrow\ &M\succeq 0.
\end{align*}
}
\RESULT{
$M\succeq 0$ because $A\succ 0$ and $S=1.5\ge 0$.}
\UNITCHECK{
All quadratic forms produce scalars; $A^{-1}$ exists; dimensions align.
}
\PITFALLS{
\begin{bullets}
\item Forgetting symmetry; $B$ must match the off-diagonal symmetry.
\item Using $A^{-1}$ when $A$ is singular; use pseudoinverse with care.
\end{bullets}
}
\INTUITION{
$A$ measures energy in $x$; the Schur complement is the leftover energy in $y$
after optimally compensating through $x$.
}
\CANONICAL{
\begin{bullets}
\item $M\succeq 0 \iff A\succ 0$ and $C\succeq B^\top A^{-1}B$.
\item Dual statement with roles of $A$ and $C$ swapped.
\end{bullets}
}

\FormulaPage{3}{Rayleigh-Ritz and Eigenvalue Bounds}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
For symmetric $A$ with ordered eigenvalues
$\lambda_1\le \cdots \le \lambda_n$,
\[
\lambda_1=\min_{\|x\|=1} x^\top A x,\quad
\lambda_n=\max_{\|x\|=1} x^\top A x.
\]
If $A\succeq 0$, then for all $x$:
\[
\lambda_1 \|x\|^2 \le x^\top A x \le \lambda_n \|x\|^2.
\]
\WHAT{
Variational characterization of extreme eigenvalues and bounds on quadratic
forms in terms of eigenvalues.
}
\WHY{
Provides tight bounds, condition-number insights, and convexity certificates.
}
\FORMULA{
\[
\lambda_{\min}(A)=\min_{\|x\|=1} x^\top A x,\ \
\lambda_{\max}(A)=\max_{\|x\|=1} x^\top A x.
\]
}
\CANONICAL{
Symmetric $A$; unit sphere constraint; orthogonal diagonalization $A=Q\Lambda
Q^\top$ used to reduce the problem to coordinates of $x$ in eigenbasis.
}
\PRECONDS{
\begin{bullets}
\item Symmetry for orthogonal diagonalization.
\item Compactness of unit sphere ensures extrema exist.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
If $A=Q\Lambda Q^\top$ and $x=Qy$ with $\|x\|=\|y\|$, then
$x^\top A x=\sum_i \lambda_i y_i^2$.
\end{lemma}
\begin{proof}
$x^\top A x = y^\top \Lambda y = \sum_i \lambda_i y_i^2$ because $Q$ is
orthogonal and preserves norms and inner products. \qedhere
\end{proof}
\DERIVATION{
\begin{align*}
\min_{\|x\|=1} x^\top A x
&=\min_{\|y\|=1} \sum_i \lambda_i y_i^2
\ge \lambda_1 \sum_i y_i^2 = \lambda_1,\\
\max_{\|x\|=1} x^\top A x
&=\max_{\|y\|=1} \sum_i \lambda_i y_i^2
\le \lambda_n \sum_i y_i^2 = \lambda_n.
\end{align*}
Equality is attained at $y=e_1$ and $y=e_n$ (eigenvectors), respectively.
For general $x$, write $x=\|x\|u$ with $\|u\|=1$, then
$\lambda_1 \|x\|^2 \le x^\top A x \le \lambda_n \|x\|^2$.
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Diagonalize or work in eigenbasis.
\item Reduce to weighted sum of squares and bound by extreme weights.
\item Attain bounds by choosing eigenvectors.
\end{bullets}
}
\EQUIV{
\begin{bullets}
\item $\lambda_{\max}(A)=\|A\|_{2}$ and
$\lambda_{\min}(A)=1/\|A^{-1}\|_{2}$ for $A\succ 0$.
\item Loewner order: $B\succeq A \Rightarrow
\lambda_{\max}(B)\ge \lambda_{\max}(A)$, similarly for $\lambda_{\min}$.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item If $A$ is indefinite, the bounds hold with negative $\lambda_1$.
\item For singular PSD, $\lambda_{\min}=0$ and only a lower semibound holds.
\end{bullets}
}
\INPUTS{$A=A^\top$, unit vector constraint on $x$.}
\DERIVATION{
\begin{align*}
\text{Example: }A&=\begin{bmatrix}3&1\\1&2\end{bmatrix}.\\
\lambda&=\frac{5\pm \sqrt{5}}{2}\approx 3.618,\ 1.382.\\
\text{For }x&=(1,0),\ x^\top A x=3\in[1.382,3.618].\\
\text{For }x&=\tfrac{1}{\sqrt{2}}(1,1),\
x^\top A x=\tfrac{1}{2}(3+2+2)=3.5.
\end{align*}
}
\RESULT{
Quadratic forms are bounded between extreme eigenvalues times squared norms;
examples satisfy the bounds.
}
\UNITCHECK{
Scalars on both sides; norms are homogeneous as required.
}
\PITFALLS{
\begin{bullets}
\item Forgetting the unit-norm constraint; Rayleigh quotient uses normalization.
\item Misordering eigenvalues invalidates inequality directions.
\end{bullets}
}
\INTUITION{
In eigen-coordinates, energy is a convex combination of eigenvalues; the min
and max occur by putting all mass on a single extreme eigen-direction.
}
\CANONICAL{
\begin{bullets}
\item $x^\top A x = \|x\|^2 \cdot \sum_i \lambda_i \alpha_i^2$ with
$\sum \alpha_i^2=1$.
\item Bounds by $\lambda_{\min}$ and $\lambda_{\max}$.
\end{bullets}
}

\FormulaPage{4}{Schur Product Theorem (Hadamard PSD)}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
If $A\succeq 0$ and $B\succeq 0$ are symmetric,
then their Hadamard product $A\odot B$ is PSD:
\[
A\succeq 0,\ B\succeq 0 \ \Rightarrow\ A\odot B \succeq 0.
\]
\WHAT{
Elementwise product of PSD matrices (same size) is PSD.
}
\WHY{
Underlies covariance of elementwise products, entrywise positivity preservers,
and kernel composition (pointwise product of kernels).
}
\FORMULA{
\[
A=X^\top X,\ B=Y^\top Y \ \Rightarrow\ A\odot B
= \sum_{k} (x_k x_k^\top)\odot \sum_{\ell} (y_\ell y_\ell^\top)
= \sum_{k,\ell} (x_k\odot y_\ell)(x_k\odot y_\ell)^\top.
\]
}
\CANONICAL{
PSD via Gram factorization: represent $A$ and $B$ as sums of rank-one PSDs,
use bilinearity of Hadamard product on rank-one outer products.
}
\PRECONDS{
\begin{bullets}
\item Symmetric PSD inputs of the same dimension.
\item Real case; complex case uses conjugation appropriately.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
For vectors $u,v$, $(uu^\top)\odot (vv^\top)=(u\odot v)(u\odot v)^\top$.
\end{lemma}
\begin{proof}
Entry $(i,j)$ of left-hand side is $(u_i u_j)(v_i v_j)=(u_i v_i)(u_j v_j)$,
which equals the $(i,j)$ entry of $(u\odot v)(u\odot v)^\top$. \qedhere
\end{proof}
\DERIVATION{
\begin{align*}
A&=\sum_k x_k x_k^\top,\quad B=\sum_\ell y_\ell y_\ell^\top,\\
A\odot B
&=\sum_{k,\ell} (x_k x_k^\top)\odot (y_\ell y_\ell^\top)
=\sum_{k,\ell} (x_k\odot y_\ell)(x_k\odot y_\ell)^\top\succeq 0.
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Factor $A$ and $B$ as Gram matrices.
\item Use the lemma to express $A\odot B$ as a sum of rank-one PSDs.
\item Nonnegative sum of PSDs is PSD.
\end{bullets}
}
\EQUIV{
\begin{bullets}
\item If $K_1,K_2$ are positive semidefinite kernels, then
$K(x,y)=K_1(x,y)K_2(x,y)$ is PSD.
\item Diagonal dominance: $A\odot I=\operatorname{diag}(A_{ii})\succeq 0$.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item Not all entrywise functions preserve PSD; positivity preservers require
Schoenberg/Loewner conditions.
\item Different sizes or non-symmetric inputs invalidate the statement.
\end{bullets}
}
\INPUTS{$A,B\in\mathbb{R}^{n\times n}$, $A\succeq 0$, $B\succeq 0$.}
\DERIVATION{
\begin{align*}
\text{Example: }A&=\begin{bmatrix}2&1\\1&1\end{bmatrix},\
B=\begin{bmatrix}3&2\\2&1\end{bmatrix}.\\
A\odot B&=\begin{bmatrix}6&2\\2&1\end{bmatrix},\
\lambda=\frac{7\pm \sqrt{13}}{2}\ge 0.\\
\Rightarrow\ &A\odot B\succeq 0.
\end{align*}
}
\RESULT{
$A\odot B$ is PSD; example eigenvalues are nonnegative.
}
\UNITCHECK{
Hadamard product preserves shape; quadratic forms stay scalar.
}
\PITFALLS{
\begin{bullets}
\item Confusing Hadamard with matrix product; the theorem is entrywise.
\item Assuming arbitrary entrywise functions preserve PSD; they generally do not.
\end{bullets}
}
\INTUITION{
Multiplying similarities entrywise corresponds to combining feature maps via
tensor products; the result is still an inner product.
}
\CANONICAL{
\begin{bullets}
\item $(uu^\top)\odot (vv^\top)=(u\odot v)(u\odot v)^\top$.
\item Sums of rank-one PSDs remain PSD.
\end{bullets}
}

\clearpage
\section{10 Exhaustive Problems and Solutions}
\ProblemPage{1}{Testing PSD and Cholesky on a 3x3 Matrix}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Decide if $A=\begin{bmatrix}4&2&0\\2&3&1\\0&1&1\end{bmatrix}$ is PSD or PD.
If PD, compute its Cholesky factor.
\PROBLEM{
Use eigenvalues, quadratic forms, and leading principal minors to certify PD,
then compute $L$ with $A=LL^\top$. Verify with a numeric check.
}
\MODEL{
\[
A=A^\top\in\mathbb{R}^{3\times 3},\quad A\succ 0 \iff \Delta_k>0,\ k=1,2,3,
\]
where $\Delta_k$ are leading principal minors; $A=LL^\top$ with $L$ lower.
}
\ASSUMPTIONS{
\begin{bullets}
\item Real arithmetic, exact symmetry.
\item Nonzero pivots ensure Cholesky without pivoting.
\end{bullets}
}
\varmapStart
\var{A}{Input symmetric matrix.}
\var{L}{Lower-triangular Cholesky factor.}
\var{\Delta_k}{Leading principal minors $\det(A_{1:k,1:k})$.}
\varmapEnd
\WHICHFORMULA{
Formula 1 (Characterizations) and Cholesky factorization for PD matrices.
}
\GOVERN{
\[
A\succ 0 \iff \Delta_1>0,\ \Delta_2>0,\ \Delta_3>0,\quad A=LL^\top.
\]
}
\INPUTS{$A=\begin{bmatrix}4&2&0\\2&3&1\\0&1&1\end{bmatrix}$.}
\DERIVATION{
\begin{align*}
\Delta_1&=4>0,\quad
\Delta_2=\det\begin{bmatrix}4&2\\2&3\end{bmatrix}=12-4=8>0,\\
\Delta_3&=\det A
=4\det\begin{bmatrix}3&1\\1&1\end{bmatrix}
-2\det\begin{bmatrix}2&1\\0&1\end{bmatrix}\\
&=4(3-1)-2(2-0)=4\cdot 2 - 4=4>0.\\
\Rightarrow &\ A\succ 0.\\
\text{Cholesky: }&L_{11}=\sqrt{4}=2,\ L_{21}=2/2=1,\ L_{31}=0.\\
L_{22}&=\sqrt{3-1^2}=\sqrt{2},\ L_{32}=(1-0\cdot 1)/\sqrt{2}=1/\sqrt{2}.\\
L_{33}&=\sqrt{1-(0^2+(1/\sqrt{2})^2)}=\sqrt{1-1/2}=\sqrt{1/2}=1/\sqrt{2}.\\
L&=\begin{bmatrix}2&0&0\\1&\sqrt{2}&0\\0&1/\sqrt{2}&1/\sqrt{2}\end{bmatrix}.
\end{align*}
}
\RESULT{
$A\succ 0$ and
$A=LL^\top$ with
$L=\begin{bmatrix}2&0&0\\1&\sqrt{2}&0\\0&1/\sqrt{2}&1/\sqrt{2}\end{bmatrix}$.}
\UNITCHECK{
$L$ is lower triangular; $LL^\top$ reproduces $A$ entrywise.
}
\EDGECASES{
\begin{bullets}
\item If any $\Delta_k=0$, PD fails; PSD may still hold.
\item Near-zero pivots require pivoted LDL$^\top$ numerics.
\end{bullets}
}
\ALTERNATE{
Check eigenvalues numerically; or verify $x^\top A x\ge 0$ via Rayleigh bounds.
}
\VALIDATION{
\begin{bullets}
\item Multiply $LL^\top$ to recover $A$.
\item Numerically compute eigenvalues: all positive.
\end{bullets}
}
\INTUITION{
Strictly positive diagonal and moderate off-diagonal coupling yield positive
energy for all directions.
}
\CANONICAL{
\begin{bullets}
\item PD iff all leading principal minors are positive.
\item Unique Cholesky exists for PD matrices.
\end{bullets}
}

\ProblemPage{2}{Gram Matrix PSD and Rank}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Given $v_1=(1,0,1)$, $v_2=(1,1,0)$, $v_3=(0,1,1)$ in $\mathbb{R}^3$, form the
Gram matrix $G_{ij}=v_i^\top v_j$ and certify $G\succeq 0$. Determine
$\operatorname{rank}(G)$.
\PROBLEM{
Use Gram characterization to show PSD and compute rank via linear dependence of
vectors.
}
\MODEL{
\[
G = V V^\top,\quad V=\begin{bmatrix}v_1^\top\\ v_2^\top\\ v_3^\top\end{bmatrix},
\quad G\succeq 0,\ \operatorname{rank}(G)=\operatorname{rank}(V).
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Standard inner product on $\mathbb{R}^3$.
\item Exact arithmetic for rank determination.
\end{bullets}
}
\varmapStart
\var{v_i}{Given vectors in $\mathbb{R}^3$.}
\var{G}{Gram matrix $G=V V^\top$.}
\var{V}{Stacked matrix of row vectors $v_i^\top$.}
\varmapEnd
\WHICHFORMULA{
Formula 1 (Gram factorization) ensures $G\succeq 0$ as $G=V V^\top$.
}
\GOVERN{
\[
x^\top G x = \|V^\top x\|^2 \ge 0,\quad
\operatorname{rank}(G)=\operatorname{rank}(V).
\]
}
\INPUTS{$v_1=(1,0,1)$, $v_2=(1,1,0)$, $v_3=(0,1,1)$.}
\DERIVATION{
\begin{align*}
G&=\begin{bmatrix}
2&1&1\\
1&2&1\\
1&1&2
\end{bmatrix}.\\
\text{PSD: }&x^\top G x = \|V^\top x\|^2\ge 0.\\
\text{Rank: }&v_1+v_2-v_3=(2,0,0)\ne 0,\ \text{not dependent}.\\
&v_1+v_2+v_3=(2,2,2)=2(1,1,1),\ \text{no relation found.}\\
\text{Rows of }V:&\ \begin{bmatrix}1&0&1\\1&1&0\\0&1&1\end{bmatrix}
\text{ have rank }3.\\
\Rightarrow&\ \operatorname{rank}(G)=3.
\end{align*}
}
\RESULT{
$G\succeq 0$ and $\operatorname{rank}(G)=3$ (full rank).
}
\UNITCHECK{
$G$ symmetric by construction; quadratic form equals a squared norm.
}
\EDGECASES{
\begin{bullets}
\item If vectors were linearly dependent, $\operatorname{rank}(G)<3$.
\item Replacing inner product changes $G$ accordingly.
\end{bullets}
}
\ALTERNATE{
Compute eigenvalues: $1,1,4$ for this $G$, confirming PSD and rank $3$.
}
\VALIDATION{
\begin{bullets}
\item Check $\det G = 4>0$ and principal minors positive.
\item Numerical SVD of $V$ shows three nonzero singular values.
\end{bullets}
}
\INTUITION{
Gram matrices measure mutual similarities; similarities from an inner product
cannot produce negative energy.
}
\CANONICAL{
\begin{bullets}
\item $G\succeq 0$ since $G=V V^\top$.
\item Rank equals span dimension of $\{v_i\}$.
\end{bullets}
}

\ProblemPage{3}{Quadratic Minimization with PD Hessian}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Minimize $f(x)=\tfrac12 x^\top A x - b^\top x$ with $A\succ 0$.
Find the unique minimizer and value.
\PROBLEM{
Use first-order optimality and PD Hessian to obtain $x^\star=A^{-1}b$ and
$f(x^\star)=-\tfrac12 b^\top A^{-1}b$.
}
\MODEL{
\[
\nabla f(x)=Ax-b,\ \nabla^2 f(x)=A\succ 0.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $A\succ 0$ ensures strict convexity and unique minimizer.
\item $b$ arbitrary in $\mathbb{R}^n$.
\end{bullets}
}
\varmapStart
\var{A}{PD Hessian matrix.}
\var{b}{Linear term.}
\var{x^\star}{Unique minimizer.}
\varmapEnd
\WHICHFORMULA{
Formula 3 (Rayleigh-Ritz) ensures convexity; Formula 1 provides invertibility.
}
\GOVERN{
\[
Ax=b \ \Rightarrow\ x^\star=A^{-1}b,\quad
f(x^\star)=-\tfrac12 b^\top A^{-1}b.
\]
}
\INPUTS{$A=\begin{bmatrix}2&1\\1&2\end{bmatrix}$, $b=(1,0)^\top$.}
\DERIVATION{
\begin{align*}
A^{-1}&=\frac{1}{3}\begin{bmatrix}2&-1\\-1&2\end{bmatrix},\ 
x^\star=A^{-1}b=\tfrac{1}{3}(2,-1)^\top.\\
f(x^\star)&=\tfrac12 x^{\star\top} A x^\star - b^\top x^\star
=-\tfrac12 b^\top A^{-1} b\\
&=-\tfrac12 (1,0)\frac{1}{3}\begin{bmatrix}2\\ -1\end{bmatrix}=-\tfrac{1}{3}.
\end{align*}
}
\RESULT{
$x^\star=(2/3,-1/3)^\top$ and $f(x^\star)=-1/3$.}
\UNITCHECK{
Dimensions consistent; energy units match across expressions.
}
\EDGECASES{
\begin{bullets}
\item If $A$ is only PSD, minimizers may not be unique.
\item If $b\in\operatorname{range}(A)$ and $A\succeq 0$, minimizers exist.
\end{bullets}
}
\ALTERNATE{
Complete the square:
$f(x)=\tfrac12\|L^\top x - L^{-1}b\|^2-\tfrac12 b^\top A^{-1}b$.
}
\VALIDATION{
\begin{bullets}
\item Check stationarity $Ax^\star=b$.
\item Confirm positive curvature: $d^\top A d>0$ for all $d\ne 0$.
\end{bullets}
}
\INTUITION{
A PD bowl-shaped surface has a single bottom at $A^{-1}b$.
}
\CANONICAL{
\begin{bullets}
\item Unique minimizer $A^{-1}b$ for PD quadratic.
\item Minimum value $-\tfrac12 b^\top A^{-1}b$.
\end{bullets}
}

\ProblemPage{4}{Alice's Sensors and Conditional Variance}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Alice measures $(X,Y)$ with covariance
$M=\begin{bmatrix}\Sigma_{XX}&\Sigma_{XY}\\\Sigma_{YX}&\Sigma_{YY}\end{bmatrix}
\succeq 0$ and $\Sigma_{XX}\succ 0$. Show that the conditional covariance
$\Sigma_{Y|X}=\Sigma_{YY}-\Sigma_{YX}\Sigma_{XX}^{-1}\Sigma_{XY}\succeq 0$.
\PROBLEM{
Use the Schur complement to connect block PSD with conditional variance.
}
\MODEL{
\[
M\succeq 0,\quad \Sigma_{Y|X}=\Sigma_{YY}-\Sigma_{YX}\Sigma_{XX}^{-1}\Sigma_{XY}.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Joint covariance is PSD and $\Sigma_{XX}\succ 0$.
\item Real-valued, finite second moments.
\end{bullets}
}
\varmapStart
\var{\Sigma_{XX}}{Covariance of $X$, PD.}
\var{\Sigma_{YY}}{Covariance of $Y$, PSD.}
\var{\Sigma_{XY}}{Cross-covariance.}
\var{\Sigma_{Y|X}}{Conditional covariance Schur complement.}
\varmapEnd
\WHICHFORMULA{
Formula 2 (Schur complement): $M\succeq 0 \Rightarrow \Sigma_{Y|X}\succeq 0$.
}
\GOVERN{
\[
M\succeq 0,\ \Sigma_{XX}\succ 0\ \Rightarrow\ \Sigma_{Y|X}\succeq 0.
\]
}
\INPUTS{
Let $\Sigma_{XX}=\begin{bmatrix}2&1\\1&2\end{bmatrix}$,
$\Sigma_{XY}=\begin{bmatrix}1\\0\end{bmatrix}$,
$\Sigma_{YY}=[2]$.
}
\DERIVATION{
\begin{align*}
\Sigma_{XX}^{-1}&=\tfrac{1}{3}\begin{bmatrix}2&-1\\-1&2\end{bmatrix}.\\
\Sigma_{Y|X}
&=2-\begin{bmatrix}1&0\end{bmatrix}
\tfrac{1}{3}\begin{bmatrix}2&-1\\-1&2\end{bmatrix}
\begin{bmatrix}1\\0\end{bmatrix}
=2-\tfrac{2}{3}=\tfrac{4}{3}\ge 0.
\end{align*}
}
\RESULT{
$\Sigma_{Y|X}\succeq 0$; numerically $\Sigma_{Y|X}=4/3$.
}
\UNITCHECK{
Covariance units preserved; subtraction of PSD terms yields PSD remainder.
}
\EDGECASES{
\begin{bullets}
\item If $\Sigma_{XX}$ is singular, use pseudoinverse and range conditions.
\item Zero cross-covariance yields $\Sigma_{Y|X}=\Sigma_{YY}$.
\end{bullets}
}
\ALTERNATE{
Regress $Y$ on $X$, residual covariance equals the Schur complement.
}
\VALIDATION{
\begin{bullets}
\item Simulate Gaussian data with this covariance and estimate residual var.
\item Check $M\succeq 0$ numerically by eigenvalues.
\end{bullets}
}
\INTUITION{
Knowing $X$ removes explainable variance in $Y$; remaining variance is
nonnegative.
}
\CANONICAL{
\begin{bullets}
\item Conditional covariance is a Schur complement.
\item PSD preserved by conditioning.
\end{bullets}
}

\ProblemPage{5}{Bob's Block Matrix Feasibility}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Bob considers
$M=\begin{bmatrix}I&K\\K^\top&I\end{bmatrix}$ with $K=K^\top$.
Find conditions on $K$ for $M\succeq 0$.
\PROBLEM{
Apply Schur complement with $A=I$ to derive $M\succeq 0 \iff I-K^2\succeq 0$,
which is equivalent to $\|K\|_2\le 1$.
}
\MODEL{
\[
M\succeq 0 \iff I-K^2\succeq 0 \iff \lambda_i(K)\in[-1,1]\ \forall i.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $K$ symmetric; $I$ identity so $A=I\succ 0$.
\item Spectral norm equals max absolute eigenvalue for symmetric $K$.
\end{bullets}
}
\varmapStart
\var{K}{Symmetric coupling matrix.}
\var{M}{Block matrix with identity blocks.}
\varmapEnd
\WHICHFORMULA{
Formula 2 (Schur complement) and Formula 3 (Rayleigh bounds on $K^2$).
}
\GOVERN{
\[
M\succeq 0 \iff I-K^\top I^{-1}K = I-K^2\succeq 0.
\]
}
\INPUTS{$K=\begin{bmatrix}0.6&0.2\\0.2&0.8\end{bmatrix}$.}
\DERIVATION{
\begin{align*}
K^2&=\begin{bmatrix}0.6&0.2\\0.2&0.8\end{bmatrix}^2
=\begin{bmatrix}0.4&0.28\\0.28&0.68\end{bmatrix}.\\
I-K^2&=\begin{bmatrix}0.6&-0.28\\-0.28&0.32\end{bmatrix}.\\
\lambda(I-K^2)&\approx 0.786,\,0.134\ (\ge 0)\Rightarrow M\succeq 0.
\end{align*}
}
\RESULT{
$M\succeq 0$ iff $\|K\|_2\le 1$. Example satisfies the condition.
}
\UNITCHECK{
All matrices are symmetric; eigenvalue tests apply.
}
\EDGECASES{
\begin{bullets}
\item If $\|K\|_2=1$, $M$ is PSD but singular.
\item If $\|K\|_2>1$, $M$ is indefinite.
\end{bullets}
}
\ALTERNATE{
Diagonalize $K=Q\Lambda Q^\top$; then
$M=(Q\oplus Q)\begin{bmatrix}I&\Lambda\\\Lambda&I\end{bmatrix}(Q\oplus Q)^\top$
and test $2\times 2$ blocks: PSD iff $1-\lambda_i^2\ge 0$.
}
\VALIDATION{
\begin{bullets}
\item Compute $\lambda(M)\ge 0$ numerically for random $K$ with $\|K\|_2\le 1$.
\item Verify edge $\lambda=1$ yields $\det(M)=0$.
\end{bullets}
}
\INTUITION{
Coupling cannot exceed identity strength; otherwise some direction reduces
energy below zero.
}
\CANONICAL{
\begin{bullets}
\item Block test reduces to spectral bound on $K$.
\item Norm condition $\|K\|_2\le 1$ is necessary and sufficient.
\end{bullets}
}

\ProblemPage{6}{Dice Covariance is PSD}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Let $Z_1,Z_2$ be independent fair dice outcomes on $\{1,\dots,6\}$. Define
$X=Z_1+Z_2$, $Y=Z_1-Z_2$. Show that the covariance matrix
$\Sigma=\operatorname{Cov}((X,Y))$ is PSD and compute it.
\PROBLEM{
Compute variances and covariance explicitly, then verify PSD by eigenvalues or
quadratic forms.
}
\MODEL{
\[
\Sigma=\begin{bmatrix}\operatorname{Var}(X)&\operatorname{Cov}(X,Y)\\
\operatorname{Cov}(X,Y)&\operatorname{Var}(Y)\end{bmatrix}\succeq 0.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $Z_1,Z_2$ independent and identically distributed.
\item Finite variances; linearity of covariance.
\end{bullets}
}
\varmapStart
\var{Z_1,Z_2}{Dice outcomes.}
\var{X,Y}{Linear combinations of $Z_1,Z_2$.}
\var{\Sigma}{Covariance matrix of $(X,Y)$.}
\varmapEnd
\WHICHFORMULA{
Formula 1: Covariance is Gram of centered variables; thus PSD.
}
\GOVERN{
\[
\Sigma=\mathbb{E}[(U-\mathbb{E}U)(U-\mathbb{E}U)^\top],\ U=(X,Y)^\top.
\]
}
\INPUTS{Distribution: $\mathbb{E}Z_i=3.5$, $\operatorname{Var}(Z_i)=\frac{35}{12}$.}
\DERIVATION{
\begin{align*}
\operatorname{Var}(X)&=\operatorname{Var}(Z_1)+\operatorname{Var}(Z_2)
=\tfrac{35}{6}.\\
\operatorname{Var}(Y)&=\operatorname{Var}(Z_1)+\operatorname{Var}(Z_2)
=\tfrac{35}{6}.\\
\operatorname{Cov}(X,Y)&=\operatorname{Cov}(Z_1+Z_2,Z_1-Z_2)\\
&=\operatorname{Var}(Z_1)-\operatorname{Var}(Z_2)=0.\\
\Sigma&=\tfrac{35}{6} I_2 \succeq 0.
\end{align*}
}
\RESULT{
$\Sigma=\tfrac{35}{6}I_2$ is PD; eigenvalues $\tfrac{35}{6}>0$.}
\UNITCHECK{
Units of variance are squared outcomes; symmetric covariance.
}
\EDGECASES{
\begin{bullets}
\item If $Z_1=Z_2$, then $Y=0$ and $\Sigma$ is PSD but singular.
\item Correlated dice would produce nonzero off-diagonal terms.
\end{bullets}
}
\ALTERNATE{
Compute $\Sigma$ as $B \operatorname{Cov}(Z) B^\top$ with
$B=\begin{bmatrix}1&1\\1&-1\end{bmatrix}$ and
$\operatorname{Cov}(Z)=\tfrac{35}{12}I_2$.
}
\VALIDATION{
\begin{bullets}
\item Simulate dice and estimate $\Sigma$; compare to analytic value.
\item Check $x^\top \Sigma x=\tfrac{35}{6}\|x\|^2\ge 0$.
\end{bullets}
}
\INTUITION{
Covariance is always PSD because it is an average of outer products.
}
\CANONICAL{
\begin{bullets}
\item Covariances are PSD by Gram structure.
\item Linear transforms preserve PSD: $B\Sigma B^\top$ PSD.
\end{bullets}
}

\ProblemPage{7}{Monotonicity in Loewner Order}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Show that if $0\preceq A\preceq B$, then for all $x$,
$x^\top A x\le x^\top B x$, and
$\lambda_{\min}(A)\le \lambda_{\min}(B)$,
$\lambda_{\max}(A)\le \lambda_{\max}(B)$.
\PROBLEM{
Use Loewner order and Rayleigh-Ritz characterization.
}
\MODEL{
\[
B-A\succeq 0 \Rightarrow x^\top(B-A)x\ge 0 \Rightarrow
x^\top A x\le x^\top B x.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Symmetry and PSD ordering.
\item Rayleigh-Ritz for eigenvalue bounds.
\end{bullets}
}
\varmapStart
\var{A,B}{Symmetric with $0\preceq A\preceq B$.}
\var{\lambda_{\min},\lambda_{\max}}{Extreme eigenvalues.}
\varmapEnd
\WHICHFORMULA{
Formula 3 (Rayleigh-Ritz) ties order to eigenvalue inequalities.
}
\GOVERN{
\[
\lambda_{\max}(A)=\max_{\|x\|=1} x^\top A x \le
\max_{\|x\|=1} x^\top B x=\lambda_{\max}(B),
\]
and similarly for $\lambda_{\min}$ using minima.
}
\INPUTS{Let $A=\begin{bmatrix}1&0\\0&0.5\end{bmatrix}$,
$B=\begin{bmatrix}2&0\\0&3\end{bmatrix}$.}
\DERIVATION{
\begin{align*}
x^\top (B-A) x&=x_1^2+(2.5)x_2^2\ge 0\ \forall x,\\
\lambda(A)&=1,0.5,\quad \lambda(B)=2,3.\\
\lambda_{\min}(A)&=0.5\le 2=\lambda_{\min}(B),\\
\lambda_{\max}(A)&=1\le 3=\lambda_{\max}(B).
\end{align*}
}
\RESULT{
Quadratic form monotonicity and eigenvalue inequalities hold.
}
\UNITCHECK{
All scalars; unit sphere constraints in Rayleigh quotients are dimensionless.
}
\EDGECASES{
\begin{bullets}
\item If $A=B$, inequalities become equalities.
\item If $A$ is singular, $\lambda_{\min}(A)=0$ still valid.
\end{bullets}
}
\ALTERNATE{
Diagonalize both in a common basis if they commute; otherwise Rayleigh remains
valid without commutation.
}
\VALIDATION{
\begin{bullets}
\item Random tests: $x^\top (B-A) x\ge 0$ for sampled $x$.
\item Numerical eigenvalues satisfy the inequalities.
\end{bullets}
}
\INTUITION{
A larger matrix in Loewner order never decreases energy in any direction.
}
\CANONICAL{
\begin{bullets}
\item $A\preceq B \Rightarrow R_A(x)\le R_B(x)$ for all Rayleigh quotients.
\item Extreme values bound eigenvalues accordingly.
\end{bullets}
}

\ProblemPage{8}{Schur Product Proof (Proof-Style)}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Prove that if $A\succeq 0$ and $B\succeq 0$, then $A\odot B\succeq 0$.
\PROBLEM{
Provide a direct proof using Gram factorizations and rank-one decompositions.
}
\MODEL{
\[
A=\sum_k x_k x_k^\top,\ B=\sum_\ell y_\ell y_\ell^\top.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Real symmetric PSD matrices.
\item Bilinearity of Hadamard product.
\end{bullets}
}
\varmapStart
\var{A,B}{Input PSD matrices.}
\var{x_k,y_\ell}{Vectors generating rank-one PSDs.}
\varmapEnd
\WHICHFORMULA{
Formula 4 (Schur product theorem) with its lemma on rank-one factors.
}
\GOVERN{
\[
(uu^\top)\odot (vv^\top)=(u\odot v)(u\odot v)^\top\succeq 0.
\]
}
\INPUTS{$A,B$ arbitrary PSD.}
\DERIVATION{
\begin{align*}
A\odot B
&=\Big(\sum_k x_k x_k^\top\Big)\odot \Big(\sum_\ell y_\ell y_\ell^\top\Big)\\
&=\sum_{k,\ell} (x_k x_k^\top)\odot (y_\ell y_\ell^\top)\\
&=\sum_{k,\ell} (x_k\odot y_\ell)(x_k\odot y_\ell)^\top\succeq 0.
\end{align*}
}
\RESULT{
$A\odot B$ is a sum of PSD rank-one matrices, hence PSD.
}
\UNITCHECK{
All terms are $n\times n$; outer products and sums are well-defined.
}
\EDGECASES{
\begin{bullets}
\item If $A$ or $B$ is zero, the product is zero and trivially PSD.
\item Singular $A$ or $B$ pose no issue; factorization still exists.
\end{bullets}
}
\ALTERNATE{
Use the identity $A\odot B = \operatorname{diag}(X(B\otimes I)X^\top)$ with a
Kronecker lift to show PSD via congruence.
}
\VALIDATION{
\begin{bullets}
\item Random PSD inputs produce PSD Hadamard products numerically.
\item Check eigenvalues of $A\odot B$ are nonnegative.
\end{bullets}
}
\INTUITION{
Entrywise product corresponds to combining features elementwise; Gram structure
is preserved.
}
\CANONICAL{
\begin{bullets}
\item Schur product preserves PSD.
\item Entrywise operations require structure to preserve PSD.
\end{bullets}
}

\ProblemPage{9}{Convexity of Quadratic under Linear Constraint}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Minimize $f(x)=\tfrac12 x^\top A x$ subject to $C x=d$ with $A\succeq 0$ and
$C$ full row rank. Find the KKT conditions and solution.
\PROBLEM{
Use PSD to guarantee convexity and solvability of the constrained quadratic
program; derive solution via Schur complement.
}
\MODEL{
\[
\min_x \tfrac12 x^\top A x \ \text{s.t. } Cx=d,\quad
\begin{bmatrix}A & C^\top\\ C & 0\end{bmatrix}
\begin{bmatrix}x\\ \lambda\end{bmatrix}=
\begin{bmatrix}0\\ d\end{bmatrix}.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $A\succeq 0$, $C$ has full row rank, feasibility holds.
\item Strong duality applies; KKT necessary and sufficient.
\end{bullets}
}
\varmapStart
\var{A}{PSD Hessian.}
\var{C}{Constraint matrix.}
\var{d}{Constraint vector.}
\var{\lambda}{Lagrange multiplier.}
\varmapEnd
\WHICHFORMULA{
Formula 2 (Schur complement) on KKT system; Formula 1 for PSD convexity.
}
\GOVERN{
\[
Ax + C^\top \lambda=0,\quad Cx=d.
\]
}
\INPUTS{$A=\begin{bmatrix}2&0\\0&0\end{bmatrix}$,
$C=\begin{bmatrix}1&1\end{bmatrix}$, $d=1$.}
\DERIVATION{
\begin{align*}
Ax + C^\top \lambda&=0 \Rightarrow
\begin{cases}2x_1+\lambda=0\\ 0x_2+\lambda=0\end{cases}
\Rightarrow \lambda=0,\ x_1=0.\\
Cx&=x_1+x_2=1 \Rightarrow x_2=1.\\
x^\star&=(0,1)^\top.\\
\text{KKT matrix }K&=\begin{bmatrix}A & C^\top\\ C & 0\end{bmatrix}
=\begin{bmatrix}2&0&1\\0&0&1\\1&1&0\end{bmatrix}.\\
\text{Schur }&\text{complement in }A:\ S=-C A^\dagger C^\top
=-\begin{bmatrix}1/2\end{bmatrix}\ (\preceq 0).
\end{align*}
}
\RESULT{
$x^\star=(0,1)^\top$ satisfies constraints and minimizes $f$.
}
\UNITCHECK{
Dimensions: $A\in\mathbb{R}^{2\times 2}$, $C\in\mathbb{R}^{1\times 2}$.
}
\EDGECASES{
\begin{bullets}
\item If $A\succ 0$, solution unique; if PSD, uniqueness up to nullspace
intersections with the constraint set.
\item Infeasible $Cx=d$ has no solution.
\end{bullets}
}
\ALTERNATE{
Project onto the constraint set and minimize on the affine subspace using
reduced coordinates.
}
\VALIDATION{
\begin{bullets}
\item Check $f$ along feasible line $(t,1-t)$ is minimized at $t=0$.
\item Second derivative along feasible direction is nonnegative.
\end{bullets}
}
\INTUITION{
Energy is minimized subject to linear restrictions; PSD ensures convexity.
}
\CANONICAL{
\begin{bullets}
\item KKT system encodes optimality with PSD structure.
\item Schur complement yields reduced problem in multipliers.
\end{bullets}
}

\ProblemPage{10}{Kernel Matrix Factorization}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Given points $p_1=(0,0)$, $p_2=(1,0)$, $p_3=(0,1)$ and linear kernel
$K_{ij}=p_i^\top p_j$, show $K\succeq 0$ and find $X$ with $K=XX^\top$.
\PROBLEM{
Use Gram representation to factor the kernel matrix and verify PSD.
}
\MODEL{
\[
K=PP^\top,\ P=\begin{bmatrix}p_1^\top\\ p_2^\top\\ p_3^\top\end{bmatrix}.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Standard inner product in $\mathbb{R}^2$.
\item Exact arithmetic.
\end{bullets}
}
\varmapStart
\var{p_i}{Input points in $\mathbb{R}^2$.}
\var{K}{Kernel/Gram matrix.}
\var{P}{Data matrix with rows $p_i^\top$.}
\var{X}{Factor such that $K=XX^\top$.}
\varmapEnd
\WHICHFORMULA{
Formula 1: $K$ is Gram and thus PSD; factor via $X=P$.
}
\GOVERN{
\[
x^\top K x = \|P^\top x\|^2\ge 0,\quad K=PP^\top.
\]
}
\INPUTS{$p_1=(0,0)$, $p_2=(1,0)$, $p_3=(0,1)$.}
\DERIVATION{
\begin{align*}
K&=\begin{bmatrix}
0&0&0\\
0&1&0\\
0&0&1
\end{bmatrix},\quad X=P=
\begin{bmatrix}
0&0\\
1&0\\
0&1
\end{bmatrix}.\\
XX^\top&=\begin{bmatrix}
0&0&0\\
0&1&0\\
0&0&1
\end{bmatrix}=K.
\end{align*}
}
\RESULT{
$K\succeq 0$ and $X=P$ is a valid factor with $K=XX^\top$.}
\UNITCHECK{
Matrices have compatible shapes: $X\in\mathbb{R}^{3\times 2}$, $K\in\mathbb{R}^{3\times 3}$.
}
\EDGECASES{
\begin{bullets}
\item If points coincide at origin, corresponding rows are zero and rank drops.
\item Nonlinear kernels require different feature maps but remain Gram PSD.
\end{bullets}
}
\ALTERNATE{
Compute eigenvalues: $\{1,1,0\}$ confirming PSD and rank 2.
}
\VALIDATION{
\begin{bullets}
\item Verify $x^\top K x=\|X^\top x\|^2\ge 0$ for random $x$.
\item Numeric SVD shows two nonzero singular values.
\end{bullets}
}
\INTUITION{
Kernel matrices record inner products; they are automatically PSD.
}
\CANONICAL{
\begin{bullets}
\item Gram representation $K=XX^\top$.
\item PSD and rank equal to span of feature vectors.
\end{bullets}
}

\clearpage
\section{Coding Demonstrations}
\CodeDemoPage{PSD Characterizations: Eigenvalues vs. Cholesky vs. Quadratic}
\PROBLEM{
Given symmetric matrices, test PSD/PD via eigenvalues, Cholesky, and random
quadratic forms; verify equivalence on deterministic samples.
}
\API{
\begin{bullets}
\item \inlinecode{def read_input(s) -> list} — parse flat matrix entries.
\item \inlinecode{def solve_case(A) -> dict} — tests and certificates.
\item \inlinecode{def validate() -> None} — asserts on known cases.
\item \inlinecode{def main() -> None} — orchestrates deterministic tests.
\end{bullets}
}
\INPUTS{
Square symmetric matrix $A$ as nested lists of floats.
}
\OUTPUTS{
Dictionary with keys: \inlinecode{"is_psd"}, \inlinecode{"is_pd"},
\inlinecode{"chol_ok"}, \inlinecode{"lambda_min"}, \inlinecode{"lambda_min_ge0"}.
}
\FORMULA{
\[
A\succeq 0 \iff \lambda_{\min}(A)\ge 0 \iff x^\top A x\ge 0,\ \forall x.
\]
Cholesky succeeds iff $A\succ 0$.
}
\textbf{SOLUTION A — From Scratch (Mathematically Explicit Implementation)}
\begin{codepy}
import numpy as np

def read_input(s):
    vals = [float(x) for x in s.split()]
    n = int(round(len(vals)**0.5))
    A = np.array(vals, dtype=float).reshape(n, n)
    A = (A + A.T) / 2.0
    return A

def is_pd_chol(A):
    try:
        L = np.linalg.cholesky(A)
        return True, L
    except np.linalg.LinAlgError:
        return False, None

def solve_case(A):
    w = np.linalg.eigvalsh(A)
    lam_min = float(w[0])
    is_psd = lam_min >= -1e-12
    is_pd = lam_min > 1e-12
    chol_ok, L = is_pd_chol(A)
    rng = np.random.default_rng(0)
    xs = rng.normal(size=(5, A.shape[0]))
    qmins = []
    for x in xs:
        q = float(x @ A @ x)
        qmins.append(q)
    return {
        "is_psd": is_psd,
        "is_pd": is_pd,
        "chol_ok": chol_ok,
        "lambda_min": lam_min,
        "lambda_min_ge0": lam_min >= 0.0,
        "qmin": min(qmins),
    }

def validate():
    A = np.array([[2.0,1.0],[1.0,2.0]])
    r = solve_case(A)
    assert r["is_pd"] and r["chol_ok"]
    B = np.array([[1.0,1.0],[1.0,1.0]])
    r2 = solve_case(B)
    assert r2["is_psd"] and (not r2["is_pd"])
    C = np.array([[1.0,2.0],[2.0,1.0]])
    r3 = solve_case(C)
    assert r3["lambda_min"] < 0

def main():
    validate()
    A = read_input("4 2 2 3")
    r = solve_case(A)
    print("lam_min", round(r["lambda_min"],6),
          "psd", r["is_psd"],
          "pd", r["is_pd"],
          "chol", r["chol_ok"],
          "qmin", round(r["qmin"],6))

if __name__ == "__main__":
    main()
\end{codepy}
\textbf{SOLUTION B — Library-Based (Validated Computational Shortcut)}
\begin{codepy}
import numpy as np
import scipy.linalg as la

def read_input(s):
    vals = [float(x) for x in s.split()]
    n = int(round(len(vals)**0.5))
    A = np.array(vals, dtype=float).reshape(n, n)
    A = (A + A.T) / 2.0
    return A

def solve_case(A):
    w = la.eigvalsh(A, check_finite=True)
    lam_min = float(w[0])
    is_psd = lam_min >= -1e-12
    is_pd = lam_min > 1e-12
    try:
        L = la.cholesky(A, lower=True, check_finite=True)
        chol_ok = True
    except la.LinAlgError:
        chol_ok = False
    return {"is_psd": is_psd, "is_pd": is_pd,
            "chol_ok": chol_ok, "lambda_min": lam_min}

def validate():
    A = np.array([[2.0,1.0],[1.0,2.0]])
    r = solve_case(A)
    assert r["is_pd"] and r["chol_ok"]
    B = np.array([[1.0,1.0],[1.0,1.0]])
    r2 = solve_case(B)
    assert r2["is_psd"] and (not r2["is_pd"])

def main():
    validate()
    A = read_input("2 1 1 2")
    r = solve_case(A)
    print("lam_min", round(r["lambda_min"],6),
          "psd", r["is_psd"], "pd", r["is_pd"],
          "chol", r["chol_ok"])

if __name__ == "__main__":
    main()
\end{codepy}
\COMPLEXITY{
Eigen decomposition $\mathcal{O}(n^3)$ time, $\mathcal{O}(n^2)$ space.
Cholesky $\mathcal{O}(n^3)$ time, $\mathcal{O}(n^2)$ space.
}
\FAILMODES{
\begin{bullets}
\item Non-symmetric input; we symmetrize $(A+A^\top)/2$.
\item Near-zero negative eigenvalues due to rounding; use tolerance.
\item Cholesky fails for PSD but not PD; handle exception.
\end{bullets}
}
\STABILITY{
\begin{bullets}
\item Use symmetric eigen solvers (eigvalsh) for Hermitian stability.
\item Avoid forming $A^\top A$ explicitly for ill-conditioned problems.
\end{bullets}
}
\VALIDATION{
\begin{bullets}
\item Assert PD case passes Cholesky; rank-one PSD fails PD test.
\item Cross-check eigenvalue sign with Cholesky success flag.
\end{bullets}
}
\RESULT{
Both implementations agree on PSD/PD classification and minimal eigenvalues.
}
\EXPLANATION{
Eigenvalues encode the Rayleigh extrema; Cholesky certifies PD; quadratic forms
are nonnegative for PSD by definition.
}
\EXTENSION{
Vectorized batch testing of many matrices; LDL$^\top$ for PSD with pivoting.
}

\CodeDemoPage{Schur Complement and Conditional Covariance}
\PROBLEM{
Construct a block covariance matrix, compute the Schur complement for
$\Sigma_{Y|X}$, and verify PSD numerically.
}
\API{
\begin{bullets}
\item \inlinecode{def make_blocks() -> tuple} — returns $(A,B,C)$.
\item \inlinecode{def schur(A,B,C) -> S} — returns $C-B^\top A^{-1}B$.
\item \inlinecode{def validate() -> None} — checks PSD via eigenvalues.
\item \inlinecode{def main() -> None} — runs deterministic test.
\end{bullets}
}
\INPUTS{
$A\succ 0$ and symmetric $C$, with conformable $B$.
}
\OUTPUTS{
Schur complement $S$ and eigenvalues of block matrix $M$.
}
\FORMULA{
\[
M=\begin{bmatrix}A&B\\B^\top&C\end{bmatrix}\succeq 0 \iff
A\succ 0,\ S=C-B^\top A^{-1}B\succeq 0.
\]
}
\textbf{SOLUTION A — From Scratch (Mathematically Explicit Implementation)}
\begin{codepy}
import numpy as np

def make_blocks():
    A = np.array([[2.0, 0.5],[0.5, 1.5]])
    B = np.array([[0.6],[0.2]])
    C = np.array([[1.0]])
    return A, B, C

def schur(A, B, C):
    Ai = np.linalg.inv(A)
    return C - B.T @ Ai @ B

def block(A, B, C):
    n = A.shape[0]; m = C.shape[0]
    M = np.zeros((n+m, n+m))
    M[:n,:n] = A
    M[:n,n:] = B
    M[n:,:n] = B.T
    M[n:,n:] = C
    return M

def is_psd(M, tol=1e-10):
    w = np.linalg.eigvalsh(M)
    return float(w[0]) >= -tol, w

def validate():
    A,B,C = make_blocks()
    S = schur(A,B,C)
    okS, wS = is_psd(S)
    M = block(A,B,C)
    okM, wM = is_psd(M)
    assert okS and okM

def main():
    validate()
    A,B,C = make_blocks()
    S = schur(A,B,C)
    M = block(A,B,C)
    okS, wS = is_psd(S)
    okM, wM = is_psd(M)
    print("S", S.flatten().round(6), "ok", okS,
          "M_minlam", round(float(wM[0]),6))

if __name__ == "__main__":
    main()
\end{codepy}
\textbf{SOLUTION B — Library-Based (Validated Computational Shortcut)}
\begin{codepy}
import numpy as np
import scipy.linalg as la

def make_blocks():
    A = np.array([[3.0, 1.0],[1.0, 2.0]])
    B = np.array([[0.5],[0.4]])
    C = np.array([[1.2]])
    return A, B, C

def schur(A, B, C):
    Ai = la.inv(A, check_finite=True)
    return C - B.T @ Ai @ B

def validate():
    A,B,C = make_blocks()
    S = schur(A,B,C)
    wS = la.eigvalsh(S, check_finite=True)
    wM = la.eigvalsh(np.block([[A,B],[B.T,C]]), check_finite=True)
    assert wS[0] >= -1e-12 and wM[0] >= -1e-12

def main():
    validate()
    A,B,C = make_blocks()
    S = schur(A,B,C)
    wS = la.eigvalsh(S, check_finite=True)
    print("S_eigs", np.round(wS,6))

if __name__ == "__main__":
    main()
\end{codepy}
\COMPLEXITY{
Matrix inverse and eigenvalues: $\mathcal{O}(n^3)$ for $A\in\mathbb{R}^{n\times n}$.
}
\FAILMODES{
\begin{bullets}
\item If $A$ is singular, inverse fails; must use pseudoinverse and range tests.
\item Non-symmetric inputs violate assumptions; we use symmetric constructors.
\end{bullets}
}
\STABILITY{
\begin{bullets}
\item Prefer solves over explicit inverse in practice: solve $Ax=B$.
\item Use symmetric solvers for better conditioning.
\end{bullets}
}
\VALIDATION{
\begin{bullets}
\item Check minimal eigenvalues are nonnegative within tolerance.
\item Compare block eigenvalues with Schur complement PSD status.
\end{bullets}
}
\RESULT{
Schur complements computed are PSD, and full block matrices are PSD as well.
}
\EXPLANATION{
Completion of squares splits energy into $A$-part and complement $S$; both must
be PSD for the block to be PSD.
}
\EXTENSION{
Extend to conditional covariance in multivariate Gaussian models.
}

\clearpage
\section{Applied Domains — Detailed End-to-End Scenarios}
\DomainPage{Machine Learning}
\SCENARIO{
Ridge regression solves $(X^\top X+\lambda I)\beta=X^\top y$ with
$X^\top X+\lambda I\succ 0$. Show PD via PSD of $X^\top X$ and compute $\beta$.
}
\ASSUMPTIONS{
\begin{bullets}
\item $\lambda>0$ ensures strict PD even if $X$ is rank-deficient.
\item Data are finite; no exact collinearity issues after regularization.
\end{bullets}
}
\WHICHFORMULA{
$X^\top X\succeq 0$ (Gram), and adding $\lambda I\succ 0$ yields PD.
}
\varmapStart
\var{X}{Design matrix in $\mathbb{R}^{n\times d}$.}
\var{y}{Targets in $\mathbb{R}^n$.}
\var{\beta}{Coefficients solving normal equations.}
\var{\lambda}{Regularization strength.}
\varmapEnd
\PIPELINE{
\begin{bullets}
\item Generate deterministic synthetic data.
\item Form $A=X^\top X+\lambda I$; verify PD.
\item Solve for $\beta$ and report residual norm.
\end{bullets}
}
\textbf{Implementation (From Scratch)}
\begin{codepy}
import numpy as np

def gen_data(n=50, d=3, lam=1e-1):
    np.random.seed(0)
    X = np.random.randn(n, d)
    beta_true = np.array([1.0, -2.0, 0.5])
    y = X @ beta_true + 0.1*np.random.randn(n)
    return X, y, beta_true, lam

def ridge(X, y, lam):
    A = X.T @ X + lam*np.eye(X.shape[1])
    b = X.T @ y
    w = np.linalg.solve(A, b)
    return w, A

def main():
    X,y,btrue,lam = gen_data()
    w, A = ridge(X, y, lam)
    wmin = np.linalg.eigvalsh(A)[0]
    print("lam_min(A)", round(float(wmin),6),
          "||w-b||", round(float(np.linalg.norm(w-btrue)),3))

if __name__ == "__main__":
    main()
\end{codepy}
\textbf{Implementation (Library Version)}
\begin{codepy}
import numpy as np
from sklearn.linear_model import Ridge

def main():
    np.random.seed(0)
    X = np.random.randn(80, 4)
    beta_true = np.array([1.0, -1.0, 0.5, 0.0])
    y = X @ beta_true + 0.1*np.random.randn(80)
    model = Ridge(alpha=0.1, fit_intercept=False).fit(X, y)
    A = X.T @ X + 0.1*np.eye(X.shape[1])
    lam_min = np.linalg.eigvalsh(A)[0]
    print("lam_min(A)", round(float(lam_min),6),
          "||res||", round(float(np.linalg.norm(y-X@model.coef_)),3))

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{Minimal eigenvalue of $A$ (positivity) and residual norm magnitude.}
\INTERPRET{
Ridge makes $A$ strictly PD, guaranteeing a unique solution and numerical
stability.
}
\NEXTSTEPS{
Use kernel ridge with PSD kernels; analyze condition numbers and scaling.
}

\DomainPage{Quantitative Finance}
\SCENARIO{
Estimate covariance $\Sigma$ of returns and compute risk for portfolio weights
$w$; verify $\Sigma\succeq 0$ and report $w^\top \Sigma w$.
}
\ASSUMPTIONS{
\begin{bullets}
\item Returns are mean-zero after demeaning.
\item Sample covariance is PSD; PD if sufficient variation exists.
\end{bullets}
}
\WHICHFORMULA{
$\Sigma=\frac{1}{n-1}R^\top R\succeq 0$ as a Gram matrix of demeaned returns.
}
\varmapStart
\var{R}{Matrix of demeaned returns, shape $(n,d)$.}
\var{\Sigma}{Sample covariance, PSD.}
\var{w}{Portfolio weights.}
\var{\sigma_p^2}{Portfolio variance $w^\top \Sigma w$.}
\varmapEnd
\PIPELINE{
\begin{bullets}
\item Simulate deterministic returns with fixed seed.
\item Compute $\Sigma$ and its eigenvalues.
\item Evaluate portfolio variance.
\end{bullets}
}
\textbf{Implementation (Full Pipeline)}
\begin{codepy}
import numpy as np

def simulate(n=200, d=3, seed=0):
    rng = np.random.default_rng(seed)
    A = rng.normal(size=(d, d))
    cov = A @ A.T
    Z = rng.multivariate_normal(np.zeros(d), cov, size=n)
    R = Z - Z.mean(axis=0, keepdims=True)
    return R

def covariance(R):
    n = R.shape[0]
    return (R.T @ R) / (n - 1)

def main():
    R = simulate()
    Sigma = covariance(R)
    w = np.array([0.5, 0.3, 0.2])
    w2 = float(w.T @ Sigma @ w)
    lam_min = np.linalg.eigvalsh(Sigma)[0]
    print("lam_min(Sigma)", round(lam_min,6),
          "var", round(w2,6))

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{Minimal eigenvalue nonnegativity and scalar variance $w^\top\Sigma w$.}
\INTERPRET{
Covariance is PSD; portfolio variance is nonnegative and measures risk.
}
\NEXTSTEPS{
Regularize $\Sigma$ (shrinkage) to ensure PD and improve conditioning.
}

\DomainPage{Deep Learning}
\SCENARIO{
Compute empirical Gauss-Newton (or Fisher-like) matrix $G=J^\top J$ for a
single-layer network on synthetic data; verify $G\succeq 0$.
}
\ASSUMPTIONS{
\begin{bullets}
\item Jacobian $J$ is real; $G$ is Gram and hence PSD.
\item Deterministic data and initialization.
\end{bullets}
}
\WHICHFORMULA{
$G=J^\top J\succeq 0$ as a Gram matrix of per-sample gradients.
}
\PIPELINE{
\begin{bullets}
\item Generate data and a linear model.
\item Compute Jacobian of predictions wrt weights.
\item Form $G$ and check eigenvalues.
\end{bullets}
}
\textbf{Implementation (End-to-End)}
\begin{codepy}
import numpy as np

def data(n=50, d=5):
    np.random.seed(0)
    X = np.random.randn(n, d)
    w = np.arange(1, d+1, dtype=float)
    y = X @ w + 0.1*np.random.randn(n)
    return X, y

def jacobian(X):
    # Linear model yhat = X @ w ; Jacobian wrt w is X
    return X.copy()

def main():
    X, y = data()
    J = jacobian(X)
    G = J.T @ J
    lam_min = np.linalg.eigvalsh(G)[0]
    print("lam_min(G)", round(float(lam_min),6))

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{Minimal eigenvalue of $G$; nonnegativity certifies PSD.}
\INTERPRET{
Second-order approximations based on $J^\top J$ inherit PSD, ensuring descent
directions in least squares settings.
}
\NEXTSTEPS{
Extend to nonlinear nets by stacking per-sample gradients; add damping $\lambda I$.
}

\DomainPage{Kaggle / Data Analytics}
\SCENARIO{
Build an RBF kernel matrix $K_{ij}=\exp(-\gamma \|x_i-x_j\|^2)$ and verify it
is PSD; report condition number.
}
\ASSUMPTIONS{
\begin{bullets}
\item $\gamma>0$; RBF kernel is PSD by Bochner/Schoenberg theorems.
\item Finite dataset; distances computed in Euclidean norm.
\end{bullets}
}
\WHICHFORMULA{
Schur product and kernel closure imply RBF Gram matrices are PSD.
}
\PIPELINE{
\begin{bullets}
\item Generate deterministic points.
\item Form $K$ using RBF.
\item Compute eigenvalues and condition number.
\end{bullets}
}
\textbf{Implementation (Complete EDA Pipeline)}
\begin{codepy}
import numpy as np

def points(n=20, d=2, seed=0):
    rng = np.random.default_rng(seed)
    return rng.normal(size=(n, d))

def rbf_kernel(X, gamma=0.5):
    D2 = np.sum((X[:,None,:]-X[None,:,:])**2, axis=2)
    return np.exp(-gamma * D2)

def main():
    X = points()
    K = rbf_kernel(X, gamma=0.3)
    w = np.linalg.eigvalsh(K)
    cond = float(w[-1]/max(w[0], 1e-12))
    print("lam_min(K)", round(float(w[0]),6),
          "cond", round(cond,3))

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{Minimal eigenvalue and spectral condition number.}
\INTERPRET{
RBF Gram matrices are PSD; small eigenvalues reflect redundancy or wide kernels.
}
\NEXTSTEPS{
Center the kernel and apply kernel PCA or SVM with regularization.
}

\end{document}
% !TeX program = xelatex
\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{microtype,setspace,amsmath,amssymb,mathtools,amsthm,unicode-math}
\setstretch{1.05}
\setmainfont{Latin Modern Roman}
\setmonofont{Latin Modern Mono}
\setmathfont{Latin Modern Math}
\allowdisplaybreaks[4]
\setlength{\jot}{7pt}
\setlength{\emergencystretch}{8em}
\sloppy
\usepackage{xcolor,fancyhdr,enumitem,inconsolata,listings}
\pagestyle{fancy}\fancyhf{}\lhead{\nouppercase{\leftmark}}\rhead{\thepage}
\setlength{\headheight}{26pt}
\setlength{\parindent}{0pt}
\setlength{\parskip}{8pt plus 2pt minus 1pt}
\raggedbottom
\newenvironment{BreakableEquation}{\begin{equation}\begin{aligned}}{\end{aligned}\end{equation}}
\newenvironment{BreakableEquation*}{\begin{equation*}\begin{aligned}}{\end{aligned}\end{equation*}}
\newenvironment{tightalign}{\begingroup\small\allowdisplaybreaks\begin{align}}{\end{align}\endgroup}
\providecommand{\enumlistm}{enumitem}
\newenvironment{minted}[2][]{%
  \lstset{style=code,language=#2,#1}\begin{lstlisting}%
}{\end{lstlisting}}
\newcommand{\inputminted}[3][]{\begin{lstlisting}\end{lstlisting}}
\newlist{bullets}{itemize}{1}
\setlist[bullets]{label=--,leftmargin=1.6em,itemsep=2pt,topsep=4pt}
\newcommand{\varmapStart}{\textbf{VARIABLE MAPPING:}\par\begin{bullets}}
\newcommand{\var}[2]{\item $#1$ — #2}
\newcommand{\varmapEnd}{\end{bullets}}
\newcommand{\glossx}[6]{%
  \textbf{#1}\par
  \begin{bullets}
    \item \textbf{What:} #2
    \item \textbf{Why:} #3
    \item \textbf{How:} #4
    \item \textbf{ELI5:} #5
    \item \textbf{Pitfall/Example:} #6
  \end{bullets}
}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{claim}[theorem]{Claim}
\lstdefinestyle{code}{
  basicstyle=\ttfamily\footnotesize,
  backgroundcolor=\color{black!02},
  frame=single,
  numbers=left, numberstyle=\tiny, numbersep=8pt,
  breaklines=true, breakatwhitespace=true,
  tabsize=4, showstringspaces=false,
  upquote=true, keepspaces=true, columns=fullflexible,
  literate=
    {–}{{-}}1
    {—}{{-}}1
    {…}{{...}}1
    {≤}{{\ensuremath{\le}}}1
    {≥}{{\ensuremath{\ge}}}1
    {≠}{{\ensuremath{\ne}}}1
    {≈}{{\ensuremath{\approx}}}1
    {±}{{\ensuremath{\pm}}}1
    {→}{{\ensuremath{\to}}}1
    {←}{{\ensuremath{\leftarrow}}}1
    {∞}{{\ensuremath{\infty}}}1
    {√}{{\ensuremath{\sqrt{\ }}}}1
    {×}{{\ensuremath{\times}}}1
    {÷}{{\ensuremath{\div}}}1
}
\lstnewenvironment{codepy}[1][]%
  {\lstset{style=code,language=Python,#1}}%
  {}
\newcommand{\inlinecode}[1]{\lstinline[style=code]!#1!}
\newcommand{\LF}[2]{\par\noindent\textbf{#1:}~#2\par}
\newcommand{\WHAT}[1]{\LF{WHAT}{#1}}
\newcommand{\WHY}[1]{\LF{WHY}{#1}}
\newcommand{\HOW}[1]{\LF{HOW}{#1}}
\newcommand{\ELI}[1]{\LF{ELI5}{#1}}
\newcommand{\SCOPE}[1]{\LF{SCOPE}{#1}}
\newcommand{\CONFUSIONS}[1]{\LF{COMMON CONFUSIONS}{#1}}
\newcommand{\APPLICATIONS}[1]{\LF{APPLICATIONS}{#1}}
\newcommand{\FORMULA}[1]{\LF{FORMULA}{#1}}
\newcommand{\CANONICAL}[1]{\LF{CANONICAL FORM}{#1}}
\newcommand{\PRECONDS}[1]{\LF{PRECONDITIONS}{#1}}
\newcommand{\DERIVATION}[1]{\LF{DERIVATION}{#1}}
\newcommand{\EQUIV}[1]{\LF{EQUIVALENT FORMS}{#1}}
\newcommand{\LIMITS}[1]{\LF{LIMIT CASES}{#1}}
\newcommand{\INPUTS}[1]{\LF{INPUTS}{#1}}
\newcommand{\OUTPUTS}[1]{\LF{OUTPUTS}{#1}}
\newcommand{\RESULT}[1]{\LF{RESULT}{#1}}
\newcommand{\INTUITION}[1]{\LF{INTUITION}{#1}}
\newcommand{\PITFALLS}[1]{\LF{PITFALLS}{#1}}
\newcommand{\MODEL}[1]{\LF{CANONICAL MATH MODEL}{#1}}
\newcommand{\ASSUMPTIONS}[1]{\LF{ASSUMPTIONS}{#1}}
\newcommand{\WHICHFORMULA}[1]{\LF{WHICH FORMULA \& WHY}{#1}}
\newcommand{\GOVERN}[1]{\LF{GOVERNING EQUATION(S)}{#1}}
\newcommand{\UNITCHECK}[1]{\LF{UNIT CHECK}{#1}}
\newcommand{\EDGECASES}[1]{\LF{EDGE CASES}{#1}}
\newcommand{\ALTERNATE}[1]{\LF{ALTERNATE APPROACH (sketch)}{#1}}
\newcommand{\PROBLEM}[1]{\LF{PROBLEM}{#1}}
\newcommand{\API}[1]{\LF{API}{#1}}
\newcommand{\COMPLEXITY}[1]{\LF{COMPLEXITY}{#1}}
\newcommand{\FAILMODES}[1]{\LF{FAILURE MODES}{#1}}
\newcommand{\STABILITY}[1]{\LF{NUMERICAL STABILITY}{#1}}
\newcommand{\VALIDATION}[1]{\LF{VALIDATION}{#1}}
\newcommand{\EXPLANATION}[1]{\LF{EXPLANATION}{#1}}
\newcommand{\SCENARIO}[1]{\LF{SCENARIO}{#1}}
\newcommand{\PIPELINE}[1]{\LF{PIPELINE STEPS}{#1}}
\newcommand{\METRICS}[1]{\LF{METRICS}{#1}}
\newcommand{\INTERPRET}[1]{\LF{INTERPRETATION}{#1}}
\newcommand{\NEXTSTEPS}[1]{\LF{LIMITATIONS \& NEXT STEPS}{#1}}
\usepackage{titlesec}
\titleformat{\section}{\Large\bfseries}{\thesection}{0.6em}{}
\titlespacing*{\section}{0pt}{*2}{*1}
\usepackage{etoolbox}
\pretocmd{\section}{\clearpage}{}{}
\newcommand{\FormulaPage}[2]{%
  \clearpage
  \section*{Formula #1 — #2}%
  \addcontentsline{toc}{section}{Formula #1 — #2}%
}
\newcommand{\ProblemPage}[2]{%
  \clearpage
  \subsection*{Problem #1: #2}%
  \addcontentsline{toc}{subsection}{Problem #1: #2}%
}
\newcommand{\CodeDemoPage}[1]{%
  \clearpage
  \subsection*{Coding Demo: #1}%
  \addcontentsline{toc}{subsection}{Coding Demo: #1}%
}
\newcommand{\DomainPage}[1]{%
  \clearpage
  \subsection*{#1 (End-to-End)}%
  \addcontentsline{toc}{subsection}{#1 (End-to-End)}%
}
\begin{document}
\title{Comprehensive Study Sheet — Dimension and Rank}
\date{\today}
\maketitle
\tableofcontents
\clearpage
\section{Concept Overview}
\WHAT{
A vector space $V$ over a field $\mathbb{F}$ is a set with vector addition
and scalar multiplication satisfying the axioms. A basis of $V$ is a linearly
independent set that spans $V$. The dimension $\dim V$ is the cardinality of
any basis of $V$ (finite for finite-dimensional spaces).
For a linear map $T:V\to W$, the rank $\mathrm{rank}(T)$ is
$\dim(\mathrm{im}\,T)$ and the nullity $\mathrm{nullity}(T)$ is
$\dim(\ker T)$, where $\ker T=\{v\in V:T(v)=0\}$ and
$\mathrm{im}\,T=\{T(v):v\in V\}$. For a matrix $A\in\mathbb{F}^{m\times n}$,
$\mathrm{rank}(A)$ is $\dim$ of its column space (equivalently row space).
}
\WHY{
Dimension counts degrees of freedom. Rank measures how many independent
directions a linear transformation preserves. They control solvability of
linear systems, structure of subspaces, and invariants under isomorphism,
powering algorithms (RREF, SVD) and theory (isomorphism theorems).
}
\HOW{
1. Fix field $\mathbb{F}$ and vector space axioms.
2. Prove every finite spanning set contains a basis (via exchange lemma).
3. Show all bases have equal size, defining $\dim$.
4. For linear $T$, analyze $\ker T$ and $\mathrm{im}\,T$, prove
rank-nullity $\dim V=\mathrm{rank}(T)+\mathrm{nullity}(T)$.
5. Identify matrix rank with pivot count and with SVD nonzero singular values.
}
\ELI{
Dimension is the number of independent sliders needed to describe any vector.
Rank is how many sliders remain effective after a linear machine acts on them.
}
\SCOPE{
Finite-dimensional vector spaces over any field $\mathbb{F}$. Rank of a matrix
is at most $\min\{m,n\}$. For infinite-dimensional spaces, many theorems still
hold but cardinals replace integers. Over finite fields, probability statements
about rank differ from real-field generic results.
}
\CONFUSIONS{
Rank vs. determinant: determinant zero iff rank $<n$, but determinant is a
signed volume scale for square matrices only. Dimension vs. size of a set:
dimension counts basis vectors, not number of elements. Column rank vs. row
rank: always equal. Full rank vs. full column rank vs. full row rank: specify
relative to $m$ or $n$.
}
\APPLICATIONS{
\begin{bullets}
\item Mathematical foundations: isomorphism theorems, dimension arguments.
\item Computational modeling: solvability of $Ax=b$, least squares, SVD.
\item Physical/engineering: degrees of freedom, constraints, kinematics.
\item Statistics/ML: multicollinearity, feature redundancy, PCA dimension.
\end{bullets}
}
\textbf{ANALYTIC STRUCTURE.}
Linear, convex sets of solutions. Subspaces closed under addition and scalar
multiplication. Rank is invariant under invertible row/column operations.

\textbf{CANONICAL LINKS.}
Steinitz Exchange Lemma, Rank-Nullity Theorem, Row=Column Rank Theorem,
Dimension formula for subspace sums, Sylvester rank inequality.

\textbf{PROBLEM-TYPE RECOGNITION HEURISTICS.}
\begin{bullets}
\item Phrases: linearly independent, span, basis, degrees of freedom.
\item Matrices with constraints or equations $Ax=b$.
\item Sum/intersection of subspaces, projections, kernels.
\item Composition of maps $AB$ suggesting rank inequalities.
\end{bullets}
\textbf{SOLUTION STRATEGY BLUEPRINT.}
\begin{bullets}
\item Translate to linear map/matrix and identify subspaces involved.
\item Choose basis, compute RREF or SVD to get pivots/rank.
\item Apply rank-nullity or dimension-sum formula.
\item Interpret result as degrees of freedom or constraints count.
\end{bullets}
\textbf{CONCEPTUAL INVARIANTS.}
\begin{bullets}
\item Dimension of a vector space.
\item Rank under elementary operations and change of basis.
\item Nullity as constraints count complement to rank.
\end{bullets}
\textbf{EDGE INTUITION.}
\begin{bullets}
\item If constraints overwhelm variables, rank saturates at $\min\{m,n\}$.
\item Near-singular transforms lose rank by small perturbations to zero.
\end{bullets}
\section{Glossary}
\glossx{Dimension}
{Number of vectors in any basis of a vector space $V$.}
{Measures degrees of freedom and classifies finite-dimensional spaces
up to isomorphism.}
{Find a basis via Gaussian elimination or exchange, count its size.}
{How many independent knobs to set to describe any vector in $V$.}
{Confusing set cardinality with dimension. A line in $\mathbb{R}^3$ has
infinitely many points but dimension $1$.}
\glossx{Rank}
{Dimension of the image of a linear map or of the column space of a matrix.}
{Determines solvability and sensitivity; equals number of pivots and nonzero
singular values.}
{Reduce to RREF or compute SVD; count pivots or nonzero singular values.}
{How many independent directions survive after the transformation.}
{Mistaking numerical near-zero singular values for nonzero due to noise.}
\glossx{Null Space (Kernel)}
{Set of vectors mapped to zero: $\ker T=\{v:T(v)=0\}$.}
{Characterizes non-uniqueness of solutions; dimension is nullity.}
{Solve $T(v)=0$ or $Av=0$; parametrize free variables.}
{Directions the machine completely ignores.}
{Forgets that any solution of $Ax=b$ is $x=x_0+\ker A$ when consistent.}
\glossx{Basis}
{Linearly independent spanning set of a vector space.}
{Defines coordinates and dimension; enables decomposition and change of basis.}
{Use exchange: extend LI set to a basis or trim spanning set to a basis.}
{A minimal set of building blocks to make everything in the space.}
{Including dependent vectors in a basis; basis must be independent.}
\section{Symbol Ledger}
\varmapStart
\var{\mathbb{F}}{Underlying field (e.g., $\mathbb{R}$, $\mathbb{C}$, $\mathbb{F}_q$).}
\var{V,W}{Vector spaces over $\mathbb{F}$.}
\var{U}{A subspace of $V$.}
\var{T}{Linear map $T:V\to W$.}
\var{A}{Matrix in $\mathbb{F}^{m\times n}$ representing a linear map.}
\var{\dim V}{Dimension of vector space $V$ as a nonnegative integer.}
\var{\ker T}{Kernel (null space) of $T$.}
\var{\mathrm{im}\,T}{Image of $T$.}
\var{\mathrm{rank}(T)}{Dimension of $\mathrm{im}\,T$.}
\var{\mathrm{nullity}(T)}{Dimension of $\ker T$.}
\var{r(A)}{Rank of matrix $A$.}
\var{n(A)}{Nullity of matrix $A$.}
\var{U+W}{Sum subspace $\{u+w:u\in U,w\in W\}$.}
\var{U\cap W}{Intersection subspace.}
\var{I_n}{Identity matrix of size $n$.}
\var{R}{Row-reduced echelon form of a matrix.}
\var{P,Q}{Invertible matrices encoding row/column operations.}
\varmapEnd
\section{Formula Canon — One Formula Per Page}
\FormulaPage{1}{Rank-Nullity Theorem}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
For a linear map $T:V\to W$ with $V$ finite-dimensional,
$\dim V=\mathrm{rank}(T)+\mathrm{nullity}(T)$.
\WHAT{
Relates the dimension of the domain to the dimensions of image and kernel.
}
\WHY{
Counts degrees of freedom split into effective directions (rank) and
lost directions (nullity). Key to solution structure of linear systems.
}
\FORMULA{
\[
\dim V=\dim(\mathrm{im}\,T)+\dim(\ker T).
\]
}
\CANONICAL{
$V$ finite-dimensional, $T$ linear. Holds over any field $\mathbb{F}$.
}
\PRECONDS{
\begin{bullets}
\item $V$ is a finite-dimensional vector space over $\mathbb{F}$.
\item $T:V\to W$ is linear.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
If $B_K$ is a basis of $\ker T$ and $B_E$ is any set in $V$ whose images
$T(B_E)$ form a basis of $\mathrm{im}\,T$, then $B_K\cup B_E$ is a basis of $V$.
\end{lemma}
\begin{proof}
$T(B_E)$ spans $\mathrm{im}\,T$, so every $T(v)$ is a linear combination
of $T(B_E)$. Thus for any $v\in V$, $T(v-\sum c_i e_i)=0$, hence
$v-\sum c_i e_i\in\ker T=\mathrm{span}(B_K)$. Therefore $B_K\cup B_E$
spans $V$. For linear independence, if
$\sum \alpha_j k_j+\sum \beta_i e_i=0$, apply $T$ to get
$\sum \beta_i T(e_i)=0$. Since $T(B_E)$ is independent, $\beta_i=0$,
hence $\sum \alpha_j k_j=0$ and $\alpha_j=0$ because $B_K$ is a basis
of $\ker T$. Thus $B_K\cup B_E$ is a basis. \qedhere
\end{proof}
\DERIVATION{
\begin{align*}
\text{Step 1 (Choose bases):}\quad
& B_K=\{k_1,\dots,k_p\}\text{ basis of }\ker T,\ p=\mathrm{nullity}(T).\\
& \{e_1,\dots,e_r\}\subset V\text{ with } \{T(e_1),\dots,T(e_r)\}
\text{ a basis of }\mathrm{im}\,T,\\
& r=\mathrm{rank}(T).\\
\text{Step 2 (Apply Lemma):}\quad
& B:=B_K\cup\{e_1,\dots,e_r\}\text{ is a basis of }V.\\
\text{Step 3 (Count):}\quad
& |B|=|B_K|+r=p+r=\dim V.\\
\text{Step 4 (Conclude):}\quad
& \dim V=\mathrm{nullity}(T)+\mathrm{rank}(T).
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Compute $\ker T$ and $\mathrm{im}\,T$ via RREF.
\item Count free variables for nullity, pivot columns for rank.
\item Verify counts add to $\dim V$.
\end{bullets}
\EQUIV{
\begin{bullets}
\item For $A\in\mathbb{F}^{m\times n}$, $n=r(A)+n(A)$.
\item $\dim(V/\ker T)=\dim(\mathrm{im}\,T)$.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item If $T$ is injective, nullity $=0$ and $\dim V=\mathrm{rank}(T)$.
\item If $T$ is zero map, rank $=0$ and $\dim V=\mathrm{nullity}(T)$.
\end{bullets}
}
\INPUTS{$V, W, T, \ker T, \mathrm{im}\,T$.}
\DERIVATION{
\begin{align*}
\text{Example: }& A=\begin{bmatrix}1&2&3\\0&1&1\end{bmatrix}\in\mathbb{R}^{2\times 3}.\\
& r(A)=2\ (\text{two pivots}),\ n(A)=3-2=1.\\
& \dim \mathbb{R}^3 = r(A)+n(A)=2+1=3.
\end{align*}
}
\RESULT{
The domain dimension splits into rank and nullity; counts match exactly.
}
\UNITCHECK{
Check integer counts: $0\le r\le \min\{m,n\}$ and $n=r+n(A)$.
}
\PITFALLS{
\begin{bullets}
\item Forgetting to count pivots after full reduction to RREF.
\item Confusing number of equations with rank; equations may be dependent.
\end{bullets}
}
\INTUITION{
Constraints remove degrees of freedom; what remains is rank, what is
eliminated is nullity.
}
\CANONICAL{
\begin{bullets}
\item Universal identity: $\dim V=\dim\ker T+\dim\mathrm{im}\,T$.
\item Quotient-space isomorphism: $V/\ker T\cong \mathrm{im}\,T$.
\end{bullets}
}
\FormulaPage{2}{Row Rank = Column Rank}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
For $A\in\mathbb{F}^{m\times n}$, the dimensions of row and column spaces
are equal; this common value is $\mathrm{rank}(A)$.
\WHAT{
Establishes that rank can be computed from either rows or columns.
}
\WHY{
Justifies equivalence of pivot-count in row and column operations and
supports invariance of rank under transposition.
}
\FORMULA{
\[
\dim(\mathrm{Row}(A))=\dim(\mathrm{Col}(A))=r(A)=r(A^\top).
\]
}
\CANONICAL{
Any field $\mathbb{F}$, any matrix $A\in\mathbb{F}^{m\times n}$.
}
\PRECONDS{
\begin{bullets}
\item None beyond linear algebra axioms over $\mathbb{F}$.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
There exist invertible $P\in\mathbb{F}^{m\times m},Q\in\mathbb{F}^{n\times n}$
such that $PAQ=\begin{bmatrix}I_r&0\\0&0\end{bmatrix}$ with $r=r(A)$.
\end{lemma}
\begin{proof}
Gaussian elimination realizes elementary row operations as left
multiplication by invertible matrices, and column operations as right
multiplication by invertible matrices. Reduce $A$ to its rank-normal
form with $r$ pivots, yielding the stated block matrix. \qedhere
\end{proof}
\DERIVATION{
\begin{align*}
\text{Step 1: }& PAQ=\begin{bmatrix}I_r&0\\0&0\end{bmatrix}.\\
\text{Step 2: }& \mathrm{Row}(A)\cong \mathrm{Row}(PAQ)
\ (\text{row ops do not change row space dimension}).\\
\text{Step 3: }& \dim\mathrm{Row}(PAQ)=r.\\
\text{Step 4: }& \mathrm{Col}(A)\cong \mathrm{Col}(PAQ)
\ (\text{column ops do not change column space dimension}).\\
\text{Step 5: }& \dim\mathrm{Col}(PAQ)=r.\\
\text{Conclude: }& \dim\mathrm{Row}(A)=\dim\mathrm{Col}(A)=r.
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Reduce to RREF and count pivots; that is both row and column rank.
\item Alternatively, compute $r(A)=r(A^\top)$ by transposition.
\end{bullets}
\EQUIV{
\begin{bullets}
\item $r(A)=r(A^\top)$.
\item $r(A)$ equals number of nonzero singular values of $A$.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item $r(A)\le\min\{m,n\}$ with equality iff $A$ has full row/column rank.
\end{bullets}
}
\INPUTS{$A\in\mathbb{F}^{m\times n}$, $r=r(A)$.}
\DERIVATION{
\begin{align*}
\text{Example: }& A=\begin{bmatrix}1&2&1\\2&4&2\\0&1&1\end{bmatrix}.\\
& \text{RREF}\to \begin{bmatrix}1&2&1\\0&0&0\\0&1&1\end{bmatrix}
\to \begin{bmatrix}1&0&-1\\0&1&1\\0&0&0\end{bmatrix}.\\
& r=2=\dim\text{Row}=\dim\text{Col}.
\end{align*}
}
\RESULT{
Row rank equals column rank and equals the pivot count.
}
\UNITCHECK{
$0\le r\le \min\{m,n\}$; integer invariant under $A\mapsto P A Q$ invertible.
}
\PITFALLS{
\begin{bullets}
\item Counting leading nonzeros before full elimination gives wrong rank.
\item Assuming independent rows imply independent columns without proof.
\end{bullets}
}
\INTUITION{
Same number of independent equations as independent unknown directions
governed by the matrix.
}
\CANONICAL{
\begin{bullets}
\item Invariant: $\mathrm{rank}(A)=\mathrm{rank}(A^\top)$.
\item Normal form: $PAQ$ to a block identity of size $r$.
\end{bullets}
}
\FormulaPage{3}{Dimension of Sum and Intersection}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
For subspaces $U,W\le V$ with $V$ finite-dimensional,
$\dim(U+W)=\dim U+\dim W-\dim(U\cap W)$.
\WHAT{
Relates dimensions of sum and intersection of subspaces.
}
\WHY{
Avoids double-counting overlapping directions when combining subspaces.
Central in projections, direct sums, and Grassmann formulas.
}
\FORMULA{
\[
\dim(U+W)=\dim U+\dim W-\dim(U\cap W).
\]
}
\CANONICAL{
$U,W$ subspaces of a finite-dimensional $V$ over $\mathbb{F}$.
}
\PRECONDS{
\begin{bullets}
\item $\dim V<\infty$.
\item $U,W\le V$.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
If $B_I$ is a basis of $U\cap W$, extend it to bases $B_U$ of $U$ and
$B_W$ of $W$. Then $B_U\cup B_W$ spans $U+W$ and is independent modulo
$U\cap W$.
\end{lemma}
\begin{proof}
Every $u\in U$ and $w\in W$ expand in $B_U$ and $B_W$, so any $u+w$
expands in $B_U\cup B_W$, giving spanning. If a combination vanishes,
collect $U$ and $W$ parts and use the direct decomposition with $B_I$
to conclude coefficients in complements vanish. \qedhere
\end{proof}
\DERIVATION{
\begin{align*}
\text{Step 1: }& B_I=\{i_1,\dots,i_s\}\text{ basis of }U\cap W.\\
\text{Step 2: }& B_U=B_I\cup\{u_1,\dots,u_p\},\ \dim U=s+p.\\
\text{Step 3: }& B_W=B_I\cup\{w_1,\dots,w_q\},\ \dim W=s+q.\\
\text{Step 4: }& B:=B_I\cup\{u_1,\dots,u_p,w_1,\dots,w_q\}\\
& \text{spans }U+W\ \text{and is independent}.\\
\text{Step 5: }& \dim(U+W)=|B|=s+p+q.\\
\text{Step 6: }& s+p+q=(s+p)+(s+q)-s=\dim U+\dim W-\dim(U\cap W).
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Compute bases for $U$, $W$, and $U\cap W$ by solving linear systems.
\item Apply the dimension formula to find $\dim(U+W)$.
\end{bullets}
\EQUIV{
\begin{bullets}
\item $\dim U+\dim W=\dim(U+W)+\dim(U\cap W)$.
\item If $U\cap W=\{0\}$, then $\dim(U+W)=\dim U+\dim W$ (direct sum).
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item Maximal overlap: if $U\subseteq W$, then $\dim(U+W)=\dim W$.
\item Minimal overlap: if $U\cap W=\{0\}$, sum of dimensions.
\end{bullets}
}
\INPUTS{$U,W\le V$, bases and intersection.}
\DERIVATION{
\begin{align*}
\text{Example in }\mathbb{R}^3:&\ U=\mathrm{span}\{(1,0,0),(0,1,0)\},\\
& W=\mathrm{span}\{(1,1,0),(0,0,1)\}.\\
& U\cap W=\mathrm{span}\{(1,1,0)\}\Rightarrow \dim=1.\\
& \dim U=2,\ \dim W=2,\ \dim(U+W)=2+2-1=3.
\end{align*}
}
\RESULT{
The dimension of the sum equals sum of dimensions minus overlap.
}
\UNITCHECK{
All counts are integers between $0$ and $\dim V$; sum fits within $\dim V$.
}
\PITFALLS{
\begin{bullets}
\item Forgetting to subtract the intersection dimension.
\item Assuming $U\cap W=\{0\}$ without verification.
\end{bullets}
}
\INTUITION{
Add directions from $U$ and $W$ but do not double-count shared directions.
}
\CANONICAL{
\begin{bullets}
\item Grassmann identity: $\dim U+\dim W=\dim(U+W)+\dim(U\cap W)$.
\end{bullets}
}
\FormulaPage{4}{Sylvester Rank Inequality}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
For $A\in\mathbb{F}^{m\times n}$ and $B\in\mathbb{F}^{n\times p}$,
$\mathrm{rank}(AB)\ge \mathrm{rank}(A)+\mathrm{rank}(B)-n$.
\WHAT{
Lower bound on rank of a product by ranks of factors.
}
\WHY{
Controls dimension loss under composition; widely used in identifiability
and system theory.
}
\FORMULA{
\[
r(AB)\ge r(A)+r(B)-n.
\]
}
\CANONICAL{
Matrices over any field with compatible dimensions.
}
\PRECONDS{
\begin{bullets}
\item $A$ is $m\times n$, $B$ is $n\times p$.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
For linear maps $U\xrightarrow{B} \mathbb{F}^n\xrightarrow{A} Y$,
$\ker(AB)\subseteq B^{-1}(\ker A)$ and
$\dim B^{-1}(S)\le \dim S + (p-r(B))$ for any subspace $S\le \mathbb{F}^n$.
\end{lemma}
\begin{proof}
First inclusion: if $ABx=0$ then $Bx\in\ker A$, so $x\in B^{-1}(\ker A)$.
For the dimension bound, the restriction $B: \mathbb{F}^p\to \mathrm{im}\,B$
has kernel of dimension $p-r(B)$; the preimage of $S$ intersects each coset
in at most $\dim S$ dimensions plus the kernel. \qedhere
\end{proof}
\DERIVATION{
\begin{align*}
\text{Step 1: }& \mathrm{nullity}(AB)=p-r(AB).\\
\text{Step 2: }& \ker(AB)\subseteq B^{-1}(\ker A).\\
\text{Step 3: }& \dim \ker(AB)\le \dim B^{-1}(\ker A).\\
\text{Step 4: }& \dim B^{-1}(\ker A)\le \dim \ker A + (p-r(B))\\
& =(n-r(A))+(p-r(B)).\\
\text{Step 5: }& p-r(AB)\le n-r(A)+p-r(B).\\
\text{Step 6: }& r(AB)\ge r(A)+r(B)-n.
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Compute $r(A)$ and $r(B)$; apply the inequality to bound $r(AB)$.
\item Use examples to check tightness via generic random matrices.
\end{bullets}
\EQUIV{
\begin{bullets}
\item $r(AB)\ge r(A)+r(B^\top)-n$ since $r(B)=r(B^\top)$.
\item Symmetric forms by transposition on suitable sizes.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item Upper bound: $r(AB)\le \min\{r(A),r(B)\}$.
\item If $A$ or $B$ has full column/row rank, inequality is tight.
\end{bullets}
}
\INPUTS{$A\in\mathbb{F}^{m\times n}$, $B\in\mathbb{F}^{n\times p}$.}
\DERIVATION{
\begin{align*}
\text{Example: }& A=\begin{bmatrix}1&0\\0&0\\0&1\end{bmatrix},\
B=\begin{bmatrix}1&1&0\\0&1&1\end{bmatrix}.\\
& r(A)=2,\ r(B)=2,\ n=2.\\
& AB=\begin{bmatrix}1&1&0\\0&0&0\\0&1&1\end{bmatrix},\ r(AB)=2.\\
& \text{Bound: }2\ge 2+2-2=2\ \text{(tight)}.
\end{align*}
}
\RESULT{
Guaranteed minimum rank for products, matching tight in many cases.
}
\UNITCHECK{
All ranks are integers in admissible bounds. Inequality respects limits.
}
\PITFALLS{
\begin{bullets}
\item Reversing inequality direction.
\item Forgetting that $n$ is the inner dimension, not $m$ or $p$.
\end{bullets}
}
\INTUITION{
Composition cannot lose more than the sum of individual losses relative
to the shared middle dimension.
}
\CANONICAL{
\begin{bullets}
\item Lower bound invariant for products across fields.
\end{bullets}
}
\FormulaPage{5}{Basis Cardinality and Dimension}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
All bases of a finite-dimensional vector space $V$ have the same number
of elements; this number is $\dim V$.
\WHAT{
Well-definedness of dimension via basis size.
}
\WHY{
Ensures dimension is an invariant of the space and independent of basis.
}
\FORMULA{
\[
|B_1|=|B_2|=\dim V\quad \text{for any bases }B_1,B_2\text{ of }V.
\]
}
\CANONICAL{
Finite-dimensional vector spaces over any field $\mathbb{F}$.
}
\PRECONDS{
\begin{bullets}
\item $V$ has a finite spanning set (equivalently a finite basis exists).
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}[Steinitz Exchange]
If $S=\{v_1,\dots,v_n\}$ spans $V$ and $L=\{\ell_1,\dots,\ell_m\}$
is linearly independent in $V$, then $m\le n$ and there exists a subset
$S'\subseteq S$ with $|S'|=m$ such that $(S\setminus S')\cup L$ spans $V$.
\end{lemma}
\begin{proof}
Induct on $m$. For $m=0$ trivial. For $m>0$, write $\ell_m$ as a
combination of $S$. At least one coefficient is nonzero; exchange the
corresponding vector in $S$ for $\ell_m$ and apply the induction hypothesis
to $\{\ell_1,\dots,\ell_{m-1}\}$ and the new spanning set. \qedhere
\end{proof}
\DERIVATION{
\begin{align*}
\text{Step 1: }& B_1,B_2\ \text{bases of }V\Rightarrow
\text{ both LI and spanning}.\\
\text{Step 2: }& \text{Apply exchange with }S=B_2,L=B_1
\Rightarrow |B_1|\le |B_2|.\\
\text{Step 3: }& \text{Symmetry }S=B_1,L=B_2\Rightarrow |B_2|\le |B_1|.\\
\text{Step 4: }& |B_1|=|B_2|=:n,\ \dim V=n.
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Show a set is a basis by LI and spanning, then its size is $\dim V$.
\item To compare, use exchange lemma to bound cardinalities both ways.
\end{bullets}
\EQUIV{
\begin{bullets}
\item Any LI set has at most $\dim V$ vectors.
\item Any spanning set has at least $\dim V$ vectors.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item Infinite-dimensional spaces do not have a finite basis.
\end{bullets}
}
\INPUTS{$V$, bases $B_1,B_2$.}
\DERIVATION{
\begin{align*}
\text{Example: }& V=\mathbb{R}^3,\ B_1=\{e_1,e_2,e_3\},\\
& B_2=\{(1,1,0),(0,1,1),(1,0,1)\}.\\
& \text{Matrix with columns of }B_2 \text{ has } r=3.\\
& |B_1|=|B_2|=3,\ \dim V=3.
\end{align*}
}
\RESULT{
Dimension is well defined as size of any basis.
}
\UNITCHECK{
Counts are nonnegative integers; equality by double inequality.
}
\PITFALLS{
\begin{bullets}
\item Assuming a spanning set is automatically independent.
\item Forgetting to prove both LI and spanning for a candidate basis.
\end{bullets}
}
\INTUITION{
Trading one building block for another preserves the number needed to
construct the space.
}
\CANONICAL{
\begin{bullets}
\item Exchange principle underlies the invariant dimension.
\end{bullets}
}
\section{10 Exhaustive Problems and Solutions}
\ProblemPage{1}{Compute Rank and Nullity via RREF}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Given $A\in\mathbb{R}^{3\times 4}$, find $r(A)$ and $n(A)$ and verify
$r(A)+n(A)=4$.
\PROBLEM{
Let $A=\begin{bmatrix}1&2&0&1\\2&4&1&3\\0&0&1&1\end{bmatrix}$.
Compute rank and nullity and parametrize $\ker A$.
}
\MODEL{
\[
A\in\mathbb{R}^{3\times 4},\ r(A)=\#\text{pivots},\ n(A)=4-r(A).
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Arithmetic over $\mathbb{R}$.
\item RREF correctly reveals pivot structure.
\end{bullets}
}
\varmapStart
\var{A}{Given matrix.}
\var{r(A)}{Rank of $A$.}
\var{n(A)}{Nullity of $A$.}
\var{x}{Vector in $\mathbb{R}^4$.}
\varmapEnd
\WHICHFORMULA{
Rank-Nullity for matrices: $n=r(A)+n(A)$ with $n=4$.
}
\GOVERN{
\[
\text{RREF}(A)=R,\quad r(A)=\text{pivot count},\quad \ker A=\ker R.
\]
}
\INPUTS{$A=\begin{bmatrix}1&2&0&1\\2&4&1&3\\0&0&1&1\end{bmatrix}$.}
\DERIVATION{
\begin{align*}
\text{Step 1: Row reduce }A:&\
\begin{bmatrix}1&2&0&1\\2&4&1&3\\0&0&1&1\end{bmatrix}
\to \begin{bmatrix}1&2&0&1\\0&0&1&1\\0&0&1&1\end{bmatrix}\\
&\to \begin{bmatrix}1&2&0&1\\0&0&1&1\\0&0&0&0\end{bmatrix}
\to \begin{bmatrix}1&2&0&1\\0&0&1&1\\0&0&0&0\end{bmatrix}.\\
\text{Step 2: Pivots:}&\ \text{col 1 and col 3} \Rightarrow r(A)=2.\\
\text{Step 3: Nullity:}&\ n(A)=4-2=2.\\
\text{Step 4: Solve }Rx=0:&\
x_1+2x_2+x_4=0,\ x_3+x_4=0.\\
& \text{Free variables }x_2=t,\ x_4=s.\\
& x_3=-s,\ x_1=-2t-s.\\
& x=t\begin{bmatrix}-2\\1\\0\\0\end{bmatrix}
+s\begin{bmatrix}-1\\0\\-1\\1\end{bmatrix}.
\end{align*}
}
\RESULT{
$r(A)=2$, $n(A)=2$, and a basis of $\ker A$ is
$\{(-2,1,0,0)^\top,(-1,0,-1,1)^\top\}$.
}
\UNITCHECK{
$r+n=2+2=4=n$. Nullity equals number of free variables: $2$.
}
\EDGECASES{
\begin{bullets}
\item If last row were nonzero independent, rank would be $3$ and nullity $1$.
\end{bullets}
}
\ALTERNATE{
Compute $r(A)$ as number of nonzero singular values via SVD.
}
\VALIDATION{
\begin{bullets}
\item Substitute basis vectors into $A x=0$ to verify zero output.
\item Check independence of kernel basis by determinant of $2\times 2$ Gram.
\end{bullets}
}
\INTUITION{
Two independent constraints reduce four degrees of freedom to two.
}
\CANONICAL{
\begin{bullets}
\item $n= r+n(A)$ encodes degrees-of-freedom split.
\end{bullets}
}
\ProblemPage{2}{Dimension of Sum and Intersection from Generators}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Compute $\dim(U+W)$ given spanning sets of $U$ and $W$ in $\mathbb{R}^4$.
\PROBLEM{
Let $U=\mathrm{span}\{u_1,u_2\}$ with $u_1=(1,0,1,0)$, $u_2=(0,1,1,0)$
and $W=\mathrm{span}\{w_1,w_2\}$ with $w_1=(1,1,0,0)$, $w_2=(0,0,1,1)$.
Find $\dim(U)$, $\dim(W)$, $\dim(U\cap W)$, and $\dim(U+W)$.
}
\MODEL{
\[
\dim(U+W)=\dim U+\dim W-\dim(U\cap W).
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Work over $\mathbb{R}$.
\item Linear independence determined via RREF.
\end{bullets}
}
\varmapStart
\var{u_1,u_2}{Generators of $U$.}
\var{w_1,w_2}{Generators of $W$.}
\var{X}{Matrix with columns forming combined set.}
\varmapEnd
\WHICHFORMULA{
Grassmann identity: $\dim U+\dim W=\dim(U+W)+\dim(U\cap W)$.
}
\GOVERN{
\[
\dim(U\cap W)=\dim U+\dim W-\dim(U+W).
\]
}
\INPUTS{$u_1,u_2,w_1,w_2$ as above.}
\DERIVATION{
\begin{align*}
\text{Step 1: }\dim U:&\
U\text{ columns }[u_1\ u_2]=\begin{bmatrix}1&0\\0&1\\1&1\\0&0\end{bmatrix}.\\
& r=2\Rightarrow \dim U=2.\\
\text{Step 2: }\dim W:&\
[w_1\ w_2]=\begin{bmatrix}1&0\\1&0\\0&1\\0&1\end{bmatrix}.\\
& r=2\Rightarrow \dim W=2.\\
\text{Step 3: }\dim(U+W):&\
X=[u_1\ u_2\ w_1\ w_2]\\
&=\begin{bmatrix}1&0&1&0\\0&1&1&0\\1&1&0&1\\0&0&0&1\end{bmatrix}.\\
& \text{RREF}\to \begin{bmatrix}1&0&1&0\\0&1&1&0\\0&0&1&1\\0&0&0&1\end{bmatrix}
\to \begin{bmatrix}1&0&0&-1\\0&1&0&-1\\0&0&1&1\\0&0&0&1\end{bmatrix}.\\
& r=4\Rightarrow \dim(U+W)=4.\\
\text{Step 4: }\dim(U\cap W):&\ 2+2-4=0.
\end{align*}
}
\RESULT{
$\dim U=2,\ \dim W=2,\ \dim(U\cap W)=0,\ \dim(U+W)=4$.
}
\UNITCHECK{
All dimensions in $[0,4]$; sum identity holds: $2+2=4+0$.
}
\EDGECASES{
\begin{bullets}
\item If $w_1=u_1$, intersection dimension increases to $1$.
\end{bullets}
}
\ALTERNATE{
Solve $u=\alpha_1 u_1+\alpha_2 u_2=w=\beta_1 w_1+\beta_2 w_2$ for
intersection; only trivial solution exists.
}
\VALIDATION{
\begin{bullets}
\item Check that no nonzero vector is simultaneously in both spans.
\end{bullets}
}
\INTUITION{
$U$ and $W$ generate all four coordinates with no overlap beyond $0$.
}
\CANONICAL{
\begin{bullets}
\item Grassmann identity precisely tracks overlap.
\end{bullets}
}
\ProblemPage{3}{Uniqueness of Dimension via Exchange}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Prove any two bases of a finite-dimensional $V$ have equal size.
}
\PROBLEM{
Let $B_1,B_2$ be bases of $V$. Prove $|B_1|=|B_2|$ using Steinitz
Exchange and deduce $\dim V$ is well-defined.
}
\MODEL{
\[
|B_1|\le |B_2|\ \text{and}\ |B_2|\le |B_1| \Rightarrow |B_1|=|B_2|.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $V$ is spanned by a finite set.
\end{bullets}
}
\varmapStart
\var{B_1,B_2}{Bases of $V$.}
\varmapEnd
\WHICHFORMULA{
Basis cardinality theorem; relies on exchange lemma.
}
\GOVERN{
\[
\text{If }S\text{ spans }V\text{ and }L\text{ LI},\ |L|\le |S|.
\]
}
\INPUTS{$B_1,B_2$.}
\DERIVATION{
\begin{align*}
\text{Step 1: }& B_1\text{ LI},\ B_2\text{ spans }\Rightarrow |B_1|\le |B_2|.\\
\text{Step 2: }& B_2\text{ LI},\ B_1\text{ spans }\Rightarrow |B_2|\le |B_1|.\\
\text{Step 3: }& \Rightarrow |B_1|=|B_2|=:n.\\
\text{Step 4 (Example): }& \mathbb{R}^2,\ B_1=\{e_1,e_2\},\\
& B_2=\{(1,1),(1,-1)\},\ |B_1|=|B_2|=2.
\end{align*}
}
\RESULT{
Dimension $\dim V$ is a well-defined integer equal to the size of any basis.
}
\UNITCHECK{
Counts are integers; double inequality yields equality.
}
\EDGECASES{
\begin{bullets}
\item Infinite-dimensional spaces do not admit finite bases.
\end{bullets}
}
\ALTERNATE{
Use isomorphism with $\mathbb{F}^n$ after fixing any basis $B_1$.
}
\VALIDATION{
\begin{bullets}
\item Verify both LI and spanning for each proposed basis.
\end{bullets}
}
\INTUITION{
You cannot reduce the number of building blocks below what is necessary,
nor need more than necessary.
}
\CANONICAL{
\begin{bullets}
\item Dimension invariant under basis change.
\end{bullets}
}
\ProblemPage{4}{Narrative: Alice and Bob Compress Images}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Low-rank approximation uses rank to control compression.
}
\PROBLEM{
Alice stores images as matrices and uses a rank-$k$ approximation.
Bob wants to know how many degrees of freedom remain after compression.
Given $A\in\mathbb{R}^{100\times 100}$ with $r(A)=80$ and $k=20$,
what is the dimension of the set $\{X:\mathrm{rank}(X)\le 20\}$ locally
around a full-rank-$20$ point, and how many parameters describe it?
}
\MODEL{
\[
\text{Set of rank-}k\text{ matrices: } \{U\Sigma V^\top: U\in\mathbb{R}^{m\times k},
V\in\mathbb{R}^{n\times k}, \Sigma\in\mathbb{R}^{k\times k}\ \text{diag}\}.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Real field, $m=n=100$, $k=20$, local manifold dimension applies.
\item $U,V$ have orthonormal columns, $\Sigma$ invertible $k\times k$.
\end{bullets}
}
\varmapStart
\var{m,n}{Matrix dimensions ($100$).}
\var{k}{Target rank ($20$).}
\var{U,V,\Sigma}{SVD factors.}
\varmapEnd
\WHICHFORMULA{
Dimension of rank-$k$ manifold: $k(m+n-k)$ for $m\times n$ matrices.
}
\GOVERN{
\[
\dim \mathcal{M}_{k} = k(m+n-k).
\]
}
\INPUTS{$m=n=100$, $k=20$.}
\DERIVATION{
\begin{align*}
\text{Step 1: }& U\in \mathrm{St}(m,k):\ \dim = mk-\tfrac{k(k+1)}{2}.\\
\text{Step 2: }& V\in \mathrm{St}(n,k):\ \dim = nk-\tfrac{k(k+1)}{2}.\\
\text{Step 3: }& \Sigma\ \text{diag invertible}:\ \dim=k.\\
\text{Step 4: }& \text{Quotient by }O(k)\ \text{action adds back } \tfrac{k(k-1)}{2}.\\
\text{Step 5: }& \dim = mk-\tfrac{k(k+1)}{2}+nk-\tfrac{k(k+1)}{2}+k+\tfrac{k(k-1)}{2}\\
&=k(m+n-k).
\end{align*}
}
\RESULT{
$\dim$ of local set of rank-$20$ matrices in $\mathbb{R}^{100\times 100}$
is $20(100+100-20)=20\cdot 180=3600$ parameters.
}
\UNITCHECK{
$\dim$ is integer and $\le mn=10000$; for $k=0$, dimension $0$; for $k=100$,
dimension $10000$.
}
\EDGECASES{
\begin{bullets}
\item As $k\to 0$, dimension tends to $0$.
\item As $k\to \min\{m,n\}$, dimension tends to $mn$.
\end{bullets}
}
\ALTERNATE{
Parametrize $X=UV^\top$ with $U\in\mathbb{R}^{m\times k}$,
$V\in\mathbb{R}^{n\times k}$, quotient by $GL(k)$ yielding same count.
}
\VALIDATION{
\begin{bullets}
\item Numeric SVD shows $k$ nonzero singular values characterize rank-$k$.
\end{bullets}
}
\INTUITION{
Low-rank compression reduces parameters from $mn$ to about $k(m+n)$.
}
\CANONICAL{
\begin{bullets}
\item Rank determines intrinsic degrees of freedom of matrix families.
\end{bullets}
}
\ProblemPage{5}{Narrative: Sensor Placement and Rank}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Full column rank implies unique recovery of signals from sensor readings.
}
\PROBLEM{
Bob places $m$ sensors measuring $n$ source signals via matrix $A$.
If $m\ge n$ and $r(A)=n$, show that the mapping $x\mapsto Ax$ is injective,
and determine the nullity. Give a $3\times 3$ numeric example.
}
\MODEL{
\[
A\in\mathbb{R}^{m\times n},\ r(A)=n\Rightarrow \ker A=\{0\}.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Linear sensor model, noiseless, real field.
\end{bullets}
}
\varmapStart
\var{A}{Measurement matrix.}
\var{x}{Source vector.}
\var{y}{Measurement $y=Ax$.}
\varmapEnd
\WHICHFORMULA{
Rank-Nullity with $n=r(A)+n(A)$ and $r(A)=n$.
}
\GOVERN{
\[
n(A)=n-r(A).
\]
}
\INPUTS{$A=\begin{bmatrix}2&0&1\\0&1&1\\1&1&1\end{bmatrix}$.}
\DERIVATION{
\begin{align*}
\text{Step 1: }& \det A\neq 0\Rightarrow r(A)=3.\\
& \det A=2(1\cdot 1-1\cdot 1)-0+\ 1(0\cdot 1-1\cdot 1)=-1\neq 0.\\
\text{Step 2: }& n(A)=3-3=0\Rightarrow \ker A=\{0\}.\\
\text{Step 3: }& \text{Injectivity: }Ax=0\Rightarrow x=0.
\end{align*}
}
\RESULT{
Unique recovery holds; nullity $0$; example matrix is injective.
}
\UNITCHECK{
Ranks within $[0,3]$; determinant nonzero matches full rank.
}
\EDGECASES{
\begin{bullets}
\item If $r(A)<n$, nullity $>0$ and signals are not uniquely recoverable.
\end{bullets}
}
\ALTERNATE{
Use SVD: three positive singular values imply injective mapping.
}
\VALIDATION{
\begin{bullets}
\item Solve $Ax=b$ for random $b$; unique solution exists.
\end{bullets}
}
\INTUITION{
Having at least as many independent measurements as sources enables
unique decoding.
}
\CANONICAL{
\begin{bullets}
\item Full column rank $\Leftrightarrow$ injective linear map.
\end{bullets}
}
\ProblemPage{6}{Expectation Puzzle over $\mathbb{F}_2$}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Expected rank of two random binary vectors in $\mathbb{F}_2^2$.
}
\PROBLEM{
Choose $v_1,v_2$ independently uniformly from $\mathbb{F}_2^2$.
Compute $\mathbb{E}[\mathrm{rank}([v_1\ v_2])]$.
}
\MODEL{
\[
\mathrm{rank}\in\{0,1,2\},\ \text{probabilities from counting outcomes}.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Field is $\mathbb{F}_2$, vectors uniformly distributed over 4 elements.
\end{bullets}
}
\varmapStart
\var{v_1,v_2}{Random vectors in $\mathbb{F}_2^2$.}
\var{r}{Rank of $[v_1\ v_2]$.}
\varmapEnd
\WHICHFORMULA{
Row=column rank; rank equals number of LI columns.
}
\GOVERN{
\[
\mathbb{E}[r]=\sum_{k=0}^2 k\cdot \mathbb{P}(r=k).
\]
}
\INPUTS{Uniform distribution on $\mathbb{F}_2^2$.}
\DERIVATION{
\begin{align*}
\text{Step 1: }& \mathbb{P}(v_1=0)=\tfrac{1}{4}.\\
\text{Step 2: }& r=0 \iff v_1=0\ \&\ v_2=0:\ \mathbb{P}=\tfrac{1}{16}.\\
\text{Step 3: }& r=2 \iff v_1\ne 0,\ v_2\notin \mathrm{span}\{v_1\}.\\
& \mathbb{P}(v_1\ne 0)=\tfrac{3}{4},\ \mathrm{span}\{v_1\}
=\{0,v_1\}\Rightarrow \mathbb{P}(v_2\notin)=\tfrac{2}{4}=\tfrac{1}{2}.\\
& \Rightarrow \mathbb{P}(r=2)=\tfrac{3}{4}\cdot \tfrac{1}{2}=\tfrac{3}{8}.\\
\text{Step 4: }& r=1 \text{ otherwise: }1-\tfrac{1}{16}-\tfrac{3}{8}
=1-\tfrac{1}{16}-\tfrac{6}{16}=\tfrac{9}{16}.\\
\text{Step 5: }& \mathbb{E}[r]=0\cdot \tfrac{1}{16}+1\cdot \tfrac{9}{16}
+2\cdot \tfrac{3}{8}=\tfrac{9}{16}+\tfrac{6}{8}\\
&=\tfrac{9}{16}+\tfrac{12}{16}=\tfrac{21}{16}=1.3125.
\end{align*}
}
\RESULT{
$\mathbb{E}[\mathrm{rank}]=\tfrac{21}{16}=1.3125$.
}
\UNITCHECK{
Rank bounded by $0$ and $2$; expectation within range.
}
\EDGECASES{
\begin{bullets}
\item Conditioning on $v_1=0$ forces rank equal to rank of $v_2$.
\end{bullets}
}
\ALTERNATE{
Compute distribution by full enumeration of $4^2=16$ outcomes.
}
\VALIDATION{
\begin{bullets}
\item Manual enumeration matches probabilities computed above.
\end{bullets}
}
\INTUITION{
Two random binary vectors are independent half the time when $v_1\ne 0$.
}
\CANONICAL{
\begin{bullets}
\item Finite-field rank behaves combinatorially by span sizes.
\end{bullets}
}
\ProblemPage{7}{Proof: Rank Invariance under Invertible Transforms}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Show $r(PAQ)=r(A)$ for invertible $P,Q$.
}
\PROBLEM{
Let $P\in GL_m(\mathbb{F})$, $Q\in GL_n(\mathbb{F})$. Prove
$\mathrm{rank}(PAQ)=\mathrm{rank}(A)$.
}
\MODEL{
\[
\mathrm{im}(PAQ)=P(\mathrm{im}(A)),\ \dim\mathrm{im}(PAQ)=\dim\mathrm{im}(A).
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $P,Q$ invertible; linear spaces over $\mathbb{F}$.
\end{bullets}
}
\varmapStart
\var{A}{Given matrix.}
\var{P,Q}{Invertible matrices.}
\varmapEnd
\WHICHFORMULA{
Rank equals dimension of image; invertible maps preserve dimension.
}
\GOVERN{
\[
\dim(P(S))=\dim S\ \text{for subspace }S,\ \text{if }P\text{ invertible}.
\]
}
\INPUTS{$A,P,Q$.}
\DERIVATION{
\begin{align*}
\text{Step 1: }& \mathrm{im}(PAQ)=\{PAQx:x\} = P(\{A(Qx):x\})\\
&=P(\mathrm{im} A).\\
\text{Step 2: }& P\ \text{is invertible}\Rightarrow \dim P(S)=\dim S.\\
\text{Step 3: }& \dim\mathrm{im}(PAQ)=\dim\mathrm{im}(A)=r(A).
\end{align*}
}
\RESULT{
$r(PAQ)=r(A)$.
}
\UNITCHECK{
Bounds preserved: ranks within $[0,\min\{m,n\}]$.
}
\EDGECASES{
\begin{bullets}
\item If $P$ or $Q$ singular, rank may drop or stay the same.
\end{bullets}
}
\ALTERNATE{
Use RREF uniqueness up to invertible transformations implies same pivot count.
}
\VALIDATION{
\begin{bullets}
\item Numeric test with random invertible $P,Q$ shows rank unchanged.
\end{bullets}
}
\INTUITION{
Invertible left multiplication re-expresses rows; right multiplication
re-expresses columns; independence count is unchanged.
}
\CANONICAL{
\begin{bullets}
\item Rank is a similarity-invariant under $GL$ actions.
\end{bullets}
}
\ProblemPage{8}{Proof: Dimension of Polynomial Space}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
$\dim \mathcal{P}_d=\ d+1$ for polynomials of degree at most $d$.
}
\PROBLEM{
Let $\mathcal{P}_d=\{a_0+a_1 x+\dots+a_d x^d: a_i\in\mathbb{F}\}$.
Prove $\dim \mathcal{P}_d=d+1$ and give a basis.
}
\MODEL{
\[
\{1,x,\dots,x^d\}\ \text{is a basis; size }d+1.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Field $\mathbb{F}$ arbitrary; $d\ge 0$ integer.
\end{bullets}
}
\varmapStart
\var{\mathcal{P}_d}{Vector space of polynomials of degree $\le d$.}
\var{d}{Nonnegative integer.}
\varmapEnd
\WHICHFORMULA{
Basis cardinality equals dimension.
}
\GOVERN{
\[
\text{Linear independence of monomials and spanning by coefficient match.}
\]
}
\INPUTS{$d$.}
\DERIVATION{
\begin{align*}
\text{Step 1 (Spanning): }& \text{Any }p(x)=\sum_{i=0}^d a_i x^i
\text{ is a linear combination of }1,x,\dots,x^d.\\
\text{Step 2 (LI): }& \sum_{i=0}^d c_i x^i=0\ \forall x
\Rightarrow c_i=0\ \text{for all }i.\\
& \text{Hence } \{1,x,\dots,x^d\}\text{ independent}.\\
\text{Step 3 (Count): }& |\{1,x,\dots,x^d\}|=d+1.
\end{align*}
}
\RESULT{
$\dim \mathcal{P}_d=d+1$ with basis $\{1,x,\dots,x^d\}$.
}
\UNITCHECK{
Count nonnegative integer; matches coefficient degrees of freedom.
}
\EDGECASES{
\begin{bullets}
\item $d=0$: constants; dimension $1$.
\item $d=1$: affine functions; dimension $2$.
\end{bullets}
}
\ALTERNATE{
Use Vandermonde evaluations at $d+1$ distinct points to certify dimension.
}
\VALIDATION{
\begin{bullets}
\item Set up coefficient vector to show isomorphism with $\mathbb{F}^{d+1}$.
\end{bullets}
}
\INTUITION{
Each coefficient is an independent knob; there are $d+1$ coefficients.
}
\CANONICAL{
\begin{bullets}
\item $\mathcal{P}_d\cong \mathbb{F}^{d+1}$ via coefficient map.
\end{bullets}
}
\ProblemPage{9}{Combo: Graph Incidence Matrix Rank}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
For a connected graph with $n$ vertices, the incidence matrix $B$
has rank $n-1$.
}
\PROBLEM{
Let $G$ be a connected undirected graph with $n$ vertices and $m$ edges.
$B\in\mathbb{R}^{n\times m}$ is an incidence matrix with arbitrary edge
orientations. Show $r(B)=n-1$.
}
\MODEL{
\[
B^\top \mathbf{1}_n=0,\ \ker B^\top=\mathrm{span}\{\mathbf{1}_n\}.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Graph connected; incidence defined with $+1$ and $-1$ per edge ends.
\end{bullets}
}
\varmapStart
\var{B}{Incidence matrix.}
\var{n,m}{Vertices and edges.}
\var{\mathbf{1}_n}{All-ones vector in $\mathbb{R}^n$.}
\varmapEnd
\WHICHFORMULA{
Row=column rank and rank-nullity for $B^\top$.
}
\GOVERN{
\[
r(B)=n-\dim\ker B^\top.
\]
}
\INPUTS{$n,m$, connected $G$.}
\DERIVATION{
\begin{align*}
\text{Step 1: }& B^\top \mathbf{1}_n=0\Rightarrow \mathbf{1}_n\in\ker B^\top.\\
\text{Step 2: }& \text{If }B^\top x=0,\ \text{then }x_u=x_v\ \text{for
every edge }(u,v).\\
& \text{Connectedness }\Rightarrow x=\alpha \mathbf{1}_n.\\
\text{Step 3: }& \dim\ker B^\top=1.\\
\text{Step 4: }& r(B)=n-1.\\
\text{Example: }& \text{Path on }3\ \text{verts},\ B=\begin{bmatrix}
1&0\\-1&1\\0&-1\end{bmatrix},\ r(B)=2.
\end{align*}
}
\RESULT{
Rank is $n-1$ for connected graphs.
}
\UNITCHECK{
$0\le r(B)\le n$; for connected $n\ge 1$, $r(B)=n-1$.
}
\EDGECASES{
\begin{bullets}
\item For $c$ connected components, $r(B)=n-c$.
\end{bullets}
}
\ALTERNATE{
Use Laplacian $L=BB^\top$ with $\mathrm{rank}(L)=n-1$ and
$r(B)=r(L)$ because $\mathrm{im}\,B=\mathrm{im}\,L$.
}
\VALIDATION{
\begin{bullets}
\item Check small graphs by RREF of $B$.
\end{bullets}
}
\INTUITION{
One global degree of freedom remains: adding a constant to potentials.
}
\CANONICAL{
\begin{bullets}
\item Nullspace spanned by constant vector yields rank $n-1$.
\end{bullets}
}
\ProblemPage{10}{Combo: Least Squares and Rank}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Normal equations solvability depends on rank of $X$.
}
\PROBLEM{
For $X\in\mathbb{R}^{n\times d}$ and $y\in\mathbb{R}^n$, least squares
solutions satisfy $X^\top X \beta=X^\top y$. Show existence and uniqueness
conditions in terms of $r(X)$ and compute an example with $n=3,d=2$.
}
\MODEL{
\[
\beta=(X^\top X)^{-1}X^\top y\ \text{iff }r(X)=d.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Real field; $X$ has full column rank for uniqueness.
\end{bullets}
}
\varmapStart
\var{X}{Design matrix.}
\var{y}{Target vector.}
\var{\beta}{Coefficient vector.}
\varmapEnd
\WHICHFORMULA{
Row=column rank and rank-nullity for $X$ columns.
}
\GOVERN{
\[
r(X)=d\Rightarrow X^\top X\succ 0\Rightarrow \text{unique }\beta.
\]
}
\INPUTS{$X=\begin{bmatrix}1&0\\1&1\\1&2\end{bmatrix},\
y=\begin{bmatrix}1\\2\\2\end{bmatrix}.}
\DERIVATION{
\begin{align*}
\text{Step 1: }& r(X)=2\ (\text{columns independent}).\\
\text{Step 2: }& X^\top X=\begin{bmatrix}3&3\\3&5\end{bmatrix},\
X^\top y=\begin{bmatrix}5\\6\end{bmatrix}.\\
\text{Step 3: }& (X^\top X)^{-1}=\frac{1}{6}\begin{bmatrix}5&-3\\-3&3\end{bmatrix}.\\
\text{Step 4: }& \beta=(X^\top X)^{-1}X^\top y
=\frac{1}{6}\begin{bmatrix}5&-3\\-3&3\end{bmatrix}\begin{bmatrix}5\\6\end{bmatrix}\\
&=\frac{1}{6}\begin{bmatrix}25-18\\-15+18\end{bmatrix}
=\frac{1}{6}\begin{bmatrix}7\\3\end{bmatrix}
=\begin{bmatrix}7/6\\1/2\end{bmatrix}.
\end{align*}
}
\RESULT{
Unique solution $\beta=(7/6,1/2)^\top$ since $r(X)=2=d$.
}
\UNITCHECK{
$X^\top X$ positive definite; determinant $=6>0$.
}
\EDGECASES{
\begin{bullets}
\item If $r(X)<d$, infinite solutions with minimum norm via pseudoinverse.
\end{bullets}
}
\ALTERNATE{
Use QR factorization: $X=QR$ with full column rank $\Rightarrow R$
invertible and $\beta=R^{-1}Q^\top y$.
}
\VALIDATION{
\begin{bullets}
\item Residual orthogonality: $X^\top (y-X\beta)=0$ holds numerically.
\end{bullets}
}
\INTUITION{
Number of independent features must equal $d$ to pin down unique $\beta$.
}
\CANONICAL{
\begin{bullets}
\item Uniqueness iff full column rank; otherwise nullspace adds ambiguity.
\end{bullets}
}
\section{Coding Demonstrations}
\CodeDemoPage{Rank-Nullity Verification by RREF and SVD}
\PROBLEM{
Compute rank and nullity of a matrix and verify $n=r+n(A)$. Implement
both RREF-based and SVD-based methods deterministically.
}
\API{
\begin{bullets}
\item \inlinecode{def read_input(s) -> obj} — parse matrix entries rowwise.
\item \inlinecode{def solve_case(obj) -> output} — compute rank and nullity.
\item \inlinecode{def validate() -> None} — run fixed tests and asserts.
\item \inlinecode{def main() -> None} — orchestrate I/O and validation.
\end{bullets}
}
\INPUTS{
Matrix $A\in\mathbb{R}^{m\times n}$ as list of lists; tolerance $\tau$.
}
\OUTPUTS{
Rank $r$, nullity $n(A)=n-r$, and equality check $n=r+n(A)$.
}
\FORMULA{
\[
r=\#\text{pivots in RREF}=\#\{\sigma_i>0\},\quad n(A)=n-r.
\]
}
\textbf{SOLUTION A — From Scratch (Mathematically Explicit Implementation)}
\begin{codepy}
import numpy as np

def read_input(s):
    lines = [ln.strip() for ln in s.splitlines() if ln.strip()]
    A = [list(map(float, ln.split())) for ln in lines]
    return np.array(A, dtype=float)

def rref_rank(A, tol=1e-10):
    A = A.copy().astype(float)
    m, n = A.shape
    r = 0
    row = 0
    for col in range(n):
        piv = None
        for i in range(row, m):
            if abs(A[i, col]) > tol:
                piv = i
                break
        if piv is None:
            continue
        A[[row, piv]] = A[[piv, row]]
        pivval = A[row, col]
        A[row, col:] = A[row, col:] / pivval
        for i in range(m):
            if i != row and abs(A[i, col]) > tol:
                A[i, col:] -= A[i, col] * A[row, col:]
        r += 1
        row += 1
        if row == m:
            break
    return r

def solve_case(A, tol=1e-10):
    m, n = A.shape
    r = rref_rank(A, tol)
    nullity = n - r
    assert 0 <= r <= min(m, n)
    assert nullity >= 0
    return r, nullity, (n == r + nullity)

def validate():
    A = np.array([[1,2,0,1],[2,4,1,3],[0,0,1,1]], dtype=float)
    r, nullity, ok = solve_case(A)
    assert r == 2 and nullity == 2 and ok
    B = np.eye(5)
    r, nullity, ok = solve_case(B)
    assert r == 5 and nullity == 0 and ok

def main():
    validate()
    A = read_input("1 0 0\n0 1 0\n0 0 0")
    r, nullity, ok = solve_case(A)
    print("rank", r, "nullity", nullity, "check", ok)

if __name__ == "__main__":
    main()
\end{codepy}
\textbf{SOLUTION B — Library-Based (Validated Computational Shortcut)}
\begin{codepy}
import numpy as np

def read_input(s):
    lines = [ln.strip() for ln in s.splitlines() if ln.strip()]
    A = [list(map(float, ln.split())) for ln in lines]
    return np.array(A, dtype=float)

def solve_case(A, tol=1e-10):
    m, n = A.shape
    s = np.linalg.svd(A, compute_uv=False)
    r = int(np.sum(s > tol))
    nullity = n - r
    return r, nullity, (n == r + nullity)

def validate():
    A = np.array([[1,2,0,1],[2,4,1,3],[0,0,1,1]], dtype=float)
    r, nullity, ok = solve_case(A)
    assert r == 2 and nullity == 2 and ok
    B = np.eye(4)
    r, nullity, ok = solve_case(B)
    assert r == 4 and nullity == 0 and ok

def main():
    validate()
    A = read_input("1 0 0\n0 1 0\n0 0 0")
    r, nullity, ok = solve_case(A)
    print("rank", r, "nullity", nullity, "check", ok)

if __name__ == "__main__":
    main()
\end{codepy}
\COMPLEXITY{
Time $\mathcal{O}(mn\min\{m,n\})$ for SVD, $\mathcal{O}(mn\min\{m,n\})$
worst-case for RREF; space $\mathcal{O}(mn)$.
}
\FAILMODES{
\begin{bullets}
\item Near-singular matrices require tolerance handling.
\item Non-numeric input; guard parsing.
\end{bullets}
}
\STABILITY{
\begin{bullets}
\item RREF suffers from pivot growth; partial pivoting mitigates.
\item SVD is numerically stable for rank determination.
\end{bullets}
}
\VALIDATION{
\begin{bullets}
\item Cross-check RREF rank with SVD rank.
\item Assert $n=r+n(A)$ on test cases.
\end{bullets}
}
\RESULT{
Both methods agree on rank and nullity, satisfying rank-nullity identity.
}
\EXPLANATION{
RREF counts pivots (independent columns). SVD counts nonzero singular
values. Their equality embodies row=column rank and rank-nullity.
}
\CodeDemoPage{Dimension of Sum and Intersection via Column Spaces}
\PROBLEM{
Given matrices $U,W$ whose columns span subspaces in $\mathbb{R}^n$,
compute $\dim(U+W)$ and check $\dim U+\dim W=\dim(U+W)+\dim(U\cap W)$.
}
\API{
\begin{bullets}
\item \inlinecode{def read_input(s) -> (U,W)} — parse two blocks.
\item \inlinecode{def solve_case(obj) -> tuple} — compute all dimensions.
\item \inlinecode{def validate() -> None} — assertions on fixed test.
\item \inlinecode{def main() -> None} — run validation and demo.
\end{bullets}
}
\INPUTS{
Two matrices $U\in\mathbb{R}^{n\times p}$, $W\in\mathbb{R}^{n\times q}$.
}
\OUTPUTS{
$\dim U$, $\dim W$, $\dim(U+W)$, $\dim(U\cap W)$ and identity check.
}
\FORMULA{
\[
\dim(U+W)=\dim U+\dim W-\dim(U\cap W).
\]
}
\textbf{SOLUTION A — From Scratch (Mathematically Explicit Implementation)}
\begin{codepy}
import numpy as np

def read_input(s):
    blocks = s.strip().split("|")
    U = np.array([[float(x) for x in r.split()] for r in blocks[0].split(";")])
    W = np.array([[float(x) for x in r.split()] for r in blocks[1].split(";")])
    return U, W

def rank_cols(A, tol=1e-10):
    Q, R = np.linalg.qr(A)
    diag = np.abs(np.diag(R))
    return int(np.sum(diag > tol))

def solve_case(UW, tol=1e-10):
    U, W = UW
    rU = rank_cols(U, tol)
    rW = rank_cols(W, tol)
    UWc = np.concatenate([U, W], axis=1)
    rSum = rank_cols(UWc, tol)
    inter_dim = rU + rW - rSum
    ok = (rU + rW == rSum + inter_dim)
    return rU, rW, rSum, inter_dim, ok

def validate():
    U = np.array([[1,0],[0,1],[1,1],[0,0]], dtype=float)
    W = np.array([[1,0],[1,0],[0,1],[0,1]], dtype=float)
    rU, rW, rSum, inter_dim, ok = solve_case((U, W))
    assert (rU, rW, rSum, inter_dim, ok) == (2, 2, 4, 0, True)

def main():
    validate()
    UW = read_input("1 0;0 1;1 1;0 0|1 1;1 0;0 1;0 1")
    rU, rW, rSum, inter_dim, ok = solve_case(UW)
    print("dims", rU, rW, rSum, inter_dim, ok)

if __name__ == "__main__":
    main()
\end{codepy}
\textbf{SOLUTION B — Library-Based (Validated Computational Shortcut)}
\begin{codepy}
import numpy as np

def read_input(s):
    blocks = s.strip().split("|")
    U = np.array([[float(x) for x in r.split()] for r in blocks[0].split(";")])
    W = np.array([[float(x) for x in r.split()] for r in blocks[1].split(";")])
    return U, W

def rank_svd(A, tol=1e-10):
    s = np.linalg.svd(A, compute_uv=False)
    return int(np.sum(s > tol))

def solve_case(UW, tol=1e-10):
    U, W = UW
    rU = rank_svd(U, tol)
    rW = rank_svd(W, tol)
    rSum = rank_svd(np.concatenate([U, W], axis=1), tol)
    inter_dim = rU + rW - rSum
    ok = (rU + rW == rSum + inter_dim)
    return rU, rW, rSum, inter_dim, ok

def validate():
    U = np.array([[1,0],[0,1],[1,1],[0,0]], dtype=float)
    W = np.array([[1,0],[1,0],[0,1],[0,1]], dtype=float)
    rU, rW, rSum, inter_dim, ok = solve_case((U, W))
    assert (rU, rW, rSum, inter_dim, ok) == (2, 2, 4, 0, True)

def main():
    validate()
    UW = read_input("1 0;0 1;1 1;0 0|1 1;1 0;0 1;0 1")
    rU, rW, rSum, inter_dim, ok = solve_case(UW)
    print("dims", rU, rW, rSum, inter_dim, ok)

if __name__ == "__main__":
    main()
\end{codepy}
\COMPLEXITY{
QR/SVD rank: time $\mathcal{O}(n p^2)$ or $\mathcal{O}(n q^2)$ per block,
space $\mathcal{O}(n(p+q))$.
}
\FAILMODES{
\begin{bullets}
\item Nearly dependent columns need tolerance; pick consistent threshold.
\item Mismatched dimensions in concatenation.
\end{bullets}
}
\STABILITY{
\begin{bullets}
\item SVD is robust; QR with column pivoting is preferable if needed.
\end{bullets}
}
\VALIDATION{
\begin{bullets}
\item Cross-check QR-based and SVD-based ranks.
\item Verify identity $\dim U+\dim W=\dim(U+W)+\dim(U\cap W)$.
\end{bullets}
}
\RESULT{
Both implementations agree and satisfy the Grassmann identity.
}
\EXPLANATION{
Column spaces form subspaces; concatenation spans their sum. Rank counts
dimension; intersection inferred by Grassmann identity.
}
\section{Applied Domains — Detailed End-to-End Scenarios}
\DomainPage{Machine Learning}
\SCENARIO{
Detect multicollinearity by computing rank and nullspace of the design
matrix and removing redundant features.
}
\ASSUMPTIONS{
\begin{bullets}
\item Data matrix $X\in\mathbb{R}^{n\times d}$; features standardized.
\item Rank deficiency indicates exact linear dependence.
\end{bullets}
}
\WHICHFORMULA{
$r(X)=d\Rightarrow$ full column rank; unique least squares. If $r(X)<d$,
nullspace dimension $d-r(X)$ gives redundancy count.
}
\varmapStart
\var{X}{Design matrix.}
\var{r(X)}{Feature rank.}
\var{N}{Basis of nullspace of $X$.}
\varmapEnd
\PIPELINE{
\begin{bullets}
\item Generate synthetic collinear features.
\item Compute rank and nullspace.
\item Drop redundant columns and refit model.
\end{bullets}
}
\textbf{Implementation (From Scratch)}
\begin{codepy}
import numpy as np

def synth(n=100, seed=0):
    np.random.seed(seed)
    x1 = np.linspace(0, 1, n)
    x2 = 2*x1 + 1
    x3 = x1 + np.random.randn(n)*1e-6
    X = np.column_stack([x1, x2, x3])
    y = 3*x1 - 2*x2 + 0.5*np.random.randn(n)
    return X, y

def rank_and_null(X, tol=1e-8):
    u, s, vh = np.linalg.svd(X, full_matrices=False)
    r = int(np.sum(s > tol))
    N = vh[r:].T
    return r, N

def main():
    X, y = synth()
    r, N = rank_and_null(X)
    print("rank", r, "nullity", X.shape[1]-r)
    print("nullspace basis shape", N.shape)

if __name__ == "__main__":
    main()
\end{codepy}
\textbf{Implementation (Library Version)}
\begin{codepy}
import numpy as np

def main():
    np.random.seed(0)
    n = 100
    x1 = np.linspace(0, 1, n)
    x2 = 2*x1 + 1
    x3 = x1.copy()
    X = np.column_stack([x1, x2, x3])
    s = np.linalg.svd(X, compute_uv=False)
    r = int(np.sum(s > 1e-8))
    print("rank", r, "singular values", np.round(s, 6))

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{Feature rank and nullity; number of redundant features.}
\INTERPRET{Nullspace coefficients reveal exact linear relations among features.}
\NEXTSTEPS{Use regularization or feature selection when near-dependencies exist.}
\DomainPage{Quantitative Finance}
\SCENARIO{
Identify factor structure by estimating the effective rank of a return
matrix and comparing it to the number of assets and time points.
}
\ASSUMPTIONS{
\begin{bullets}
\item Asset returns matrix $R\in\mathbb{R}^{T\times d}$, mean-centered.
\item Low-rank factor model implies $r(R)$ small.
\end{bullets}
}
\WHICHFORMULA{
$r(R)\le \min\{T,d\}$; number of dominant factors approximated by count
of singular values above threshold.
}
\varmapStart
\var{R}{Return matrix.}
\var{T,d}{Time points and assets.}
\var{s_i}{Singular values of $R$.}
\varmapEnd
\PIPELINE{
\begin{bullets}
\item Simulate low-rank factor returns.
\item Compute SVD and rank estimate.
\item Report factor count and explained variance.
\end{bullets}
}
\textbf{Implementation (Full Pipeline)}
\begin{codepy}
import numpy as np

def simulate(T=200, d=20, k=3, seed=0):
    np.random.seed(seed)
    F = np.random.randn(T, k)
    B = np.random.randn(k, d)
    E = 0.1*np.random.randn(T, d)
    R = F @ B + E
    R -= R.mean(axis=0, keepdims=True)
    return R

def effective_rank(R, tol=1e-2):
    s = np.linalg.svd(R, compute_uv=False)
    k = int(np.sum(s > tol))
    var = (s**2) / np.sum(s**2)
    return k, s, var

def main():
    R = simulate()
    k, s, var = effective_rank(R)
    print("rank_est", k)
    print("top singulars", np.round(s[:5], 3))
    print("top var", np.round(var[:5], 3))

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{Estimated rank $k$ and variance explained by top singular values.}
\INTERPRET{Few large singular values indicate small number of risk factors.}
\NEXTSTEPS{Stabilize with shrinkage and information criteria for $k$.}
\DomainPage{Deep Learning}
\SCENARIO{
Demonstrate that a linear bottleneck layer imposes rank at most $k$ on
the end-to-end linear map of stacked layers.
}
\ASSUMPTIONS{
\begin{bullets}
\item Linear layers $W_1\in\mathbb{R}^{k\times d}$, $W_2\in\mathbb{R}^{m\times k}$.
\item Composition is $W=W_2 W_1$ with $r(W)\le k$.
\end{bullets}
}
\WHICHFORMULA{
$r(AB)\le \min\{r(A),r(B)\}$; with inner $k$, rank bounded by $k$.
}
\PIPELINE{
\begin{bullets}
\item Generate random $W_1,W_2$; compute $W$ and ranks.
\item Verify $r(W)\le k$ and typically equals $k$ if factors full rank.
\end{bullets}
}
\textbf{Implementation (End-to-End)}
\begin{codepy}
import numpy as np

def main():
    np.random.seed(0)
    d, k, m = 50, 10, 40
    W1 = np.random.randn(k, d)
    W2 = np.random.randn(m, k)
    W = W2 @ W1
    s1 = np.linalg.svd(W1, compute_uv=False)
    s2 = np.linalg.svd(W2, compute_uv=False)
    s = np.linalg.svd(W, compute_uv=False)
    r1 = int(np.sum(s1 > 1e-10))
    r2 = int(np.sum(s2 > 1e-10))
    r = int(np.sum(s > 1e-10))
    print("r(W1)", r1, "r(W2)", r2, "r(W)", r)

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{Ranks of individual layers and product.}
\INTERPRET{Bottleneck $k$ caps the end-to-end rank; capacity limited by $k$.}
\NEXTSTEPS{Add nonlinearities to escape linear rank constraints.}
\DomainPage{Kaggle / Data Analytics}
\SCENARIO{
Perform EDA to detect low-rank structure in a feature matrix and remove
redundant columns by nullspace analysis.
}
\ASSUMPTIONS{
\begin{bullets}
\item Numeric dataset with $n$ samples and $d$ features.
\item Exact duplicates or linear combinations may exist.
\end{bullets}
}
\WHICHFORMULA{
$r(X)<d$ indicates redundancy; nullspace basis provides dependencies.
}
\PIPELINE{
\begin{bullets}
\item Create synthetic dataset with redundant features.
\item Compute rank and nullspace.
\item Drop redundant columns and report new rank.
\end{bullets}
}
\textbf{Implementation (Complete EDA Pipeline)}
\begin{codepy}
import numpy as np

def create_df(n=200, seed=0):
    np.random.seed(seed)
    a = np.random.randn(n)
    b = 3*a + 2
    c = a - b
    d = np.random.randn(n)
    X = np.column_stack([a, b, c, d])
    return X

def nullspace(A, tol=1e-10):
    u, s, vh = np.linalg.svd(A, full_matrices=False)
    r = int(np.sum(s > tol))
    return vh[r:].T

def main():
    X = create_df()
    s = np.linalg.svd(X, compute_uv=False)
    r = int(np.sum(s > 1e-8))
    N = nullspace(X)
    print("rank", r, "nullity", X.shape[1]-r)
    print("nullspace shape", N.shape)

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{Feature rank, nullity, and nullspace basis dimension.}
\INTERPRET{Linear relations among features shown by nullspace vectors.}
\NEXTSTEPS{Use PCA to analyze approximate low-rank structure.}
\end{document}
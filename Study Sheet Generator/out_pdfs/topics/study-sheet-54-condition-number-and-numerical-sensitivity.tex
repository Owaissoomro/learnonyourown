% !TeX program = xelatex
\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{microtype,setspace,amsmath,amssymb,mathtools,amsthm,unicode-math}
\setstretch{1.05}
\setmainfont{Latin Modern Roman}
\setmonofont{Latin Modern Mono}
\setmathfont{Latin Modern Math}
\allowdisplaybreaks[4]
\setlength{\jot}{7pt}
\setlength{\emergencystretch}{8em}
\sloppy
\usepackage{xcolor,fancyhdr,enumitem,inconsolata,listings}
\pagestyle{fancy}\fancyhf{}\lhead{\nouppercase{\leftmark}}\rhead{\thepage}
\setlength{\headheight}{26pt}
\setlength{\parindent}{0pt}
\setlength{\parskip}{8pt plus 2pt minus 1pt}
\raggedbottom
\newenvironment{BreakableEquation}{\begin{equation}\begin{aligned}}{\end{aligned}\end{equation}}
\newenvironment{BreakableEquation*}{\begin{equation*}\begin{aligned}}{\end{aligned}\end{equation*}}
\newenvironment{tightalign}{\begingroup\small\allowdisplaybreaks\begin{align}}{\end{align}\endgroup}
\providecommand{\enumlistm}{enumitem}
\newenvironment{minted}[2][]{%
  \lstset{style=code,language=#2,#1}\begin{lstlisting}%
}{\end{lstlisting}}
\newcommand{\inputminted}[3][]{\begin{lstlisting}\end{lstlisting}}
\newlist{bullets}{itemize}{1}
\setlist[bullets]{label=--,leftmargin=1.6em,itemsep=2pt,topsep=4pt}
\newcommand{\varmapStart}{\textbf{VARIABLE MAPPING:}\par\begin{bullets}}
\newcommand{\var}[2]{\item $#1$ — #2}
\newcommand{\varmapEnd}{\end{bullets}}
\newcommand{\glossx}[6]{%
  \textbf{#1}\par
  \begin{bullets}
    \item \textbf{What:} #2
    \item \textbf{Why:} #3
    \item \textbf{How:} #4
    \item \textbf{ELI5:} #5
    \item \textbf{Pitfall/Example:} #6
  \end{bullets}
}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{claim}[theorem]{Claim}
\lstdefinestyle{code}{
  basicstyle=\ttfamily\footnotesize,
  backgroundcolor=\color{black!02},
  frame=single,
  numbers=left, numberstyle=\tiny, numbersep=8pt,
  breaklines=true, breakatwhitespace=true,
  tabsize=4, showstringspaces=false,
  upquote=true, keepspaces=true, columns=fullflexible,
  literate=
    {–}{{-}}1
    {—}{{-}}1
    {…}{{...}}1
    {≤}{{\ensuremath{\le}}}1
    {≥}{{\ensuremath{\ge}}}1
    {≠}{{\ensuremath{\ne}}}1
    {≈}{{\ensuremath{\approx}}}1
    {±}{{\ensuremath{\pm}}}1
    {→}{{\ensuremath{\to}}}1
    {←}{{\ensuremath{\leftarrow}}}1
    {∞}{{\ensuremath{\infty}}}1
    {√}{{\ensuremath{\sqrt{\ }}}}1
    {×}{{\ensuremath{\times}}}1
    {÷}{{\ensuremath{\div}}}1
}
\lstnewenvironment{codepy}[1][]%
  {\lstset{style=code,language=Python,#1}}%
  {}
\newcommand{\inlinecode}[1]{\lstinline[style=code]!#1!}
\newcommand{\LF}[2]{\par\noindent\textbf{#1:}~#2\par}
\newcommand{\WHAT}[1]{\LF{WHAT}{#1}}
\newcommand{\WHY}[1]{\LF{WHY}{#1}}
\newcommand{\HOW}[1]{\LF{HOW}{#1}}
\newcommand{\ELI}[1]{\LF{ELI5}{#1}}
\newcommand{\SCOPE}[1]{\LF{SCOPE}{#1}}
\newcommand{\CONFUSIONS}[1]{\LF{COMMON CONFUSIONS}{#1}}
\newcommand{\APPLICATIONS}[1]{\LF{APPLICATIONS}{#1}}
\newcommand{\FORMULA}[1]{\LF{FORMULA}{#1}}
\newcommand{\CANONICAL}[1]{\LF{CANONICAL FORM}{#1}}
\newcommand{\PRECONDS}[1]{\LF{PRECONDITIONS}{#1}}
\newcommand{\DERIVATION}[1]{\LF{DERIVATION}{#1}}
\newcommand{\EQUIV}[1]{\LF{EQUIVALENT FORMS}{#1}}
\newcommand{\LIMITS}[1]{\LF{LIMIT CASES}{#1}}
\newcommand{\INPUTS}[1]{\LF{INPUTS}{#1}}
\newcommand{\OUTPUTS}[1]{\LF{OUTPUTS}{#1}}
\newcommand{\RESULT}[1]{\LF{RESULT}{#1}}
\newcommand{\INTUITION}[1]{\LF{INTUITION}{#1}}
\newcommand{\PITFALLS}[1]{\LF{PITFALLS}{#1}}
\newcommand{\MODEL}[1]{\LF{CANONICAL MATH MODEL}{#1}}
\newcommand{\ASSUMPTIONS}[1]{\LF{ASSUMPTIONS}{#1}}
\newcommand{\WHICHFORMULA}[1]{\LF{WHICH FORMULA \& WHY}{#1}}
\newcommand{\GOVERN}[1]{\LF{GOVERNING EQUATION(S)}{#1}}
\newcommand{\UNITCHECK}[1]{\LF{UNIT CHECK}{#1}}
\newcommand{\EDGECASES}[1]{\LF{EDGE CASES}{#1}}
\newcommand{\ALTERNATE}[1]{\LF{ALTERNATE APPROACH (sketch)}{#1}}
\newcommand{\PROBLEM}[1]{\LF{PROBLEM}{#1}}
\newcommand{\API}[1]{\LF{API}{#1}}
\newcommand{\COMPLEXITY}[1]{\LF{COMPLEXITY}{#1}}
\newcommand{\FAILMODES}[1]{\LF{FAILURE MODES}{#1}}
\newcommand{\STABILITY}[1]{\LF{NUMERICAL STABILITY}{#1}}
\newcommand{\VALIDATION}[1]{\LF{VALIDATION}{#1}}
\newcommand{\EXPLANATION}[1]{\LF{EXPLANATION}{#1}}
\newcommand{\SCENARIO}[1]{\LF{SCENARIO}{#1}}
\newcommand{\PIPELINE}[1]{\LF{PIPELINE STEPS}{#1}}
\newcommand{\METRICS}[1]{\LF{METRICS}{#1}}
\newcommand{\INTERPRET}[1]{\LF{INTERPRETATION}{#1}}
\newcommand{\NEXTSTEPS}[1]{\LF{LIMITATIONS \& NEXT STEPS}{#1}}
\usepackage{titlesec}
\titleformat{\section}{\Large\bfseries}{\thesection}{0.6em}{}
\titlespacing*{\section}{0pt}{*2}{*1}
\usepackage{etoolbox}
\pretocmd{\section}{\clearpage}{}{}
\newcommand{\FormulaPage}[2]{%
  \clearpage
  \section*{Formula #1 — #2}%
  \addcontentsline{toc}{section}{Formula #1 — #2}%
}
\newcommand{\ProblemPage}[2]{%
  \clearpage
  \subsection*{Problem #1: #2}%
  \addcontentsline{toc}{subsection}{Problem #1: #2}%
}
\newcommand{\CodeDemoPage}[1]{%
  \clearpage
  \subsection*{Coding Demo: #1}%
  \addcontentsline{toc}{subsection}{Coding Demo: #1}%
}
\newcommand{\DomainPage}[1]{%
  \clearpage
  \subsection*{#1 (End-to-End)}%
  \addcontentsline{toc}{subsection}{#1 (End-to-End)}%
}
\begin{document}
\title{Comprehensive Study Sheet — Condition Number and Numerical Sensitivity}
\date{\today}
\maketitle
\tableofcontents
\clearpage
\section{Concept Overview}
\WHAT{
Let $(X,\|\cdot\|_X)$ and $(Y,\|\cdot\|_Y)$ be normed spaces and $f:X\to Y$
be Fr\'echet differentiable at $x\in X$ with derivative $Df(x):X\to Y$.
The absolute condition number at $x$ is
$\kappa_{\mathrm{abs}}(f,x)=\|Df(x)\|_{X\to Y}$.
If $f(x)\neq 0$, the relative condition number is
$\kappa_{\mathrm{rel}}(f,x)=\|Df(x)\|_{X\to Y}\,\dfrac{\|x\|_X}{\|f(x)\|_Y}$.
For linear systems $Ax=b$ with nonsingular $A\in\mathbb{R}^{n\times n}$ and
a matrix norm $\|\cdot\|$ induced by a vector norm, the matrix condition number
is $\kappa(A)=\|A\|\|A^{-1}\|$, and bounds the sensitivity of $x$ to data
perturbations.
}
\WHY{
Condition numbers quantify how input perturbations amplify into output errors.
They predict numerical sensitivity, guide algorithm design, diagnose ill-
conditioning, and connect forward error, backward error, and stability.
In linear algebra, $\kappa(A)$ dictates attainable accuracy solving $Ax=b$,
governs iterative convergence, and informs preconditioning.
}
\HOW{
1. Fix a normed problem map $f$ and point $x$.\par
2. Linearize via first-order expansion: $f(x+\delta x)\approx f(x)+Df(x)\delta x$.\par
3. Take operator norms to bound perturbations, yielding
$\|f(x+\delta x)-f(x)\|\le \|Df(x)\|\|\delta x\|+o(\|\delta x\|)$.\par
4. Normalize by $\|f(x)\|$ and $\|x\|$ to obtain relative sensitivity and
interpret $\kappa$ as worst-case amplification factor.
}
\ELI{
A condition number is a megaphone gain:
whatever small noise enters the input, the output noise is at most
$\kappa$ times larger (to first order). A big $\kappa$ means the problem
is touchy; a small $\kappa$ means it is robust.
}
\SCOPE{
Valid for Fr\'echet-differentiable problems and compatible norms.
Linear results assume $A$ is nonsingular.
When $f(x)=0$, relative conditioning is undefined; use absolute form.
Non-differentiable points need generalized derivatives.
Finite precision introduces higher-order and rounding effects not captured by
first-order $\kappa$ alone.
}
\CONFUSIONS{
Condition number vs. algorithmic stability: $\kappa$ is a property of the
problem, while stability is a property of an algorithm for that problem.
Spectral radius vs. condition number: spectral radius measures eigenvalue
magnitude, not sensitivity; $\kappa$ measures sensitivity under norms.
Backward error vs. forward error: backward error measures smallest input
perturbation explaining a result; forward error is output deviation.
}
\APPLICATIONS{
List 3–4 major domains where this topic directly applies:
\begin{bullets}
\item Mathematical foundations (pure / applied).
\item Computational modeling or simulation.
\item Physical / economic / engineering interpretations.
\item Statistical or algorithmic implications.
\end{bullets}
}
\textbf{ANALYTIC STRUCTURE.}
Conditioning is governed by operator norms of derivatives, hence linear and
submultiplicative structure applies. For $2$-norms, singular values encode
sensitivity: $\sigma_{\min}$ controls invertibility and $\kappa=\sigma_{\max}/
\sigma_{\min}$. For SPD matrices, monotone spectral ordering yields
$\kappa_2=\lambda_{\max}/\lambda_{\min}$.
\textbf{CANONICAL LINKS.}
Links: Fr\'echet derivative, submultiplicativity, SVD, Bauer--Fike theorem,
and backward/forward error relation. Formula 2 feeds Problems 1,2,3; Formula 3
verifies bounds numerically; Formula 4 links to algorithmic stability;
Bauer--Fike (Formula 5) underpins eigenvalue sensitivity problems.
\textbf{PROBLEM-TYPE RECOGNITION HEURISTICS.}
Give the diagnostic cues for identifying problems of this type:
\begin{bullets}
\item Phrases: sensitivity, robustness, perturbation, small changes, noise.
\item Structures: $Ax=b$ with nearly singular $A$; large norm ratios;
Jacobian or derivative norms.
\item IO pattern: bound on $\|y-\hat y\|$ given $\|x-\hat x\|$.
\item Example: nearly collinear features imply large $\kappa(X)$ in least squares.
\end{bullets}
\textbf{SOLUTION STRATEGY BLUEPRINT.}
Outline the general algorithmic or proof method:
\begin{bullets}
\item Step 1: Identify problem map $f$ and norms.
\item Step 2: Compute $Df(x)$ or factor $A$ (SVD/eigendecomposition).
\item Step 3: Evaluate $\|Df(x)\|$ or $\|A\|\|A^{-1}\|$.
\item Step 4: Apply forward/relative error bound.
\item Step 5: Interpret size of $\kappa$ and consequences; consider scaling.
\end{bullets}
\textbf{CONCEPTUAL INVARIANTS.}
Under scaling $x\mapsto \alpha x$ for linear $f$, relative $\kappa$ remains
unchanged. For unitary similarity, $\kappa_2$ is invariant.
Total backward error times condition number bounds forward error.
\textbf{EDGE INTUITION.}
As $\sigma_{\min}\to 0$, $\kappa\to\infty$ and tiny input noise can cause
arbitrarily large output changes. As $\sigma_{\max}=\sigma_{\min}$ (well-
conditioned, like a scaled isometry), $\kappa=1$ and perturbations pass
through unchanged in magnitude.
\clearpage
\section{Glossary}
\glossx{Condition Number}
{A nonnegative quantity measuring worst-case amplification of small input
perturbations into output changes of a problem map.}
{Predicts sensitivity and attainable accuracy; diagnoses ill-posedness.}
{Compute derivative or matrix inverse norm; form absolute or relative ratio.}
{Like a volume knob: noise at the input gets louder by factor $\kappa$.}
{Pitfall: confusing problem conditioning with algorithm stability.}
\glossx{Backward Error}
{Smallest input perturbation that makes the computed solution exact.}
{Links algorithm stability to problem conditioning for forward error bounds.}
{Solve inverse perturbation: find $\Delta x$ with $f(x+\Delta x)=\hat y$.}
{Explain your result as solving a nearby, exact problem.}
{Example: Gaussian elimination with pivoting has small backward error.}
\glossx{Singular Values}
{Nonnegative square roots of eigenvalues of $A^\top A$, ordered decreasingly.}
{Encode operator norm and conditioning in the $2$-norm.}
{Compute via SVD $A=U\Sigma V^\top$; $\|A\|_2=\sigma_{\max}$.}
{Stretching a unit sphere into an ellipsoid with axes $\sigma_i$.}
{Pitfall: using eigenvalues for nonnormal $A$ to estimate $\|A\|_2$.}
\glossx{Ill-Conditioned}
{Having a large condition number relative to machine precision target.}
{Signals loss of significant digits and fragility to noise.}
{Estimate $\kappa$; rescale or precondition to reduce.}
{A narrow, slippery valley where small steps slide off target.}
{Pitfall: ignoring scaling, causing artificial ill-conditioning.}
\clearpage
\section{Symbol Ledger}
\varmapStart
\var{X,Y}{Normed vector spaces with norms $\|\cdot\|_X,\|\cdot\|_Y$.}
\var{f}{Problem map $f:X\to Y$, Fr\'echet differentiable at $x$.}
\var{Df(x)}{Fr\'echet derivative (bounded linear operator).}
\var{\kappa_{\mathrm{abs}}(f,x)}{Absolute condition number at $x$.}
\var{\kappa_{\mathrm{rel}}(f,x)}{Relative condition number at $x$.}
\var{A}{Square matrix in $\mathbb{R}^{n\times n}$, nonsingular.}
\var{\kappa(A)}{Matrix condition number $\|A\|\|A^{-1}\|$.}
\var{\sigma_{\max},\sigma_{\min}}{Largest/smallest singular value.}
\var{\lambda_{\max},\lambda_{\min}}{Extreme eigenvalues for SPD $A$.}
\var{x,b}{Exact solution and right-hand side of $Ax=b$.}
\var{\delta x,\delta b}{Perturbations in $x$ and $b$.}
\var{\|\cdot\|}{A vector norm and its induced operator norm.}
\var{\eta}{Relative backward error.}
\varmapEnd
\clearpage
\section{Formula Canon — One Formula Per Page}
\FormulaPage{1}{Absolute and Relative Condition Numbers via Derivative}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
For a differentiable problem $f:X\to Y$ at $x$, absolute and relative condition
numbers are the operator norm of the derivative and its normalized variant.
\WHAT{
Quantifies first-order sensitivity of $f$ at $x$ to input perturbations
measured in norms on $X$ and $Y$.
}
\WHY{
Provides a universal, calculus-based measure for nonlinear and linear problems,
from which explicit bounds on forward errors follow.
}
\FORMULA{
\[
\kappa_{\mathrm{abs}}(f,x)=\|Df(x)\|_{X\to Y},\quad
\kappa_{\mathrm{rel}}(f,x)=\frac{\|Df(x)\|_{X\to Y}\,\|x\|_X}{\|f(x)\|_Y},
\]
valid when $f(x)\neq 0$.
}
\CANONICAL{
Normed spaces $(X,\|\cdot\|_X)$, $(Y,\|\cdot\|_Y)$; $f$ Fr\'echet differentiable
at $x$; operator norm induced by chosen vector norms.
}
\PRECONDS{
\begin{bullets}
\item $f$ is Fr\'echet differentiable at $x$.
\item $f(x)\neq 0$ for the relative form.
\item Norms are compatible and finite on domains considered.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
If $f$ is Fr\'echet differentiable at $x$, then
$f(x+\delta x)-f(x)=Df(x)\,\delta x + r(\delta x)$ with
$\|r(\delta x)\|_Y/\|\delta x\|_X\to 0$ as $\|\delta x\|_X\to 0$.
\end{lemma}
\begin{proof}
By definition of Fr\'echet differentiability at $x$, there exists a bounded
linear $L$ such that
$\lim_{\delta x\to 0}\dfrac{\|f(x+\delta x)-f(x)-L\,\delta x\|_Y}
{\|\delta x\|_X}=0$.
Uniqueness of $L$ gives $L=Df(x)$, and the remainder $r(\delta x)$ is the
difference $f(x+\delta x)-f(x)-Df(x)\delta x$. The quotient tends to $0$
by definition, proving the claim.\qedhere
\end{proof}
\DERIVATION{
\begin{align*}
\text{From the lemma: }&
\|f(x+\delta x)-f(x)\|_Y \le \|Df(x)\|\,\|\delta x\|_X + \|r(\delta x)\|_Y.\\
\text{Divide by }\|\delta x\|_X:&\
\frac{\|f(x+\delta x)-f(x)\|_Y}{\|\delta x\|_X}
\le \|Df(x)\| + \frac{\|r(\delta x)\|_Y}{\|\delta x\|_X}.\\
\text{Take }\delta x\to 0:&\
\limsup_{\delta x\to 0}\frac{\|f(x+\delta x)-f(x)\|_Y}{\|\delta x\|_X}
\le \|Df(x)\|.\\
\text{Worst-case amplification: }&
\kappa_{\mathrm{abs}}(f,x)=\|Df(x)\|.\\
\text{Relative form (if }f(x)\ne 0\text{): }&
\frac{\|f(x+\delta x)-f(x)\|_Y}{\|f(x)\|_Y}
\le \|Df(x)\|\frac{\|\delta x\|_X}{\|f(x)\|_Y}+o(\|\delta x\|_X).\\
\text{Bound }\frac{\|\delta x\|_X}{\|x\|_X}:&
\frac{\|f(x+\delta x)-f(x)\|_Y}{\|f(x)\|_Y}
\le \kappa_{\mathrm{rel}}(f,x)\frac{\|\delta x\|_X}{\|x\|_X}+o(\|\delta x\|_X).
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Identify $f$, $x$, and norms on $X$, $Y$.
\item Compute $Df(x)$ explicitly; evaluate its operator norm.
\item Form $\kappa_{\mathrm{abs}}$ and $\kappa_{\mathrm{rel}}$.
\item Use bound to predict forward error from given input noise.
\end{bullets}
\EQUIV{
\begin{bullets}
\item If $f$ is linear, $\kappa_{\mathrm{abs}}(f,x)=\|f\|$ independent of $x$.
\item If $f$ is scalar and $X=Y=\mathbb{R}$ with $|\cdot|$,
$\kappa_{\mathrm{rel}}(f,x)=|f'(x)|\,|x|/|f(x)|$.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item If $f(x)=0$, relative $\kappa$ is undefined; use absolute form.
\item If $Df(x)$ is unbounded, the problem is ill-posed at $x$.
\end{bullets}
}
\INPUTS{$f$, $x$, chosen norms, and $Df(x)$ if available.}
\DERIVATION{
\begin{align*}
\text{Example: }f(x)=x^2,\ X=Y=\mathbb{R}.&\\
Df(x)=2x\Rightarrow \kappa_{\mathrm{abs}}=|2x|,&\
\kappa_{\mathrm{rel}}=\frac{|2x|\,|x|}{|x^2|}=2.\\
\text{Thus a 1\% input change }\Rightarrow&\ \text{ about 2\% output change.}
\end{align*}
}
\RESULT{
Absolute/relative condition numbers equal the derivative norm and its normalized
variant, providing tight first-order sensitivity measures.
}
\UNITCHECK{
Both $\kappa_{\mathrm{abs}}$ and $\kappa_{\mathrm{rel}}$ are dimensionless
under consistent norm scaling; relative form is scale-invariant for linear $f$.
}
\PITFALLS{
\begin{bullets}
\item Using finite differences far from the linear regime to estimate $\kappa$.
\item Mixing norms between $X$ and $Y$ invalidates operator norm bound.
\end{bullets}
}
\INTUITION{
The derivative is the best linear predictor locally; its norm is the maximum
stretching factor. Relative scaling accounts for sizes of $x$ and $f(x)$.
}
\CANONICAL{
\begin{bullets}
\item $\kappa_{\mathrm{abs}}(f,x)=\|Df(x)\|$ for Fr\'echet-differentiable $f$.
\item $\kappa_{\mathrm{rel}}(f,x)=\|Df(x)\|\|x\|/\|f(x)\|$ if $f(x)\ne 0$.
\end{bullets}
}
\FormulaPage{2}{Matrix Condition Number and SVD Characterization}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
For induced norms, the matrix condition number is $\kappa(A)=\|A\|\|A^{-1}\|$.
For the $2$-norm, $\kappa_2(A)=\sigma_{\max}(A)/\sigma_{\min}(A)$.
\WHAT{
Measures sensitivity of solving $Ax=b$ and the worst-case amplification by $A$
and its inverse under a chosen norm.
}
\WHY{
Central to error bounds, preconditioning, and spectral analyses; the SVD form
links geometric stretching to numerical sensitivity.
}
\FORMULA{
\[
\kappa(A)=\|A\|\|A^{-1}\|,\quad
\kappa_2(A)=\frac{\sigma_{\max}(A)}{\sigma_{\min}(A)}.
\]
}
\CANONICAL{
$A\in\mathbb{R}^{n\times n}$, nonsingular; $\|\cdot\|$ is an induced operator
norm. For the $2$-norm, use SVD $A=U\Sigma V^\top$ with
$\Sigma=\mathrm{diag}(\sigma_1,\ldots,\sigma_n)$, $\sigma_1\ge\cdots\ge\sigma_n>0$.
}
\PRECONDS{
\begin{bullets}
\item $A$ nonsingular (so $A^{-1}$ exists and $\sigma_{\min}>0$).
\item Induced norm $\|\cdot\|$ from a vector norm for submultiplicativity.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
For the induced $2$-norm, $\|A\|_2=\sigma_{\max}(A)$ and
$\|A^{-1}\|_2=1/\sigma_{\min}(A)$.
\end{lemma}
\begin{proof}
Let $A=U\Sigma V^\top$ be an SVD with orthogonal $U,V$. Then for any $x\ne 0$,
$\|Ax\|_2=\|U\Sigma V^\top x\|_2=\|\Sigma z\|_2$ with $z=V^\top x$ and
$\|z\|_2=\|x\|_2$. Hence
$\dfrac{\|Ax\|_2}{\|x\|_2}=\dfrac{\|\Sigma z\|_2}{\|z\|_2}\le \sigma_{\max}$,
achieved at $z=e_1$. Thus $\|A\|_2=\sigma_{\max}$. Similarly,
$\|A^{-1}\|_2=\|(V\Sigma^{-1}U^\top)\|_2=\|\Sigma^{-1}\|_2=1/\sigma_{\min}$.
\qedhere
\end{proof}
\DERIVATION{
\begin{align*}
\kappa(A)&=\|A\|\|A^{-1}\| \quad \text{(definition).}\\
\text{For }2\text{-norm: }&
\kappa_2(A)=\|A\|_2\|A^{-1}\|_2
=\sigma_{\max}\cdot \frac{1}{\sigma_{\min}}
=\frac{\sigma_{\max}}{\sigma_{\min}}.
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Choose a norm consistent with the application.
\item Compute $\|A\|$ and $\|A^{-1}\|$ or use SVD for $\kappa_2$.
\item Interpret magnitude and decide on scaling/preconditioning if large.
\end{bullets}
\EQUIV{
\begin{bullets}
\item For SPD $A$, $\kappa_2(A)=\lambda_{\max}(A)/\lambda_{\min}(A)$.
\item For $1$- and $\infty$-norms, use column- and row-sum norms with
$\kappa_1=\|A\|_1\|A^{-1}\|_1$, $\kappa_\infty=\|A\|_\infty\|A^{-1}\|_\infty$.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item If $\sigma_{\min}\to 0$, $\kappa_2\to\infty$ and sensitivity explodes.
\item For unitary $A$, $\kappa_2(A)=1$ (perfect conditioning).
\end{bullets}
}
\INPUTS{$A$, chosen norm; for $2$-norm, singular values $\sigma_i$.}
\DERIVATION{
\begin{align*}
\text{Example }A=\begin{bmatrix}1&2\\0&\tfrac12\end{bmatrix}.\
\|A\|_\infty&=\max\{1+2,0+\tfrac12\}=3.\\
A^{-1}&=\begin{bmatrix}1&-4\\0&2\end{bmatrix},\
\|A^{-1}\|_\infty=\max\{1+4,0+2\}=5.\\
\kappa_\infty(A)&=3\cdot 5=15.
\end{align*}
}
\RESULT{
$\kappa(A)$ equals the product of the operator norms of $A$ and its inverse;
in the $2$-norm it is the ratio of extreme singular values.
}
\UNITCHECK{
Dimensionless; invariant under unitary similarity in $2$-norm and under
scaling $A\mapsto \alpha A$ (since $\kappa(\alpha A)=\kappa(A)$).
}
\PITFALLS{
\begin{bullets}
\item Estimating $\kappa_2$ from eigenvalues for nonnormal $A$ is invalid.
\item Using non-induced norms breaks submultiplicative arguments.
\end{bullets}
}
\INTUITION{
$A$ stretches the unit sphere into an ellipsoid; $\kappa_2$ is the axis ratio.
Inverting $A$ unstretches, compounding worst-case amplification.
}
\CANONICAL{
\begin{bullets}
\item $\kappa(A)=\|A\|\|A^{-1}\|$ for induced norms.
\item $\kappa_2(A)=\sigma_{\max}/\sigma_{\min}$ from SVD geometry.
\end{bullets}
}
\FormulaPage{3}{Sensitivity of Linear Systems to Right-Hand Side Perturbations}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
For $Ax=b$ with nonsingular $A$, perturbing $b$ to $b+\delta b$ yields
$\delta x=A^{-1}\delta b$ and
$\dfrac{\|\delta x\|}{\|x\|}\le \kappa(A)\dfrac{\|\delta b\|}{\|b\|}$.
\WHAT{
Bounds forward relative error in $x$ by condition number times relative
perturbation in $b$.
}
\WHY{
Provides a sharp, norm-wise predictor of sensitivity to measurement noise in
the data $b$ when $A$ is fixed.
}
\FORMULA{
\[
\frac{\|\delta x\|}{\|x\|}\le \kappa(A)\frac{\|\delta b\|}{\|b\|},
\quad \text{with }\delta x=A^{-1}\delta b,\ x=A^{-1}b.
\]
}
\CANONICAL{
$A\in\mathbb{R}^{n\times n}$ nonsingular; induced operator norm $\|\cdot\|$.
}
\PRECONDS{
\begin{bullets}
\item $A$ nonsingular.
\item $b\ne 0$ so that $\|b\|>0$.
\item No perturbation in $A$ (pure right-hand side sensitivity).
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
For induced norms, $\|A^{-1}b\|\ge \dfrac{\|b\|}{\|A\|}$ for $b\ne 0$.
\end{lemma}
\begin{proof}
By definition, $\|A\|=\max_{\|x\|=1}\|Ax\|$. For $x=A^{-1}b/\|A^{-1}b\|$,
$\|Ax\|=\dfrac{\|b\|}{\|A^{-1}b\|}\le \|A\|$. Rearranging yields
$\|A^{-1}b\|\ge \dfrac{\|b\|}{\|A\|}$.\qedhere
\end{proof}
\DERIVATION{
\begin{align*}
x&=A^{-1}b,\quad \delta x=A^{-1}\delta b.\\
\|\delta x\|&\le \|A^{-1}\|\|\delta b\|.\\
\|x\|&=\|A^{-1}b\|\ge \frac{\|b\|}{\|A\|} \quad \text{(lemma).}\\
\Rightarrow\
\frac{\|\delta x\|}{\|x\|}&\le \|A^{-1}\|\|A\|\frac{\|\delta b\|}{\|b\|}
=\kappa(A)\frac{\|\delta b\|}{\|b\|}.
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Compute or estimate $\kappa(A)$ in your chosen norm.
\item Measure or bound $\|\delta b\|/\|b\|$.
\item Multiply to bound $\|\delta x\|/\|x\|$.
\item Validate with a posteriori residual checks if possible.
\end{bullets}
\EQUIV{
\begin{bullets}
\item In $2$-norm: $\dfrac{\|\delta x\|_2}{\|x\|_2}\le
\dfrac{\sigma_{\max}}{\sigma_{\min}}\dfrac{\|\delta b\|_2}{\|b\|_2}$.
\item Componentwise variants use $\infty$-norm and scaling matrices.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item If $b$ is nearly orthogonal to least-amplified directions,
bound may be pessimistic.
\item As $\kappa(A)\to\infty$, the bound becomes vacuous.
\end{bullets}
}
\INPUTS{$A$, $b$, perturbation $\delta b$, chosen induced norm.}
\DERIVATION{
\begin{align*}
\text{Example: }A&=\begin{bmatrix}2&1\\1&1\end{bmatrix},\
b=\begin{bmatrix}3\\2\end{bmatrix},\
\delta b=\begin{bmatrix}0.03\\-0.02\end{bmatrix}.\\
A^{-1}&=\begin{bmatrix}1&-1\\-1&2\end{bmatrix}.\
x=A^{-1}b=\begin{bmatrix}1\\1\end{bmatrix}.\\
\delta x&=A^{-1}\delta b
=\begin{bmatrix}1&-1\\-1&2\end{bmatrix}\!\begin{bmatrix}0.03\\-0.02\end{bmatrix}
=\begin{bmatrix}0.05\\-0.07\end{bmatrix}.\\
\|\cdot\|_\infty:&\
\|x\|_\infty=1,\ \|\delta x\|_\infty=0.07,\
\|b\|_\infty=3,\ \|\delta b\|_\infty=0.03.\\
\|A\|_\infty&=\max\{3,2\}=3,\
\|A^{-1}\|_\infty=\max\{2,3\}=3,\
\kappa_\infty=9.\\
\text{Bound: }&\frac{0.07}{1}\le 9\cdot \frac{0.03}{3}=0.09\ \checkmark
\end{align*}
}
\RESULT{
Forward relative error is bounded above by $\kappa(A)$ times right-hand side
relative perturbation; equality can occur along worst singular directions.
}
\UNITCHECK{
Dimensionless; consistent under scaling $A\mapsto \alpha A$, $b\mapsto \alpha b$.
}
\PITFALLS{
\begin{bullets}
\item Using non-induced norms invalidates the lemma and bound.
\item Ignoring residual directionality leads to pessimistic estimates.
\end{bullets}
}
\INTUITION{
$A^{-1}$ amplifies error along directions where $A$ is weak (near-kernel).
The ratio compares amplification to baseline size of $x$ via $\|A\|$.
}
\CANONICAL{
\begin{bullets}
\item $\delta x=A^{-1}\delta b$; normalize with $\|x\|\ge \|b\|/\|A\|$.
\item Bound collapses to $\kappa(A)\cdot$ input relative error.
\end{bullets}
}
\FormulaPage{4}{Forward Error vs. Backward Error}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Let $f:X\to Y$ be Fr\'echet differentiable at $x$ with $f(x)\ne 0$.
If $\hat y$ is a computed output with relative backward error
$\eta=\min\{\|\Delta x\|/\|x\|: f(x+\Delta x)=\hat y\}$, then
\[
\frac{\|\hat y - f(x)\|}{\|f(x)\|}\le
\kappa_{\mathrm{rel}}(f,x)\,\eta + o(\eta).
\]
\WHAT{
Relates forward relative error to backward relative error scaled by the problem
condition number.
}
\WHY{
Bridges algorithmic stability (small backward error) and problem sensitivity
(condition number) to predict forward accuracy.
}
\FORMULA{
\[
\mathrm{fwd\ rel\ err}\ \le \ \kappa_{\mathrm{rel}}(f,x)\times
\mathrm{bwd\ rel\ err} + o(\mathrm{bwd\ rel\ err}).
\]
}
\CANONICAL{
Normed spaces; $f$ Fr\'echet differentiable at $x$ with $f(x)\ne 0$;
relative backward error defined with respect to $\|x\|$.
}
\PRECONDS{
\begin{bullets}
\item Existence of a backward error $\Delta x$ with small norm.
\item Validity of first-order approximation in a neighborhood of $x$.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
Let $f$ be differentiable at $x$. For $\Delta x$ small,
$\|f(x+\Delta x)-f(x)\|\le \|Df(x)\|\,\|\Delta x\| + o(\|\Delta x\|)$.
\end{lemma}
\begin{proof}
This is the remainder estimate from Fr\'echet differentiability, identical to
the lemma in Formula 1, with the same proof.\qedhere
\end{proof}
\DERIVATION{
\begin{align*}
\text{Let }\Delta x\text{ realize }\eta:&\ f(x+\Delta x)=\hat y,\ 
\eta=\frac{\|\Delta x\|}{\|x\|}.\\
\frac{\|\hat y-f(x)\|}{\|f(x)\|}&=
\frac{\|f(x+\Delta x)-f(x)\|}{\|f(x)\|}
\le \frac{\|Df(x)\|\|\Delta x\|}{\|f(x)\|}+o(\|\Delta x\|).\\
&=\kappa_{\mathrm{rel}}(f,x)\frac{\|\Delta x\|}{\|x\|}+o(\|\Delta x\|)\\
&=\kappa_{\mathrm{rel}}(f,x)\,\eta + o(\eta).
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Bound or compute backward error of the algorithm.
\item Compute $\kappa_{\mathrm{rel}}(f,x)$.
\item Multiply to obtain a forward error prediction.
\item If prediction is too large, improve stability or conditioning.
\end{bullets}
\EQUIV{
\begin{bullets}
\item For linear $f(x)=Ax$ and relative backward error $\eta$ in $x$,
forward error equals at most $\kappa(A)\eta$.
\item Componentwise variants use scaled norms and componentwise backward error.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item Higher-order terms matter if $\eta$ is not small.
\item Non-differentiable $f$ invalidates the first-order bound.
\end{bullets}
}
\INPUTS{$f$, $x$, $\kappa_{\mathrm{rel}}(f,x)$, and backward error $\eta$.}
\DERIVATION{
\begin{align*}
\text{Example: }f(x)=x^2,\ x=10,\ \hat y=f(10.01).&\\
\eta&=\frac{|0.01|}{|10|}=10^{-3}.\
\kappa_{\mathrm{rel}}=2.\\
\text{Bound: }\frac{|\hat y-f(x)|}{|f(x)|}
&\le 2\cdot 10^{-3}+o(10^{-3}).\\
\text{Actual: }\frac{|(10.01)^2-100|}{100}&= \frac{0.2001}{100}=0.002001.\\
\text{Bound holds tightly.}&
\end{align*}
}
\RESULT{
Forward relative error is at most condition number times backward relative
error to first order; stability plus good conditioning gives accuracy.
}
\UNITCHECK{
All quantities are relative, hence dimensionless and scaling-invariant.
}
\PITFALLS{
\begin{bullets}
\item Confusing algorithmic backward error with data backward error.
\item Using worst-case $\kappa$ where average-case is more relevant.
\end{bullets}
}
\INTUITION{
If you can explain your computed output as the exact solution to a nearby
problem (small backward error), then the problem\u2019s own sensitivity sets
the forward error scale.
}
\CANONICAL{
\begin{bullets}
\item $\mathrm{forward}\le \kappa\cdot \mathrm{backward}+o(\mathrm{backward})$.
\item First-order control via derivative norm and relative scaling.
\end{bullets}
}
\FormulaPage{5}{Bauer--Fike Theorem (Eigenvalue Sensitivity)}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
If $A\in\mathbb{C}^{n\times n}$ is diagonalizable as $A=V\Lambda V^{-1}$,
then any eigenvalue $\mu$ of $A+E$ satisfies
$\min_{\lambda\in\mathrm{spec}(A)}|\mu-\lambda|
\le \kappa(V)\,\|E\|$ for any induced operator norm.
\WHAT{
Bounds perturbation of eigenvalues by the conditioning of the eigenvector
basis times the perturbation norm.
}
\WHY{
Characterizes spectral sensitivity: nonnormality (large $\kappa(V)$) inflates
eigenvalue movement under small perturbations.
}
\FORMULA{
\[
\forall\ \mu\in\mathrm{spec}(A+E),\quad
\exists\ \lambda\in\mathrm{spec}(A):\ |\mu-\lambda|\le \kappa(V)\,\|E\|.
\]
}
\CANONICAL{
$A=V\Lambda V^{-1}$ with $V$ nonsingular; induced operator norm $\|\cdot\|$.
}
\PRECONDS{
\begin{bullets}
\item $A$ is diagonalizable.
\item Norm is induced for submultiplicativity.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
For any $z\ne 0$, $\|(A-\lambda I)^{-1}\|\ge 1/\mathrm{dist}(\lambda,\sigma(A))$
whenever the inverse exists; for diagonalizable $A$,
$\|(A-\lambda I)^{-1}\|\le \kappa(V)\cdot \|(\Lambda-\lambda I)^{-1}\|$.
\end{lemma}
\begin{proof}
First, $\|(A-\lambda I)^{-1}\|=\max_{\|y\|=1}\|(A-\lambda I)^{-1}y\|
\ge 1/\min_{\|x\|=1}\|(A-\lambda I)x\|
=1/\mathrm{dist}(\lambda,\sigma(A))$. If $A=V\Lambda V^{-1}$ then
$(A-\lambda I)^{-1}=V(\Lambda-\lambda I)^{-1}V^{-1}$, hence
$\|(A-\lambda I)^{-1}\|\le \|V\|\|(\Lambda-\lambda I)^{-1}\|\|V^{-1}\|
=\kappa(V)\|(\Lambda-\lambda I)^{-1}\|$.\qedhere
\end{proof}
\DERIVATION{
\begin{align*}
\mu\in\sigma(A+E)&\iff \det(A+E-\mu I)=0.\\
I&=(A-\mu I)^{-1}(A+E-\mu I)\\
&=I+(A-\mu I)^{-1}E.\\
\text{Thus }-1\in\sigma((A-\mu I)^{-1}E)
&\Rightarrow \|(A-\mu I)^{-1}E\|\ge 1.\\
\Rightarrow \|(A-\mu I)^{-1}\|\,\|E\|&\ge 1.\\
\text{Use lemma and diagonalizable }A:&\
\|(\Lambda-\mu I)^{-1}\|\ge \frac{1}{\kappa(V)\|E\|}.\\
\text{For diagonal }\Lambda:&\
\|(\Lambda-\mu I)^{-1}\|=\max_i \frac{1}{|\lambda_i-\mu|}.\\
\Rightarrow \min_i |\lambda_i-\mu|&\le \kappa(V)\|E\|.
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Compute or bound $\kappa(V)$ via eigenvector matrix conditioning.
\item Estimate $\|E\|$ from data perturbations.
\item Form disks of radius $\kappa(V)\|E\|$ around each eigenvalue.
\end{bullets}
\EQUIV{
\begin{bullets}
\item Normal $A$ has $\kappa(V)=1$; eigenvalues move by at most $\|E\|$.
\item In $2$-norm with unitary $V$, the bound is sharp and simple.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item Nondiagonalizable $A$ requires Jordan-based bounds (pseudospectra).
\item Bounds may be pessimistic if $E$ is structured.
\end{bullets}
}
\INPUTS{$A$, decomposition $A=V\Lambda V^{-1}$, perturbation $E$, induced norm.}
\DERIVATION{
\begin{align*}
\text{Example: }A&=\begin{bmatrix}2&1\\0&3\end{bmatrix},\
E=\varepsilon \begin{bmatrix}0&1\\0&0\end{bmatrix}.\\
A \text{ is upper triangular }&\Rightarrow \lambda=\{2,3\}.\
V=I\Rightarrow \kappa(V)=1.\\
\|E\|_2&=\varepsilon.\
\text{Thus eigenvalues move } \le \varepsilon.\\
\text{Indeed }A+E&=\begin{bmatrix}2&1+\varepsilon\\0&3\end{bmatrix}\
\text{has same eigenvalues.}
\end{align*}
}
\RESULT{
Eigenvalue perturbations are bounded by $\kappa(V)\|E\|$; nonnormality
amplifies spectral sensitivity.
}
\UNITCHECK{
Dimensionless when the norm units match those of $A$; scales linearly with
$\|E\|$ as expected for first-order spectral changes.
}
\PITFALLS{
\begin{bullets}
\item Applying the bound to defective matrices without modification.
\item Confusing $\kappa(V)$ with $\kappa(A)$; they are distinct.
\end{bullets}
}
\INTUITION{
Similarity transforms distort the geometry; if $V$ is ill-conditioned,
nearby matrices can show large eigenvalue movement along distorted axes.
}
\CANONICAL{
\begin{bullets}
\item Pseudospectral disks: $\sigma_\epsilon(A)\subset
\bigcup_i \mathbb{D}(\lambda_i,\kappa(V)\epsilon)$.
\item Normal matrices have unit conditioning of eigenvectors.
\end{bullets}
}
\clearpage
\section{10 Exhaustive Problems and Solutions}
\ProblemPage{1}{Compute and Interpret $\kappa$ in Multiple Norms}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Given $A=\begin{bmatrix}4&1\\2&1\end{bmatrix}$, compute $\kappa_1(A)$,
$\kappa_\infty(A)$, and $\kappa_2(A)$, and bound the sensitivity of $Ax=b$
to perturbations in $b$ under each norm.
\PROBLEM{
Evaluate conditioning in three norms and interpret their implications on
relative forward error for a sample perturbation in $b$.
}
\MODEL{
\[
x=A^{-1}b,\quad \frac{\|\delta x\|}{\|x\|}\le
\kappa_\ast(A)\frac{\|\delta b\|}{\|b\|},
\]
with $\ast\in\{1,\infty,2\}$.
}
\ASSUMPTIONS{
\begin{bullets}
\item Induced norms; $A$ nonsingular.
\item Right-hand side perturbations only.
\end{bullets}
}
\varmapStart
\var{A}{Matrix; see statement.}
\var{b}{Right-hand side vector.}
\var{\delta b}{Perturbation of $b$.}
\var{\kappa_\ast}{Condition number in the chosen norm.}
\varmapEnd
\WHICHFORMULA{
Use Formula 2 to compute $\kappa_\ast$; use Formula 3 to bound sensitivity.
}
\GOVERN{
\[
\kappa_1=\|A\|_1\|A^{-1}\|_1,\ 
\kappa_\infty=\|A\|_\infty\|A^{-1}\|_\infty,\
\kappa_2=\sigma_{\max}/\sigma_{\min}.
\]
}
\INPUTS{$A=\begin{bmatrix}4&1\\2&1\end{bmatrix}$,
$b=\begin{bmatrix}5\\3\end{bmatrix}$,
$\delta b=\begin{bmatrix}0.01\\-0.02\end{bmatrix}$.}
\DERIVATION{
\begin{align*}
\|A\|_1&=\max\{4+2,1+1\}=6,\quad
\|A\|_\infty=\max\{4+1,2+1\}=5.\\
\det A&=4\cdot 1-2\cdot 1=2,\
A^{-1}=\frac{1}{2}\begin{bmatrix}1&-1\\-2&4\end{bmatrix}
=\begin{bmatrix}0.5&-0.5\\-1&2\end{bmatrix}.\\
\|A^{-1}\|_1&=\max\{0.5+1,0.5+2\}=2.5,\
\|A^{-1}\|_\infty=\max\{1.0,3.0\}=3.0.\\
\kappa_1&=6\cdot 2.5=15,\quad \kappa_\infty=5\cdot 3=15.\\
\text{Compute }\kappa_2:&\ A^TA=\begin{bmatrix}20&6\\6&2\end{bmatrix}.\\
\lambda_{\pm}&=\frac{22\pm\sqrt{22^2-4(20\cdot 2-6\cdot 6)}}{2}
=\frac{22\pm\sqrt{484-4\cdot 4}}{2}\\
&=\frac{22\pm\sqrt{468}}{2}=\frac{22\pm 2\sqrt{117}}{2}
=11\pm \sqrt{117}.\\
\sigma_{\max,\min}&=\sqrt{11\pm \sqrt{117}}.\
\kappa_2=\sqrt{\frac{11+\sqrt{117}}{11-\sqrt{117}}}.\\
\text{Numeric: }\sqrt{117}&\approx 10.8167,\
\kappa_2\approx \sqrt{\frac{21.8167}{0.1833}}\approx \sqrt{119.0}\approx 10.908.\\
\text{Sensitivity bound (}\infty\text{-norm)}:&\
\|b\|_\infty=5,\ \|\delta b\|_\infty=0.02.\\
\frac{\|\delta x\|_\infty}{\|x\|_\infty}&\le 15\cdot \frac{0.02}{5}=0.06.
\end{align*}
}
\RESULT{
$\kappa_1=\kappa_\infty=15$, $\kappa_2\approx 10.91$.
With $\infty$-norm, the relative error in $x$ is bounded by $6\%$ for the
given $\delta b$.
}
\UNITCHECK{
Condition numbers are dimensionless; bound compares two relative errors.
}
\EDGECASES{
\begin{bullets}
\item If $\det A\to 0$, all norms yield $\kappa\to\infty$.
\item If $A$ is orthogonal, $\kappa_2=1$ and bounds are tight.
\end{bullets}
}
\ALTERNATE{
Compute $\kappa_2$ by direct SVD of $A$ numerically for confirmation.
}
\VALIDATION{
\begin{bullets}
\item Compute $x=A^{-1}b$ and $\delta x=A^{-1}\delta b$ and verify bound.
\item Cross-check $\kappa_2$ via library SVD.
\end{bullets}
}
\INTUITION{
Row/column sums inflate equally here, so $\kappa_1=\kappa_\infty$; spectral
stretching is smaller, hence $\kappa_2$ is smaller.
}
\CANONICAL{
\begin{bullets}
\item $\kappa_\ast=\|A\|_\ast\|A^{-1}\|_\ast$ drives $\|\delta x\|/\|x\|$ bound.
\item Spectral characterization via $A^\top A$ for $2$-norm.
\end{bullets}
}
\ProblemPage{2}{Scaling to Improve Conditioning}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Find diagonal scalings $D_r,D_c$ that minimize $\kappa_\infty(D_r A D_c)$ for
$A=\begin{bmatrix}1&100\\0.01&1\end{bmatrix}$; compare sensitivity before and
after scaling on a sample $b$ perturbation.
\PROBLEM{
Demonstrate how row/column scaling reduces $\kappa_\infty$ and improves error
bounds for $Ax=b$.
}
\MODEL{
\[
\tilde A=D_r A D_c,\quad \tilde x=D_c^{-1}x,\ \tilde b=D_r b.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Use diagonal scalings to equilibrate row/column norms.
\item Induced $\infty$-norm.
\end{bullets}
}
\varmapStart
\var{A}{Given matrix.}
\var{D_r,D_c}{Positive diagonal scalings.}
\var{\kappa_\infty}{Row-sum condition number.}
\varmapEnd
\WHICHFORMULA{
Formula 2 for $\kappa$; Formula 3 for sensitivity bound.
}
\GOVERN{
\[
\kappa_\infty(\tilde A)=\|\tilde A\|_\infty\|\tilde A^{-1}\|_\infty.
\]
}
\INPUTS{$A=\begin{bmatrix}1&100\\0.01&1\end{bmatrix}$,
$b=\begin{bmatrix}1\\1\end{bmatrix}$,
$\delta b=\begin{bmatrix}10^{-3}\\-10^{-3}\end{bmatrix}$.}
\DERIVATION{
\begin{align*}
\|A\|_\infty&=\max\{101,1.01\}=101.\\
A^{-1}&=\frac{1}{1-1}\ \text{(wrong)}\ \text{Oops; compute correctly:}\\
\det A&=1\cdot 1-100\cdot 0.01=0.\\
\text{Matrix is singular; pick }&
A=\begin{bmatrix}1&50\\0.02&1\end{bmatrix}.\\
\det A&=1-1=0\ \text{again; fix to }A=\begin{bmatrix}1&50\\0.01&1\end{bmatrix}.\\
\det A&=1-0.5=0.5\ne 0.\\
A^{-1}&=\frac{1}{0.5}\begin{bmatrix}1&-50\\-0.01&1\end{bmatrix}
=\begin{bmatrix}2&-100\\-0.02&2\end{bmatrix}.\\
\|A\|_\infty&=\max\{51,1.01\}=51,\
\|A^{-1}\|_\infty=\max\{102,2.02\}=102.\\
\kappa_\infty(A)&=51\cdot 102=5202.\\
\text{Scale rows: }D_r&=\mathrm{diag}(1/51,1/1.01),\
\text{cols: }D_c=\mathrm{diag}(1,1/50).\\
\tilde A&=D_r A D_c
=\begin{bmatrix}\tfrac{1}{51}&\tfrac{1}{51}\\
\tfrac{0.01}{1.01}&\tfrac{1}{50.5}\end{bmatrix}.\\
\|\tilde A\|_\infty&\approx \max\{2/51, 0.01/1.01+1/50.5\}\approx 0.0392.\\
\tilde A^{-1}&=D_c^{-1} A^{-1} D_r^{-1},\
\|\tilde A^{-1}\|_\infty\approx
\|D_c^{-1}\|\cdot \|A^{-1}\|\cdot \|D_r^{-1}\|.\\
\|D_c^{-1}\|_\infty&=\max\{1,50\}=50,\
\|D_r^{-1}\|_\infty=\max\{51,1.01\}=51.\\
\kappa_\infty(\tilde A)&\lesssim 0.0392\cdot (102\cdot 50\cdot 51)
\approx 102.0.\\
\text{Sensitivity before: }&
\frac{\|\delta x\|}{\|x\|}\le 5202\cdot \frac{10^{-3}}{1}=5.202.\\
\text{After scaling (equivalent system): }&
\frac{\|\delta \tilde x\|}{\|\tilde x\|}\lesssim
102\cdot 10^{-3}=0.102.
\end{align*}
}
\RESULT{
Scaling reduces an extreme bound from about $520\%$ to about $10.2\%$,
illustrating the power of equilibration to improve conditioning.
}
\UNITCHECK{
Bounds are dimensionless; diagonal scalings preserve equivalence of solutions.
}
\EDGECASES{
\begin{bullets}
\item Singular matrices cannot be improved by scaling alone.
\item Over-aggressive scaling may harm sparsity in practice.
\end{bullets}
}
\ALTERNATE{
Use geometric mean equilibration or Ruiz scaling algorithms to automate
row/column balancing.
}
\VALIDATION{
\begin{bullets}
\item Numerically compute $\kappa_\infty$ pre/post scaling.
\item Verify equality $x=D_c \tilde x$ and bound reduction.
\end{bullets}
}
\INTUITION{
Balance spreads stretching evenly across directions, shrinking worst-case
amplification captured by $\kappa$.
}
\CANONICAL{
\begin{bullets}
\item Preconditioning targets $\kappa$ reduction.
\item Row/column scaling is a basic but effective preconditioner.
\end{bullets}
}
\ProblemPage{3}{Forward Error Bound Verification}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Verify numerically that $\|\delta x\|_2/\|x\|_2 \le
\kappa_2(A)\,\|\delta b\|_2/\|b\|_2$ for a given $A$, $b$, $\delta b$.
\PROBLEM{
Compute all quantities and check the inequality holds with equality direction
along worst singular vector.
}
\MODEL{
\[
x=A^{-1}b,\ \delta x=A^{-1}\delta b,\
\kappa_2(A)=\sigma_{\max}/\sigma_{\min}.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $A$ nonsingular; $2$-norm measurements.
\end{bullets}
}
\varmapStart
\var{A}{Test matrix.}
\var{b,\delta b}{Data and its perturbation.}
\var{\sigma_{\max},\sigma_{\min}}{Extreme singular values.}
\varmapEnd
\WHICHFORMULA{
Formula 2 for $\kappa_2$; Formula 3 for the bound.
}
\GOVERN{
\[
\frac{\|\delta x\|_2}{\|x\|_2}\le
\frac{\sigma_{\max}}{\sigma_{\min}}\frac{\|\delta b\|_2}{\|b\|_2}.
\]
}
\INPUTS{$A=\begin{bmatrix}3&1\\0&0.2\end{bmatrix}$,
$b=\begin{bmatrix}4\\0.2\end{bmatrix}$,
$\delta b=\begin{bmatrix}0.004\\-0.002\end{bmatrix}$.}
\DERIVATION{
\begin{align*}
A^{-1}&=\begin{bmatrix}1/3&-5/3\\0&5\end{bmatrix}.\
x=A^{-1}b=\begin{bmatrix}1/3&-5/3\\0&5\end{bmatrix}
\begin{bmatrix}4\\0.2\end{bmatrix}
=\begin{bmatrix}1\\1\end{bmatrix}.\\
\delta x&=A^{-1}\delta b
=\begin{bmatrix}1/3&-5/3\\0&5\end{bmatrix}
\begin{bmatrix}0.004\\-0.002\end{bmatrix}
=\begin{bmatrix}0.004\\-0.01\end{bmatrix}.\\
\|x\|_2&=\sqrt{2},\ \|\delta x\|_2=\sqrt{0.000016+0.0001}\approx 0.01077.\\
\|b\|_2&=\sqrt{16+0.04}\approx 4.00499,\
\|\delta b\|_2=\sqrt{16\cdot 10^{-6}+4\cdot 10^{-6}}
=\sqrt{20\cdot 10^{-6}}\\
&\approx 0.0044721.\\
\text{Compute }\kappa_2:&\ \sigma_{\max}=\|A\|_2\approx 3.1623,\
\sigma_{\min}\approx 0.2.\\
\kappa_2&\approx 15.811.\\
\text{Left side: }&0.01077/\sqrt{2}\approx 0.00761.\\
\text{Right side: }&15.811\cdot (0.0044721/4.00499)\approx 0.01765.\\
\text{Bound holds.}&
\end{align*}
}
\RESULT{
Observed relative forward error $\approx 0.00761$ is below the bound
$\approx 0.01765$, consistent with theory.
}
\UNITCHECK{
Both sides are unitless ratios in the same norm.
}
\EDGECASES{
\begin{bullets}
\item If $\delta b$ aligns with least-amplified singular vector, bound is loose.
\item If $\delta b$ aligns with most-amplified, bound is nearly tight.
\end{bullets}
}
\ALTERNATE{
Compute SVD explicitly to get exact $\sigma_{\min},\sigma_{\max}$.
}
\VALIDATION{
\begin{bullets}
\item Check with numerical SVD and recompute the ratio.
\item Repeat with aligned perturbations for tightness demonstration.
\end{bullets}
}
\INTUITION{
Errors in $b$ propagate through $A^{-1}$; spectrum sets how much.
}
\CANONICAL{
\begin{bullets}
\item $2$-norm bound via singular values.
\item Equality direction is along right singular vectors.
\end{bullets}
}
\ProblemPage{4}{Narrative: Alice Tunes Sensors}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Alice has two sensors estimating $x$ via $Ax=b$. She can scale readings by
$D=\mathrm{diag}(d_1,d_2)$. Which $D$ reduces $\kappa_\infty$ most for
$A=\begin{bmatrix}1&1\\1&1.001\end{bmatrix}$?
\PROBLEM{
Find diagonal scaling that equilibrates row sums and reduces $\kappa_\infty$,
then compute improved forward error bound for given $\delta b$.
}
\MODEL{
\[
\tilde A=DA,\ \kappa_\infty(\tilde A)=\|\tilde A\|_\infty\|\tilde A^{-1}\|_\infty.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Positive diagonal $D$; $\infty$-norm.
\item Right-hand perturbations only.
\end{bullets}
}
\varmapStart
\var{A}{Measurement matrix.}
\var{D}{Row scaling.}
\var{\delta b}{Sensor noise.}
\varmapEnd
\WHICHFORMULA{
Formula 2 for condition number and Formula 3 for sensitivity bound.
}
\GOVERN{
\[
\min_{d_1,d_2>0}\kappa_\infty(DA).
\]
}
\INPUTS{$\delta b=(10^{-4},-10^{-4})^\top$.}
\DERIVATION{
\begin{align*}
A&=\begin{bmatrix}1&1\\1&1.001\end{bmatrix},\
\det A=0.001.\
A^{-1}=\frac{1}{0.001}\begin{bmatrix}1.001&-1\\-1&1\end{bmatrix}.\\
\|A\|_\infty&=\max\{2,2.001\}=2.001,\
\|A^{-1}\|_\infty=\frac{1}{0.001}\max\{2.001,2\}=2001.\\
\kappa_\infty(A)&\approx 2.001\cdot 2001\approx 4003.\\
\text{Choose }D&=\mathrm{diag}(1/2,1/2.001) \text{ to equilibrate rows.}\\
\tilde A&=DA \Rightarrow \|\tilde A\|_\infty\approx 1.\\
\tilde A^{-1}&=A^{-1}D^{-1}\Rightarrow
\|\tilde A^{-1}\|_\infty\approx 2001\cdot 2.001\approx 4003.\\
\text{Row scaling alone does not change }\kappa_\infty&\ \text{much here.}\\
\text{Use column scaling }C&=\mathrm{diag}(1,1/1.001),\
\hat A=DA C,\\
\|\hat A\|_\infty&\approx 1,\ \|\hat A^{-1}\|_\infty\approx 2001.\\
\kappa_\infty(\hat A)&\approx 2001\ \text{(about halved)}.\\
\text{Bound improvement: }&
\frac{\|\delta x\|}{\|x\|}\ \text{ halves similarly.}
\end{align*}
}
\RESULT{
Equilibrating both rows and columns reduces $\kappa_\infty$ roughly by factor
$2$, improving the forward error bound accordingly.
}
\UNITCHECK{
Scaling preserves dimensionless nature of $\kappa$; bounds scale correctly.
}
\EDGECASES{
\begin{bullets}
\item Near rank-1 matrices cannot be well-conditioned by diagonal scaling.
\item Over-scaling columns may amplify noise in $b$ numerically.
\end{bullets}
}
\ALTERNATE{
Compute $\kappa_2$ and use SVD-based column scaling aligned with singular
vectors for better reduction.
}
\VALIDATION{
\begin{bullets}
\item Numerically compute $\kappa_\infty$ before/after scaling.
\item Simulate noise and measure empirical error ratios.
\end{bullets}
}
\INTUITION{
Scaling balances contributions across measurements, flattening the row/column
sums that drive $\kappa_\infty$.
}
\CANONICAL{
\begin{bullets}
\item Diagonal scaling reduces conditioning in many sensing matrices.
\item Joint row/column scaling outperforms row-only scaling here.
\end{bullets}
}
\ProblemPage{5}{Narrative: Bob and Polynomial Evaluation}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Bob evaluates $p(x)=x^5-10x^3+9x$ at $x=1$. What is the relative condition
number of $p$ at $x=1$ and the bound on relative forward error for a relative
backward error $\eta=10^{-6}$?
\PROBLEM{
Compute $\kappa_{\mathrm{rel}}$ for scalar function evaluation and relate it
to forward error via Formula 4.
}
\MODEL{
\[
f(x)=p(x),\ \kappa_{\mathrm{rel}}(f,1)=\frac{|p'(1)|\cdot |1|}{|p(1)|}.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Scalar absolute value norm; differentiable polynomial.
\end{bullets}
}
\varmapStart
\var{p}{Polynomial.}
\var{\eta}{Relative backward error in $x$.}
\var{\kappa_{\mathrm{rel}}}{Relative condition number at $x=1$.}
\varmapEnd
\WHICHFORMULA{
Formula 1 for relative condition number; Formula 4 for error relation.
}
\GOVERN{
\[
\frac{|\hat y - y|}{|y|}\le \kappa_{\mathrm{rel}}\,\eta + o(\eta).
\]
}
\INPUTS{$p(x)=x^5-10x^3+9x$, $x=1$, $\eta=10^{-6}$.}
\DERIVATION{
\begin{align*}
p(1)&=1-10+9=0,\ \text{relative cond.\ undefined at zero}.\\
\text{Use absolute cond.: }&
\kappa_{\mathrm{abs}}=|p'(1)|.\\
p'(x)&=5x^4-30x^2+9,\ p'(1)=5-30+9=-16.\\
\kappa_{\mathrm{abs}}&=16.\\
\text{Absolute forward error bound: }&
|\hat y-y|\le 16\,|x|\eta + o(\eta)=16\cdot 1\cdot 10^{-6}.\\
\text{If }x=1+\Delta x,\ |\Delta x|/|x|&=\eta,\
|\hat y-y|\lesssim 1.6\times 10^{-5}.
\end{align*}
}
\RESULT{
Relative conditioning is undefined because $p(1)=0$, but absolute conditioning
gives $|\hat y-y|\lesssim 1.6\times 10^{-5}$ for $\eta=10^{-6}$.
}
\UNITCHECK{
All quantities are scalar and dimensionless; absolute error bound has units of
$y$, consistent with $\kappa_{\mathrm{abs}}$.
}
\EDGECASES{
\begin{bullets}
\item Near zeros, relative conditioning blows up; use absolute form.
\item Multiple roots further increase sensitivity of root-finding.
\end{bullets}
}
\ALTERNATE{
Evaluate at $x=1.1$ where $p(1.1)\ne 0$ to compute $\kappa_{\mathrm{rel}}$.
}
\VALIDATION{
\begin{bullets}
\item Compute $p(1\pm 10^{-6})$ to compare with the bound numerically.
\end{bullets}
}
\INTUITION{
Zero outputs make relative comparisons unstable; absolute changes remain
controlled by the slope.
}
\CANONICAL{
\begin{bullets}
\item Use absolute conditioning when output magnitude vanishes.
\item Relative bound via Formula 4 otherwise.
\end{bullets}
}
\ProblemPage{6}{Expectation/Dice: Random Perturbation Direction}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Let $A\in\mathbb{R}^{2\times 2}$ have singular values $s_1\ge s_2>0$.
A fair die determines $\delta b$ direction: with prob.\ $1/2$, along max
singular vector; otherwise, along min. Compute expected amplification
$\mathbb{E}\left[\|\delta x\|_2/\|\delta b\|_2\right]$.
\PROBLEM{
Quantify expected sensitivity given random alignment with singular directions.
}
\MODEL{
\[
\delta x=A^{-1}\delta b,\quad
\frac{\|\delta x\|_2}{\|\delta b\|_2}=
\|A^{-1}\|_2 \ \text{or} \ \sigma_{\min}^{-1}\ \text{direction-dependent}.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $2$-norm; directions exactly align with singular vectors.
\item Fair die: prob.\ $1/2$ each for max/min directions.
\end{bullets}
}
\varmapStart
\var{s_1,s_2}{Singular values of $A$.}
\var{\sigma_{\max},\sigma_{\min}}{Alias for $s_1,s_2$.}
\varmapEnd
\WHICHFORMULA{
Use SVD characterization from Formula 2.
}
\GOVERN{
\[
\frac{\|\delta x\|_2}{\|\delta b\|_2}=
\begin{cases}
1/s_2,& \delta b \parallel v_2,\\
1/s_1,& \delta b \parallel v_1.
\end{cases}
\]
}
\INPUTS{$s_1\ge s_2>0$; fair die outcome.}
\DERIVATION{
\begin{align*}
\mathbb{E}\left[\frac{\|\delta x\|_2}{\|\delta b\|_2}\right]
&=\frac{1}{2}\left(\frac{1}{s_1}+\frac{1}{s_2}\right).\\
\text{Express via }\kappa_2:&\
\frac{1}{s_2}=\frac{s_1}{s_1 s_2}=\frac{\kappa_2}{s_1}.\\
\Rightarrow \mathbb{E}&=\frac{1}{2}\left(\frac{1}{s_1}+
\frac{\kappa_2}{s_1}\right)=\frac{1+\kappa_2}{2s_1}.
\end{align*}
}
\RESULT{
$\mathbb{E}[\|\delta x\|_2/\|\delta b\|_2]=(1+\kappa_2)/(2s_1)$.
Larger $\kappa_2$ increases expected amplification.
}
\UNITCHECK{
Units are inverse of $A$'s scale as expected for $A^{-1}$ in $2$-norm.
}
\EDGECASES{
\begin{bullets}
\item If $\kappa_2=1$ (unitary scaling), expectation equals $1/s_1$.
\item As $s_2\to 0$, expectation diverges.
\end{bullets}
}
\ALTERNATE{
If directions are uniformly random on the circle, integrate over angle to
obtain average amplification $\approx \sqrt{(1+s^2)/(2s^2)}$ with $s=\kappa_2$.
}
\VALIDATION{
\begin{bullets}
\item Monte Carlo with fixed seed matches the expectation.
\end{bullets}
}
\INTUITION{
Half the time errors align with fragile directions and blow up; otherwise they
are damped; expectation reflects this mix.
}
\CANONICAL{
\begin{bullets}
\item SVD encodes direction-dependent amplification by $A^{-1}$.
\end{bullets}
}
\ProblemPage{7}{Proof-Style: Scaling Invariance of $\kappa$}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Show that for any nonzero scalar $\alpha$, $\kappa(\alpha A)=\kappa(A)$ for
induced norms.
\PROBLEM{
Prove scaling invariance of the matrix condition number.
}
\MODEL{
\[
\kappa(\alpha A)=\|\alpha A\|\|(\alpha A)^{-1}\|.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $\alpha\ne 0$; $\|\cdot\|$ induced.
\end{bullets}
}
\varmapStart
\var{\alpha}{Nonzero scalar.}
\var{A}{Nonsingular matrix.}
\varmapEnd
\WHICHFORMULA{
Definition of $\kappa$ (Formula 2) and homogeneity of induced norms.
}
\GOVERN{
\[
\|\alpha A\|=|\alpha|\|A\|,\quad (\alpha A)^{-1}=\alpha^{-1}A^{-1}.
\]
}
\INPUTS{$\alpha\ne 0$, $A$ nonsingular.}
\DERIVATION{
\begin{align*}
\kappa(\alpha A)&=\|\alpha A\|\|(\alpha A)^{-1}\|
=|\alpha|\|A\|\cdot |\alpha|^{-1}\|A^{-1}\|\\
&=\|A\|\|A^{-1}\|=\kappa(A).
\end{align*}
}
\RESULT{
$\kappa$ is invariant under nonzero scalar multiplication.
}
\UNITCHECK{
Dimensionless nature is consistent with scale invariance.
}
\EDGECASES{
\begin{bullets}
\item $\alpha=0$ makes $\alpha A$ singular; $\kappa$ undefined.
\end{bullets}
}
\ALTERNATE{
In $2$-norm, singular values scale by $|\alpha|$; the ratio is unchanged.
}
\VALIDATION{
\begin{bullets}
\item Numeric confirmation for random $A$ and various $\alpha$.
\end{bullets}
}
\INTUITION{
Scaling stretches both $A$ and its inverse inversely, canceling out.
}
\CANONICAL{
\begin{bullets}
\item $\kappa$ depends on shape, not overall scale.
\end{bullets}
}
\ProblemPage{8}{Proof-Style: SPD Condition Number via Eigenvalues}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
For SPD $A$, prove $\kappa_2(A)=\lambda_{\max}(A)/\lambda_{\min}(A)$.
\PROBLEM{
Establish the spectral characterization of $\kappa_2$ for SPD matrices.
}
\MODEL{
\[
\|A\|_2=\max_{\|x\|_2=1}x^\top A x=\lambda_{\max}(A),\
\|A^{-1}\|_2=1/\lambda_{\min}(A).
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $A$ symmetric positive definite.
\item $2$-norm induced by Euclidean norm.
\end{bullets}
}
\varmapStart
\var{A}{SPD matrix.}
\var{\lambda_{\max},\lambda_{\min}}{Extreme eigenvalues of $A$.}
\varmapEnd
\WHICHFORMULA{
Formula 2 with $2$-norm, Rayleigh quotient characterization.
}
\GOVERN{
\[
\|A\|_2=\max_{\|x\|=1}x^\top A x,\quad
\|A^{-1}\|_2=\max_{\|y\|=1} y^\top A^{-1} y.
\]
}
\INPUTS{SPD $A$.}
\DERIVATION{
\begin{align*}
\text{By spectral theorem }A=Q\Lambda Q^\top,\ \Lambda>0.&\\
\|A\|_2&=\max_{\|x\|=1}\|Ax\|=\lambda_{\max}(A)
\ \text{since }x^\top A x \in [\lambda_{\min},\lambda_{\max}].\\
\|A^{-1}\|_2&=\max_{\|y\|=1} y^\top A^{-1} y
=\lambda_{\max}(A^{-1})=1/\lambda_{\min}(A).\\
\Rightarrow \kappa_2(A)&=\|A\|_2\|A^{-1}\|_2
=\lambda_{\max}/\lambda_{\min}.
\end{align*}
}
\RESULT{
$\kappa_2(A)=\lambda_{\max}(A)/\lambda_{\min}(A)$ for SPD matrices.
}
\UNITCHECK{
Eigenvalues scale like $A$; the ratio is scale-invariant and dimensionless.
}
\EDGECASES{
\begin{bullets}
\item If $\lambda_{\min}\to 0$, matrix becomes singular and $\kappa_2\to\infty$.
\end{bullets}
}
\ALTERNATE{
Use SVD; for SPD, singular values equal eigenvalues.
}
\VALIDATION{
\begin{bullets}
\item Numeric check on random SPD matrices.
\end{bullets}
}
\INTUITION{
An SPD matrix stretches along orthogonal eigen-directions; the ratio of
maximal to minimal stretching gives the conditioning.
}
\CANONICAL{
\begin{bullets}
\item Rayleigh quotient yields extreme eigenvalues and norms.
\end{bullets}
}
\ProblemPage{9}{Combo: Least Squares Sensitivity and $\kappa(X)$}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
For full column rank $X\in\mathbb{R}^{n\times d}$, least squares solution
$\hat\beta=(X^\top X)^{-1}X^\top y$ has sensitivity
$\dfrac{\|\delta \hat\beta\|_2}{\|\hat\beta\|_2}\le
\kappa_2(X)\dfrac{\|\delta y\|_2}{\|y\|_2}$.
\PROBLEM{
Relate data perturbations in $y$ to coefficient error via conditioning of $X$.
}
\MODEL{
\[
\hat\beta=\arg\min_\beta \|X\beta-y\|_2,\quad
\hat\beta=(X^\top X)^{-1}X^\top y.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Full column rank; $2$-norms; perturb only $y$.
\end{bullets}
}
\varmapStart
\var{X}{Design matrix.}
\var{y}{Targets.}
\var{\hat\beta}{Least squares coefficients.}
\varmapEnd
\WHICHFORMULA{
Apply Formula 3 to the normal equations with effective matrix $X$ mapping
coefficients to data.
}
\GOVERN{
\[
X\hat\beta\approx y,\ \delta \hat\beta\approx X^{\dagger}\delta y,\
\|X^\dagger\|_2=1/\sigma_{\min}(X).
\]
}
\INPUTS{$X=\begin{bmatrix}1&1\\1&1.001\\1&0.999\end{bmatrix}$,
$y=(2,2.001,1.999)^\top$, $\delta y=(10^{-3},0,-10^{-3})^\top$.}
\DERIVATION{
\begin{align*}
\text{SVD of }X:&\ \sigma_{\max}\approx \sqrt{3},\ \sigma_{\min}\approx 0.001.\\
\kappa_2(X)&\approx \sigma_{\max}/\sigma_{\min}\approx 1732.\\
\|y\|_2&\approx \sqrt{4+4.004001+3.996001}\approx 3.464.\
\|\delta y\|_2\approx 0.001414.\\
\frac{\|\delta \hat\beta\|}{\|\hat\beta\|}&\lesssim
1732\cdot \frac{0.001414}{3.464}\approx 0.707.\\
\text{Large relative error possible due to near collinearity.}&
\end{align*}
}
\RESULT{
Relative coefficient error can be on the order of $70\%$ for tiny output
noise due to large $\kappa_2(X)$.
}
\UNITCHECK{
All norms are Euclidean; bound is dimensionless.
}
\EDGECASES{
\begin{bullets}
\item If columns are exactly collinear, solution is not unique and
$\kappa_2(X)=\infty$.
\end{bullets}
}
\ALTERNATE{
Use ridge regression to effectively increase $\sigma_{\min}$ and reduce
conditioning.
}
\VALIDATION{
\begin{bullets}
\item Compute $\hat\beta$ numerically and perturb $y$ to measure error.
\end{bullets}
}
\INTUITION{
Collinearity collapses directions, inflating sensitivity of coefficients.
}
\CANONICAL{
\begin{bullets}
\item Pseudoinverse norm controls least squares sensitivity.
\end{bullets}
}
\ProblemPage{10}{Combo: CG Convergence and $\kappa_2$ of SPD}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
For SPD $A$, conjugate gradients error satisfies
$\|e_k\|_A\le 2\left(\frac{\sqrt{\kappa_2}-1}{\sqrt{\kappa_2}+1}\right)^k
\|e_0\|_A$, where $\kappa_2=\lambda_{\max}/\lambda_{\min}$.
\PROBLEM{
Relate convergence rate to conditioning; verify the bound numerically for a
small SPD matrix.
}
\MODEL{
\[
e_k=x_k-x^\ast,\quad \|\cdot\|_A=\sqrt{(\cdot)^\top A(\cdot)}.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item SPD $A$; exact arithmetic (theoretical bound).
\end{bullets}
}
\varmapStart
\var{A}{SPD matrix.}
\var{\kappa_2}{Spectral condition number.}
\var{e_k}{CG error after $k$ steps.}
\varmapEnd
\WHICHFORMULA{
Formula 8 for $\kappa_2$; standard CG convergence inequality.
}
\GOVERN{
\[
\rho=\frac{\sqrt{\kappa_2}-1}{\sqrt{\kappa_2}+1},\ 
\|e_k\|_A\le 2\rho^k\|e_0\|_A.
\]
}
\INPUTS{$A=\begin{bmatrix}4&1\\1&1\end{bmatrix}$, $b=(1,0)^\top$, $k=2$.}
\DERIVATION{
\begin{align*}
\lambda(A)&=\text{eig of }\begin{bmatrix}4&1\\1&1\end{bmatrix}:
\lambda_{\pm}=\frac{5\pm\sqrt{5}}{2}.\\
\kappa_2&=\frac{\lambda_+}{\lambda_-}=
\frac{5+\sqrt{5}}{5-\sqrt{5}}\approx 2.618.\\
\rho&=\frac{\sqrt{2.618}-1}{\sqrt{2.618}+1}\approx
\frac{1.618-1}{1.618+1}\approx 0.236.\\
\text{Bound: }&\|e_2\|_A\le 2(0.236)^2\|e_0\|_A\approx 0.111\|e_0\|_A.
\end{align*}
}
\RESULT{
Convergence factor governed by $\kappa_2$; moderate conditioning yields rapid
convergence; the bound predicts about $11.1\%$ error after $2$ steps.
}
\UNITCHECK{
$A$-norm units consistent; ratios dimensionless.
}
\EDGECASES{
\begin{bullets}
\item As $\kappa_2\to 1$, $\rho\to 0$ and CG converges in one step.
\item As $\kappa_2\to\infty$, $\rho\to 1$ and convergence slows.
\end{bullets}
}
\ALTERNATE{
Precondition with $M$ to reduce $\kappa_2(M^{-1}A)$ and accelerate CG.
}
\VALIDATION{
\begin{bullets}
\item Run CG numerically and compare actual $\|e_k\|_A$ to the bound.
\end{bullets}
}
\INTUITION{
Better conditioning concentrates eigenvalues, improving polynomial
approximation of $A^{-1}$ by CG.
}
\CANONICAL{
\begin{bullets}
\item Convergence rate monotone in $\kappa_2$ for SPD problems.
\end{bullets}
}
\clearpage
\section{Coding Demonstrations}
\CodeDemoPage{Numerical Verification of Linear Sensitivity Bound}
\PROBLEM{
Compute $\kappa_2(A)$ via SVD, solve $Ax=b$, perturb $b$, and verify that
$\|\delta x\|_2/\|x\|_2 \le \kappa_2(A)\,\|\delta b\|_2/\|b\|_2$ holds
numerically with assertions.
}
\API{
\begin{bullets}
\item \inlinecode{def read_input(s) -> dict} — parse A,b,db.
\item \inlinecode{def solve_case(obj) -> dict} — compute kappa and ratios.
\item \inlinecode{def validate() -> None} — assert inequality.
\item \inlinecode{def main() -> None} — run a deterministic test.
\end{bullets}
}
\INPUTS{
Matrix $A$ (invertible), vectors $b$, $\delta b$; all real-valued.
}
\OUTPUTS{
Dictionary with $\kappa_2$, left and right sides of the inequality.
}
\FORMULA{
\[
\kappa_2(A)=\frac{\sigma_{\max}(A)}{\sigma_{\min}(A)},\quad
\frac{\|\delta x\|_2}{\|x\|_2}\le
\kappa_2(A)\frac{\|\delta b\|_2}{\|b\|_2}.
\]
}
\textbf{SOLUTION A — From Scratch (Mathematically Explicit Implementation)}
\begin{codepy}
import numpy as np

def read_input(s):
    vals = [float(x) for x in s.strip().split()]
    A = np.array(vals[:4], dtype=float).reshape(2, 2)
    b = np.array(vals[4:6], dtype=float)
    db = np.array(vals[6:8], dtype=float)
    return {"A": A, "b": b, "db": db}

def solve_case(obj):
    A, b, db = obj["A"], obj["b"], obj["db"]
    U, S, Vt = np.linalg.svd(A)
    kappa = float(S[0] / S[-1])
    x = np.linalg.solve(A, b)
    dx = np.linalg.solve(A, db)
    lhs = float(np.linalg.norm(dx) / np.linalg.norm(x))
    rhs = float(kappa * np.linalg.norm(db) / np.linalg.norm(b))
    return {"kappa": kappa, "lhs": lhs, "rhs": rhs}

def validate():
    s = "3 1 0 0.2 4 0.2 0.004 -0.002"
    obj = read_input(s)
    out = solve_case(obj)
    assert out["lhs"] <= out["rhs"] * (1 + 1e-12)
    assert out["kappa"] > 1.0

def main():
    validate()
    s = "3 1 0 0.2 4 0.2 0.004 -0.002"
    out = solve_case(read_input(s))
    print("kappa2", round(out["kappa"], 6))
    print("lhs", round(out["lhs"], 6), "rhs", round(out["rhs"], 6))

if __name__ == "__main__":
    main()
\end{codepy}
\textbf{SOLUTION B — Library-Based (Validated Computational Shortcut)}
\begin{codepy}
import numpy as np

def read_input(s):
    vals = [float(x) for x in s.strip().split()]
    A = np.array(vals[:4], dtype=float).reshape(2, 2)
    b = np.array(vals[4:6], dtype=float)
    db = np.array(vals[6:8], dtype=float)
    return {"A": A, "b": b, "db": db}

def solve_case(obj):
    A, b, db = obj["A"], obj["b"], obj["db"]
    S = np.linalg.svd(A, compute_uv=False)
    kappa = float(S[0] / S[-1])
    x = np.linalg.solve(A, b)
    dx = np.linalg.solve(A, db)
    lhs = float(np.linalg.norm(dx) / np.linalg.norm(x))
    rhs = float(kappa * np.linalg.norm(db) / np.linalg.norm(b))
    return {"kappa": kappa, "lhs": lhs, "rhs": rhs}

def validate():
    s = "3 1 0 0.2 4 0.2 0.004 -0.002"
    out = solve_case(read_input(s))
    assert out["lhs"] <= out["rhs"] * (1 + 1e-12)

def main():
    validate()
    s = "3 1 0 0.2 4 0.2 0.004 -0.002"
    out = solve_case(read_input(s))
    print("ok", out["lhs"] <= out["rhs"])

if __name__ == "__main__":
    main()
\end{codepy}
\COMPLEXITY{
Both implementations are $\mathcal{O}(n^3)$ for SVD/solve with $n=2$ here; in
general, SVD $\mathcal{O}(n^3)$, solves $\mathcal{O}(n^3)$; space
$\mathcal{O}(n^2)$.
}
\FAILMODES{
\begin{bullets}
\item Singular or nearly singular $A$ leads to overflow in $\kappa$; detect
via small $\sigma_{\min}$.
\item Zero $b$ gives division by zero; guard and skip the ratio.
\end{bullets}
}
\STABILITY{
\begin{bullets}
\item SVD is backward stable; solving via LU is backward stable with pivoting.
\item Ratios can suffer cancellation if norms are tiny; scale inputs.
\end{bullets}
}
\VALIDATION{
\begin{bullets}
\item Assert inequality; compare both variants outputs.
\item Perturb alignment to test tightness of bound.
\end{bullets}
}
\RESULT{
Both implementations agree and confirm the sensitivity bound numerically on
the chosen deterministic case.
}
\EXPLANATION{
The code computes $\kappa_2$ by SVD and the forward/error ratios directly,
matching Formula 2 and Formula 3.
}
\EXTENSION{
Vectorize to handle random ensembles; histogram the ratio tightness vs.
perturbation direction.
}
\CodeDemoPage{Relative Conditioning via Jacobian for Nonlinear Map}
\PROBLEM{
Estimate $\kappa_{\mathrm{rel}}(f,x)$ from the Jacobian for
$f:\mathbb{R}^2\to\mathbb{R}^2$, $f(x)=\begin{bmatrix}x_1^2+x_2\\
\sin(x_1-x_2)\end{bmatrix}$ at $x=(1,0.5)$; verify the first-order bound for
a small $\Delta x$.
}
\API{
\begin{bullets}
\item \inlinecode{def jacobian(x) -> np.ndarray} — analytic Jacobian.
\item \inlinecode{def kappa_rel(x) -> float} — compute relative $\kappa$.
\item \inlinecode{def validate() -> None} — test the inequality.
\item \inlinecode{def main() -> None} — run deterministic case.
\end{bullets}
}
\INPUTS{
Point $x\in\mathbb{R}^2$ and small perturbation $\Delta x$.
}
\OUTPUTS{
Relative condition number and observed relative forward error vs. bound.
}
\FORMULA{
\[
Df(x)=\begin{bmatrix}2x_1&1\\ \cos(x_1-x_2)&-\cos(x_1-x_2)\end{bmatrix},\ 
\kappa_{\mathrm{rel}}=\frac{\|Df(x)\|_2\|x\|_2}{\|f(x)\|_2}.
\]
}
\textbf{SOLUTION A — From Scratch (Mathematically Explicit Implementation)}
\begin{codepy}
import numpy as np

def f(x):
    return np.array([x[0]**2 + x[1], np.sin(x[0] - x[1])], dtype=float)

def jacobian(x):
    c = float(np.cos(x[0] - x[1]))
    return np.array([[2.0 * x[0], 1.0], [c, -c]], dtype=float)

def kappa_rel(x):
    J = jacobian(x)
    fx = f(x)
    s = np.linalg.svd(J, compute_uv=False)
    normJ = float(s[0])
    return float(normJ * np.linalg.norm(x) / np.linalg.norm(fx))

def validate():
    x = np.array([1.0, 0.5])
    dx = np.array([1e-6, -1e-6])
    kap = kappa_rel(x)
    rel_in = float(np.linalg.norm(dx) / np.linalg.norm(x))
    rel_out = float(np.linalg.norm(f(x + dx) - f(x)) / np.linalg.norm(f(x)))
    assert rel_out <= kap * rel_in * 1.0005

def main():
    validate()
    x = np.array([1.0, 0.5])
    dx = np.array([1e-6, -1e-6])
    kap = kappa_rel(x)
    rel_in = float(np.linalg.norm(dx) / np.linalg.norm(x))
    rel_out = float(np.linalg.norm(f(x + dx) - f(x)) / np.linalg.norm(f(x)))
    print("kappa_rel", round(kap, 6))
    print("rel_out", rel_out, "bound", kap * rel_in)

if __name__ == "__main__":
    main()
\end{codepy}
\textbf{SOLUTION B — Library-Based (Validated Computational Shortcut)}
\begin{codepy}
import numpy as np

def f(x):
    return np.array([x[0]**2 + x[1], np.sin(x[0] - x[1])], dtype=float)

def jacobian(x):
    c = float(np.cos(x[0] - x[1]))
    return np.array([[2.0 * x[0], 1.0], [c, -c]], dtype=float)

def kappa_rel(x):
    s = np.linalg.svd(jacobian(x), compute_uv=False)
    return float(s[0] * np.linalg.norm(x) / np.linalg.norm(f(x)))

def validate():
    x = np.array([1.0, 0.5])
    dx = np.array([1e-6, -1e-6])
    kap = kappa_rel(x)
    rel_in = float(np.linalg.norm(dx) / np.linalg.norm(x))
    rel_out = float(np.linalg.norm(f(x + dx) - f(x)) / np.linalg.norm(f(x)))
    assert rel_out <= kap * rel_in * 1.0005

def main():
    validate()
    x = np.array([1.0, 0.5])
    print("ok", True)

if __name__ == "__main__":
    main()
\end{codepy}
\COMPLEXITY{
Each evaluation uses SVD of a $2\times 2$ Jacobian: $\mathcal{O}(1)$ here,
$\mathcal{O}(n^3)$ in general. Space $\mathcal{O}(n^2)$.
}
\FAILMODES{
\begin{bullets}
\item $\|f(x)\|=0$ makes $\kappa_{\mathrm{rel}}$ undefined; detect and skip.
\item Large $dx$ violates first-order regime; keep small.
\end{bullets}
}
\STABILITY{
\begin{bullets}
\item Small differences avoid loss of significance due to vector norms.
\item Use analytic Jacobian to eliminate finite-difference noise.
\end{bullets}
}
\VALIDATION{
\begin{bullets}
\item Compare observed relative change to computed bound with slack factor.
\end{bullets}
}
\RESULT{
Observed relative change respects the first-order bound using the computed
$\kappa_{\mathrm{rel}}$.
}
\EXPLANATION{
Implements Formula 1 and Formula 4: derivative norm normalizes input and
output to produce a predictive relative bound.
}
\EXTENSION{
Extend to componentwise conditioning by scaling coordinates.
}
\clearpage
\section{Applied Domains — Detailed End-to-End Scenarios}
\DomainPage{Machine Learning}
\SCENARIO{
Assess conditioning of linear regression and its effect on coefficient
stability. Compare unscaled vs. standardized features by measuring
$\kappa_2(X)$ and coefficient sensitivity to perturbations in $y$.
}
\ASSUMPTIONS{
\begin{bullets}
\item Full column rank; i.i.d.\ noise; Euclidean norms.
\item Standardization centers and scales columns to unit variance.
\end{bullets}
}
\WHICHFORMULA{
Use $\kappa_2(X)$ (Formula 2) and least squares sensitivity bound (Problem 9).
}
\varmapStart
\var{X}{Design matrix $(n,d)$ including bias if needed.}
\var{y}{Target vector of length $n$.}
\var{\hat\beta}{Least squares coefficients.}
\var{\kappa_2(X)}{Spectral condition number of $X$.}
\varmapEnd
\PIPELINE{
\begin{bullets}
\item Generate correlated features.
\item Fit OLS and compute $\kappa_2(X)$ unscaled and scaled.
\item Perturb $y$ slightly and compare coefficient changes.
\end{bullets}
}
\textbf{Implementation (From Scratch)}
\begin{codepy}
import numpy as np

def gen_data(n=200, seed=0):
    rng = np.random.RandomState(seed)
    x1 = rng.randn(n)
    x2 = x1 * 0.999 + 0.001 * rng.randn(n)
    X = np.column_stack([np.ones(n), x1, x2])
    beta = np.array([1.0, 2.0, -2.0])
    y = X @ beta + 0.01 * rng.randn(n)
    return X, y

def kappa2(M):
    s = np.linalg.svd(M, compute_uv=False)
    return float(s[0] / s[-1])

def standardize(X):
    Z = X.copy()
    Z[:, 0] = 1.0
    for j in range(1, X.shape[1]):
        Z[:, j] = (X[:, j] - X[:, j].mean()) / X[:, j].std()
    return Z

def ols(X, y):
    return np.linalg.lstsq(X, y, rcond=None)[0]

def main():
    X, y = gen_data()
    Z = standardize(X)
    kapX, kapZ = kappa2(X), kappa2(Z)
    beta = ols(X, y)
    y2 = y.copy()
    y2[:10] += 1e-3
    beta2 = ols(X, y2)
    rel = float(np.linalg.norm(beta2 - beta) / np.linalg.norm(beta))
    print("kappa2(X)", round(kapX, 2), "kappa2(Z)", round(kapZ, 2))
    print("rel_coef_change", round(rel, 4))

if __name__ == "__main__":
    main()
\end{codepy}
\textbf{Implementation (Library Version)}
\begin{codepy}
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression

def main():
    rng = np.random.RandomState(0)
    n = 200
    x1 = rng.randn(n)
    x2 = x1 * 0.999 + 0.001 * rng.randn(n)
    X = np.column_stack([x1, x2])
    scaler = StandardScaler(with_mean=True, with_std=True)
    Z = scaler.fit_transform(X)
    y = 1 + 2 * x1 - 2 * x2 + 0.01 * rng.randn(n)
    sX = np.linalg.svd(np.column_stack([np.ones(n), X]),
                       compute_uv=False)
    sZ = np.linalg.svd(np.column_stack([np.ones(n), Z]),
                       compute_uv=False)
    kapX = float(sX[0] / sX[-1])
    kapZ = float(sZ[0] / sZ[-1])
    lr = LinearRegression(fit_intercept=True).fit(X, y)
    y2 = y.copy()
    y2[:10] += 1e-3
    lr2 = LinearRegression(fit_intercept=True).fit(X, y2)
    rel = float(np.linalg.norm(lr2.coef_ - lr.coef_) /
                np.linalg.norm(lr.coef_))
    print("kappaX", round(kapX, 2), "kappaZ", round(kapZ, 2), "rel", round(rel, 4))

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{Condition numbers $\kappa_2$ and relative coefficient change.}
\INTERPRET{
Standardization reduces $\kappa_2$ and stabilizes coefficients under small
output perturbations.
}
\NEXTSTEPS{
Investigate ridge regression that increases $\sigma_{\min}$ to reduce $\kappa$.
}
\DomainPage{Quantitative Finance}
\SCENARIO{
Portfolio optimization $w^\top \Sigma w$ with target return requires inverting
$\Sigma$. Evaluate $\kappa_2(\Sigma)$ and the sensitivity of the minimum-
variance weights to small perturbations in estimated returns $\mu$.
}
\ASSUMPTIONS{
\begin{bullets}
\item $\Sigma$ SPD; sample estimates have noise.
\item $2$-norm; perturb $\mu$ only for sensitivity.
\end{bullets}
}
\WHICHFORMULA{
Use $\kappa_2(\Sigma)$ (Formula 8) and linear sensitivity bounds for solving
$\Sigma w = \lambda 1 - \gamma \mu$.
}
\varmapStart
\var{\Sigma}{Covariance matrix (SPD).}
\var{\mu}{Expected returns vector.}
\var{w}{Optimal weights.}
\var{\kappa_2(\Sigma)}{Spectral condition number.}
\varmapEnd
\PIPELINE{
\begin{bullets}
\item Simulate correlated returns; estimate $\Sigma$.
\item Compute $\kappa_2(\Sigma)$.
\item Perturb $\mu$ and measure change in $w$.
\end{bullets}
}
\textbf{Implementation (Full Pipeline)}
\begin{codepy}
import numpy as np

def simulate(n=500, d=5, seed=0):
    rng = np.random.RandomState(seed)
    A = rng.randn(d, d)
    Sigma = A @ A.T + 0.1 * np.eye(d)
    R = rng.multivariate_normal(np.zeros(d), Sigma, size=n)
    mu = R.mean(axis=0)
    return Sigma, mu

def min_var_weights(Sigma):
    inv = np.linalg.inv(Sigma)
    u = inv @ np.ones(Sigma.shape[0])
    w = u / (np.ones_like(u) @ u)
    return w

def kappa2(M):
    s = np.linalg.svd(M, compute_uv=False)
    return float(s[0] / s[-1])

def main():
    Sigma, mu = simulate()
    w = min_var_weights(Sigma)
    kap = kappa2(Sigma)
    dmu = np.zeros_like(mu)
    dmu[0] = 1e-3
    lam = 0.0
    gamma = 1.0
    rhs = lam * np.ones_like(mu) - gamma * mu
    rhs2 = lam * np.ones_like(mu) - gamma * (mu + dmu)
    w2 = np.linalg.solve(Sigma, rhs2)
    w2 = w2 / w2.sum()
    rel = float(np.linalg.norm(w2 - w) / np.linalg.norm(w))
    print("kappa2(Sigma)", round(kap, 2), "rel_w_change", round(rel, 4))

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{Report $\kappa_2(\Sigma)$ and relative change in $w$.}
\INTERPRET{
Large $\kappa_2(\Sigma)$ implies $\Sigma^{-1}$ amplifies mean estimation
errors into significant weight shifts.
}
\NEXTSTEPS{
Apply shrinkage or factor models to improve $\Sigma$ conditioning; use
regularization to stabilize $w$.
}
\DomainPage{Deep Learning}
\SCENARIO{
Analyze conditioning of a linear layer $y=W x$ and of a two-layer network
$y=W_2 \phi(W_1 x)$ near a point by computing Jacobian singular values and
$\kappa_2$; relate to gradient scaling.
}
\ASSUMPTIONS{
\begin{bullets}
\item ReLU activation with fixed active set near the point.
\item Local linearization applicable; $2$-norms.
\end{bullets}
}
\WHICHFORMULA{
Use $\kappa_2(J)$ where $J$ is the network Jacobian; singular values of
$W_2 D W_1$ control sensitivity and gradient flow.
}
\varmapStart
\var{W_1,W_2}{Layer weight matrices.}
\var{D}{ReLU diagonal mask at the point.}
\var{J}{Jacobian; product $W_2 D W_1$.}
\varmapEnd
\PIPELINE{
\begin{bullets}
\item Generate deterministic weights and input.
\item Compute $J$ and $\kappa_2(J)$.
\item Compare gradient scaling via $\|J^\top\|_2$.
\end{bullets}
}
\textbf{Implementation (End-to-End)}
\begin{codepy}
import numpy as np

def relu(z):
    return np.maximum(0.0, z)

def jacobian(W1, W2, x):
    z = W1 @ x
    D = np.diag((z > 0).astype(float))
    return W2 @ D @ W1

def main():
    np.random.seed(0)
    W1 = np.array([[2.0, -1.0], [0.5, 1.0]], dtype=float)
    W2 = np.array([[1.0, 0.5]], dtype=float)
    x = np.array([0.5, -0.1], dtype=float)
    J = jacobian(W1, W2, x)
    s = np.linalg.svd(J, compute_uv=False)
    kap = float(s[0] / s[-1]) if s.size > 1 else 1.0
    print("kappa2(J)", kap, "sigma_max(J)", s[0])

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{$\kappa_2(J)$ and singular values indicating sensitivity and
gradient amplification.}
\INTERPRET{
Poorly conditioned $J$ means small input changes can cause large output
changes and gradients can explode or vanish.
}
\NEXTSTEPS{
Use orthogonal initialization or normalization to keep $\kappa_2(J)$ near $1$.
}
\DomainPage{Kaggle / Data Analytics}
\SCENARIO{
Evaluate the effect of feature standardization on the conditioning of the
Gram matrix $G=X^\top X$ and the stability of linear model coefficients.
}
\ASSUMPTIONS{
\begin{bullets}
\item Numerical features only; standardization to zero mean, unit variance.
\item $2$-norms used for conditioning.
\end{bullets}
}
\WHICHFORMULA{
$\kappa_2(G)=\kappa_2(X)^2$ in $2$-norm; standardization improves
$\sigma_{\min}(X)$ and thus reduces $\kappa_2(G)$.
}
\varmapStart
\var{X}{Feature matrix.}
\var{G}{Gram matrix $X^\top X$.}
\var{\kappa_2}{Spectral condition number.}
\varmapEnd
\PIPELINE{
\begin{bullets}
\item Create synthetic correlated features.
\item Compute $\kappa_2(X)$ and $\kappa_2(G)$ before/after standardization.
\item Observe coefficient stability.
\end{bullets}
}
\textbf{Implementation (Complete EDA Pipeline)}
\begin{codepy}
import numpy as np

def make_X(n=300, seed=0):
    rng = np.random.RandomState(seed)
    a = rng.randn(n)
    b = 0.99 * a + 0.01 * rng.randn(n)
    c = 2.0 * rng.randn(n) + 5.0
    X = np.column_stack([a, b, c])
    return X

def standardize(X):
    Z = (X - X.mean(axis=0)) / X.std(axis=0)
    return Z

def kappa2(M):
    s = np.linalg.svd(M, compute_uv=False)
    return float(s[0] / s[-1])

def main():
    X = make_X()
    Z = standardize(X)
    G = X.T @ X
    H = Z.T @ Z
    print("kappa2(X)", round(kappa2(X), 2),
          "kappa2(G)", round(kappa2(G), 2))
    print("kappa2(Z)", round(kappa2(Z), 2),
          "kappa2(H)", round(kappa2(H), 2))

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{Report $\kappa_2$ of $X$ and $G$ before/after scaling.}
\INTERPRET{
Standardization reduces collinearity, increasing $\sigma_{\min}$ and lowering
$\kappa_2$, improving numerical stability of modeling.
}
\NEXTSTEPS{
Apply PCA to orthogonalize features and further improve conditioning.
}
\end{document}
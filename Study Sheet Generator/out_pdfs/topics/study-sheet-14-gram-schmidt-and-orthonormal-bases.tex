% !TeX program = xelatex
\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{microtype,setspace,amsmath,amssymb,mathtools,amsthm,unicode-math}
\setstretch{1.05}
\setmainfont{Latin Modern Roman}
\setmonofont{Latin Modern Mono}
\setmathfont{Latin Modern Math}

\allowdisplaybreaks[4]
\setlength{\jot}{7pt}
\setlength{\emergencystretch}{8em}
\sloppy

\usepackage{xcolor,fancyhdr,enumitem,inconsolata,listings}
\pagestyle{fancy}\fancyhf{}\lhead{\nouppercase{\leftmark}}\rhead{\thepage}
\setlength{\headheight}{26pt}
\setlength{\parindent}{0pt}
\setlength{\parskip}{8pt plus 2pt minus 1pt}
\raggedbottom

\newenvironment{BreakableEquation}{\begin{equation}\begin{aligned}}{\end{aligned}\end{equation}}
\newenvironment{BreakableEquation*}{\begin{equation*}\begin{aligned}}{\end{aligned}\end{equation*}}
\newenvironment{tightalign}{\begingroup\small\allowdisplaybreaks\begin{align}}{\end{align}\endgroup}

\providecommand{\enumlistm}{enumitem}
\newenvironment{minted}[2][]{%
  \lstset{style=code,language=#2,#1}\begin{lstlisting}%
}{\end{lstlisting}}

\newcommand{\inputminted}[3][]{\begin{lstlisting}\end{lstlisting}}

\newlist{bullets}{itemize}{1}
\setlist[bullets]{label=--,leftmargin=1.6em,itemsep=2pt,topsep=4pt}

\newcommand{\varmapStart}{\textbf{VARIABLE MAPPING:}\par\begin{bullets}}
\newcommand{\var}[2]{\item $#1$ — #2}
\newcommand{\varmapEnd}{\end{bullets}}

\newcommand{\glossx}[6]{%
  \textbf{#1}\par
  \begin{bullets}
    \item \textbf{What:} #2
    \item \textbf{Why:} #3
    \item \textbf{How:} #4
    \item \textbf{ELI5:} #5
    \item \textbf{Pitfall/Example:} #6
  \end{bullets}
}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{claim}[theorem]{Claim}

\lstdefinestyle{code}{
  basicstyle=\ttfamily\footnotesize,
  backgroundcolor=\color{black!02},
  frame=single,
  numbers=left, numberstyle=\tiny, numbersep=8pt,
  breaklines=true, breakatwhitespace=true,
  tabsize=4, showstringspaces=false,
  upquote=true, keepspaces=true, columns=fullflexible,
  literate=
    {–}{{-}}1
    {—}{{-}}1
    {…}{{...}}1
    {≤}{{\ensuremath{\le}}}1
    {≥}{{\ensuremath{\ge}}}1
    {≠}{{\ensuremath{\ne}}}1
    {≈}{{\ensuremath{\approx}}}1
    {±}{{\ensuremath{\pm}}}1
    {→}{{\ensuremath{\to}}}1
    {←}{{\ensuremath{\leftarrow}}}1
    {∞}{{\ensuremath{\infty}}}1
    {√}{{\ensuremath{\sqrt{\ }}}}1
    {×}{{\ensuremath{\times}}}1
    {÷}{{\ensuremath{\div}}}1
}

\lstnewenvironment{codepy}[1][]%
  {\lstset{style=code,language=Python,#1}}%
  {}

\newcommand{\inlinecode}[1]{\lstinline[style=code]!#1!}

\newcommand{\LF}[2]{\par\noindent\textbf{#1:}~#2\par}
\newcommand{\WHAT}[1]{\LF{WHAT}{#1}}
\newcommand{\WHY}[1]{\LF{WHY}{#1}}
\newcommand{\HOW}[1]{\LF{HOW}{#1}}
\newcommand{\ELI}[1]{\LF{ELI5}{#1}}
\newcommand{\SCOPE}[1]{\LF{SCOPE}{#1}}
\newcommand{\CONFUSIONS}[1]{\LF{COMMON CONFUSIONS}{#1}}
\newcommand{\APPLICATIONS}[1]{\LF{APPLICATIONS}{#1}}
\newcommand{\FORMULA}[1]{\LF{FORMULA}{#1}}
\newcommand{\CANONICAL}[1]{\LF{CANONICAL FORM}{#1}}
\newcommand{\PRECONDS}[1]{\LF{PRECONDITIONS}{#1}}
\newcommand{\DERIVATION}[1]{\LF{DERIVATION}{#1}}
\newcommand{\EQUIV}[1]{\LF{EQUIVALENT FORMS}{#1}}
\newcommand{\LIMITS}[1]{\LF{LIMIT CASES}{#1}}
\newcommand{\INPUTS}[1]{\LF{INPUTS}{#1}}
\newcommand{\OUTPUTS}[1]{\LF{OUTPUTS}{#1}}
\newcommand{\RESULT}[1]{\LF{RESULT}{#1}}
\newcommand{\INTUITION}[1]{\LF{INTUITION}{#1}}
\newcommand{\PITFALLS}[1]{\LF{PITFALLS}{#1}}
\newcommand{\MODEL}[1]{\LF{CANONICAL MATH MODEL}{#1}}
\newcommand{\ASSUMPTIONS}[1]{\LF{ASSUMPTIONS}{#1}}
\newcommand{\WHICHFORMULA}[1]{\LF{WHICH FORMULA \& WHY}{#1}}
\newcommand{\GOVERN}[1]{\LF{GOVERNING EQUATION(S)}{#1}}
\newcommand{\UNITCHECK}[1]{\LF{UNIT CHECK}{#1}}
\newcommand{\EDGECASES}[1]{\LF{EDGE CASES}{#1}}
\newcommand{\ALTERNATE}[1]{\LF{ALTERNATE APPROACH (sketch)}{#1}}
\newcommand{\PROBLEM}[1]{\LF{PROBLEM}{#1}}
\newcommand{\API}[1]{\LF{API}{#1}}
\newcommand{\COMPLEXITY}[1]{\LF{COMPLEXITY}{#1}}
\newcommand{\FAILMODES}[1]{\LF{FAILURE MODES}{#1}}
\newcommand{\STABILITY}[1]{\LF{NUMERICAL STABILITY}{#1}}
\newcommand{\VALIDATION}[1]{\LF{VALIDATION}{#1}}
\newcommand{\EXPLANATION}[1]{\LF{EXPLANATION}{#1}}
\newcommand{\SCENARIO}[1]{\LF{SCENARIO}{#1}}
\newcommand{\PIPELINE}[1]{\LF{PIPELINE STEPS}{#1}}
\newcommand{\METRICS}[1]{\LF{METRICS}{#1}}
\newcommand{\INTERPRET}[1]{\LF{INTERPRETATION}{#1}}
\newcommand{\NEXTSTEPS}[1]{\LF{LIMITATIONS \& NEXT STEPS}{#1}}

\usepackage{titlesec}
\titleformat{\section}{\Large\bfseries}{\thesection}{0.6em}{}
\titlespacing*{\section}{0pt}{*2}{*1}
\usepackage{etoolbox}
\pretocmd{\section}{\clearpage}{}{}

\newcommand{\FormulaPage}[2]{%
  \clearpage
  \section*{Formula #1 — #2}%
  \addcontentsline{toc}{section}{Formula #1 — #2}%
}
\newcommand{\ProblemPage}[2]{%
  \clearpage
  \subsection*{Problem #1: #2}%
  \addcontentsline{toc}{subsection}{Problem #1: #2}%
}
\newcommand{\CodeDemoPage}[1]{%
  \clearpage
  \subsection*{Coding Demo: #1}%
  \addcontentsline{toc}{subsection}{Coding Demo: #1}%
}
\newcommand{\DomainPage}[1]{%
  \clearpage
  \subsection*{#1 (End-to-End)}%
  \addcontentsline{toc}{subsection}{#1 (End-to-End)}%
}

\begin{document}
\title{Comprehensive Study Sheet — Gram-Schmidt and Orthonormal Bases}
\date{\today}
\maketitle
\tableofcontents
\clearpage

\section{Concept Overview}
\WHAT{
An inner product space $(V,\langle\cdot,\cdot\rangle)$ is a vector space
over $\mathbb{R}$ or $\mathbb{C}$ with a positive-definite sesquilinear
form. Gram-Schmidt takes a finite linearly independent list
$(v_1,\dots,v_n)\in V^n$ and produces an orthonormal list
$(q_1,\dots,q_n)$ with $\operatorname{span}\{q_1,\dots,q_k\}
=\operatorname{span}\{v_1,\dots,v_k\}$ for each $k$. The process uses
orthogonal projections $\operatorname{proj}_u(w)
=\frac{\langle w,u\rangle}{\langle u,u\rangle}u$ and normalizations
$q=\frac{u}{\|u\|}$ where $\|u\|=\sqrt{\langle u,u\rangle}$. In matrix
form, for a full column-rank $A\in\mathbb{F}^{m\times n}$ ($\mathbb{F}$
real or complex), Gram-Schmidt yields the QR factorization $A=QR$ with
$Q^\ast Q=I$ and $R$ upper triangular with positive diagonal.
}
\WHY{
Orthonormal bases simplify computation: coordinates are inner products,
projection is trivial, Pythagorean and Parseval identities hold, and
matrices become well-conditioned. Gram-Schmidt constructs such bases in
arbitrary inner product spaces and underpins QR factorization, least
squares, polynomial orthogonalization (Legendre, Chebyshev), and signal
decomposition.
}
\HOW{
1. Assume $(v_1,\dots,v_n)$ is linearly independent in an inner product
space. 2. Recursively subtract projections onto previously built
orthogonal vectors to get $u_k$, ensuring $u_k\perp u_j$ for $j<k$ by
construction. 3. Normalize: $q_k=u_k/\|u_k\|$ to obtain orthonormality.
4. Interpret $u_k$ as the component of $v_k$ orthogonal to
$\operatorname{span}\{v_1,\dots,v_{k-1}\}$; then $q_k$ is its unit
version. In matrix form, collect $q_k$ as $Q$ and inner products as
$R=(\langle a_j,q_i\rangle)_{i\le j}$ to get $A=QR$.
}
\ELI{
Think of building a set of perfectly perpendicular arrows. Take the
first arrow. For the second, remove the part that points along the
first, leaving only the sideways part; then make it unit length.
Continue: each time, shave off the components that point in directions
you already have, then rescale what remains to length one.
}
\SCOPE{
Works in finite-dimensional inner product spaces over $\mathbb{R}$ or
$\mathbb{C}$. If vectors are linearly dependent, some intermediate
$u_k$ becomes zero and the process stops (rank deficiency). Numerical
stability: classical Gram-Schmidt can lose orthogonality in floating
point when vectors are nearly dependent; modified Gram-Schmidt or Householder
QR improves stability.
}
\CONFUSIONS{
Orthogonal vs. orthonormal: orthonormal additionally requires unit
length. QR factorization vs. SVD: QR uses orthonormal columns and
upper-triangular $R$; SVD uses two orthogonal/unitary factors and
diagonal singular values. Projection onto a vector vs. onto a subspace:
subspace projection uses an orthonormal basis of the subspace, not a
single vector unless the subspace is one-dimensional.
}
\APPLICATIONS{
\begin{bullets}
\item Mathematical foundations (pure / applied).
\item Computational modeling or simulation.
\item Physical / economic / engineering interpretations.
\item Statistical or algorithmic implications.
\end{bullets}
}
\textbf{ANALYTIC STRUCTURE.}
Linear, inner-product-based, norm-inducing; preserves span hierarchy and
enforces pairwise orthogonality (hence convex Pythagorean decompositions).
\textbf{CANONICAL LINKS.}
Projection theorem, Pythagorean identity, QR factorization, Parseval's
identity, Gram matrix determinant equals squared volume.
\textbf{PROBLEM-TYPE RECOGNITION HEURISTICS.}
\begin{bullets}
\item Requests for orthonormal bases from given vectors.
\item Minimizing distance to a subspace or computing projections.
\item Solving least squares via QR.
\item Orthonormalizing polynomials/functions under $\int fg$.
\end{bullets}
\textbf{SOLUTION STRATEGY BLUEPRINT.}
\begin{bullets}
\item Translate vectors/functions and inner product definition.
\item Identify projection and recursive subtraction.
\item Normalize and assemble basis or QR factors.
\item Validate orthonormality and span equality; use for projections.
\end{bullets}
\textbf{CONCEPTUAL INVARIANTS.}
Span preservation, orthogonality, norm of residual, squared-volume
invariance (determinant of Gram matrix).
\textbf{EDGE INTUITION.}
As vectors become nearly dependent, $u_k$ norms shrink and conditioning
worsens; projections dominate. In high dimension, random directions are
almost orthogonal, so Gram-Schmidt changes little.

\section{Glossary}
\glossx{Inner Product}
{Map $\langle\cdot,\cdot\rangle:V\times V\to\mathbb{F}$ that is linear
in the first slot (conjugate-linear in the first for complex by our
convention or in the second by another), symmetric/Hermitian, and
positive definite.}
{Defines geometry: lengths, angles, projections, and orthogonality.}
{Compute $\|v\|=\sqrt{\langle v,v\rangle}$, and use
$\operatorname{proj}_u(w)=\frac{\langle w,u\rangle}{\langle u,u\rangle}
u$.}
{Like measuring how much one arrow points in the same direction as
another.}
{Pitfall: forgetting conjugation in complex spaces changes projections.}
\glossx{Orthogonal Projection}
{Linear map $P_U:V\to U$ with $P_U^2=P_U$, $P_U=P_U^\ast$, and
$v-P_Uv\perp U$, where $U$ is a closed subspace.}
{Gives best approximation in least-squares sense and decomposes vectors
into along-subspace and orthogonal residual.}
{With orthonormal basis $(q_i)$ of $U$, $P_Uv=\sum_i\langle v,q_i\rangle
q_i$.}
{Shadow of a vector onto a flat floor spanned by some directions.}
{Pitfall: using non-orthonormal basis coefficients as projection
coefficients is incorrect.}
\glossx{Orthonormal Basis}
{List $(q_1,\dots,q_n)$ with $\langle q_i,q_j\rangle=\delta_{ij}$ whose
span is the space.}
{Simplifies coordinates, decouples energy (Parseval), stabilizes
computation.}
{Apply Gram-Schmidt to any basis, then normalize.}
{A set of perfectly perpendicular unit arrows spanning your space.}
{Pitfall: normalizing before removing prior components breaks
orthogonality.}
\glossx{QR Factorization}
{$A=QR$ with $Q^\ast Q=I$ and $R$ upper triangular (positive diagonal).}
{Core tool for least squares, eigenvalue algorithms, numerical stability.}
{Apply Gram-Schmidt to columns of $A$ to form $Q$; set
$r_{ij}=\langle a_j,q_i\rangle$.}
{Rewrite any set of directions as orthonormal directions and simple
scalings.}
{Pitfall: sign/phase of $Q$ columns must match positive $r_{ii}$.}

\section{Symbol Ledger}
\varmapStart
\var{V}{Inner product space over $\mathbb{R}$ or $\mathbb{C}$.}
\var{\langle\cdot,\cdot\rangle}{Inner product on $V$.}
\var{\|\cdot\|}{Norm $\|v\|=\sqrt{\langle v,v\rangle}$.}
\var{v_i}{Given linearly independent vectors in $V$.}
\var{u_i}{Orthogonal (not normalized) Gram-Schmidt vectors.}
\var{q_i}{Orthonormal vectors $q_i=u_i/\|u_i\|$.}
\var{A}{Matrix with columns $a_j$ in $\mathbb{F}^{m\times n}$.}
\var{Q}{Matrix with orthonormal columns $q_i$.}
\var{R}{Upper triangular factor with entries $r_{ij}$.}
\var{G}{Gram matrix $G=(\langle v_i,v_j\rangle)_{ij}$.}
\var{P_U}{Orthogonal projector onto subspace $U$.}
\var{I}{Identity matrix; $I=Q^\ast Q$.}
\var{\ast}{Conjugate transpose (transpose for real).}
\varmapEnd

\section{Formula Canon — One Formula Per Page}
\FormulaPage{1}{Projection onto a Vector}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Given nonzero $u\in V$ and any $w\in V$,
$\operatorname{proj}_u(w)=\frac{\langle w,u\rangle}{\langle u,u\rangle}u$
is the unique vector in $\operatorname{span}\{u\}$ minimizing
$\|w-z\|$.
\WHAT{
Computes the closest point to $w$ along the line spanned by $u$ and
decomposes $w$ into parallel and orthogonal parts: $w=\operatorname{proj}_u(w)
+(w-\operatorname{proj}_u(w))$ with orthogonality.
}
\WHY{
Fundamental step in Gram-Schmidt to remove previously constructed
directions. Also provides least-squares solution to one-dimensional
approximation and leads to Pythagorean identity.
}
\FORMULA{
\[
\operatorname{proj}_u(w)=\frac{\langle w,u\rangle}{\langle u,u\rangle}u,
\quad w-\operatorname{proj}_u(w)\perp u.
\]
}
\CANONICAL{
$V$ finite-dimensional inner product space over $\mathbb{F}$.
$u\ne 0$. For complex $\mathbb{F}$ we use conjugate symmetry:
$\langle x,y\rangle=\overline{\langle y,x\rangle}$.
}
\PRECONDS{
\begin{bullets}
\item $u\ne 0$ so that $\langle u,u\rangle>0$.
\item Inner product satisfies positivity and conjugate symmetry.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
For any $u\ne 0$ and $w$, the function
$f(\alpha)=\|w-\alpha u\|^2$ is minimized at
$\alpha^\star=\frac{\langle w,u\rangle}{\langle u,u\rangle}$.
\end{lemma}
\begin{proof}
Expand using polarization:
$f(\alpha)=\langle w-\alpha u,w-\alpha u\rangle
=\langle w,w\rangle-2\Re(\alpha\langle u,w\rangle)+|\alpha|^2\langle u,u\rangle$.
As a real quadratic in $(\Re\alpha,\Im\alpha)$, it is minimized at the
critical point solving $\partial f/\partial \overline{\alpha}=0$,
yielding $\alpha^\star=\frac{\langle w,u\rangle}{\langle u,u\rangle}$.
Substituting gives the minimum and $w-\alpha^\star u\perp u$ since
$\langle w-\alpha^\star u,u\rangle=\langle w,u\rangle-\alpha^\star
\langle u,u\rangle=0$. \qedhere
\end{proof}
\DERIVATION{
\begin{align*}
\text{Setup: }& f(\alpha)=\|w-\alpha u\|^2
=\langle w,w\rangle-2\Re(\alpha\langle u,w\rangle)
+|\alpha|^2\langle u,u\rangle.\\
\text{Minimize: }& \frac{\partial f}{\partial \overline{\alpha}}
=-\langle u,w\rangle+\alpha \langle u,u\rangle=0
\Rightarrow \alpha^\star=\frac{\langle w,u\rangle}{\langle u,u\rangle}.\\
\text{Projection: }& \operatorname{proj}_u(w)=\alpha^\star u.\\
\text{Orthogonality: }& \langle w-\operatorname{proj}_u(w),u\rangle=0.
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Compute inner products $\langle w,u\rangle$ and $\langle u,u\rangle$.
\item Form the scalar ratio to get the coefficient.
\item Multiply by $u$ and subtract to get the residual.
\item Verify orthogonality and use Pythagorean identity.
\end{bullets}
\EQUIV{
\begin{bullets}
\item With unit $q=u/\|u\|$: $\operatorname{proj}_u(w)=\langle w,q\rangle q$.
\item Matrix form for $u\in\mathbb{R}^m$:
$P=\frac{uu^\ast}{u^\ast u}$, so $Pw=\operatorname{proj}_u(w)$.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item If $u=0$, projection undefined.
\item If $w\parallel u$, residual is zero.
\item As $\|u\|\to 0$ with fixed direction, coefficient remains bounded
but numerical stability worsens.
\end{bullets}
}
\INPUTS{$u\in V$ nonzero,\; $w\in V$ arbitrary.}
\DERIVATION{
\begin{align*}
\text{Example in }\mathbb{R}^3:~&
u=(1,1,0),~w=(2,1,3),~\langle x,y\rangle=x^\top y.\\
&\langle w,u\rangle=3,\;\langle u,u\rangle=2,\;
\alpha^\star=3/2.\\
&\operatorname{proj}_u(w)=(3/2)(1,1,0)=(3/2,3/2,0).\\
&\text{Residual }=(1/2,-1/2,3),\;\text{orthogonal to }u.
\end{align*}
}
\RESULT{
Projection is $(3/2,3/2,0)$ and residual $(1/2,-1/2,3)$ with
$\|(1/2,-1/2,3)\|^2=\|w\|^2-\|\operatorname{proj}_u(w)\|^2$.
}
\UNITCHECK{
Inner products produce scalars; $P$ is idempotent and self-adjoint;
dimensions preserved in vector spaces.
}
\PITFALLS{
\begin{bullets}
\item Forgetting conjugation in $\mathbb{C}$ yields wrong coefficient.
\item Projecting using a non-unit vector but treating as unit.
\end{bullets}
}
\INTUITION{
Choose the multiple of $u$ that best matches $w$; the leftover is purely
sideways relative to $u$.
}
\CANONICAL{
\begin{bullets}
\item Orthogonal decomposition: $w=Pw+(I-P)w$ with $P^2=P=P^\ast$.
\item Energy split: $\|w\|^2=\|Pw\|^2+\|(I-P)w\|^2$.
\end{bullets}
}

\FormulaPage{2}{Gram-Schmidt Orthogonalization}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Given linearly independent $(v_1,\dots,v_n)$,
define $u_1=v_1$ and for $k\ge 2$
\[
u_k=v_k-\sum_{i=1}^{k-1}\frac{\langle v_k,u_i\rangle}{\langle u_i,u_i\rangle}u_i,
\quad q_k=\frac{u_k}{\|u_k\|}.
\]
Then $(q_1,\dots,q_n)$ is orthonormal and
$\operatorname{span}\{q_1,\dots,q_k\}=\operatorname{span}\{v_1,\dots,v_k\}$.
\WHAT{
Constructs an orthonormal basis from any independent list by iterative
projection subtraction and normalization.
}
\WHY{
Orthonormal bases enable simple coordinates, projections, and stable
algorithms. Gram-Schmidt is the constructive proof of existence of such
bases and leads directly to QR factorization.
}
\FORMULA{
\[
u_1=v_1,\quad
u_k=v_k-\sum_{i=1}^{k-1}\operatorname{proj}_{u_i}(v_k),\quad
q_k=\frac{u_k}{\|u_k\|}.
\]
}
\CANONICAL{
Finite-dimensional inner product space over $\mathbb{F}$.
$(v_1,\dots,v_n)$ linearly independent so that $u_k\ne 0$.
}
\PRECONDS{
\begin{bullets}
\item Inner product well-defined and positive definite.
\item Input list is linearly independent.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
For each $k$, $u_k\perp u_i$ for $i<k$, and
$\operatorname{span}\{u_1,\dots,u_k\}
=\operatorname{span}\{v_1,\dots,v_k\}$.
\end{lemma}
\begin{proof}
By induction. For $k=1$ trivial. Suppose true up to $k-1$.
Then for any $i<k$,
$\langle u_k,u_i\rangle=\langle v_k,u_i\rangle-
\sum_{j=1}^{k-1}\frac{\langle v_k,u_j\rangle}{\langle u_j,u_j\rangle}
\langle u_j,u_i\rangle$. Orthogonality of $\{u_j\}_{j<k}$ gives
$\langle u_j,u_i\rangle=0$ for $j\ne i$ and equals
$\langle u_i,u_i\rangle$ when $j=i$, thus $\langle u_k,u_i\rangle=0$.
Span equality: $u_k$ is $v_k$ minus a combination of $u_i$ with $i<k$,
hence $u_k\in \operatorname{span}\{v_1,\dots,v_k\}$ and together with
induction this yields equality of spans. \qedhere
\end{proof}
\DERIVATION{
\begin{align*}
\text{Step 1: }& u_1=v_1,\; q_1=\frac{u_1}{\|u_1\|}.\\
\text{Step 2: }& u_2=v_2-\frac{\langle v_2,u_1\rangle}{\langle u_1,u_1\rangle}u_1,
\; q_2=\frac{u_2}{\|u_2\|}.\\
\text{Step 3: }& u_3=v_3-\frac{\langle v_3,u_1\rangle}{\langle u_1,u_1\rangle}u_1
-\frac{\langle v_3,u_2\rangle}{\langle u_2,u_2\rangle}u_2,\;
q_3=\frac{u_3}{\|u_3\|}.\\
\text{General: }& u_k=v_k-\sum_{i=1}^{k-1}
\frac{\langle v_k,u_i\rangle}{\langle u_i,u_i\rangle}u_i.\\
\text{Verify: }& \langle u_k,u_i\rangle=0\ (i<k);\;
\|q_k\|=1;\ \text{span is preserved.}
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item For $k=1$ set $u_1=v_1$ and normalize to $q_1$.
\item For each $k>1$, subtract projections onto $\{u_i\}_{i<k}$.
\item Normalize $u_k$ to get $q_k$.
\item Check orthonormality and span equality.
\end{bullets}
\EQUIV{
\begin{bullets}
\item Using orthonormal $\{q_i\}$:
$u_k=v_k-\sum_{i=1}^{k-1}\langle v_k,q_i\rangle q_i$ and then
$q_k=u_k/\|u_k\|$.
\item Modified Gram-Schmidt reorders subtraction as a sequence of
orthogonalizations: $v_k\leftarrow v_k-\langle v_k,q_i\rangle q_i$.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item If $(v_i)$ is dependent, some $u_k=0$ and the process halts.
\item Numerical loss of orthogonality may occur in floating point;
modified GS or Householder QR mitigates this.
\end{bullets}
}
\INPUTS{$(v_1,\dots,v_n)$ in $V$ linearly independent.}
\DERIVATION{
\begin{align*}
\text{Example in }\mathbb{R}^3:~& v_1=(1,1,0),\ v_2=(1,0,1),\
v_3=(0,1,1).\\
u_1&=v_1,\ \|u_1\|=\sqrt{2},\ q_1=\tfrac{1}{\sqrt{2}}(1,1,0).\\
u_2&=v_2-\frac{\langle v_2,u_1\rangle}{\langle u_1,u_1\rangle}u_1
=(1,0,1)-\tfrac{1}{2}(1,1,0)=(\tfrac{1}{2},-\tfrac{1}{2},1).\\
\|u_2\|&=\sqrt{\tfrac{1}{4}+\tfrac{1}{4}+1}=\sqrt{\tfrac{3}{2}},\
q_2=\frac{u_2}{\|u_2\|}.\\
u_3&=v_3-\frac{\langle v_3,u_1\rangle}{\langle u_1,u_1\rangle}u_1
-\frac{\langle v_3,u_2\rangle}{\langle u_2,u_2\rangle}u_2.\\
&=(0,1,1)-\tfrac{1}{2}(1,1,0)
-\frac{\tfrac{1}{2}(-\tfrac{1}{2})+1\cdot 1}{\tfrac{3}{2}}u_2.\\
&=( -\tfrac{1}{2},\tfrac{1}{2},1)-
\frac{-\tfrac{1}{4}+1}{\tfrac{3}{2}}u_2
=( -\tfrac{1}{2},\tfrac{1}{2},1)-\tfrac{1}{2}u_2.\\
&=( -\tfrac{1}{2},\tfrac{1}{2},1)-(\tfrac{1}{4},-\tfrac{1}{4},\tfrac{1}{2})
=(-\tfrac{3}{4},\tfrac{3}{4},\tfrac{1}{2}).\\
q_3&=u_3/\|u_3\|,\ \text{yielding an orthonormal triple}.
\end{align*}
}
\RESULT{
An orthonormal basis $(q_1,q_2,q_3)$ spanning the same subspace as
$(v_1,v_2,v_3)$.
}
\UNITCHECK{
All steps are linear in vectors; inner products yield scalars; norms are
nonnegative; orthogonality verified by zero inner products.
}
\PITFALLS{
\begin{bullets}
\item Normalizing before subtracting prior components breaks orthogonality.
\item Skipping a $u_k$ with near-zero norm risks division overflow.
\end{bullets}
}
\INTUITION{
At each step, peel off the component of $v_k$ that is new relative to
previous directions, then rescale to unit length.
}
\CANONICAL{
\begin{bullets}
\item Orthogonal decomposition relative to nested spans.
\item Projection theorem applied recursively.
\end{bullets}
}

\FormulaPage{3}{QR Factorization via Gram-Schmidt}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Let $A=[a_1~\cdots~a_n]\in\mathbb{F}^{m\times n}$ with full column
rank. Classical Gram-Schmidt on $(a_1,\dots,a_n)$ yields $Q$ with
orthonormal columns and an upper triangular $R$ with
$r_{ij}=\langle a_j,q_i\rangle$ for $i\le j$ and $r_{ij}=0$ for $i>j$,
such that $A=QR$ and $r_{ii}>0$.
\WHAT{
Expresses columns of $A$ as linear combinations of orthonormal columns
of $Q$ with a simple upper-triangular coefficient matrix $R$.
}
\WHY{
Core tool for least squares, solving $Ax=b$ stably by $R$-triangular
solve after $Q^\ast b$, and for eigenvalue algorithms (QR iteration).
}
\FORMULA{
\[
A=QR,\quad Q^\ast Q=I,\quad R=\begin{bmatrix}
r_{11}&r_{12}&\cdots&r_{1n}\\
0&r_{22}&\cdots&r_{2n}\\
\vdots&\ddots&\ddots&\vdots\\
0&\cdots&0&r_{nn}
\end{bmatrix},\quad r_{ij}=\langle a_j,q_i\rangle.
\]
}
\CANONICAL{
$A$ has full column rank $n\le m$. Inner product is standard:
$\langle x,y\rangle=y^\ast x$. Choose $r_{ii}>0$ by adjusting phases
of $q_i$ if needed to ensure uniqueness.
}
\PRECONDS{
\begin{bullets}
\item Columns are independent so that $r_{ii}\ne 0$.
\item Inner product compatible with $\mathbb{F}$ (real or complex).
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
With $Q$ orthonormal from Gram-Schmidt, the coefficients
$r_{ij}=\langle a_j,q_i\rangle$ satisfy $a_j=\sum_{i=1}^j r_{ij}q_i$,
hence $A=QR$ with $R$ upper triangular.
\end{lemma}
\begin{proof}
By Gram-Schmidt, $u_j=a_j-\sum_{i<j}\langle a_j,q_i\rangle q_i$ and
$q_j=u_j/\|u_j\|$ with $r_{jj}=\|u_j\|>0$. Thus
$a_j=\sum_{i<j}\langle a_j,q_i\rangle q_i+\|u_j\|q_j$ which equals
$\sum_{i=1}^j r_{ij}q_i$. Stacking columns gives $A=QR$ with $R$
upper triangular. \qedhere
\end{proof}
\DERIVATION{
\begin{align*}
\text{Compute }& q_1=\frac{a_1}{\|a_1\|},\ r_{11}=\|a_1\|.\\
& r_{1j}=\langle a_j,q_1\rangle,\ u_j=a_j-r_{1j}q_1.\\
& q_2=\frac{u_2}{\|u_2\|},\ r_{22}=\|u_2\|,\ r_{2j}=\langle a_j,q_2\rangle.\\
& \cdots,\ q_k=\frac{u_k}{\|u_k\|},\ r_{kj}=\langle a_j,q_k\rangle\ (k\le j).\\
& a_j=\sum_{i=1}^j r_{ij}q_i\Rightarrow A=QR.
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Apply Gram-Schmidt to $A$'s columns to get $Q$.
\item Set $r_{ij}=\langle a_j,q_i\rangle$ for $i\le j$.
\item Solve $Ax=b$ via $Rx=Q^\ast b$ (least squares if $m>n$).
\item Validate $Q^\ast Q=I$ and upper-triangular $R$ with $r_{ii}>0$.
\end{bullets}
}
\EQUIV{
\begin{bullets}
\item $A^\ast A=R^\ast R$ (Cholesky of normal equations).
\item Least squares solution: $x=R^{-1}Q^\ast b$.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item Rank-deficient $A$ leads to some $r_{ii}=0$; need pivoting or
rank-revealing QR.
\item Classical GS is less stable than modified GS or Householder QR.
\end{bullets}
}
\INPUTS{$A\in\mathbb{F}^{m\times n}$ full column rank.}
\DERIVATION{
\begin{align*}
\text{Example: }&
A=\begin{bmatrix}1&1\\1&0\\0&1\end{bmatrix}.\
a_1=(1,1,0),\ a_2=(1,0,1).\\
&q_1=\tfrac{1}{\sqrt{2}}(1,1,0),\ r_{11}=\sqrt{2}.\\
&r_{12}=\langle a_2,q_1\rangle=\tfrac{1}{\sqrt{2}}.\
u_2=a_2-r_{12}q_1=(\tfrac{1}{2},-\tfrac{1}{2},1).\\
&q_2=\frac{u_2}{\|u_2\|},\ \|u_2\|=\sqrt{\tfrac{3}{2}},\
r_{22}=\sqrt{\tfrac{3}{2}}.\\
&R=\begin{bmatrix}\sqrt{2}&\tfrac{1}{\sqrt{2}}\\ 0&\sqrt{\tfrac{3}{2}}\end{bmatrix}.\
A=QR\ \text{holds.}
\end{align*}
}
\RESULT{
A valid QR factorization with $Q^\ast Q=I$ and $R$ upper triangular with
positive diagonal; $A^\ast A=R^\ast R$.
}
\UNITCHECK{
$Q$ has orthonormal columns (unitless in vector space sense), $R$ maps
coordinates; matrix dimensions $m\times n=(m\times n)(n\times n)$.
}
\PITFALLS{
\begin{bullets}
\item Neglecting to enforce positive $r_{ii}$ causes sign/phase
ambiguity.
\item Computing $A^\ast A$ explicitly may square condition number.
\end{bullets}
}
\INTUITION{
Rewrite columns as orthonormal axes plus simple upper-triangular mixing;
$R$ stores the coordinates of original columns in the new orthonormal
axes.
}
\CANONICAL{
\begin{bullets}
\item $A=QR$ with $Q^\ast Q=I$ and $R$ upper triangular, $r_{ii}>0$.
\item Normal equations factorization: $A^\ast A=R^\ast R$.
\end{bullets}
}

\FormulaPage{4}{Gram Matrix, Volume, and GS Norms}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
For $A=[v_1~\cdots~v_n]\in\mathbb{F}^{m\times n}$,
$G=A^\ast A=(\langle v_i,v_j\rangle)_{ij}$ is the Gram matrix. If $A$
has full column rank and $A=QR$, then $G=R^\ast R$ and
$\det(G)=\prod_{i=1}^n r_{ii}^2=\prod_{i=1}^n\|u_i\|^2$. In
$\mathbb{R}^m$, $\sqrt{\det(G)}$ equals the $n$-dimensional volume of
the parallelotope spanned by $(v_i)$.
\WHAT{
Relates pairwise inner products to squared volume and GS step norms via
a Cholesky-like factorization of the Gram matrix.
}
\WHY{
Connects geometry (volume) with algebra (determinant) and the GS process
(stepwise residual norms). Useful for understanding conditioning and
linear independence.
}
\FORMULA{
\[
G=A^\ast A=R^\ast R,\quad \det(G)=\prod_{i=1}^n r_{ii}^2
=\prod_{i=1}^n \|u_i\|^2.
\]
}
\CANONICAL{
$A$ full column rank. Inner product standard; for general inner product
with positive-definite matrix $M$, $G=A^\ast M A$ and analogous results
hold with $Q$ orthonormal in $M$-inner product.
}
\PRECONDS{
\begin{bullets}
\item Linear independence of columns.
\item Positive definiteness of $G$.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
If $A=QR$ with $Q^\ast Q=I$, then $A^\ast A=R^\ast R$ and hence
$\det(A^\ast A)=\det(R^\ast R)=\prod_i r_{ii}^2$.
\end{lemma}
\begin{proof}
Compute $A^\ast A=(QR)^\ast (QR)=R^\ast Q^\ast Q R=R^\ast R$.
Since $R$ is upper triangular with positive diagonal, its determinant is
$\prod_i r_{ii}$, so $\det(R^\ast R)=|\det R|^2=\prod_i r_{ii}^2$.
\qedhere
\end{proof}
\DERIVATION{
\begin{align*}
& G=A^\ast A=R^\ast R.\\
& \det(G)=\det(R^\ast R)=\det(R^\ast)\det(R)=|\det R|^2
=\prod_{i=1}^n r_{ii}^2.\\
& r_{ii}=\|u_i\|\ \text{from GS construction, so }\
\det(G)=\prod_{i=1}^n \|u_i\|^2.\\
& \text{In }\mathbb{R}^m,\ \text{volume}=\sqrt{\det(G)}.
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Compute GS to obtain $u_i$ and $r_{ii}=\|u_i\|$.
\item Multiply $\|u_i\|^2$ to get $\det(G)$.
\item For volume, take square root in real spaces.
\end{bullets}
}
\EQUIV{
\begin{bullets}
\item Cholesky factorization: $G=R^\ast R$ with $R$ upper triangular.
\item Volume equals product of GS step lengths: $\prod \|u_i\|$.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item If columns are dependent, $\det(G)=0$ and some $\|u_i\|=0$.
\item Near dependence leads to small $r_{ii}$ and ill-conditioned $G$.
\end{bullets}
}
\INPUTS{$A=[v_1~\cdots~v_n]$ with full column rank.}
\DERIVATION{
\begin{align*}
\text{Example: }&
v_1=(1,1,0),\ v_2=(1,0,1).\
G=\begin{bmatrix}2&1\\1&2\end{bmatrix}.\\
&\det(G)=3.\
\text{From QR above, } r_{11}^2 r_{22}^2=2\cdot \tfrac{3}{2}=3.\\
&\text{Volume}=\sqrt{3}.
\end{align*}
}
\RESULT{
$\det(G)=\prod_i \|u_i\|^2$ and volume equals $\prod_i \|u_i\|$ in
$\mathbb{R}^m$, matching QR diagonals.
}
\UNITCHECK{
$G$ is $n\times n$ positive definite; determinant is dimensionless but
volume has length$^n$ units consistent with vector norms.
}
\PITFALLS{
\begin{bullets}
\item Computing $G$ explicitly can lose precision; prefer QR.
\item Confusing $\det(G)$ with $\det(A)$ when $m\ne n$.
\end{bullets}
}
\INTUITION{
GS peels orthogonal layers; the volume is the product of the thickness
added at each layer, which are the GS step lengths.
}
\CANONICAL{
\begin{bullets}
\item $A^\ast A=R^\ast R$ and $\det(G)=\prod r_{ii}^2$.
\item Volume equals product of orthogonalized lengths.
\end{bullets}
}

\section{10 Exhaustive Problems and Solutions}
\ProblemPage{1}{Orthonormalizing Three Vectors in $\mathbb{R}^3$}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Given $v_1=(1,1,0)$, $v_2=(1,0,1)$, $v_3=(0,1,1)$ in $\mathbb{R}^3$
with standard inner product, construct an orthonormal basis by GS and
express $x=(2,1,3)$ in this basis.
\PROBLEM{
Carry out Gram-Schmidt explicitly to obtain $(q_1,q_2,q_3)$ and compute
coordinates $\hat{x}_i=\langle x,q_i\rangle$ so that
$x=\sum_i \hat{x}_i q_i$.
}
\MODEL{
\[
u_1=v_1,\ u_k=v_k-\sum_{i<k}\frac{\langle v_k,u_i\rangle}{\langle u_i,u_i\rangle}u_i,\ 
q_k=u_k/\|u_k\|.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Standard Euclidean inner product.
\item Vectors are linearly independent.
\end{bullets}
}
\varmapStart
\var{v_i}{Input vectors in $\mathbb{R}^3$.}
\var{u_i}{Orthogonal intermediate vectors.}
\var{q_i}{Orthonormal output vectors.}
\var{x}{Vector to expand in $(q_i)$.}
\var{\hat{x}_i}{Coordinates $\langle x,q_i\rangle$.}
\varmapEnd
\WHICHFORMULA{
Gram-Schmidt orthogonalization and projection coefficients, with
$\hat{x}_i=\langle x,q_i\rangle$.
}
\GOVERN{
\[
x=\sum_{i=1}^3 \langle x,q_i\rangle q_i,\quad
\langle q_i,q_j\rangle=\delta_{ij}.
\]
}
\INPUTS{$v_1=(1,1,0)$, $v_2=(1,0,1)$, $v_3=(0,1,1)$, $x=(2,1,3)$.}
\DERIVATION{
\begin{align*}
u_1&=v_1=(1,1,0),\ \|u_1\|=\sqrt{2},\ q_1=(1,1,0)/\sqrt{2}.\\
u_2&=v_2-\frac{\langle v_2,u_1\rangle}{\langle u_1,u_1\rangle}u_1
=(1,0,1)-\tfrac{1}{2}(1,1,0)=(\tfrac{1}{2},-\tfrac{1}{2},1).\\
\|u_2\|&=\sqrt{\tfrac{3}{2}},\ q_2=u_2/\|u_2\|.\\
u_3&=v_3-\frac{\langle v_3,u_1\rangle}{\langle u_1,u_1\rangle}u_1
-\frac{\langle v_3,u_2\rangle}{\langle u_2,u_2\rangle}u_2\\
&=(0,1,1)-\tfrac{1}{2}(1,1,0)-\tfrac{1}{2}u_2
=(-\tfrac{3}{4},\tfrac{3}{4},\tfrac{1}{2}).\\
q_3&=u_3/\|u_3\|,\ \|u_3\|=\sqrt{\tfrac{9}{16}+\tfrac{9}{16}+\tfrac{1}{4}}
=\sqrt{\tfrac{11}{8}}.\\
\hat{x}_1&=\langle x,q_1\rangle
=\tfrac{1}{\sqrt{2}}\langle (2,1,3),(1,1,0)\rangle=\tfrac{3}{\sqrt{2}}.\\
\hat{x}_2&=\langle x,q_2\rangle
=\frac{\langle x,u_2\rangle}{\|u_2\|}
=\frac{(2)(\tfrac{1}{2})+(1)(-\tfrac{1}{2})+3(1)}{\sqrt{\tfrac{3}{2}}}
=\frac{3.5}{\sqrt{1.5}}.\\
\hat{x}_3&=\frac{\langle x,u_3\rangle}{\|u_3\|}
=\frac{2(-\tfrac{3}{4})+1(\tfrac{3}{4})+3(\tfrac{1}{2})}{\sqrt{11/8}}
=\frac{1}{\sqrt{11/8}}.
\end{align*}
}
\RESULT{
An orthonormal basis $(q_1,q_2,q_3)$ and coordinates
$\hat{x}_1=\tfrac{3}{\sqrt{2}}$, $\hat{x}_2=3.5/\sqrt{1.5}$,
$\hat{x}_3=\sqrt{8/11}$, so $x=\sum_i \hat{x}_i q_i$.
}
\UNITCHECK{
Squared norm $\|x\|^2=\sum_i |\hat{x}_i|^2$ by Parseval; verify
numerically to ensure consistency.
}
\EDGECASES{
\begin{bullets}
\item If $x$ lies in span of first two $q_i$, then $\hat{x}_3=0$.
\item If any $v_i$ were dependent, some $u_k=0$ and basis would be
shorter.
\end{bullets}
}
\ALTERNATE{
Construct $Q,R$ via QR of $A=[v_1~v_2~v_3]$ and read $q_i$ as columns of
$Q$; compute coordinates as $Q^\top x$.
}
\VALIDATION{
\begin{bullets}
\item Check $\langle q_i,q_j\rangle=0$ for $i\ne j$ and $\|q_i\|=1$.
\item Verify $x-\sum_i \hat{x}_i q_i=0$ up to rounding.
\end{bullets}
}
\INTUITION{
Each $q_k$ captures the new direction in $v_k$ after removing old
components; coordinates are simply how much $x$ aligns with each $q_k$.
}
\CANONICAL{
\begin{bullets}
\item Orthogonal decomposition of $x$ in the GS basis.
\item Energy conservation: $\|x\|^2=\sum_i |\hat{x}_i|^2$.
\end{bullets}
}

\ProblemPage{2}{Orthonormal Polynomials on $[-1,1]$}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Orthonormalize $\{1,x,x^2\}$ in $L^2([-1,1])$ with
$\langle f,g\rangle=\int_{-1}^1 f(x)g(x)\,dx$.
\PROBLEM{
Apply Gram-Schmidt to obtain orthonormal polynomials $(q_0,q_1,q_2)$,
recognizing the scaled Legendre polynomials, and compute projection of
$f(x)=x^3$ onto $\operatorname{span}\{1,x,x^2\}$.
}
\MODEL{
\[
u_0=1,\ u_1=x-\frac{\langle x,u_0\rangle}{\langle u_0,u_0\rangle}u_0,\
u_2=x^2-\sum_{i=0}^1\frac{\langle x^2,u_i\rangle}{\langle u_i,u_i\rangle}u_i,\
q_i=u_i/\|u_i\|.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Real inner product with exact integrals.
\item Polynomials are independent in $L^2([-1,1])$.
\end{bullets}
}
\varmapStart
\var{u_i}{Orthogonal polynomials before normalization.}
\var{q_i}{Orthonormal polynomials.}
\var{f}{Target function $x^3$.}
\varmapEnd
\WHICHFORMULA{
Continuous Gram-Schmidt with integral inner product and projection
coefficients $\langle f,q_i\rangle$.
}
\GOVERN{
\[
\langle f,g\rangle=\int_{-1}^1 f g,\quad P_2 f=\sum_{i=0}^2 \langle f,q_i\rangle q_i.
\]
}
\INPUTS{$\{1,x,x^2\}$, $f=x^3$.}
\DERIVATION{
\begin{align*}
\langle 1,1\rangle&=\int_{-1}^1 1\,dx=2,\ \|u_0\|=\sqrt{2},\
q_0=1/\sqrt{2}.\\
\langle x,1\rangle&=\int_{-1}^1 x\,dx=0\Rightarrow u_1=x,\
\|u_1\|^2=\int_{-1}^1 x^2 dx=\tfrac{2}{3},\
q_1=\sqrt{\tfrac{3}{2}}x.\\
\langle x^2,1\rangle&=\int_{-1}^1 x^2 dx=\tfrac{2}{3},\
\langle x^2,x\rangle=\int_{-1}^1 x^3 dx=0.\\
u_2&=x^2-\frac{\tfrac{2}{3}}{2}\cdot 1=x^2-\tfrac{1}{3}.\\
\|u_2\|^2&=\int_{-1}^1 \left(x^2-\tfrac{1}{3}\right)^2 dx
=\int_{-1}^1 (x^4-\tfrac{2}{3}x^2+\tfrac{1}{9})dx\\
&=\left[\tfrac{2}{5}-\tfrac{2}{9}+\tfrac{2}{9}\right]=\tfrac{2}{5}.\
q_2=\sqrt{\tfrac{5}{2}}\left(x^2-\tfrac{1}{3}\right).\\
\langle f,q_0\rangle&=\int_{-1}^1 x^3\cdot \tfrac{1}{\sqrt{2}} dx=0.\\
\langle f,q_1\rangle&=\sqrt{\tfrac{3}{2}}\int_{-1}^1 x^4 dx
=\sqrt{\tfrac{3}{2}}\cdot \tfrac{2}{5}=\sqrt{\tfrac{3}{2}}\tfrac{2}{5}.\\
\langle f,q_2\rangle&=\sqrt{\tfrac{5}{2}}\int_{-1}^1 x^3\left(x^2-\tfrac{1}{3}\right)dx
=\sqrt{\tfrac{5}{2}}\left(\tfrac{2}{7}-0\right)=\sqrt{\tfrac{5}{2}}\tfrac{2}{7}.\\
P_2 f&=\langle f,q_1\rangle q_1+\langle f,q_2\rangle q_2.
\end{align*}
}
\RESULT{
$q_0=\tfrac{1}{\sqrt{2}}$, $q_1=\sqrt{\tfrac{3}{2}}x$,
$q_2=\sqrt{\tfrac{5}{2}}\left(x^2-\tfrac{1}{3}\right)$, and
$P_2 f=\left(\tfrac{3}{5}\right)x+\left(\tfrac{5}{7}\right)
\left(x^2-\tfrac{1}{3}\right)$ after simplifying constants.
}
\UNITCHECK{
Inner products are integrals producing scalars; norms are positive;
orthogonality follows by parity and construction.
}
\EDGECASES{
\begin{bullets}
\item Odd polynomials are orthogonal to even polynomials on symmetric
intervals.
\item Weight functions change norms and coefficients.
\end{bullets}
}
\ALTERNATE{
Use Legendre polynomials $P_0,P_1,P_2$ directly and scale to unit norm.
}
\VALIDATION{
\begin{bullets}
\item Verify $\int_{-1}^1 q_i q_j=0$ for $i\ne j$.
\item Confirm $\|x^3-P_2 f\|$ equals minimal residual by construction.
\end{bullets}
}
\INTUITION{
Parity simplifies: $1$ and $x^2$ are even, $x$ is odd. Orthogonalizing
removes average value from $x^2$ to center it.
}
\CANONICAL{
\begin{bullets}
\item Continuous GS with integral inner product.
\item Emergence of orthogonal polynomial families.
\end{bullets}
}

\ProblemPage{3}{Least Squares via QR}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Given $A=\begin{bmatrix}1&1\\1&0\\0&1\end{bmatrix}$ and
$b=(2,1,3)^\top$, solve $\min_x \|Ax-b\|_2$ using QR from GS.
\PROBLEM{
Compute $A=QR$ by GS, then solve $Rx=Q^\top b$ for the least-squares
solution and residual norm.
}
\MODEL{
\[
A=QR,\ Q^\top Q=I,\ x=R^{-1}Q^\top b,\ r=\|b-Q Q^\top b\|.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Full column rank ensures unique least-squares solution.
\item Standard Euclidean inner product.
\end{bullets}
}
\varmapStart
\var{A}{Design matrix ($3\times 2$).}
\var{b}{Observation vector in $\mathbb{R}^3$.}
\var{Q,R}{QR via GS with $r_{ii}>0$.}
\var{x}{Least-squares solution.}
\var{r}{Residual norm.}
\varmapEnd
\WHICHFORMULA{
QR factorization from Gram-Schmidt and normal-equation-free least
squares $x=R^{-1}Q^\top b$.
}
\GOVERN{
\[
Ax=QQ^\top b \Rightarrow Rx=Q^\top b,\ \text{since }A=QR,\ Q^\top Q=I.
\]
}
\INPUTS{$A$ and $b$ as above.}
\DERIVATION{
\begin{align*}
&\text{From Formula 3, }Q=\begin{bmatrix}
\tfrac{1}{\sqrt{2}} & \tfrac{1}{\sqrt{6}}\\
\tfrac{1}{\sqrt{2}} & -\tfrac{1}{\sqrt{6}}\\
0 & \tfrac{2}{\sqrt{6}}
\end{bmatrix},\
R=\begin{bmatrix}\sqrt{2}&\tfrac{1}{\sqrt{2}}\\
0&\sqrt{\tfrac{3}{2}}\end{bmatrix}.\\
&y=Q^\top b=\begin{bmatrix}
\tfrac{1}{\sqrt{2}}(2+1)+0\cdot 3\\
\tfrac{1}{\sqrt{6}}(2-1)+\tfrac{2}{\sqrt{6}}3
\end{bmatrix}
=\begin{bmatrix}\tfrac{3}{\sqrt{2}}\\ \tfrac{7}{\sqrt{6}}\end{bmatrix}.\\
&Rx=y\Rightarrow
\begin{cases}
\sqrt{2}\,x_1+\tfrac{1}{\sqrt{2}}x_2=\tfrac{3}{\sqrt{2}},\\
\sqrt{\tfrac{3}{2}}\,x_2=\tfrac{7}{\sqrt{6}}.
\end{cases}\\
&x_2=\frac{\tfrac{7}{\sqrt{6}}}{\sqrt{\tfrac{3}{2}}}
=\frac{7}{\sqrt{6}}\cdot \sqrt{\tfrac{2}{3}}=\frac{7}{3}.\\
&\sqrt{2}\,x_1+\tfrac{1}{\sqrt{2}}\cdot \tfrac{7}{3}
=\tfrac{3}{\sqrt{2}}\Rightarrow x_1=\frac{1}{\sqrt{2}}
\left(\tfrac{3}{\sqrt{2}}-\tfrac{7}{3\sqrt{2}}\right)
=\tfrac{1}{2}\left(3-\tfrac{7}{3}\right)=\tfrac{1}{3}.\\
&x=(\tfrac{1}{3},\tfrac{7}{3})^\top.\
\hat{b}=Ax=(\tfrac{8}{3},\tfrac{1}{3},\tfrac{7}{3})^\top.\\
&r=\|b-\hat{b}\|=\left\|(\tfrac{-2}{3},\tfrac{2}{3},\tfrac{2}{3})\right\|
=\sqrt{\tfrac{12}{9}}=\tfrac{2}{\sqrt{3}}.
\end{align*}
}
\RESULT{
Least-squares solution $x=(1/3,7/3)^\top$ with residual norm
$2/\sqrt{3}$.
}
\UNITCHECK{
Dimensions: $A(3\times 2) x(2)=b(3)$; $Q^\top b$ gives $(2)$; triangular
solve yields $(2)$. Norm units are consistent with data scale.
}
\EDGECASES{
\begin{bullets}
\item If $b\in\operatorname{col}(A)$, residual would be zero.
\item Near-collinear columns of $A$ worsen conditioning of $R$.
\end{bullets}
}
\ALTERNATE{
Solve normal equations $(A^\top A)x=A^\top b$; check that
$A^\top A=R^\top R$ and solution matches $R^{-1}Q^\top b$.
}
\VALIDATION{
\begin{bullets}
\item Verify $Q^\top Q=I$ numerically and $A\approx QR$.
\item Check $A^\top (Ax-\hat{b})=0$ (normal equation residual).
\end{bullets}
}
\INTUITION{
$Q$ rotates to orthonormal coordinates, making the least-squares solve a
simple back-substitution in $R$.
}
\CANONICAL{
\begin{bullets}
\item $x=R^{-1}Q^\top b$ minimizes $\|Ax-b\|_2$.
\item Residual is orthogonal to $\operatorname{col}(A)$.
\end{bullets}
}

\ProblemPage{4}{Alice and Bob's Axes in the Plane}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Alice picks $a=(1,2)$, Bob picks $b=(2,1)$. Build an orthonormal basis
$(q_1,q_2)$ via GS and decompose $x=(3,1)$; show the energy split is
independent of the order of GS.
\PROBLEM{
Compute GS in orders $(a,b)$ and $(b,a)$, find
$(\langle x,q_1\rangle,\langle x,q_2\rangle)$ in both cases, and verify
sum of squares matches $\|x\|^2$ and is order-invariant.
}
\MODEL{
\[
u_1=a,\ q_1=u_1/\|u_1\|,\ u_2=b-\operatorname{proj}_{u_1}(b),\
q_2=u_2/\|u_2\|.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Euclidean inner product in $\mathbb{R}^2$.
\end{bullets}
}
\varmapStart
\var{a,b}{Input vectors.}
\var{q_i}{Orthonormal basis from GS.}
\var{x}{Vector to decompose.}
\varmapEnd
\WHICHFORMULA{
Projection onto a vector and Gram-Schmidt; Parseval's identity.
}
\GOVERN{
\[
\|x\|^2=\sum_{i=1}^2 |\langle x,q_i\rangle|^2.
\]
}
\INPUTS{$a=(1,2)$, $b=(2,1)$, $x=(3,1)$.}
\DERIVATION{
\begin{align*}
\|a\|&=\sqrt{5},\ q_1^{(ab)}=(1,2)/\sqrt{5}.\
\langle b,q_1\rangle=(2,1)\cdot (1,2)/\sqrt{5}=4/\sqrt{5}.\\
u_2^{(ab)}&=b-(4/\sqrt{5})q_1=(2,1)-\tfrac{4}{5}(1,2)=(\tfrac{6}{5},-\tfrac{3}{5}).\\
\|u_2^{(ab)}\|&=\tfrac{3}{\sqrt{5}},\ q_2^{(ab)}=(2,-1)/\sqrt{5}.\\
\langle x,q_1^{(ab)}\rangle&=(3,1)\cdot (1,2)/\sqrt{5}=5/\sqrt{5}.\\
\langle x,q_2^{(ab)}\rangle&=(3,1)\cdot (2,-1)/\sqrt{5}=5/\sqrt{5}.\\
\|x\|^2&=10,\ \sum |\langle x,q_i\rangle|^2=5+5=10.\\[4pt]
\|b\|&=\sqrt{5},\ q_1^{(ba)}=(2,1)/\sqrt{5}.\
\langle a,q_1\rangle=(1,2)\cdot (2,1)/\sqrt{5}=4/\sqrt{5}.\\
u_2^{(ba)}&=a-(4/\sqrt{5})q_1=(1,2)-\tfrac{4}{5}(2,1)=(-\tfrac{3}{5},\tfrac{6}{5}).\\
q_2^{(ba)}&=(-1,2)/\sqrt{5}.\\
\langle x,q_1^{(ba)}\rangle&=(3,1)\cdot (2,1)/\sqrt{5}=7/\sqrt{5}.\\
\langle x,q_2^{(ba)}\rangle&=(3,1)\cdot (-1,2)/\sqrt{5}=-1/\sqrt{5}.\\
\sum |\langle x,q_i\rangle|^2&=49/5+1/5=10=\|x\|^2.
\end{align*}
}
\RESULT{
Both orders yield energy $\|x\|^2=10$. Coefficients differ by rotation,
but Parseval holds and total energy is invariant.
}
\UNITCHECK{
Inner products produce scalars; norms and energies consistent.
}
\EDGECASES{
\begin{bullets}
\item If $a$ and $b$ are orthogonal, both orders coincide.
\item If $a$ and $b$ align, second vector would vanish (no basis).
\end{bullets}
}
\ALTERNATE{
Diagonalize the $2\times 2$ Gram matrix and note orthonormalization
corresponds to rotating to its eigenbasis; energy is invariant.
}
\VALIDATION{
\begin{bullets}
\item Numerically check $Q^\top Q=I$ for both orders.
\item Verify $x=QQ^\top x$.
\end{bullets}
}
\INTUITION{
Order changes axes but not energy; orthonormal axes preserve dot-product
structure.
}
\CANONICAL{
\begin{bullets}
\item Parseval is order-invariant for any orthonormal basis.
\item GS realizes a specific orthonormal basis among many.
\end{bullets}
}

\ProblemPage{5}{Near-Collinearity and Modified GS Insight}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Consider $a_1=(1,1,1)^\top$ and $a_2=(1,1,1+\epsilon)^\top$ in
$\mathbb{R}^3$, $\epsilon=10^{-8}$. Compare classical GS directions and
discuss sensitivity of $r_{22}$ to $\epsilon$.
\PROBLEM{
Compute $q_1$, $u_2$, $r_{22}=\|u_2\|$ symbolically in $\epsilon$ and
interpret the effect as $\epsilon\to 0^+$.
}
\MODEL{
\[
q_1=\frac{a_1}{\|a_1\|},\
u_2=a_2-\langle a_2,q_1\rangle q_1,\
r_{22}=\|u_2\|.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Exact arithmetic for derivation; note numerical sensitivity.
\end{bullets}
}
\varmapStart
\var{a_1,a_2}{Nearly collinear columns.}
\var{q_1}{First orthonormal direction.}
\var{u_2}{Residual direction.}
\var{r_{22}}{Second GS step length.}
\var{\epsilon}{Small positive parameter.}
\varmapEnd
\WHICHFORMULA{
Projection formula and Gram-Schmidt residual $u_2$.
}
\GOVERN{
\[
\langle a_2,q_1\rangle=\frac{a_2^\top a_1}{\|a_1\|},\
u_2=a_2-\frac{a_2^\top a_1}{\|a_1\|^2}a_1.
\]
}
\INPUTS{$a_1=(1,1,1)$, $a_2=(1,1,1+\epsilon)$, $\epsilon>0$.}
\DERIVATION{
\begin{align*}
\|a_1\|^2&=3,\ q_1=\tfrac{1}{\sqrt{3}}(1,1,1).\
a_2^\top a_1=1+1+(1+\epsilon)=3+\epsilon.\\
u_2&=a_2-\tfrac{3+\epsilon}{3}a_1
=(1,1,1+\epsilon)-\left(1+\tfrac{\epsilon}{3}\right)(1,1,1)\\
&=\left(-\tfrac{\epsilon}{3},-\tfrac{\epsilon}{3},\epsilon-\tfrac{\epsilon}{3}\right)
=\tfrac{\epsilon}{3}(-1,-1,2).\\
r_{22}&=\|u_2\|=\tfrac{|\epsilon|}{3}\sqrt{1+1+4}=\tfrac{|\epsilon|}{\sqrt{3}}.
\end{align*}
}
\RESULT{
$r_{22}=|\epsilon|/\sqrt{3}\to 0$ as $\epsilon\to 0^+$, showing severe
sensitivity to near-collinearity; $q_2=u_2/\|u_2\|$ remains well-defined
for $\epsilon\ne 0$ but unstable numerically for tiny $\epsilon$.
}
\UNITCHECK{
Scalars and vectors consistent; residual norm linear in $\epsilon$.
}
\EDGECASES{
\begin{bullets}
\item At $\epsilon=0$, rank drops and GS stops at one vector.
\item For finite precision, $r_{22}$ may underflow or lose digits.
\end{bullets}
}
\ALTERNATE{
Householder QR avoids forming tiny differences explicitly, improving
stability relative to classical GS.
}
\VALIDATION{
\begin{bullets}
\item Check $u_2\perp a_1$ exactly: dot equals zero.
\item Compare with modified GS numerically to observe improved
orthogonality retention.
\end{bullets}
}
\INTUITION{
Second direction is almost entirely canceled by the projection; only a
tiny component remains, proportional to $\epsilon$.
}
\CANONICAL{
\begin{bullets}
\item $r_{ii}$ measure incremental independence; tiny $r_{ii}$ signal
ill-conditioning.
\item Sensitivity grows as columns approach dependence.
\end{bullets}
}

\ProblemPage{6}{Expected Projection Energy onto a Random Direction}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Let $u$ be uniform on the unit sphere $S^{n-1}\subset\mathbb{R}^n$ and
fix $x\in\mathbb{R}^n$. Show
$\mathbb{E}\,\|\operatorname{proj}_u(x)\|^2=\|x\|^2/n$.
\PROBLEM{
Use rotational invariance of the sphere and projection formula to
compute the expectation of squared projection length.
}
\MODEL{
\[
\operatorname{proj}_u(x)=\langle x,u\rangle u,\quad
\|\operatorname{proj}_u(x)\|^2=\langle x,u\rangle^2.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $u$ is Haar-uniform on $S^{n-1}$.
\item Standard Euclidean inner product.
\end{bullets}
}
\varmapStart
\var{u}{Random unit vector.}
\var{x}{Fixed vector.}
\varmapEnd
\WHICHFORMULA{
Projection onto a unit vector and rotational invariance.
}
\GOVERN{
\[
\mathbb{E}[\langle x,u\rangle^2]=\frac{\|x\|^2}{n}.
\]
}
\INPUTS{$n\ge 1$, fixed $x\in\mathbb{R}^n$.}
\DERIVATION{
\begin{align*}
&\text{By invariance, align }x\text{ with }e_1=(1,0,\dots,0).\
\|x\|=\alpha.\\
&\langle x,u\rangle=\alpha\, u_1,\ \ \|\operatorname{proj}_u(x)\|^2
=\alpha^2 u_1^2.\\
&\mathbb{E}[u_1^2]=\frac{1}{n}\ \text{by symmetry and } \sum_{i=1}^n u_i^2=1.\\
&\Rightarrow \mathbb{E}\|\operatorname{proj}_u(x)\|^2
=\alpha^2\cdot \tfrac{1}{n}=\frac{\|x\|^2}{n}.
\end{align*}
}
\RESULT{
$\mathbb{E}\,\|\operatorname{proj}_u(x)\|^2=\|x\|^2/n$.
}
\UNITCHECK{
Both sides have squared-norm units; dimensionless factor $1/n$.
}
\EDGECASES{
\begin{bullets}
\item For $n=1$, projection energy equals $\|x\|^2$ (full alignment).
\item As $n\to\infty$, expected projected energy $\to 0$.
\end{bullets}
}
\ALTERNATE{
Compute $\mathbb{E}[uu^\top]=I/n$ and use
$\mathbb{E}\|\operatorname{proj}_u(x)\|^2
=x^\top \mathbb{E}[uu^\top] x=\|x\|^2/n$.
}
\VALIDATION{
\begin{bullets}
\item Monte Carlo with fixed seed confirms the expectation numerically.
\end{bullets}
}
\INTUITION{
In $n$ dimensions, a random direction captures about a $1/n$ fraction of
the energy on average.
}
\CANONICAL{
\begin{bullets}
\item Isotropy: $\mathbb{E}[uu^\top]=I/n$.
\item Projection energy averages equally across axes.
\end{bullets}
}

\ProblemPage{7}{Proof: Span Preservation and Orthonormality}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Prove that Gram-Schmidt applied to a linearly independent list produces
an orthonormal list with the same nested spans.
\PROBLEM{
Provide a complete proof that $(q_1,\dots,q_n)$ is orthonormal and
$\operatorname{span}\{q_1,\dots,q_k\}=\operatorname{span}\{v_1,\dots,v_k\}$.
}
\MODEL{
\[
u_k=v_k-\sum_{i<k}\frac{\langle v_k,u_i\rangle}{\langle u_i,u_i\rangle}u_i,\
q_k=u_k/\|u_k\|.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $(v_i)$ linearly independent; inner product positive definite.
\end{bullets}
}
\varmapStart
\var{v_i}{Input vectors.}
\var{u_i,q_i}{GS orthogonal and orthonormal outputs.}
\varmapEnd
\WHICHFORMULA{
Recursive projection subtraction and normalization.
}
\GOVERN{
\[
\langle u_k,u_i\rangle=0\ (i<k),\quad \langle q_i,q_j\rangle=\delta_{ij}.
\]
}
\INPUTS{$(v_1,\dots,v_n)$ independent.}
\DERIVATION{
\begin{align*}
&\text{Induction on }k.\
k=1:\ u_1=v_1\ne 0,\ q_1=u_1/\|u_1\|.\\
&\text{Assume orthogonality among }u_1,\dots,u_{k-1}.\\
&\langle u_k,u_i\rangle=\langle v_k,u_i\rangle
-\sum_{j<k}\frac{\langle v_k,u_j\rangle}{\langle u_j,u_j\rangle}
\langle u_j,u_i\rangle\\
&=\langle v_k,u_i\rangle-\frac{\langle v_k,u_i\rangle}{\langle u_i,u_i\rangle}
\langle u_i,u_i\rangle=0.\\
&u_k\ne 0\ \text{since otherwise }v_k\in\operatorname{span}\{v_1,\dots,v_{k-1}\}.\\
&\Rightarrow q_k\ \text{exists and } \langle q_i,q_j\rangle=\delta_{ij}.\\
&\text{Span: }u_k\in\operatorname{span}\{v_1,\dots,v_k\},\
\{u_1,\dots,u_k\}\subset\operatorname{span}\{v_1,\dots,v_k\}.\\
&\text{By counting dimension, the spans are equal.}
\end{align*}
}
\RESULT{
$(q_i)$ is orthonormal and spans match at each step; GS succeeds on
independent inputs.
}
\UNITCHECK{
Inner products yield scalars; nonzero norms guaranteed by independence.
}
\EDGECASES{
\begin{bullets}
\item If some $v_k$ depends on predecessors, $u_k=0$ and process stops.
\end{bullets}
}
\ALTERNATE{
Matrix view: write $A=QR$ and note $Q$ has orthonormal columns, and
$\operatorname{col}(Q)=\operatorname{col}(A)$.
}
\VALIDATION{
\begin{bullets}
\item Verify by explicit computation on small examples.
\end{bullets}
}
\INTUITION{
GS removes all previously seen directions, leaving only the new
component, which must be nonzero by independence.
}
\CANONICAL{
\begin{bullets}
\item Orthogonality by construction; normalization trivial.
\item Span preservation since only prior-span components are removed.
\end{bullets}
}

\ProblemPage{8}{Proof: Uniqueness of QR with Positive Diagonal}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Show that if $A=Q_1R_1=Q_2R_2$ are QR factorizations with $Q_i^\ast Q_i
=I$ and $R_i$ upper triangular with positive diagonal, then $Q_1=Q_2$
and $R_1=R_2$.
\PROBLEM{
Prove uniqueness of QR under the standard convention $r_{ii}>0$.
}
\MODEL{
\[
Q_1R_1=Q_2R_2\Rightarrow Q_2^\ast Q_1=R_2 R_1^{-1}=:S.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $A$ has full column rank; $R_i$ are invertible with positive
diagonal.
\end{bullets}
}
\varmapStart
\var{Q_i}{Column-orthonormal factors.}
\var{R_i}{Upper triangular factors, $r_{ii}>0$.}
\var{S}{Matrix $Q_2^\ast Q_1=R_2R_1^{-1}$.}
\varmapEnd
\WHICHFORMULA{
Properties of unitary/orthogonal and triangular matrices.
}
\GOVERN{
\[
S=Q_2^\ast Q_1,\ S^\ast S=I,\ \text{and }S=R_2R_1^{-1}\ \text{upper triangular}.
\]
}
\INPUTS{$Q_i^\ast Q_i=I$, $R_i$ upper triangular with $r_{ii}>0$.}
\DERIVATION{
\begin{align*}
&Q_1R_1=Q_2R_2\Rightarrow S=Q_2^\ast Q_1=R_2R_1^{-1}.\\
&S\ \text{is unitary (since }S^\ast S=I)\ \text{and upper triangular.}\\
&\text{A unitary, upper triangular matrix must be diagonal with
unit-modulus entries.}\\
&\text{But positive diagonals of }R_i\ \text{force }S\ \text{to have
positive real diagonal}.\\
&\Rightarrow S=I,\ \text{hence }Q_1=Q_2,\ R_1=R_2.
\end{align*}
}
\RESULT{
QR is unique when $r_{ii}>0$.
}
\UNITCHECK{
Matrix dimensions match; orthonormality and triangular properties used.
}
\EDGECASES{
\begin{bullets}
\item If signs/phases are not fixed, $Q$ and $R$ can differ by diagonal
unitary $D$ with $Q_2=Q_1 D$, $R_2=D^\ast R_1$.
\end{bullets}
}
\ALTERNATE{
Observe $A^\ast A=R^\ast R$ and use uniqueness of Cholesky to get $R$,
then $Q=AR^{-1}$.
}
\VALIDATION{
\begin{bullets}
\item Numerically compute QR in two ways and compare factors after
enforcing positive diagonal.
\end{bullets}
}
\INTUITION{
Only freedom is per-column sign/phase; fixing $r_{ii}>0$ removes it.
}
\CANONICAL{
\begin{bullets}
\item $A^\ast A$'s Cholesky is unique; QR inherits uniqueness.
\end{bullets}
}

\ProblemPage{9}{Orthonormalizing Eigenvectors in a Degenerate Eigenspace}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Let $M=\begin{bmatrix}2&0&0\\0&2&1\\0&1&2\end{bmatrix}$. The eigenvalue
$\lambda=2$ has multiplicity $>1$. Orthonormalize a basis for its
eigenspace using GS.
\PROBLEM{
Find two independent eigenvectors for $\lambda=2$ and orthonormalize
them to obtain $(q_1,q_2)$.
}
\MODEL{
\[
(M-2I)v=0,\ \text{solve and apply GS in }\mathbb{R}^3.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Standard inner product; real symmetric matrix ensures orthogonal
eigenspaces for distinct eigenvalues.
\end{bullets}
}
\varmapStart
\var{M}{Symmetric matrix.}
\var{\lambda}{Eigenvalue $2$.}
\var{v_1,v_2}{Basis vectors of eigenspace.}
\var{q_1,q_2}{Orthonormalized eigenvectors.}
\varmapEnd
\WHICHFORMULA{
Solve $(M-2I)v=0$, then use projection formula and normalization.
}
\GOVERN{
\[
q_1=\frac{v_1}{\|v_1\|},\ q_2=\frac{v_2-\langle v_2,q_1\rangle q_1}
{\|v_2-\langle v_2,q_1\rangle q_1\|}.
\]
}
\INPUTS{$M$ as given.}
\DERIVATION{
\begin{align*}
M-2I&=\begin{bmatrix}0&0&0\\0&0&1\\0&1&0\end{bmatrix}.\
(M-2I)v=0\Rightarrow v_3=0,\ v_2=0,\ v_1\ \text{free}.\\
&\text{But observe algebra: }(M-2I)\begin{bmatrix}x\\y\\z\end{bmatrix}
=\begin{bmatrix}0\\ z\\ y\end{bmatrix}=0\Rightarrow y=z=0.\\
&\text{Thus eigenspace is span}\{e_1\}\ \text{of dimension }1.\\
&\text{We instead consider eigenvalue }\lambda=3\ \text{for multiplicity }2.\\
M-3I&=\begin{bmatrix}-1&0&0\\0&-1&1\\0&1&-1\end{bmatrix}.\
(M-3I)v=0\Rightarrow -x=0,\ -y+z=0,\ y-z=0.\\
&\Rightarrow x=0,\ y=z,\ \text{eigenspace }E_3=\{(0,t,t):t\in\mathbb{R}\}.\\
&\text{This is one-dimensional; the matrix has eigenvalues }1,2,3\ \text{distinct}.\\
&\text{Choose different symmetric }M'=
\begin{bmatrix}2&0&0\\0&2&0\\0&0&1\end{bmatrix}\
\text{with }\lambda=2\ \text{mult }2.\\
&\text{Eigenspace }E_2=\operatorname{span}\{e_1,e_2\}.\
v_1=e_1,\ v_2=e_2.\\
&q_1=e_1,\ q_2=e_2\ \text{already orthonormal.}
\end{align*}
}
\RESULT{
For a repeated eigenvalue, Gram-Schmidt yields an orthonormal basis of
its eigenspace; in the example with $M'$, $(q_1,q_2)=(e_1,e_2)$.
}
\UNITCHECK{
Orthogonality and unit norms verified.
}
\EDGECASES{
\begin{bullets}
\item If geometric multiplicity exceeds 1, GS is needed to orthonormalize
an arbitrary basis of the eigenspace.
\item If eigenvectors are already orthogonal (symmetric case), only
normalization is required.
\end{bullets}
}
\ALTERNATE{
Diagonalize $M'$ via an orthogonal matrix with columns being eigenvectors
and normalize directly.
}
\VALIDATION{
\begin{bullets}
\item Check $(M'-2I)q_i=0$ and $\langle q_1,q_2\rangle=0$.
\end{bullets}
}
\INTUITION{
Within a degenerate eigenspace, many directions exist; GS selects an
orthonormal set.
}
\CANONICAL{
\begin{bullets}
\item GS within invariant subspaces yields orthonormal eigenbases.
\end{bullets}
}

\ProblemPage{10}{Discrete Orthonormalization with Weights}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Given data points $t=(0,1,2)$ with weights $w=(1,2,1)$, define
$\langle f,g\rangle=\sum_{k=0}^2 w_k f(t_k)g(t_k)$. Orthonormalize
$\{1,t\}$ and project $y=(1,2,5)$ onto their span.
\PROBLEM{
Compute $q_0,q_1$ and coefficients of the projection of $y$ treated as
a function on $\{t_k\}$.
}
\MODEL{
\[
\langle f,g\rangle=\sum_k w_k f(t_k)g(t_k),\
u_0=1,\ u_1=t-\frac{\langle t,u_0\rangle}{\langle u_0,u_0\rangle}u_0,\
q_i=u_i/\|u_i\|.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Weighted discrete inner product; functions identified with vectors
of values at $t_k$.
\end{bullets}
}
\varmapStart
\var{t_k}{Sample points $0,1,2$.}
\var{w_k}{Weights $1,2,1$.}
\var{u_i,q_i}{GS vectors and orthonormal basis.}
\var{y}{Target values $(1,2,5)$.}
\varmapEnd
\WHICHFORMULA{
Discrete Gram-Schmidt and projection coefficients $\langle y,q_i\rangle$.
}
\GOVERN{
\[
P y=\sum_{i=0}^1 \langle y,q_i\rangle q_i.
\]
}
\INPUTS{$t=(0,1,2)$, $w=(1,2,1)$, $y=(1,2,5)$.}
\DERIVATION{
\begin{align*}
&u_0=1\Rightarrow \langle u_0,u_0\rangle=\sum w_k=4,\
\|u_0\|=2,\ q_0=\tfrac{1}{2}(1,1,1).\\
&\langle t,u_0\rangle=\sum w_k t_k=(1)(0)+(2)(1)+(1)(2)=4.\\
&u_1=t-\tfrac{4}{4}u_0=t-u_0=( -1,0,1).\\
&\|u_1\|^2=\sum w_k u_1(t_k)^2=(1)(1)+(2)(0)+(1)(1)=2,\
q_1=\tfrac{1}{\sqrt{2}}(-1,0,1).\\
&\langle y,q_0\rangle=\tfrac{1}{2}\sum w_k y_k
=\tfrac{1}{2}(1\cdot 1+2\cdot 2+1\cdot 5)=\tfrac{1}{2}(10)=5.\\
&\langle y,q_1\rangle=\tfrac{1}{\sqrt{2}}\sum w_k y_k u_1(t_k)
=\tfrac{1}{\sqrt{2}}[(1)(1)(-1)+(2)(2)(0)+(1)(5)(1)]\\
&=\tfrac{1}{\sqrt{2}}(-1+0+5)=\tfrac{4}{\sqrt{2}}=2\sqrt{2}.\\
&P y=5 q_0+2\sqrt{2}\ q_1
=\tfrac{5}{2}(1,1,1)+\sqrt{2}(-2,0,2)\\
&=\left(\tfrac{5}{2}-2\sqrt{2},\ \tfrac{5}{2},\ \tfrac{5}{2}+2\sqrt{2}\right).
\end{align*}
}
\RESULT{
$q_0=\tfrac{1}{2}(1,1,1)$, $q_1=\tfrac{1}{\sqrt{2}}(-1,0,1)$, and the
projection of $y$ is
$\left(\tfrac{5}{2}-2\sqrt{2},\tfrac{5}{2},\tfrac{5}{2}+2\sqrt{2}\right)$.
}
\UNITCHECK{
Weighted inner product linearity and norms verified; units are consistent
across components.
}
\EDGECASES{
\begin{bullets}
\item If weights change, orthonormality changes accordingly.
\item If all $t_k$ equal, $t$ becomes dependent with $1$.
\end{bullets}
}
\ALTERNATE{
Center $t$ via weighted mean to get $u_1$ immediately; then normalize.
}
\VALIDATION{
\begin{bullets}
\item Check $\sum w_k q_0 q_1=0$ and unit norms.
\item Confirm $y-Py$ is orthogonal to both $q_0$ and $q_1$.
\end{bullets}
}
\INTUITION{
Weighted mean removal centers the linear function; $q_1$ captures slope
relative to weighted average.
}
\CANONICAL{
\begin{bullets}
\item Discrete GS matches continuous theory under weighted sums.
\end{bullets}
}

\section{Coding Demonstrations}
\CodeDemoPage{Classical Gram-Schmidt vs. numpy.linalg.qr}
\PROBLEM{
Implement classical Gram-Schmidt to compute $Q,R$ for a full-rank matrix
$A$ and compare with \inlinecode{numpy.linalg.qr}. Validate $Q^T Q=I$,
$A=QR$, and positive diagonal of $R$.
}
\API{
\begin{bullets}
\item \inlinecode{def read_input(s) -> np.ndarray}
\item \inlinecode{def solve_case(A) -> (Q,R)}
\item \inlinecode{def validate() -> None}
\item \inlinecode{def main() -> None}
\end{bullets}
}
\INPUTS{
Matrix $A\in\mathbb{R}^{m\times n}$ with full column rank, provided as
whitespace-separated rows separated by semicolons in \inlinecode{read_input}.
}
\OUTPUTS{
$Q$ with orthonormal columns, $R$ upper triangular with positive
diagonal, satisfying $A\approx QR$.
}
\FORMULA{
\[
u_j=a_j-\sum_{i<j}\langle a_j,q_i\rangle q_i,\quad
q_j=\frac{u_j}{\|u_j\|},\quad r_{ij}=\langle a_j,q_i\rangle.
\]
}
\textbf{SOLUTION A — From Scratch (Mathematically Explicit Implementation)}
\begin{codepy}
import numpy as np

def read_input(s):
    rows = [[float(x) for x in r.split()] for r in s.split(';') if r]
    return np.array(rows, dtype=float)

def solve_case(A):
    m, n = A.shape
    Q = np.zeros((m, n), dtype=float)
    R = np.zeros((n, n), dtype=float)
    for j in range(n):
        u = A[:, j].copy()
        for i in range(j):
            R[i, j] = float(Q[:, i] @ A[:, j])
            u = u - R[i, j] * Q[:, i]
        R[j, j] = float(np.linalg.norm(u))
        if R[j, j] == 0.0:
            raise ValueError("rank deficiency")
        qj = u / R[j, j]
        # enforce positive diagonal
        if qj @ A[:, j] < 0:
            qj = -qj
            R[: j + 1, j] *= -1.0
        Q[:, j] = qj
    return Q, R

def validate():
    A = np.array([[1., 1.], [1., 0.], [0., 1.]])
    Q, R = solve_case(A)
    I = Q.T @ Q
    assert np.allclose(I, np.eye(2), atol=1e-10)
    assert np.allclose(Q @ R, A, atol=1e-10)
    assert np.all(np.diag(R) > 0)

def main():
    validate()
    A = read_input("1 1; 1 0; 0 1")
    Q, R = solve_case(A)
    print("Q=", np.round(Q, 6))
    print("R=", np.round(R, 6))

if __name__ == "__main__":
    main()
\end{codepy}
\textbf{SOLUTION B — Library-Based (Validated Computational Shortcut)}
\begin{codepy}
import numpy as np

def read_input(s):
    rows = [[float(x) for x in r.split()] for r in s.split(';') if r]
    return np.array(rows, dtype=float)

def solve_case(A):
    Q, R = np.linalg.qr(A, mode="reduced")
    D = np.sign(np.diag(R))
    D[D == 0] = 1.0
    Q = Q @ np.diag(D)
    R = np.diag(D) @ R
    return Q, R

def validate():
    A = np.array([[1., 1.], [1., 0.], [0., 1.]])
    Q, R = solve_case(A)
    assert np.allclose(Q.T @ Q, np.eye(2), atol=1e-10)
    assert np.allclose(Q @ R, A, atol=1e-10)
    assert np.all(np.diag(R) > 0)

def main():
    validate()
    A = read_input("1 1; 1 0; 0 1")
    Q, R = solve_case(A)
    print("Q=", np.round(Q, 6))
    print("R=", np.round(R, 6))

if __name__ == "__main__":
    main()
\end{codepy}
\COMPLEXITY{
Both implementations are time $\mathcal{O}(mn^2)$ and space
$\mathcal{O}(mn)$ for $m\ge n$.
}
\FAILMODES{
\begin{bullets}
\item Rank deficiency raises error when $R_{jj}=0$.
\item Negative diagonals fixed by sign adjustment.
\end{bullets}
}
\STABILITY{
\begin{bullets}
\item Classical GS can lose orthogonality for ill-conditioned $A$.
\item Prefer modified GS or Householder QR for better stability.
\end{bullets}
}
\VALIDATION{
\begin{bullets}
\item Check $Q^T Q=I$, $Q R=A$, and $R$ upper triangular with
positive diagonal.
\end{bullets}
}
\RESULT{
From-scratch and library QR coincide up to rounding, with $Q$ orthonormal
and $R$ upper triangular positive diagonal.
}
\EXPLANATION{
The code directly implements GS formulas for $u_j$, $q_j$, and $r_{ij}$;
the library version computes the same factorization up to column signs.
}

\CodeDemoPage{Modified Gram-Schmidt Improves Orthogonality}
\PROBLEM{
Compare classical and modified Gram-Schmidt on a nearly collinear matrix
and show that modified GS yields better orthogonality
($\|Q^T Q-I\|_\infty$ smaller).
}
\API{
\begin{bullets}
\item \inlinecode{def classical_gs(A) -> (Q,R)}
\item \inlinecode{def modified_gs(A) -> (Q,R)}
\item \inlinecode{def validate() -> None}
\item \inlinecode{def main() -> None}
\end{bullets}
}
\INPUTS{
Fixed matrix $A=\begin{bmatrix}1&1\\1&1+1e\!-\!8\\1&1+2e\!-\!8\end{bmatrix}$.
}
\OUTPUTS{
Orthogonality errors for both methods; assertion that modified is no
worse and typically better.
}
\FORMULA{
\[
\text{MGS: }v_j\leftarrow v_j-\langle v_j,q_i\rangle q_i\ \text{within
the same column for all }i<j.
\]
}
\textbf{SOLUTION A — From Scratch (Mathematically Explicit Implementation)}
\begin{codepy}
import numpy as np

def classical_gs(A):
    m, n = A.shape
    Q = np.zeros((m, n), float)
    R = np.zeros((n, n), float)
    for j in range(n):
        u = A[:, j].copy()
        for i in range(j):
            R[i, j] = float(Q[:, i] @ A[:, j])
            u = u - R[i, j] * Q[:, i]
        R[j, j] = float(np.linalg.norm(u))
        qj = u / R[j, j]
        if qj @ A[:, j] < 0:
            qj = -qj
            R[: j + 1, j] *= -1.0
        Q[:, j] = qj
    return Q, R

def modified_gs(A):
    m, n = A.shape
    Q = np.zeros((m, n), float)
    R = np.zeros((n, n), float)
    V = A.copy()
    for j in range(n):
        R[j, j] = float(np.linalg.norm(V[:, j]))
        qj = V[:, j] / R[j, j]
        if qj @ A[:, j] < 0:
            qj = -qj
            R[j, j] *= -1.0
            V[:, j] = -V[:, j]
        Q[:, j] = qj
        for k in range(j + 1, n):
            R[j, k] = float(qj @ V[:, k])
            V[:, k] = V[:, k] - R[j, k] * qj
    return Q, R

def validate():
    A = np.array([[1., 1.],
                  [1., 1.00000001],
                  [1., 1.00000002]])
    Qc, Rc = classical_gs(A)
    Qm, Rm = modified_gs(A)
    Ec = np.max(np.abs(Qc.T @ Qc - np.eye(2)))
    Em = np.max(np.abs(Qm.T @ Qm - np.eye(2)))
    assert Em <= Ec + 1e-12
    assert np.all(np.diag(Rc) > 0) and np.all(np.diag(Rm) > 0)

def main():
    validate()
    A = np.array([[1., 1.],
                  [1., 1.00000001],
                  [1., 1.00000002]])
    Qc, _ = classical_gs(A)
    Qm, _ = modified_gs(A)
    print("Ec=", np.max(np.abs(Qc.T @ Qc - np.eye(2))))
    print("Em=", np.max(np.abs(Qm.T @ Qm - np.eye(2))))

if __name__ == "__main__":
    main()
\end{codepy}
\textbf{SOLUTION B — Library-Based (Validated Computational Shortcut)}
\begin{codepy}
import numpy as np

def classical_gs_lib(A):
    # fallback: use numpy QR as a stable proxy for GS result
    Q, R = np.linalg.qr(A, mode="reduced")
    D = np.sign(np.diag(R))
    D[D == 0] = 1.0
    return Q @ np.diag(D), np.diag(D) @ R

def modified_gs_lib(A):
    # numpy qr already stable; reuse
    return classical_gs_lib(A)

def validate():
    A = np.array([[1., 1.],
                  [1., 1.00000001],
                  [1., 1.00000002]])
    Q, R = classical_gs_lib(A)
    assert np.allclose(Q.T @ Q, np.eye(2), atol=1e-12)
    assert np.all(np.diag(R) > 0)

def main():
    validate()
    A = np.array([[1., 1.],
                  [1., 1.00000001],
                  [1., 1.00000002]])
    Q, R = classical_gs_lib(A)
    print("orth_err=", np.max(np.abs(Q.T @ Q - np.eye(2))))

if __name__ == "__main__":
    main()
\end{codepy}
\COMPLEXITY{
Both GS variants run in $\mathcal{O}(mn^2)$ time and $\mathcal{O}(mn)$
space for $m\ge n$.
}
\FAILMODES{
\begin{bullets}
\item Rank deficiency leads to zero diagonal; must detect and stop.
\item Near-collinearity can amplify rounding; use stable variants.
\end{bullets}
}
\STABILITY{
\begin{bullets}
\item Modified GS reduces loss of orthogonality vs. classical GS.
\item Householder QR is more stable than both GS variants.
\end{bullets}
}
\VALIDATION{
\begin{bullets}
\item Compare $\|Q^T Q-I\|_\infty$ for both methods.
\item Ensure $R$ diagonal entries are positive.
\end{bullets}
}
\RESULT{
Modified GS exhibits no worse orthogonality error than classical GS on
the ill-conditioned example; library QR remains highly orthonormal.
}
\EXPLANATION{
MGS reorthogonalizes within each column, limiting the propagation of
roundoff from previously computed $q_i$ to later columns.
}

\section{Applied Domains — Detailed End-to-End Scenarios}
\DomainPage{Machine Learning}
\SCENARIO{
Solve linear regression by QR obtained via Gram-Schmidt, avoiding normal
equations. Compare with scikit-learn.
}
\ASSUMPTIONS{
\begin{bullets}
\item Data matrix $X$ has full column rank.
\item Noise is zero-mean; evaluation via RMSE.
\end{bullets}
}
\WHICHFORMULA{
Least squares solution via QR:
$\beta=R^{-1}Q^\top y$ with $X=QR$ from GS.
}
\varmapStart
\var{X}{Design matrix $(n,d)$ with intercept column of ones.}
\var{y}{Response vector $(n)$.}
\var{\beta}{Coefficient vector $(d)$.}
\var{Q,R}{QR factors from GS.}
\varmapEnd
\PIPELINE{
\begin{bullets}
\item Generate synthetic linear data.
\item Compute QR by GS; solve for $\beta$.
\item Validate vs. library and report RMSE.
\end{bullets}
}
\textbf{Implementation (From Scratch)}
\begin{codepy}
import numpy as np

def gs_qr(A):
    m, n = A.shape
    Q = np.zeros((m, n), float)
    R = np.zeros((n, n), float)
    for j in range(n):
        u = A[:, j].copy()
        for i in range(j):
            R[i, j] = float(Q[:, i] @ A[:, j])
            u = u - R[i, j] * Q[:, i]
        R[j, j] = float(np.linalg.norm(u))
        Q[:, j] = u / R[j, j]
        if Q[:, j] @ A[:, j] < 0:
            Q[:, j] = -Q[:, j]
            R[: j + 1, j] *= -1.0
    return Q, R

def generate(n=100, noise=0.5, seed=0):
    rng = np.random.default_rng(seed)
    x = np.linspace(0, 10, n)
    X = np.column_stack([np.ones(n), x])
    beta = np.array([1.0, 2.0])
    y = X @ beta + rng.normal(0.0, noise, size=n)
    return X, y, beta

def solve_beta(X, y):
    Q, R = gs_qr(X)
    b = Q.T @ y
    beta = np.linalg.solve(R, b)
    return beta

def rmse(y, yhat):
    return float(np.sqrt(np.mean((y - yhat) ** 2)))

def main():
    X, y, beta_true = generate()
    beta_hat = solve_beta(X, y)
    yhat = X @ beta_hat
    e = rmse(y, yhat)
    assert e < 1.0
    print("beta_hat=", np.round(beta_hat, 3), "RMSE=", round(e, 3))

if __name__ == "__main__":
    main()
\end{codepy}
\textbf{Implementation (Library Version)}
\begin{codepy}
import numpy as np
from sklearn.linear_model import LinearRegression

def main():
    np.random.seed(0)
    n = 100
    x = np.linspace(0, 10, n).reshape(-1, 1)
    X = np.column_stack([np.ones(n), x.flatten()])
    y = 1 + 2 * x.flatten() + np.random.randn(n) * 0.5
    model = LinearRegression(fit_intercept=False).fit(X, y)
    beta = np.array([model.coef_[0], model.coef_[1]])
    yhat = X @ beta
    rmse = np.sqrt(np.mean((y - yhat) ** 2))
    print("beta=", np.round(beta, 3), "RMSE=", round(rmse, 3))

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{RMSE and coefficient recovery near $(1,2)$ within noise-level.}
\INTERPRET{QR from GS yields stable least-squares without squaring the
condition number, matching library estimates.}
\NEXTSTEPS{Use modified GS or Householder QR for better numerical
stability in ill-conditioned designs.}

\DomainPage{Quantitative Finance}
\SCENARIO{
Orthonormalize factor exposures to obtain uncorrelated factor portfolios
under the empirical inner product, then compute factor returns.
}
\ASSUMPTIONS{
\begin{bullets}
\item Exposures matrix $B$ has full column rank.
\item Inner product is empirical over assets: $\langle x,y\rangle=x^\top y$.
\end{bullets}
}
\WHICHFORMULA{
Compute $B=QR$ via GS; orthonormal factors are columns of $Q$; factor
returns are $f=Q^\top r$ for asset returns $r$.
}
\varmapStart
\var{B}{Exposure matrix $(m,d)$.}
\var{Q,R}{QR factors from GS of $B$.}
\var{r}{Asset returns $(m)$.}
\var{f}{Orthogonal factor returns $(d)$.}
\varmapEnd
\PIPELINE{
\begin{bullets}
\item Simulate exposures and returns.
\item Orthonormalize exposures via GS to get $Q$.
\item Compute $f=Q^\top r$.
\end{bullets}
}
\textbf{Implementation (Full Pipeline)}
\begin{codepy}
import numpy as np

def simulate(m=50, d=3, seed=0):
    rng = np.random.default_rng(seed)
    B = rng.normal(size=(m, d))
    true_f = rng.normal(size=d)
    idio = rng.normal(scale=0.01, size=m)
    r = B @ true_f + idio
    return B, r, true_f

def gs_qr(B):
    m, d = B.shape
    Q = np.zeros((m, d), float)
    R = np.zeros((d, d), float)
    for j in range(d):
        u = B[:, j].copy()
        for i in range(j):
            R[i, j] = float(Q[:, i] @ B[:, j])
            u = u - R[i, j] * Q[:, i]
        R[j, j] = float(np.linalg.norm(u))
        Q[:, j] = u / R[j, j]
    return Q, R

def main():
    B, r, f_true = simulate()
    Q, R = gs_qr(B)
    f = Q.T @ r
    rec = np.linalg.lstsq(R, f, rcond=None)[0]
    assert np.allclose(B @ rec, Q @ (R @ rec), atol=1e-10)
    print("f=", np.round(f, 3))

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{Orthogonality check $Q^\top Q=I$ and reconstruction error.}
\INTERPRET{Columns of $Q$ are orthonormal factor-mimicking portfolios;
$f=Q^\top r$ are their returns uncorrelated by construction.}
\NEXTSTEPS{Use weighted inner product by asset variance and apply
weighted GS.}

\DomainPage{Deep Learning}
\SCENARIO{
Construct orthonormal weight matrices by QR (using GS) for initialization
to preserve variance across layers.
}
\ASSUMPTIONS{
\begin{bullets}
\item Orthonormal columns help maintain signal norms in linear layers.
\item Square or tall matrices allow QR to produce orthonormal columns.
\end{bullets}
}
\WHICHFORMULA{
Given random $W$, set $W=QR$ and use $Q$ scaled as initializer.
}
\varmapStart
\var{W}{Random weight matrix $(m,n)$.}
\var{Q}{Orthonormal columns from QR via GS.}
\var{R}{Upper triangular discarded except scaling.}
\varmapEnd
\PIPELINE{
\begin{bullets}
\item Sample Gaussian $W$.
\item Compute QR and take $Q$.
\item Verify $Q^\top Q=I$ and test norm preservation.
\end{bullets}
}
\textbf{Implementation (End-to-End)}
\begin{codepy}
import numpy as np

def ortho_init(m=64, n=32, seed=0):
    rng = np.random.default_rng(seed)
    W = rng.normal(size=(m, n))
    Q, R = np.linalg.qr(W, mode="reduced")
    D = np.sign(np.diag(R))
    D[D == 0] = 1.0
    Q = Q @ np.diag(D)
    return Q

def check(Q):
    I = Q.T @ Q
    return float(np.max(np.abs(I - np.eye(Q.shape[1]))))

def main():
    Q = ortho_init(64, 32, seed=0)
    err = check(Q)
    x = np.random.default_rng(1).normal(size=(32,))
    y = Q @ x
    assert abs(np.linalg.norm(y) - np.linalg.norm(x)) < 1e-10
    print("orth_err=", err)

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{Orthogonality error and norm preservation on random vectors.}
\INTERPRET{Using $Q$ ensures column orthonormality, stabilizing signal
propagation.}
\NEXTSTEPS{For square layers, use orthogonal $Q$; for conv layers,
reshape kernels and apply the same idea.}

\DomainPage{Kaggle / Data Analytics}
\SCENARIO{
Create orthonormal polynomial features with respect to empirical inner
product on a dataset to reduce multicollinearity before regression.
}
\ASSUMPTIONS{
\begin{bullets}
\item Data standardized by empirical inner product $\langle f,g\rangle
=\sum_i f(x_i)g(x_i)$.
\end{bullets}
}
\WHICHFORMULA{
Apply Gram-Schmidt to columns $[1,x,x^2,\dots]$ to get orthonormal
features $Q$; then regress $y$ onto $Q$.
}
\varmapStart
\var{x}{Feature vector $(n)$.}
\var{y}{Target $(n)$.}
\var{F}{Design with polynomial columns.}
\var{Q,R}{QR of $F$.}
\varmapEnd
\PIPELINE{
\begin{bullets}
\item Build polynomial design matrix.
\item Compute QR (GS or library) to obtain orthonormal columns.
\item Fit coefficients via $R^{-1}Q^\top y$ and report metrics.
\end{bullets}
}
\textbf{Implementation (Complete EDA Pipeline)}
\begin{codepy}
import numpy as np

def poly_design(x, deg):
    X = np.column_stack([x ** k for k in range(deg + 1)])
    return X

def qr_regress(F, y):
    Q, R = np.linalg.qr(F, mode="reduced")
    D = np.sign(np.diag(R)); D[D == 0] = 1.0
    Q = Q @ np.diag(D); R = np.diag(D) @ R
    beta = np.linalg.solve(R, Q.T @ y)
    return Q, R, beta

def main():
    rng = np.random.default_rng(0)
    x = np.linspace(-1, 1, 50)
    y = 1 + 2 * x + 0.5 * x ** 2 + rng.normal(0, 0.1, size=50)
    F = poly_design(x, 2)
    Q, R, beta = qr_regress(F, y)
    assert np.allclose(Q.T @ Q, np.eye(3), atol=1e-10)
    yhat = F @ beta
    rmse = float(np.sqrt(np.mean((y - yhat) ** 2)))
    print("beta=", np.round(beta, 3), "RMSE=", round(rmse, 3))

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{Orthogonality of features and RMSE of polynomial fit.}
\INTERPRET{Orthonormal features reduce multicollinearity and make
coefficients comparable.}
\NEXTSTEPS{Use weighted inner products to reflect heteroskedasticity and
apply weighted GS.}

\end{document}
% !TeX program = xelatex
\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{microtype,setspace,amsmath,amssymb,mathtools,amsthm,unicode-math}
\setstretch{1.05}
\setmainfont{Latin Modern Roman}
\setmonofont{Latin Modern Mono}
\setmathfont{Latin Modern Math}

\allowdisplaybreaks[4]
\setlength{\jot}{7pt}
\setlength{\emergencystretch}{8em}
\sloppy

\usepackage{xcolor,fancyhdr,enumitem,inconsolata,listings}
\pagestyle{fancy}\fancyhf{}\lhead{\nouppercase{\leftmark}}\rhead{\thepage}
\setlength{\headheight}{26pt}
\setlength{\parindent}{0pt}
\setlength{\parskip}{8pt plus 2pt minus 1pt}
\raggedbottom

\newenvironment{BreakableEquation}{\begin{equation}\begin{aligned}}{\end{aligned}\end{equation}}
\newenvironment{BreakableEquation*}{\begin{equation*}\begin{aligned}}{\end{aligned}\end{equation*}}
\newenvironment{tightalign}{\begingroup\small\allowdisplaybreaks\begin{align}}{\end{align}\endgroup}

\providecommand{\enumlistm}{enumitem}
\newenvironment{minted}[2][]{%
  \lstset{style=code,language=#2,#1}\begin{lstlisting}%
}{\end{lstlisting}}
\newcommand{\inputminted}[3][]{\begin{lstlisting}\end{lstlisting}}

\newlist{bullets}{itemize}{1}
\setlist[bullets]{label=--,leftmargin=1.6em,itemsep=2pt,topsep=4pt}

\newcommand{\varmapStart}{\textbf{VARIABLE MAPPING:}\par\begin{bullets}}
\newcommand{\var}[2]{\item $#1$ — #2}
\newcommand{\varmapEnd}{\end{bullets}}

\newcommand{\glossx}[6]{%
  \textbf{#1}\par
  \begin{bullets}
    \item \textbf{What:} #2
    \item \textbf{Why:} #3
    \item \textbf{How:} #4
    \item \textbf{ELI5:} #5
    \item \textbf{Pitfall/Example:} #6
  \end{bullets}
}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{claim}[theorem]{Claim}
\lstdefinestyle{code}{
  basicstyle=\ttfamily\footnotesize,
  backgroundcolor=\color{black!02},
  frame=single,
  numbers=left, numberstyle=\tiny, numbersep=8pt,
  breaklines=true, breakatwhitespace=true,
  tabsize=4, showstringspaces=false,
  upquote=true, keepspaces=true, columns=fullflexible,
  literate=
    {–}{{-}}1
    {—}{{-}}1
    {…}{{...}}1
    {≤}{{\ensuremath{\le}}}1
    {≥}{{\ensuremath{\ge}}}1
    {≠}{{\ensuremath{\ne}}}1
    {≈}{{\ensuremath{\approx}}}1
    {±}{{\ensuremath{\pm}}}1
    {→}{{\ensuremath{\to}}}1
    {←}{{\ensuremath{\leftarrow}}}1
    {∞}{{\ensuremath{\infty}}}1
    {√}{{\ensuremath{\sqrt{\ }}}}1
    {×}{{\ensuremath{\times}}}1
    {÷}{{\ensuremath{\div}}}1
}

\lstnewenvironment{codepy}[1][]%
  {\lstset{style=code,language=Python,#1}}%
  {}

\newcommand{\inlinecode}[1]{\lstinline[style=code]!#1!}

\newcommand{\LF}[2]{\par\noindent\textbf{#1:}~#2\par}
\newcommand{\WHAT}[1]{\LF{WHAT}{#1}}
\newcommand{\WHY}[1]{\LF{WHY}{#1}}
\newcommand{\HOW}[1]{\LF{HOW}{#1}}
\newcommand{\ELI}[1]{\LF{ELI5}{#1}}
\newcommand{\SCOPE}[1]{\LF{SCOPE}{#1}}
\newcommand{\CONFUSIONS}[1]{\LF{COMMON CONFUSIONS}{#1}}
\newcommand{\APPLICATIONS}[1]{\LF{APPLICATIONS}{#1}}
\newcommand{\FORMULA}[1]{\LF{FORMULA}{#1}}
\newcommand{\CANONICAL}[1]{\LF{CANONICAL FORM}{#1}}
\newcommand{\PRECONDS}[1]{\LF{PRECONDITIONS}{#1}}
\newcommand{\DERIVATION}[1]{\LF{DERIVATION}{#1}}
\newcommand{\EQUIV}[1]{\LF{EQUIVALENT FORMS}{#1}}
\newcommand{\LIMITS}[1]{\LF{LIMIT CASES}{#1}}
\newcommand{\INPUTS}[1]{\LF{INPUTS}{#1}}
\newcommand{\OUTPUTS}[1]{\LF{OUTPUTS}{#1}}
\newcommand{\RESULT}[1]{\LF{RESULT}{#1}}
\newcommand{\INTUITION}[1]{\LF{INTUITION}{#1}}
\newcommand{\PITFALLS}[1]{\LF{PITFALLS}{#1}}
\newcommand{\MODEL}[1]{\LF{CANONICAL MATH MODEL}{#1}}
\newcommand{\ASSUMPTIONS}[1]{\LF{ASSUMPTIONS}{#1}}
\newcommand{\WHICHFORMULA}[1]{\LF{WHICH FORMULA \& WHY}{#1}}
\newcommand{\GOVERN}[1]{\LF{GOVERNING EQUATION(S)}{#1}}
\newcommand{\UNITCHECK}[1]{\LF{UNIT CHECK}{#1}}
\newcommand{\EDGECASES}[1]{\LF{EDGE CASES}{#1}}
\newcommand{\ALTERNATE}[1]{\LF{ALTERNATE APPROACH (sketch)}{#1}}
\newcommand{\PROBLEM}[1]{\LF{PROBLEM}{#1}}
\newcommand{\API}[1]{\LF{API}{#1}}
\newcommand{\COMPLEXITY}[1]{\LF{COMPLEXITY}{#1}}
\newcommand{\FAILMODES}[1]{\LF{FAILURE MODES}{#1}}
\newcommand{\STABILITY}[1]{\LF{NUMERICAL STABILITY}{#1}}
\newcommand{\VALIDATION}[1]{\LF{VALIDATION}{#1}}
\newcommand{\EXPLANATION}[1]{\LF{EXPLANATION}{#1}}
\newcommand{\SCENARIO}[1]{\LF{SCENARIO}{#1}}
\newcommand{\PIPELINE}[1]{\LF{PIPELINE STEPS}{#1}}
\newcommand{\METRICS}[1]{\LF{METRICS}{#1}}
\newcommand{\INTERPRET}[1]{\LF{INTERPRETATION}{#1}}
\newcommand{\NEXTSTEPS}[1]{\LF{LIMITATIONS \& NEXT STEPS}{#1}}

\usepackage{titlesec}
\titleformat{\section}{\Large\bfseries}{\thesection}{0.6em}{}
\titlespacing*{\section}{0pt}{*2}{*1}
\usepackage{etoolbox}
\pretocmd{\section}{\clearpage}{}{}

\newcommand{\FormulaPage}[2]{%
  \clearpage
  \section*{Formula #1 — #2}%
  \addcontentsline{toc}{section}{Formula #1 — #2}%
}
\newcommand{\ProblemPage}[2]{%
  \clearpage
  \subsection*{Problem #1: #2}%
  \addcontentsline{toc}{subsection}{Problem #1: #2}%
}
\newcommand{\CodeDemoPage}[1]{%
  \clearpage
  \subsection*{Coding Demo: #1}%
  \addcontentsline{toc}{subsection}{Coding Demo: #1}%
}
\newcommand{\DomainPage}[1]{%
  \clearpage
  \subsection*{#1 (End-to-End)}%
  \addcontentsline{toc}{subsection}{#1 (End-to-End)}%
}

\begin{document}
\title{Comprehensive Study Sheet — Interlacing Eigenvalues and Weyl Inequalities}
\date{\today}
\maketitle
\tableofcontents
\clearpage

\section{Concept Overview}
\WHAT{
Given Hermitian (real symmetric) matrices $A\in\mathbb{C}^{n\times n}$ and
$B\in\mathbb{C}^{n\times n}$ with eigenvalues ordered
$\lambda_1(\cdot)\ge \cdots \ge \lambda_n(\cdot)$,
interlacing concerns comparisons between spectra of compressions
(principal submatrices) and the parent matrix, while Weyl inequalities
bound eigenvalues of sums $A+B$ by those of $A$ and $B$.
The object is the ordered spectrum and its order-preserving relations
under principal submatrices and addition. Domain: Hermitian matrices;
codomain: $\mathbb{R}^n$ (eigenvalue lists) with majorization order.
}

\WHY{
Interlacing and Weyl bounds are foundational in spectral theory,
matrix perturbation, numerical linear algebra, and graph theory.
They provide monotonicity and stability controls for eigenvalues under
dimension reduction (principal submatrices) and under additive
perturbations (modeling noise, regularization, coupling).
They are used in proofs (min-max), modeling (PCA stability),
and computation (error bounds).
}

\HOW{
1. Define Hermitian structure and Rayleigh quotient $R_A(x)=x^*Ax/x^*x$. 
2. Use Courant--Fischer min-max characterization to represent ordered
eigenvalues as extremal Rayleigh quotients over subspaces.
3. For interlacing, compare extremizations on nested subspaces induced by
coordinate deletions (principal submatrices) via compression maps.
4. For Weyl, apply min-max to $A+B$ and bound $R_{A+B}(x)$ by $R_A(x)+R_B(x)$,
then optimize with appropriate index arithmetic. Interpret each bound.
}

\ELI{
Think of eigenvalues as heights of $n$ shelves. Removing a coordinate
(principal submatrix) cannot move the $k$-th shelf up by more than the
shelf above nor down below the shelf at the same position in the big
cabinet; the smaller cabinet shelves interleave with the big ones.
Adding $B$ to $A$ is like stacking two terrains; the elevation at any
point is the sum, so the $k$-th highest elevation of the sum cannot
exceed the sum of suitably high elevations of each terrain separately.
}

\SCOPE{
Valid for Hermitian matrices with real spectra. Interlacing requires
principal submatrices (or orthogonal compressions). Weyl inequalities
require Hermitian summands. Edge cases: repeated eigenvalues, rank-deficient
perturbations, or singular compressions; statements still hold with
weak inequalities. Non-Hermitian matrices require singular values analogs
(interlacing for singular values and Weyl for singular values).
}

\CONFUSIONS{
Interlacing vs. majorization: interlacing is about alternating inequalities
between two ordered lists of different lengths; majorization compares
partial sums. Weyl vs. triangle inequality: Weyl refines spectral triangle
inequality at each index. Principal submatrix vs. similarity: interlacing
needs compression, not arbitrary similarity transforms. Sum eigenvalues
vs. sum of eigenvectors: no eigenvector additivity is implied.
}

\APPLICATIONS{
List 3–4 major domains where this topic directly applies:
\begin{bullets}
\item Mathematical foundations (pure / applied).
\item Computational modeling or simulation.
\item Physical / economic / engineering interpretations.
\item Statistical or algorithmic implications.
\end{bullets}
}

\textbf{ANALYTIC STRUCTURE.}
Hermitian matrices induce a real-valued Rayleigh quotient that is convex
along lines on unit sphere directions of two-dimensional subspaces.
Ordered eigenvalues are monotone with respect to Loewner order; addition
is monotone and Lipschitz in spectral norm. Interlacing reflects lattice
structure of subspaces; Weyl reflects subadditivity of extremal functionals.

\textbf{CANONICAL LINKS.}
Courant--Fischer min-max is the backbone. Cauchy interlacing follows from
min-max on nested subspaces. Weyl inequalities follow from min-max plus
index arithmetic. Poincar\'e separation theorem is an interlacing variant.

\textbf{PROBLEM-TYPE RECOGNITION HEURISTICS.}
\begin{bullets}
\item Mentions of principal submatrix or removing a row/column.
\item Questions bounding eigenvalues of $A+B$ by those of $A$ and $B$.
\item Phrases like compression, projection, Rayleigh quotient, min-max.
\item Rank-one updates, PSD perturbations, dropping features in PCA.
\end{bullets}

\textbf{SOLUTION STRATEGY BLUEPRINT.}
\begin{bullets}
\item Step 1: Order eigenvalues decreasingly and identify indices.
\item Step 2: Invoke Courant--Fischer for the target eigenvalue.
\item Step 3: Relate feasible subspaces via inclusion or addition.
\item Step 4: Apply inequalities to Rayleigh quotients and optimize.
\item Step 5: Interpret bounds and check tightness via examples.
\end{bullets}

\textbf{CONCEPTUAL INVARIANTS.}
Ordering of eigenvalues, Loewner order monotonicity, Rayleigh quotient
extrema on invariant subspaces, unitary invariance of spectra.

\textbf{EDGE INTUITION.}
As perturbation size $\to 0$, Weyl inequalities collapse to continuity of
eigenvalues; with rank-one PSD updates, each eigenvalue moves upward by
at most the update trace. For large positive $t$, eigenvalues of $A+tB$
align with those of $tB$.

\section{Glossary}
\glossx{Courant--Fischer Min-Max Principle}
{Characterizes ordered eigenvalues of Hermitian $A$ via extremal Rayleigh
quotients over $k$-dimensional subspaces.}
{It is the engine behind interlacing and Weyl inequalities; converts
spectral statements into subspace optimization.}
{Use $k$-dimensional subspaces for max-min and $(n-k+1)$ for min-max,
optimize $x^*Ax/x^*x$.}
{Like finding the $k$-th highest hill by choosing the best $k$-dimensional
direction to maximize the worst local height.}
{Pitfall: swapping min and max or using wrong dimension index.}

\glossx{Cauchy Interlacing}
{Eigenvalues of a principal submatrix of a Hermitian matrix interleave
those of the original matrix.}
{Quantifies spectral stability under coordinate deletion or compression.}
{Apply min-max on nested subspaces linked by the inclusion map.}
{Removing one coordinate yields shelves that fit between the old ones.}
{Pitfall: requires principal (not arbitrary) submatrix.}

\glossx{Weyl Inequalities}
{Index-wise bounds on eigenvalues of $A+B$ in terms of those of $A$ and
$B$, both Hermitian.}
{Gives deterministic perturbation bounds for additive noise or coupling.}
{Bound Rayleigh quotient of $A+B$ by sum of quotients and optimize indices.}
{Heights of two terrains add; ranked heights of the sum are bounded by
sums of ranked heights of parts.}
{Pitfall: index arithmetic $i+j-1$ and $i+j-n$ must be correct.}

\glossx{Principal Submatrix}
{Matrix obtained by deleting matching rows and columns from a matrix.}
{Captures compression to a coordinate subspace; preserves Hermitian.}
{Represent as $P^*AP$ where $P$ selects coordinates, then analyze $P^*AP$.}
{Like looking at a subset of coordinates only.}
{Pitfall: non-principal submatrices need not interlace.}

\section{Symbol Ledger}
\varmapStart
\var{A,B}{Hermitian matrices in $\mathbb{C}^{n\times n}$.}
\var{\lambda_k(A)}{$k$-th largest eigenvalue of $A$, real.}
\var{\mu_k}{$k$-th largest eigenvalue of a principal submatrix.}
\var{U}{$k$-dimensional subspace of $\mathbb{C}^n$.}
\var{P}{Partial isometry selecting coordinates (column selector).}
\var{R_A(x)}{Rayleigh quotient $x^*Ax/(x^*x)$.}
\var{n,m}{Matrix sizes, with $m<n$ for submatrices.}
\var{k,i,j}{Indices with $1\le k,i,j\le n$.}
\var{\|\cdot\|_2}{Spectral norm.}
\var{\preceq}{Loewner order on Hermitian matrices.}
\var{L}{Graph Laplacian (Hermitian, PSD).}
\varmapEnd

\section{Formula Canon — One Formula Per Page}
\FormulaPage{1}{Courant--Fischer Min-Max Principle}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
For Hermitian $A\in\mathbb{C}^{n\times n}$ and ordered eigenvalues
$\lambda_1\ge\cdots\ge\lambda_n$,
\[
\lambda_k(A)=\max_{\substack{U\subset\mathbb{C}^n\\ \dim U=k}}
\;\min_{\substack{x\in U\\ x\ne 0}} R_A(x)
=\min_{\substack{W\subset\mathbb{C}^n\\ \dim W=n-k+1}}
\;\max_{\substack{x\in W\\ x\ne 0}} R_A(x).
\]
\WHAT{
Characterizes $\lambda_k(A)$ as a saddle point of Rayleigh quotients over
$k$- and $(n-k+1)$-dimensional subspaces.
}
\WHY{
This variational form underlies interlacing (via subspace inclusion) and
Weyl inequalities (via additive splitting of Rayleigh quotients).
}
\FORMULA{
\[
\lambda_k(A)=\max_{\dim U=k}\ \min_{\|x\|=1,x\in U} x^*Ax
=\min_{\dim W=n-k+1}\ \max_{\|x\|=1,x\in W} x^*Ax.
\]
}
\CANONICAL{
Hermitian $A$; eigenvalues real and ordered nonincreasingly. Unit norm
constraint is without loss of generality by homogeneity of $R_A(x)$.
}
\PRECONDS{
\begin{bullets}
\item $A=A^*$ with finite dimension $n$.
\item Standard inner product on $\mathbb{C}^n$.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
Let $A=A^*$ with eigen-decomposition $A=Q\Lambda Q^*$,
$\Lambda=\mathrm{diag}(\lambda_1,\dots,\lambda_n)$,
$Q$ unitary. Then $\min_{\|x\|=1}x^*Ax=\lambda_n$ and
$\max_{\|x\|=1}x^*Ax=\lambda_1$ with extrema at eigenvectors.
\end{lemma}
\begin{proof}
Write $x=Qy$ with $\|y\|=1$. Then $x^*Ax=y^*\Lambda y=\sum_i\lambda_i|y_i|^2$
is a convex combination of eigenvalues. The minimum is $\lambda_n$ achieved
at $y=e_n$, and maximum is $\lambda_1$ at $y=e_1$. \qedhere
\end{proof}
\DERIVATION{
\begin{align*}
\text{Step 1:}~~&\text{Order eigenpairs }(q_i,\lambda_i),~Aq_i=\lambda_i q_i.\\
\text{Step 2:}~~&\text{For any }k\text{-dim }U,\ \min_{\|x\|=1,x\in U}x^*Ax\\
&\le \min_{\|x\|=1,x\in \mathrm{span}\{q_1,\dots,q_k\}}x^*Ax=\lambda_k.\\
\text{Step 3:}~~&\text{Maximizing over }U\text{ gives }\lambda_k\text{ as an upper
bound on LHS, hence equality.}\\
\text{Step 4:}~~&\text{Symmetrically, for }(n-k+1)\text{-dim }W,\\
&\max_{\|x\|=1,x\in W}x^*Ax\ge \max_{\|x\|=1,x\in \mathrm{span}\{q_k,\dots,q_n\}}
x^*Ax=\lambda_k.\\
\text{Step 5:}~~&\text{Minimizing over }W\text{ yields equality at }
W=\mathrm{span}\{q_k,\dots,q_n\}.
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Choose appropriate dimension subspace to upper/lower bound $\lambda_k$.
\item Use invariant subspaces spanned by leading or trailing eigenvectors.
\item Translate constraints (e.g., compression) into subspace restrictions.
\item Optimize Rayleigh quotient accordingly.
\end{bullets}
\EQUIV{
\begin{bullets}
\item Ky Fan max principle: $\sum_{i=1}^k\lambda_i=\max_{\dim U=k}
\mathrm{tr}(P_U A)$, $P_U$ projector.
\item Variational characterization via exterior powers $\wedge^k A$.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item If $A$ is non-Hermitian, replace by singular values (Courant--Fischer
for $A^*A$).
\item Repeated eigenvalues give non-unique extremal subspaces but same values.
\end{bullets}
}
\INPUTS{$A\in\mathbb{C}^{n\times n}$ Hermitian, index $k\in\{1,\dots,n\}$.}
\DERIVATION{
\begin{align*}
\text{Example: }A&=\begin{bmatrix}2&1\\1&0\end{bmatrix},~
\lambda_{1,2}=1\pm\sqrt{2}.\\
\lambda_1&=\max_{\|x\|=1}x^*Ax,~\lambda_2=\min_{\|x\|=1}x^*Ax.
\end{align*}
}
\RESULT{
Min-max equalities hold and numerically recover $1\pm\sqrt{2}$ in the
example.}
\UNITCHECK{
Dimensionless because Rayleigh quotient is homogeneous of degree 0.
}
\PITFALLS{
\begin{bullets}
\item Interchanging min and max or wrong subspace dimensions.
\item Forgetting to order eigenvalues nonincreasingly.
\end{bullets}
}
\INTUITION{
Min-max selects the $k$-th highest energy direction once the best $(k-1)$
directions are secured by the maximizer, while the minimizer picks the
worst vector in that subspace.}
\CANONICAL{
\begin{bullets}
\item $\lambda_k=\max_{\dim U=k}\min_{x\in U,\|x\|=1}x^*Ax$.
\item Dual form with $\min_{\dim W=n-k+1}\max_{x\in W,\|x\|=1}x^*Ax$.
\end{bullets}
}

\FormulaPage{2}{Cauchy Interlacing Theorem}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Let $A=A^*\in\mathbb{C}^{n\times n}$ with eigenvalues
$\lambda_1\ge\cdots\ge\lambda_n$, and let $M$ be an $m\times m$
principal submatrix with eigenvalues $\mu_1\ge\cdots\ge\mu_m$, $m<n$.
Then for each $k=1,\dots,m$,
\[
\lambda_k(A)\ge \mu_k\ge \lambda_{k+n-m}(A).
\]
\WHAT{
Relates the spectrum of a principal submatrix to that of its parent by
alternate inequalities (interlacing).}
\WHY{
Quantifies spectral stability under coordinate deletion and lies at the
core of graph spectral interlacing and PCA feature removal.}
\FORMULA{
\[
\lambda_k(A)\ge \mu_k\ge \lambda_{k+n-m}(A),\quad k=1,\dots,m.
\]
}
\CANONICAL{
$A$ Hermitian, $M=P^*AP$ for a coordinate selector $P\in\mathbb{C}^{n\times m}$
with $P^*P=I_m$. Eigenvalues ordered nonincreasingly.
}
\PRECONDS{
\begin{bullets}
\item $M$ must be principal: same index set for rows and columns.
\item Hermitian structure ensures real spectra and min-max applicability.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
Let $P$ have orthonormal columns. Then for any $y\ne 0$,
$R_M(y)=R_A(Py)$ and $\{Py: y\in\mathbb{C}^m\}$ is an $m$-dim subspace.
\end{lemma}
\begin{proof}
$R_M(y)=\dfrac{y^*P^*APy}{y^*y}=\dfrac{(Py)^*A(Py)}{(Py)^*(Py)}=R_A(Py)$,
and $\dim\mathrm{range}(P)=m$ by $P^*P=I_m$. \qedhere
\end{proof}
\DERIVATION{
\begin{align*}
\text{Upper bound:}~~
\mu_k&=\max_{\dim U=k}\ \min_{\substack{y\in U\\ \|y\|=1}} y^*My\\
&=\max_{\dim U=k}\ \min_{\substack{y\in U\\ \|y\|=1}} (Py)^*A(Py)\\
&\le \max_{\dim V=k}\ \min_{\substack{x\in V\\ \|x\|=1}} x^*Ax
=\lambda_k(A).\\[4pt]
\text{Lower bound:}~~
\mu_k&=\min_{\dim W=m-k+1}\ \max_{\substack{y\in W\\ \|y\|=1}} y^*My\\
&=\min_{\dim W=m-k+1}\ \max_{\substack{x\in PW\\ \|x\|=1}} x^*Ax\\
&\ge \min_{\dim Z=n-k+1}\ \max_{\substack{x\in Z\\ \|x\|=1}} x^*Ax\\
&=\lambda_{k+n-m}(A),
\end{align*}
where $PW$ is an $(m-k+1)$-dimensional subspace of $\mathbb{C}^n$ and
can be extended to a $(n-k+1)$-dimensional subspace $Z$.
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Express $M$ as $P^*AP$ with $P^*P=I$.
\item Apply min-max for $\mu_k$ and compare feasible sets to those for
$\lambda_k(A)$.
\item Use subspace inclusion to bound min or max accordingly.
\end{bullets}
\EQUIV{
\begin{bullets}
\item Poincar\'e separation theorem (orthogonal projection compression).
\item Interlacing for graph Laplacians under vertex deletion.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item Non-principal submatrices may violate interlacing.
\item For $m=n-1$, inequalities simplify to $\lambda_k\ge \mu_k\ge \lambda_{k+1}$.
\end{bullets}
}
\INPUTS{$A\in\mathbb{C}^{n\times n}$ Hermitian; index set $S$, $|S|=m$.}
\DERIVATION{
\begin{align*}
\text{Example: }A&=\begin{bmatrix}3&1&0\\1&2&1\\0&1&1\end{bmatrix}.\\
\text{Eigenvalues }&\lambda(A)\approx(3.6180,1.6180,0.7640).\\
M&=\begin{bmatrix}3&1\\1&2\end{bmatrix}\Rightarrow
\mu\approx(3.4142,1.5858).\\
\text{Check: }&3.6180\ge 3.4142\ge 1.6180,\\
&1.6180\ge 1.5858\ge 0.7640.
\end{align*}
}
\RESULT{
Interlacing holds numerically and exactly by the min-max derivation.
}
\UNITCHECK{
Inequalities compare reals; dimensionless ordering is consistent.
}
\PITFALLS{
\begin{bullets}
\item Using $P$ not column-orthonormal breaks $R_M(y)=R_A(Py)$.
\item Sorting eigenvalues increasingly flips indices and fails checks.
\end{bullets}
}
\INTUITION{
Compressing to fewer coordinates cannot create new extreme directions;
the reduced extremes must sit between the parent extremes.}
\CANONICAL{
\begin{bullets}
\item $\lambda_k(A)\ge \mu_k\ge \lambda_{k+n-m}(A)$ with $M=P^*AP$.
\end{bullets}
}

\FormulaPage{3}{Weyl Inequalities for Eigenvalues of Sums}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
For Hermitian $A,B\in\mathbb{C}^{n\times n}$ with spectra ordered
decreasingly, for $1\le i,j\le n$ and $i+j-1\le n$,
\[
\lambda_{i+j-1}(A+B)\le \lambda_i(A)+\lambda_j(B),
\]
and for $i+j-n\ge 1$,
\[
\lambda_{i+j-n}(A+B)\ge \lambda_i(A)+\lambda_j(B).
\]
\WHAT{
Index-wise upper and lower bounds on eigenvalues of a sum $A+B$.}
\WHY{
Controls spectral shifts under additive perturbations; foundational for
stability analyses and matrix inequalities.}
\FORMULA{
\[
\begin{aligned}
\lambda_{i+j-1}(A+B)&\le \lambda_i(A)+\lambda_j(B),\\
\lambda_{i+j-n}(A+B)&\ge \lambda_i(A)+\lambda_j(B).
\end{aligned}
\]
}
\CANONICAL{
Hermitian $A,B$; eigenvalues sorted nonincreasingly. Bounds expressed via
index arithmetic constraints to keep indices in $1,\dots,n$.}
\PRECONDS{
\begin{bullets}
\item Hermitian inputs ensure real spectra and min-max applicability.
\item Proper ordering of eigenvalues is required.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
For any unit vector $x$, $x^*(A+B)x=x^*Ax+x^*Bx\le
\max_{\|u\|=1}u^*Au+\max_{\|v\|=1}v^*Bv$.
\end{lemma}
\begin{proof}
Apply the lemma on Rayleigh quotients separately and use additivity. For a
fixed $x$, the equality holds; the inequality arises after maximizing the
sum independently. \qedhere
\end{proof}
\DERIVATION{
\begin{align*}
\text{Upper bound: }~
\lambda_{i+j-1}(A+B)&=\max_{\dim U=i+j-1}\ \min_{\substack{x\in U\\ \|x\|=1}}
x^*(A+B)x\\
&\le \max_{\dim U=i+j-1}\ \min_{\substack{x\in U\\ \|x\|=1}}
\big(x^*Ax+x^*Bx\big).\\
\text{Choose }&U=U_A\oplus U_B\text{ with }\dim U_A=i,\ \dim U_B=j-1,\\
&\text{and }U\subset \mathbb{C}^n\text{ of dim }i+j-1.\\
\text{Then }&\min_{x\in U,\|x\|=1}x^*Ax\le \lambda_i(A),\\
&\min_{x\in U,\|x\|=1}x^*Bx\le \lambda_j(B).\\
\Rightarrow~
\lambda_{i+j-1}(A+B)&\le \lambda_i(A)+\lambda_j(B).\\[6pt]
\text{Lower bound: }~
\lambda_{i+j-n}(A+B)&=\min_{\dim W=n-i-j+1}\ \max_{\substack{x\in W\\ \|x\|=1}}
x^*(A+B)x\\
&\ge \min_{\dim W}\ \Big(\max_{\substack{x\in W\\ \|x\|=1}}x^*Ax+
\max_{\substack{x\in W\\ \|x\|=1}}x^*Bx\Big)\\
&\ge \lambda_i(A)+\lambda_j(B),
\end{align*}
where the last step chooses $W$ that contains an $(n-i+1)$-dimensional
subspace for $A$'s lower block and an $(n-j+1)$-dimensional subspace for
$B$, so their intersection has dimension at least $n-i-j+2$, enabling
the index $i+j-n$ bound.
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Identify target eigenvalue index and feasible subspace dimensions.
\item Use decompositions or dimension counting for subspace intersection.
\item Apply Rayleigh quotient additivity and optimize with min-max.
\end{bullets}
\EQUIV{
\begin{bullets}
\item Ky Fan inequalities for partial sums:
$\sum_{k=1}^\ell \lambda_k(A+B)\le \sum_{k=1}^\ell \lambda_k(A)+
\sum_{k=1}^\ell \lambda_k(B)$.
\item Lidskii inequality (majorization refinement).
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item Tightness achieved for commuting $A,B$ with aligned eigenbases.
\item For non-Hermitian matrices, use singular value analogs (Weyl for $A^*A$).
\end{bullets}
}
\INPUTS{$A,B\in\mathbb{C}^{n\times n}$ Hermitian; indices $i,j$.}
\DERIVATION{
\begin{align*}
\text{Example: }&A=\mathrm{diag}(3,1,0),\ B=\mathrm{diag}(2,2,-1).\\
\lambda(A)&=(3,1,0),~\lambda(B)=(2,2,-1).\\
\lambda(A+B)&=(5,3,-1).\\
\text{Check: }&\lambda_2(A+B)=3\le \lambda_1(A)+\lambda_2(B)=5,\\
&\lambda_1(A+B)=5\le \lambda_1(A)+\lambda_1(B)=5,\\
&\lambda_1(A+B)=5\ge \lambda_1(A)+\lambda_1(B)-0~\text{(tight)}.
\end{align*}
}
\RESULT{
All Weyl bounds hold with equalities for commuting diagonal matrices.}
\UNITCHECK{
Inequalities compare reals; indices within $1,\dots,n$ ensure validity.
}
\PITFALLS{
\begin{bullets}
\item Misusing indices ($i+j-1$ vs. $i+j-n$).
\item Assuming equality without commutativity or aligned eigenvectors.
\end{bullets}
}
\INTUITION{
The $k$-th largest elevation of the sum cannot exceed the sum of two
elevations chosen not too low in each component; similarly for lows.}
\CANONICAL{
\begin{bullets}
\item $\lambda_{i+j-1}(A+B)\le \lambda_i(A)+\lambda_j(B)$.
\item $\lambda_{i+j-n}(A+B)\ge \lambda_i(A)+\lambda_j(B)$.
\end{bullets}
}

\FormulaPage{4}{Weyl Monotonicity and PSD Perturbations}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
If $B\succeq 0$ (positive semidefinite), then for all $k$,
\[
\lambda_k(A)\le \lambda_k(A+B)\le \lambda_k(A)+\lambda_1(B).
\]
\WHAT{
Monotonicity and Lipschitz-type bound for eigenvalues under PSD addition.}
\WHY{
Central for regularization effects, stability under noise, and error
control using spectral norm $\|B\|_2=\lambda_1(B)$.}
\FORMULA{
\[
\lambda_k(A)\le \lambda_k(A+B)\le \lambda_k(A)+\|B\|_2.
\]
}
\CANONICAL{
$A,B$ Hermitian with $B\succeq 0$. Eigenvalues ordered nonincreasingly.
}
\PRECONDS{
\begin{bullets}
\item $B\succeq 0$ ensures $x^*Bx\ge 0$ for all $x$.
\item Spectral norm equals top eigenvalue for PSD $B$.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
If $C\succeq 0$, then for any subspace $U$, $\min_{x\in U,\|x\|=1}
x^*(A+C)x\ge \min_{x\in U,\|x\|=1}x^*Ax$.
\end{lemma}
\begin{proof}
For any unit $x$, $x^*(A+C)x=x^*Ax+x^*Cx\ge x^*Ax$. Taking minima over $U$
preserves the inequality. \qedhere
\end{proof}
\DERIVATION{
\begin{align*}
\text{Lower: }~
\lambda_k(A+B)&=\max_{\dim U=k}\ \min_{\|x\|=1,x\in U}x^*(A+B)x\\
&\ge \max_{\dim U=k}\ \min_{\|x\|=1,x\in U}x^*Ax=\lambda_k(A).\\
\text{Upper: }~
\lambda_k(A+B)&=\min_{\dim W=n-k+1}\ \max_{\|x\|=1,x\in W}x^*(A+B)x\\
&\le \min_{\dim W}\ \Big(\max_{\|x\|=1,x\in W}x^*Ax+\max_{\|x\|=1,x\in W}
x^*Bx\Big)\\
&\le \lambda_k(A)+\lambda_1(B).
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Use min-max and note $x^*Bx\ge 0$ for all $x$.
\item For the upper bound, bound $x^*Bx\le \lambda_1(B)$.
\item Combine to get sandwich inequality for each $k$.
\end{bullets}
\EQUIV{
\begin{bullets}
\item Loewner order monotonicity: $A\preceq A+B\Rightarrow
\lambda_k(A)\le \lambda_k(A+B)$.
\item Spectral norm Lipschitz: $|\lambda_k(A+B)-\lambda_k(A)|
\le \|B\|_2$ for PSD $B$ (one-sided upper bound).
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item If $B$ is indefinite, only Weyl general inequalities apply.
\item Equality may hold if $B$ annihilates the relevant eigen-subspace.
\end{bullets}
}
\INPUTS{$A\in\mathbb{C}^{n\times n}$ Hermitian; $B\succeq 0$; index $k$.}
\DERIVATION{
\begin{align*}
\text{Example: }&A=\mathrm{diag}(3,1,0),\ B=\mathrm{diag}(0.5,0,0).\\
\lambda(A+B)&=(3.5,1,0).\\
\text{Bounds: }&\lambda_k(A)\le \lambda_k(A+B)\le \lambda_k(A)+0.5\\
&(3\le 3.5\le 3.5),~(1\le 1\le 1.5),~(0\le 0\le 0.5).
\end{align*}
}
\RESULT{
Sandwich bounds verified; monotone nondecreasing movement under PSD
addition, with spectral-norm cap.}
\UNITCHECK{
All quantities are real scalars; indices match dimensions.}
\PITFALLS{
\begin{bullets}
\item Using $\|B\|_2$ with $B$ indefinite provides only two-sided bound
$|\lambda_k(A+B)-\lambda_k(A)|\le \|B\|_2$ but not monotonicity.
\item Forgetting to sort eigenvalues nonincreasingly.
\end{bullets}
}
\INTUITION{
Adding energy that is never negative can only raise each energy level, but
by at most the strongest possible contribution $\lambda_1(B)$.}
\CANONICAL{
\begin{bullets}
\item $\lambda_k(A)\le \lambda_k(A+B)\le \lambda_k(A)+\|B\|_2$, $B\succeq 0$.
\end{bullets}
}

\section{10 Exhaustive Problems and Solutions}
\ProblemPage{1}{Deriving Cauchy Interlacing via Min-Max and Example}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Prove interlacing for a principal submatrix and verify on a $3\times 3$
example.
\PROBLEM{
Let $A=A^*\in\mathbb{C}^{n\times n}$ and $M$ be an $(n-1)\times(n-1)$
principal submatrix. Show $\lambda_k(A)\ge \mu_k\ge \lambda_{k+1}(A)$ for
$k=1,\dots,n-1$, then compute for a concrete matrix.}
\MODEL{
\[
M=P^*AP,\quad P\in\mathbb{C}^{n\times (n-1)},\ P^*P=I.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $A$ Hermitian with eigenvalues ordered nonincreasingly.
\item $M$ obtained by deleting the same row and column.
\end{bullets}
}
\varmapStart
\var{A}{Parent Hermitian matrix.}
\var{M}{Principal submatrix of size $n-1$.}
\var{\lambda_k}{Eigenvalues of $A$, decreasing.}
\var{\mu_k}{Eigenvalues of $M$, decreasing.}
\varmapEnd
\WHICHFORMULA{
Cauchy Interlacing Theorem (Formula 2) derived via Courant--Fischer
(Formula 1).}
\GOVERN{
\[
\lambda_k(A)=\max_{\dim U=k}\min_{\|x\|=1,x\in U}x^*Ax,\quad
\mu_k=\max_{\dim U=k}\min_{\|y\|=1,y\in U}y^*My.
\]
}
\INPUTS{$A=\begin{bmatrix}3&1&0\\1&2&1\\0&1&1\end{bmatrix}$.}
\DERIVATION{
\begin{align*}
\text{Proof: }&\mu_k=\max_{\dim U=k}\min_{\|y\|=1,y\in U} y^*P^*APy\\
&=\max_{\dim U=k}\min_{\|y\|=1,y\in U} (Py)^*A(Py)\\
&\le \max_{\dim V=k}\min_{\|x\|=1,x\in V} x^*Ax=\lambda_k(A).\\
\text{Similarly }&\mu_k\ge \lambda_{k+1}(A).
\end{align*}
Now compute numerically.\\
\lambda(A)&\approx(3.6180,1.6180,0.7640).\\
M&=\begin{bmatrix}3&1\\1&2\end{bmatrix},~
\mu\approx(3.4142,1.5858).\\
\text{Check }&3.6180\ge 3.4142\ge 1.6180,\\
&1.6180\ge 1.5858\ge 0.7640.
\end{align*}
}
\RESULT{
Interlacing proven and verified for the example.}
\UNITCHECK{
Indices consistent: $k=1,2$ for $n-1=2$.}
\EDGECASES{
\begin{bullets}
\item If $A$ has repeated eigenvalues, equalities may occur.
\item If the deleted index corresponds to an invariant coordinate,
one $\mu_k$ equals a $\lambda_k$.
\end{bullets}
}
\ALTERNATE{
Use characteristic polynomials and Cauchy interlacing of roots by
Sylvester determinant identities.}
\VALIDATION{
\begin{bullets}
\item Numerical eigen-computation confirms inequalities.
\item Verify that $P^*P=I$ for the selector $P$.
\end{bullets}
}
\INTUITION{
Compression removes directions, so lows cannot drop below the next lower
parent level and highs cannot exceed parent highs.}
\CANONICAL{
\begin{bullets}
\item $\lambda_k(A)\ge \mu_k\ge \lambda_{k+1}(A)$ for $m=n-1$.
\end{bullets}
}

\ProblemPage{2}{Weyl Bounds for Noncommuting Sum with Numbers}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Bound $\lambda_2(A+B)$ using Weyl and check numerically for a
noncommuting pair.
\PROBLEM{
Let $A=\begin{bmatrix}2&1&0\\1&1&1\\0&1&0\end{bmatrix}$ and
$B=\begin{bmatrix}1&0&1\\0&2&0\\1&0&1\end{bmatrix}$. Compute
bounds on $\lambda_2(A+B)$ via Weyl with several $(i,j)$ and compare to
the exact value.}
\MODEL{
\[
\lambda_{i+j-1}(A+B)\le \lambda_i(A)+\lambda_j(B),\quad
\lambda_{i+j-n}(A+B)\ge \lambda_i(A)+\lambda_j(B).
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $A,B$ are Hermitian; eigenvalues sorted nonincreasingly.
\end{bullets}
}
\varmapStart
\var{A,B}{Given Hermitian matrices.}
\var{\lambda_k(\cdot)}{Ordered eigenvalues.}
\var{n}{Dimension $3$.}
\varmapEnd
\WHICHFORMULA{
Weyl inequalities (Formula 3).}
\GOVERN{
\[
\lambda_2(A+B)\le \min\{\lambda_1(A)+\lambda_2(B),
\lambda_2(A)+\lambda_1(B)\}.
\]
}
\INPUTS{$A,B$ as above; $n=3$.}
\DERIVATION{
\begin{align*}
\lambda(A)&\approx(2.8029,0.7540,-0.5569).\\
\lambda(B)&\approx(2.4142,1.0000,0.5858).\\
\text{Upper bounds: }&
\lambda_1(A)+\lambda_2(B)=2.8029+1=3.8029,\\
&\lambda_2(A)+\lambda_1(B)=0.7540+2.4142=3.1682.\\
\Rightarrow~&\lambda_2(A+B)\le 3.1682.\\
\text{Lower bounds: }&
\lambda_{2+3-3}(A+B)=\lambda_2(A+B)\\
&\ge \lambda_2(A)+\lambda_3(B)=0.7540+0.5858=1.3398,\\
&\ge \lambda_3(A)+\lambda_2(B)=-0.5569+1=0.4431.\\
\text{Exact: }&\lambda(A+B)\approx(5.0346,2.2638,-0.4826).\\
\text{Thus }&\lambda_2(A+B)=2.2638\in[1.3398,3.1682].
\end{align*}
}
\RESULT{
Weyl provides a valid interval $[1.3398,3.1682]$ containing
$\lambda_2(A+B)=2.2638$.}
\UNITCHECK{
Indices satisfy $i+j-1\le 3$ and $i+j-3\ge 1$.}
\EDGECASES{
\begin{bullets}
\item If $A,B$ commute and share eigenvectors, bounds tighten to equalities
for matching indices.
\end{bullets}
}
\ALTERNATE{
Use Ky Fan sums to bound partial sums and deduce index-wise bounds via
monotonicity.}
\VALIDATION{
\begin{bullets}
\item Compute eigenvalues numerically and assert inequalities.
\end{bullets}
}
\INTUITION{
Even when eigenvectors misalign, Weyl still constrains the sum spectrum.}
\CANONICAL{
\begin{bullets}
\item Upper: $(i,j)=(2,1)$; Lower: $(i,j)=(2,3)$.
\end{bullets}
}

\ProblemPage{3}{Explained Variance Under Feature Removal}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Interlacing bounds the variance captured by top principal components after
removing a feature (principal submatrix of covariance).
\PROBLEM{
Let $\Sigma\succeq 0$ be a $d\times d$ covariance. Remove feature $p$ to
obtain $\Sigma_{-p}$. Show that for $k\le d-1$,
$\sum_{i=1}^k\lambda_i(\Sigma_{-p})\le \sum_{i=1}^k\lambda_i(\Sigma)$ and
$\lambda_k(\Sigma)\ge \lambda_k(\Sigma_{-p})\ge \lambda_{k+1}(\Sigma)$.}
\MODEL{
\[
\Sigma_{-p}=P^*\Sigma P,\quad P^*P=I_{d-1},\ P\text{ selects }[d]\setminus\{p\}.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $\Sigma$ is Hermitian PSD.
\item Eigenvalues sorted nonincreasingly.
\end{bullets}
}
\varmapStart
\var{\Sigma}{Full covariance matrix.}
\var{\Sigma_{-p}}{Principal submatrix after removing feature $p$.}
\var{\lambda_i}{Eigenvalues in decreasing order.}
\var{d}{Dimension.}
\varmapEnd
\WHICHFORMULA{
Cauchy interlacing (Formula 2) and Ky Fan inequality (sum of top $k$ is
monotone under compression).}
\GOVERN{
\[
\lambda_i(\Sigma)\ge \lambda_i(\Sigma_{-p})\ge \lambda_{i+1}(\Sigma).
\]
}
\INPUTS{$\Sigma=\begin{bmatrix}2&1&0\\1&1&0.5\\0&0.5&1\end{bmatrix}$.}
\DERIVATION{
\begin{align*}
\lambda(\Sigma)&\approx(3.0000,0.8090,0.1910).\\
\Sigma_{-3}&=\begin{bmatrix}2&1\\1&1\end{bmatrix},~
\lambda(\Sigma_{-3})\approx(2.6180,0.3820).\\
\text{Interlacing: }&3.0000\ge 2.6180\ge 0.8090,\\
&0.8090\ge 0.3820\ge 0.1910.\\
\text{Ky Fan }k=1:~&2.6180\le 3.0000;~k=2:~3.0000\le 3.8090.
\end{align*}
}
\RESULT{
Explained variance by top-$k$ decreases after feature removal and each
eigenvalue interlaces as claimed.}
\UNITCHECK{
Trace preserved: $\mathrm{tr}(\Sigma_{-p})\le \mathrm{tr}(\Sigma)$,
consistent with sums.}
\EDGECASES{
\begin{bullets}
\item If removed feature is uncorrelated zero-variance, spectra unchanged.
\end{bullets}
}
\ALTERNATE{
Use variational characterization of Ky Fan sums:
$\sum_{i=1}^k\lambda_i(\Sigma)=\max_{\dim U=k}\mathrm{tr}(P_U\Sigma)$.}
\VALIDATION{
\begin{bullets}
\item Numerical eigenvalues confirm monotonicity and interlacing.
\end{bullets}
}
\INTUITION{
Dropping a feature cannot increase the best achievable variance in any
$k$-dimensional projection.}
\CANONICAL{
\begin{bullets}
\item $\lambda_i(\Sigma)\ge \lambda_i(\Sigma_{-p})\ge \lambda_{i+1}(\Sigma)$.
\end{bullets}
}

\ProblemPage{4}{Narrative: Alice Drops a Sensor}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Interlacing predicts shifts in principal frequencies when a sensor fails.
\PROBLEM{
Alice monitors a structure with covariance $\Sigma\in\mathbb{R}^{4\times4}$.
Sensor 4 fails, yielding $\Sigma_{-4}$. Predict bounds on new principal
frequency $\lambda_1(\Sigma_{-4})$ and second $\lambda_2(\Sigma_{-4})$
in terms of $\lambda(\Sigma)$.}
\MODEL{
\[
\lambda_1(\Sigma)\ge \lambda_1(\Sigma_{-4})\ge \lambda_2(\Sigma),\quad
\lambda_2(\Sigma)\ge \lambda_2(\Sigma_{-4})\ge \lambda_3(\Sigma).
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $\Sigma$ Hermitian PSD; eigenvalues nonincreasing.
\item Principal deletion of one coordinate.
\end{bullets}
}
\varmapStart
\var{\Sigma}{Full covariance.}
\var{\Sigma_{-4}}{Principal submatrix after removing sensor 4.}
\var{\lambda_i}{Eigenvalues of $\Sigma$.}
\var{\mu_i}{Eigenvalues of $\Sigma_{-4}$.}
\varmapEnd
\WHICHFORMULA{
Cauchy interlacing (Formula 2).}
\GOVERN{
\[
\lambda_k(\Sigma)\ge \mu_k\ge \lambda_{k+1}(\Sigma),~k=1,2,3.
\]
}
\INPUTS{$\lambda(\Sigma)=(10,6,3,1)$ (given by prior analysis).}
\DERIVATION{
\begin{align*}
\text{Bounds: }&10\ge \mu_1\ge 6,\quad 6\ge \mu_2\ge 3.\\
\text{Thus }&\mu_1\in[6,10],\ \mu_2\in[3,6].\\
\text{If }&\mu_1=8\text{ observed, then }8\ge \mu_2\ge 3\text{ and }
\mu_2\le 6.\\
\text{Trace drop: }&\mathrm{tr}(\Sigma_{-4})\le 20.
\end{align*}
}
\RESULT{
Alice can guarantee principal frequency stays between 6 and 10, and the
second between 3 and 6.}
\UNITCHECK{
Units inherit from covariance (variance units); inequalities preserve
units.}
\EDGECASES{
\begin{bullets}
\item If sensor 4 aligned with smallest eigenvector, $\mu_k=\lambda_k$.
\end{bullets}
}
\ALTERNATE{
Compute actual $\mu_k$ from $\Sigma_{-4}$ once available and verify within
bounds.}
\VALIDATION{
\begin{bullets}
\item After failure, compute $\mu_1,\mu_2$ numerically and check intervals.
\end{bullets}
}
\INTUITION{
Losing one channel cannot make the top mode exceed the old top, nor drop
below the old next mode.}
\CANONICAL{
\begin{bullets}
\item $\mu_k\in[\lambda_{k+1},\lambda_k]$ for $k=1,2,3$.
\end{bullets}
}

\ProblemPage{5}{Narrative: Bob Adds a Regularizer}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Weyl monotonicity bounds spectral shifts after adding ridge $\alpha I$.
\PROBLEM{
Bob adds $\alpha I$ to a symmetric matrix $A$ for stability. Bound the
new eigenvalues and quantify the shift.}
\MODEL{
\[
B=\alpha I\succeq 0\Rightarrow \lambda_k(A)\le \lambda_k(A+\alpha I)
\le \lambda_k(A)+\alpha.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $A$ Hermitian; $\alpha\ge 0$.
\end{bullets}
}
\varmapStart
\var{A}{Original Hermitian matrix.}
\var{\alpha}{Nonnegative scalar.}
\var{\lambda_k}{Ordered eigenvalues.}
\varmapEnd
\WHICHFORMULA{
Weyl monotonicity (Formula 4).}
\GOVERN{
\[
\lambda_k(A+\alpha I)=\lambda_k(A)+\alpha.
\]
}
\INPUTS{$A$ arbitrary; choose $\alpha=0.2$.}
\DERIVATION{
\begin{align*}
\text{Since }&I\text{ commutes with }A,\ A+\alpha I\text{ shares eigenvectors.}\\
\lambda_k(A+\alpha I)&=\lambda_k(A)+\alpha.\\
\text{Bounds }&\lambda_k(A)\le \lambda_k(A)+\alpha\le \lambda_k(A)+\alpha.
\end{align*}
}
\RESULT{
Exact shift by $\alpha$ for all $k$; Weyl bounds are tight equalities.}
\UNITCHECK{
Shifted by scalar $\alpha$; consistent with units of $A$.}
\EDGECASES{
\begin{bullets}
\item $\alpha=0$ gives identity case.
\item Negative $\alpha$ invalidates PSD assumption; use general Weyl bounds.
\end{bullets}
}
\ALTERNATE{
For general PSD $B$, use $\|B\|_2$ to cap the shift.}
\VALIDATION{
\begin{bullets}
\item Numerically verify with random $A$ and compare eigenvalues.
\end{bullets}
}
\INTUITION{
Uniform elevation raises all levels by the same amount.}
\CANONICAL{
\begin{bullets}
\item $\lambda_k(A+\alpha I)=\lambda_k(A)+\alpha$.
\end{bullets}
}

\ProblemPage{6}{Expectation Puzzle: Random Rank-One PSD Update}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Bound expected top eigenvalue after a random unit vector update.
\PROBLEM{
Let $A=A^*$ with $\lambda_1\ge\cdots\ge\lambda_n$. Let $u$ be uniform on
the unit sphere and consider $A+\beta uu^*$ with $\beta\ge 0$. Show
$\lambda_1(A)\le \mathbb{E}\,\lambda_1(A+\beta uu^*)\le \lambda_1(A)+\beta$.}
\MODEL{
\[
B=\beta uu^*\succeq 0,\ \lambda_1(B)=\beta.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $u$ independent of $A$; $\|u\|=1$; $\beta\ge 0$.
\end{bullets}
}
\varmapStart
\var{A}{Hermitian base.}
\var{u}{Uniform unit vector.}
\var{\beta}{Nonnegative scalar.}
\var{\lambda_1}{Largest eigenvalue.}
\varmapEnd
\WHICHFORMULA{
Weyl monotonicity (Formula 4).}
\GOVERN{
\[
\lambda_1(A)\le \lambda_1(A+\beta uu^*)\le \lambda_1(A)+\beta.
\]
}
\INPUTS{Parameters $\beta\ge 0$; distribution of $u$.}
\DERIVATION{
\begin{align*}
\text{Pointwise: }&B\succeq 0\Rightarrow \lambda_1(A)\le \lambda_1(A+B)
\le \lambda_1(A)+\lambda_1(B)=\lambda_1(A)+\beta.\\
\text{Take }&\mathbb{E}\text{ over }u\text{ to obtain the same bounds by
monotonicity of expectation.}
\end{align*}
}
\RESULT{
$\lambda_1(A)\le \mathbb{E}\,\lambda_1(A+\beta uu^*)\le \lambda_1(A)+\beta$.}
\UNITCHECK{
Bounds are scalars; expectation preserves ordering.}
\EDGECASES{
\begin{bullets}
\item If $u$ aligns with top eigenvector, equality at upper bound occurs.
\item If $u$ orthogonal to top eigenspace, shift may be smaller.
\end{bullets}
}
\ALTERNATE{
Use secular equation for rank-one updates to derive exact distributional
properties in low dimensions.}
\VALIDATION{
\begin{bullets}
\item Monte Carlo with fixed seed demonstrates mean within bounds.
\end{bullets}
}
\INTUITION{
A PSD rank-one bump can lift the top level by at most its height $\beta$.}
\CANONICAL{
\begin{bullets}
\item Pointwise Weyl implies expected bound immediately.
\end{bullets}
}

\ProblemPage{7}{Proof-Style: Loewner Order Implies Eigenvalue Order}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Show $A\preceq C$ implies $\lambda_k(A)\le \lambda_k(C)$ for all $k$.
\PROBLEM{
Given Hermitian $A,C$ with $C-A\succeq 0$, prove componentwise eigenvalue
monotonicity.}
\MODEL{
\[
C=A+B,\quad B\succeq 0\Rightarrow \lambda_k(A)\le \lambda_k(C).
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $A,C$ Hermitian; $B=C-A\succeq 0$.
\end{bullets}
}
\varmapStart
\var{A,C}{Hermitian matrices.}
\var{B}{PSD difference.}
\var{\lambda_k}{Eigenvalues in nonincreasing order.}
\varmapEnd
\WHICHFORMULA{
Weyl monotonicity (Formula 4) lower bound.}
\GOVERN{
\[
\lambda_k(C)=\lambda_k(A+B)\ge \lambda_k(A).
\]
}
\INPUTS{Abstract, no numerics needed.}
\DERIVATION{
\begin{align*}
\lambda_k(C)&=\max_{\dim U=k}\ \min_{\|x\|=1,x\in U}x^*(A+B)x\\
&\ge \max_{\dim U=k}\ \min_{\|x\|=1,x\in U}x^*Ax=\lambda_k(A).
\end{align*}
}
\RESULT{
$\lambda_k(A)\le \lambda_k(C)$ for all $k$.}
\UNITCHECK{
Ordering is index-wise; dimensions equal.}
\EDGECASES{
\begin{bullets}
\item Equalities for $B$ annihilating eigen-subspaces.
\end{bullets}
}
\ALTERNATE{
Use variational inequalities on projectors:
$\mathrm{tr}(P_U C)\ge \mathrm{tr}(P_U A)$ for all $k$-dim projectors.}
\VALIDATION{
\begin{bullets}
\item Verify numerically on random $A$ and $B\succeq 0$.
\end{bullets}
}
\INTUITION{
Adding a nonnegative form cannot decrease any extremal value.}
\CANONICAL{
\begin{bullets}
\item Loewner order induces eigenvalue-wise order.
\end{bullets}
}

\ProblemPage{8}{Proof-Style: Interlacing via Compression Projectors}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Generalize interlacing to orthogonal compression $P^*AP$.
\PROBLEM{
Let $P\in\mathbb{C}^{n\times m}$ with $P^*P=I_m$. Prove that eigenvalues
of $P^*AP$ interlace those of $A$.}
\MODEL{
\[
\lambda_k(A)\ge \lambda_k(P^*AP)\ge \lambda_{k+n-m}(A).
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $A$ Hermitian; $P$ column-orthonormal.
\end{bullets}
}
\varmapStart
\var{A}{Hermitian matrix.}
\var{P}{Partial isometry with $P^*P=I_m$.}
\var{\lambda_k}{Ordered eigenvalues.}
\var{m}{Compressed dimension.}
\varmapEnd
\WHICHFORMULA{
Cauchy--Poincar\'e separation (Formula 2).}
\GOVERN{
\[
\lambda_k(A)\ge \lambda_k(P^*AP)\ge \lambda_{k+n-m}(A).
\]
}
\INPUTS{Abstract proof.}
\DERIVATION{
\begin{align*}
\lambda_k(P^*AP)&=\max_{\dim U=k}\ \min_{\|y\|=1,y\in U}y^*P^*APy\\
&=\max_{\dim U=k}\ \min_{\|y\|=1,y\in U}(Py)^*A(Py)\\
&\le \max_{\dim V=k}\ \min_{\|x\|=1,x\in V}x^*Ax=\lambda_k(A),\\
\lambda_k(P^*AP)&=\min_{\dim W=m-k+1}\ \max_{\|y\|=1,y\in W}(Py)^*A(Py)\\
&\ge \min_{\dim Z=n-k+1}\ \max_{\|x\|=1,x\in Z}x^*Ax\\
&=\lambda_{k+n-m}(A).
\end{align*}
}
\RESULT{
Interlacing proved for any orthogonal compression.}
\UNITCHECK{
Index shifts respect $m<n$ and $k\in\{1,\dots,m\}$.}
\EDGECASES{
\begin{bullets}
\item If $P$ spans an invariant subspace, equalities occur. 
\end{bullets}
}
\ALTERNATE{
Use Cauchy interlacing of characteristic polynomials via
Cauchy interlacing theorem on symmetric matrices.}
\VALIDATION{
\begin{bullets}
\item Numerically test random $P$ with $P^*P=I_m$.
\end{bullets}
}
\INTUITION{
Compressing to a subspace cannot produce new extremes outside the parent
range at corresponding indices.}
\CANONICAL{
\begin{bullets}
\item Same inequalities as principal submatrices with general $P$.
\end{bullets}
}

\ProblemPage{9}{Combo: Rayleigh Quotients and Interlacing in Graphs}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Vertex deletion in a graph Laplacian yields interlacing of eigenvalues.
\PROBLEM{
Let $L$ be the Laplacian of a simple graph, and $L_{-v}$ the Laplacian
after deleting vertex $v$. Show interlacing for their spectra.}
\MODEL{
\[
L=L^*\succeq 0,\quad L_{-v}=P^*LP,\quad P^*P=I_{n-1}.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $L$ Hermitian PSD.
\item $L_{-v}$ principal submatrix corresponding to $V\setminus\{v\}$.
\end{bullets}
}
\varmapStart
\var{L}{Graph Laplacian.}
\var{L_{-v}}{Principal submatrix after deleting $v$.}
\var{\lambda_i}{Eigenvalues nondecreasing: $0=\lambda_n(L)$ in some orders.}
\var{n}{Number of vertices.}
\varmapEnd
\WHICHFORMULA{
Cauchy interlacing (Formula 2).}
\GOVERN{
\[
\lambda_k(L)\ge \lambda_k(L_{-v})\ge \lambda_{k+1}(L),\ k=1,\dots,n-1.
\]
}
\INPUTS{Example: path graph on 3 vertices.}
\DERIVATION{
\begin{align*}
L&=\begin{bmatrix}1&-1&0\\-1&2&-1\\0&-1&1\end{bmatrix},~
\lambda(L)\approx(2.0000,1.0000,0.0000).\\
L_{-2}&=\begin{bmatrix}1&0\\0&1\end{bmatrix},~
\lambda(L_{-2})=(1,1).\\
\text{Check: }&2\ge 1\ge 1,\quad 1\ge 1\ge 0.
\end{align*}
}
\RESULT{
Interlacing holds for Laplacians under vertex deletion.}
\UNITCHECK{
PSD and trace decrease preserved.}
\EDGECASES{
\begin{bullets}
\item For disconnected graphs, multiplicity of zero changes by at most one.
\end{bullets}
}
\ALTERNATE{
Use Rayleigh quotient $x^*Lx=\sum_{(i,j)}(x_i-x_j)^2$ and compression.}
\VALIDATION{
\begin{bullets}
\item Compute spectra and verify inequalities numerically.
\end{bullets}
}
\INTUITION{
Removing a node cannot create more variation than existed.}
\CANONICAL{
\begin{bullets}
\item Same interlacing inequalities specialized to Laplacians.
\end{bullets}
}

\ProblemPage{10}{Combo: Gershgorin vs. Weyl Bounds}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Compare Weyl bounds to Gershgorin intervals for top eigenvalue of $A+B$.
\PROBLEM{
Given Hermitian $A,B$, show Weyl upper bound $\lambda_1(A+B)\le
\lambda_1(A)+\lambda_1(B)$ improves over Gershgorin when off-diagonals
are not aligned. Verify on an example.}
\MODEL{
\[
\lambda_1(A+B)\le \lambda_1(A)+\lambda_1(B).
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $A,B$ Hermitian.
\end{bullets}
}
\varmapStart
\var{A,B}{Hermitian matrices.}
\var{\lambda_1}{Largest eigenvalue.}
\var{G}{Gershgorin interval bound.}
\varmapEnd
\WHICHFORMULA{
Weyl inequality upper bound (Formula 3).}
\GOVERN{
\[
\lambda_1(A+B)\le \lambda_1(A)+\lambda_1(B).
\]
}
\INPUTS{$A=\begin{bmatrix}1&1&0\\1&1&0\\0&0&0\end{bmatrix}$,
$B=\begin{bmatrix}1&-1&0\\-1&1&0\\0&0&0\end{bmatrix}$.}
\DERIVATION{
\begin{align*}
\lambda(A)&=(2,0,0),\ \lambda(B)=(2,0,0).\\
A+B&=\begin{bmatrix}2&0&0\\0&2&0\\0&0&0\end{bmatrix},\
\lambda(A+B)=(2,2,0).\\
\text{Weyl: }&\lambda_1(A+B)=2\le 2+2=4.\\
\text{Gershgorin: }&\text{Row sums give discs centered at }2\text{ with
radius }0,\\
&\text{so }\lambda_1(A+B)\le 2\ \text{(exact)}.\\
\text{Modify }&B'=\begin{bmatrix}2&0.9&0\\0.9&2&0\\0&0&0\end{bmatrix},\\
\lambda_1(B')&\approx 2.9,\ \lambda_1(A)+\lambda_1(B')\approx 4.9,\\
\lambda_1(A+B')&\le 4.9,\ \text{Gershgorin gives }2+0.9=2.9.
\end{align*}
}
\RESULT{
Weyl provides dimension-free upper bound; Gershgorin may be tighter for
diagonally dominant sums but can be looser in other regimes.}
\UNITCHECK{
Bounds compare scalars; inputs are Hermitian.}
\EDGECASES{
\begin{bullets}
\item If $A,B$ commute and are diagonal, Weyl bound equals exact value.
\end{bullets}
}
\ALTERNATE{
Use spectral norm $\|A+B\|_2\le \|A\|_2+\|B\|_2$; equals Weyl with $k=1$.}
\VALIDATION{
\begin{bullets}
\item Numerically compute eigenvalues to compare bounds.
\end{bullets}
}
\INTUITION{
Weyl sees best-case alignment across all directions; Gershgorin uses row
sums, sometimes sharper, sometimes weaker.}
\CANONICAL{
\begin{bullets}
\item $\lambda_1(A+B)\le \lambda_1(A)+\lambda_1(B)$.
\end{bullets}
}

\section{Coding Demonstrations}
\CodeDemoPage{Verify Cauchy Interlacing Numerically}
\PROBLEM{
Generate random symmetric $A$, form principal submatrix $M$, compute
eigenvalues sorted decreasing, and assert interlacing. Implements
Formula 2.}
\API{
\begin{bullets}
\item \inlinecode{def read_input(s) -> int} — parse size $n$.
\item \inlinecode{def solve_case(n) -> bool} — check interlacing.
\item \inlinecode{def validate() -> None} — run fixed tests.
\item \inlinecode{def main() -> None} — orchestrate.
\end{bullets}
}
\INPUTS{
Integer $n\ge 3$; random seed fixed for determinism.}
\OUTPUTS{
Boolean assertion pass; prints example spectra.}
\FORMULA{
\[
\lambda_k(A)\ge \mu_k\ge \lambda_{k+1}(A),\quad k=1,\dots,n-1.
\]
}
\textbf{SOLUTION A — From Scratch (Power Iteration + Deflation)}
\begin{codepy}
import numpy as np

def power_iter(A, iters=200):
    np.random.seed(0)
    n = A.shape[0]
    x = np.random.randn(n)
    x = x / np.linalg.norm(x)
    v = 0.0
    for _ in range(iters):
        x = A @ x
        x = x / np.linalg.norm(x)
        v = float(x @ (A @ x))
    return v, x

def top_k_eigs(A, k):
    A = A.copy().astype(float)
    vals = []
    vecs = []
    for _ in range(k):
        v, x = power_iter(A, iters=300)
        vals.append(v)
        vecs.append(x)
        A = A - v * np.outer(x, x)
    return np.array(vals), np.array(vecs).T

def eigh_sorted(A):
    w, _ = np.linalg.eigh(A)
    return w[::-1]

def read_input(s):
    return int(s.strip())

def solve_case(n):
    np.random.seed(1)
    G = np.random.randn(n, n)
    A = (G + G.T) / 2.0
    M = A[:-1, :-1]
    lam = eigh_sorted(A)
    mu = eigh_sorted(M)
    ok = True
    for k in range(n - 1):
        ok = ok and (lam[k] + 1e-8 >= mu[k] >= lam[k + 1] - 1e-8)
    return ok, lam, mu

def validate():
    ok, lam, mu = solve_case(6)
    assert ok
    vals3, _ = top_k_eigs(np.diag([5, 3, 1]), 2)
    assert np.allclose(vals3, [5, 3], atol=1e-2)

def main():
    validate()
    ok, lam, mu = solve_case(5)
    print("ok:", ok)
    print("lam(A):", np.round(lam, 4))
    print("mu(M):", np.round(mu, 4))

if __name__ == "__main__":
    main()
\end{codepy}
\textbf{SOLUTION B — Library-Based (Direct Eigensolver)}
\begin{codepy}
import numpy as np

def read_input(s):
    return int(s.strip())

def solve_case(n):
    np.random.seed(2)
    G = np.random.randn(n, n)
    A = (G + G.T) / 2.0
    M = A[1:, 1:]
    lam = np.linalg.eigvalsh(A)[::-1]
    mu = np.linalg.eigvalsh(M)[::-1]
    ok = np.all(lam[:-1] + 1e-10 >= mu) and np.all(mu + 1e-10 >= lam[1:])
    return ok, lam, mu

def validate():
    ok, lam, mu = solve_case(7)
    assert ok
    assert len(lam) == 7 and len(mu) == 6

def main():
    validate()
    ok, lam, mu = solve_case(5)
    print("ok:", ok)
    print("lam(A):", np.round(lam, 4))
    print("mu(M):", np.round(mu, 4))

if __name__ == "__main__":
    main()
\end{codepy}
\COMPLEXITY{
From-scratch: power iteration with $k$ deflations, time
$\mathcal{O}(k\,n^2\,T)$, space $\mathcal{O}(n^2)$. Library: eigvalsh
time $\mathcal{O}(n^3)$, space $\mathcal{O}(n^2)$.}
\FAILMODES{
\begin{bullets}
\item Degenerate top eigenvalues slow power iteration convergence; mitigate
with deflation and restarts.
\item Numerical tolerance for interlacing checks must allow roundoff.
\end{bullets}
}
\STABILITY{
\begin{bullets}
\item Power iteration sensitive to spectral gaps; increase iterations.
\item Use symmetric structure to keep computations stable.
\end{bullets}
}
\VALIDATION{
\begin{bullets}
\item Cross-check from-scratch top-$k$ results on diagonals.
\item Assert interlacing inequalities with small tolerances.
\end{bullets}
}
\RESULT{
Both implementations confirm interlacing on random tests.}
\EXPLANATION{
Interlacing is evaluated by computing spectra of $A$ and its principal
submatrix $M$ and verifying $\lambda_k(A)\ge \mu_k\ge \lambda_{k+1}(A)$.}
\EXTENSION{
Vectorize checks for all principal minors or general $P$ compressions.}

\CodeDemoPage{Verify Weyl Inequalities Numerically}
\PROBLEM{
Generate random symmetric $A,B$, compute spectra, and verify Weyl
inequalities for all valid $(i,j)$. Also check monotonicity when $B$ is
PSD.}
\API{
\begin{bullets}
\item \inlinecode{def read_input(s) -> int} — parse size $n$.
\item \inlinecode{def solve_case(n) -> bool} — verify all inequalities.
\item \inlinecode{def validate() -> None} — run deterministic tests.
\item \inlinecode{def main() -> None} — orchestrate.
\end{bullets}
}
\INPUTS{
Integer $n\ge 2$; fixed seeds for reproducibility.}
\OUTPUTS{
Boolean assertion pass; prints worst slack.}
\FORMULA{
\[
\begin{aligned}
\lambda_{i+j-1}(A+B)&\le \lambda_i(A)+\lambda_j(B),\\
\lambda_{i+j-n}(A+B)&\ge \lambda_i(A)+\lambda_j(B).
\end{aligned}
\]
}
\textbf{SOLUTION A — From Scratch (Index Scans + PSD Case)}
\begin{codepy}
import numpy as np

def read_input(s):
    return int(s.strip())

def spectra(A):
    w = np.linalg.eigvalsh(A)
    return w[::-1]

def solve_case(n):
    np.random.seed(3)
    GA = np.random.randn(n, n); GB = np.random.randn(n, n)
    A = (GA + GA.T) / 2.0
    B = (GB + GB.T) / 2.0
    lamA, lamB = spectra(A), spectra(B)
    lamS = spectra(A + B)
    ok = True
    worst_up = -1e9; worst_low = 1e9
    for i in range(1, n + 1):
        for j in range(1, n + 1):
            if i + j - 1 <= n:
                up = lamS[i + j - 2] - (lamA[i - 1] + lamB[j - 1])
                worst_up = max(worst_up, up)
                ok = ok and (up <= 1e-8)
            if i + j - n >= 1:
                lo = lamS[i + j - n - 1] - (lamA[i - 1] + lamB[j - 1])
                worst_low = min(worst_low, lo)
                ok = ok and (lo >= -1e-8)
    np.random.seed(4)
    G = np.random.randn(n, n)
    C = (G @ G.T) / n
    lamC = spectra(C)
    lamAC = spectra(A + C)
    ok = ok and np.all(lamAC + 1e-8 >= lamA)
    return ok, worst_up, worst_low

def validate():
    ok, wu, wl = solve_case(5)
    assert ok
    assert wu <= 1e-8 and wl >= -1e-8

def main():
    validate()
    ok, wu, wl = solve_case(4)
    print("ok:", ok, "worst_up:", round(wu, 3), "worst_low:", round(wl, 3))

if __name__ == "__main__":
    main()
\end{codepy}
\textbf{SOLUTION B — Library-Based (Direct Eigensolver)}
\begin{codepy}
import numpy as np

def read_input(s):
    return int(s.strip())

def check_weyl(A, B):
    lamA = np.linalg.eigvalsh(A)[::-1]
    lamB = np.linalg.eigvalsh(B)[::-1]
    lamS = np.linalg.eigvalsh(A + B)[::-1]
    n = A.shape[0]; ok = True
    for i in range(1, n + 1):
        for j in range(1, n + 1):
            if i + j - 1 <= n:
                ok = ok and (lamS[i + j - 2] <= lamA[i - 1] + lamB[j - 1] + 1e-8)
            if i + j - n >= 1:
                ok = ok and (lamS[i + j - n - 1] + 1e-8 >=
                             lamA[i - 1] + lamB[j - 1])
    return ok

def validate():
    np.random.seed(5)
    A = np.random.randn(4, 4); A = (A + A.T) / 2.0
    B = np.random.randn(4, 4); B = (B + B.T) / 2.0
    assert check_weyl(A, B)

def main():
    validate()
    np.random.seed(6)
    A = np.random.randn(3, 3); A = (A + A.T) / 2.0
    B = np.random.randn(3, 3); B = (B + B.T) / 2.0
    print("ok:", check_weyl(A, B))

if __name__ == "__main__":
    main()
\end{codepy}
\COMPLEXITY{
Both variants rely on eigvalsh internally: time $\mathcal{O}(n^3)$,
space $\mathcal{O}(n^2)$.}
\FAILMODES{
\begin{bullets}
\item Sorting order mistakes invalidate index arithmetic.
\item Tolerances too tight may fail due to roundoff.
\end{bullets}
}
\STABILITY{
\begin{bullets}
\item Symmetry exploitation yields stable eigensolvers.
\item PSD construction as $G G^T$ avoids negative eigenvalues.
\end{bullets}
}
\VALIDATION{
\begin{bullets}
\item Check all index pairs $(i,j)$ and report worst slack.
\item Re-run with fixed seeds for reproducibility.
\end{bullets}
}
\RESULT{
All Weyl inequalities verified within $10^{-8}$ tolerances.}
\EXPLANATION{
Min-max and Rayleigh quotient additivity translate into index-wise
inequalities; code evaluates spectra and checks each inequality.}
\EXTENSION{
Test Ky Fan inequalities for partial sums of eigenvalues.}

\section{Applied Domains — Detailed End-to-End Scenarios}
\DomainPage{Machine Learning}
\SCENARIO{
Stability of PCA under feature removal: interlacing bounds principal
components and explained variance after dropping a feature.}
\ASSUMPTIONS{
\begin{bullets}
\item Data centered; sample covariance $\Sigma\succeq 0$.
\item Feature removal corresponds to principal submatrix.
\end{bullets}
}
\WHICHFORMULA{
Cauchy interlacing (Formula 2) and Ky Fan sums bound.}
\varmapStart
\var{X}{Data matrix $(n,d)$, centered.}
\var{\Sigma}{Covariance $X^\top X/n$.}
\var{\Sigma_{-p}}{Covariance after removing feature $p$.}
\var{\lambda_i}{Eigenvalues of $\Sigma$ and $\Sigma_{-p}$.}
\varmapEnd
\PIPELINE{
\begin{bullets}
\item Generate synthetic correlated features and compute $\Sigma$.
\item Delete one feature and compute $\Sigma_{-p}$.
\item Compare spectra and explained variance using interlacing.
\end{bullets}
}
\textbf{Implementation (From Scratch)}
\begin{codepy}
import numpy as np

def make_data(n=200, d=5, seed=0):
    np.random.seed(seed)
    Z = np.random.randn(n, d)
    C = 0.4 * np.ones((d, d)) + 0.6 * np.eye(d)
    X = Z @ np.linalg.cholesky(C).T
    X -= X.mean(axis=0, keepdims=True)
    return X

def cov(X):
    return (X.T @ X) / X.shape[0]

def spectra(S):
    return np.linalg.eigvalsh(S)[::-1]

def main():
    X = make_data()
    S = cov(X)
    S2 = cov(X[:, :-1])
    lam = spectra(S)
    mu = spectra(S2)
    k = 2
    ev_full = lam[:k].sum() / lam.sum()
    ev_drop = mu[:k].sum() / mu.sum()
    print("interlace:", lam[0] >= mu[0] >= lam[1])
    print("explained variance drop:", round(ev_drop <= ev_full, 1))

if __name__ == "__main__":
    main()
\end{codepy}
\textbf{Implementation (Library Version)}
\begin{codepy}
import numpy as np
from sklearn.decomposition import PCA

def main():
    np.random.seed(1)
    n, d = 300, 6
    X = np.random.randn(n, d)
    X -= X.mean(axis=0, keepdims=True)
    pca = PCA().fit(X)
    lam = (pca.singular_values_ ** 2) / n
    pca2 = PCA().fit(X[:, :-1])
    mu = (pca2.singular_values_ ** 2) / n
    print("interlace:", lam[0] >= mu[0] >= lam[1])

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{Top-$k$ explained variance ratio and interlacing inequalities.}
\INTERPRET{
Dropping a feature cannot increase top eigenvalues nor top-$k$ variance.}
\NEXTSTEPS{
Use interlacing to design robust feature selection strategies.}

\DomainPage{Quantitative Finance}
\SCENARIO{
Portfolio covariance update and risk bounds: Weyl bounds track top
eigenvalue under additive noise or regime change.}
\ASSUMPTIONS{
\begin{bullets}
\item Returns covariance $\Sigma$ estimated; perturbation $\Delta\succeq 0$.
\item Risk proxies via largest eigenvalue and partial sums.
\end{bullets}
}
\WHICHFORMULA{
Weyl monotonicity (Formula 4) and general Weyl inequalities (Formula 3).}
\varmapStart
\var{\Sigma}{Base covariance.}
\var{\Delta}{PSD shock or regularizer.}
\var{\lambda_1}{Largest eigenvalue (systemic risk).}
\var{w}{Portfolio weights.}
\varmapEnd
\PIPELINE{
\begin{bullets}
\item Simulate $\Sigma$ and PSD update $\Delta$.
\item Compute $\lambda_1(\Sigma)$ and $\lambda_1(\Sigma+\Delta)$.
\item Verify $\lambda_1$ increases by at most $\lambda_1(\Delta)$. 
\end{bullets}
}
\textbf{Implementation (Full Pipeline)}
\begin{codepy}
import numpy as np

def simulate(d=5, seed=0):
    np.random.seed(seed)
    A = np.random.randn(d, d)
    Sigma = A @ A.T / d
    U = np.random.randn(d, 1)
    Delta = 0.2 * (U @ U.T) / (U.T @ U)
    return Sigma, Delta

def lam1(X):
    return float(np.linalg.eigvalsh(X)[-1:][::-1][0])

def main():
    S, D = simulate()
    lS, lSD, lD = lam1(S), lam1(S + D), lam1(D)
    print("incr:", round(lSD - lS, 6), "bound:", round(lD, 6))
    print("ok:", lSD - lS <= lD + 1e-8)

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{Increase in $\lambda_1$ and bound $\lambda_1(\Delta)$.}
\INTERPRET{
Systemic risk (top mode) cannot increase by more than the shock strength.}
\NEXTSTEPS{
Use Ky Fan sums to bound sectoral risk aggregate changes.}

\DomainPage{Deep Learning}
\SCENARIO{
Hessian spectrum under mini-batch aggregation: Weyl bounds for sum of
per-batch Hessians bound curvature growth.}
\ASSUMPTIONS{
\begin{bullets}
\item Batch Hessians $H_b$ are Hermitian; total Hessian $\sum_b H_b$.
\item PSD regularization possible.
\end{bullets}
}
\WHICHFORMULA{
Weyl inequalities (Formula 3) and monotonicity with PSD terms (Formula 4).}
\varmapStart
\var{H_b}{Per-batch Hessian.}
\var{H}{Total Hessian $\sum_b H_b$.}
\var{\lambda_k}{Curvature eigenvalues.}
\var{b}{Batch index.}
\varmapEnd
\PIPELINE{
\begin{bullets}
\item Create synthetic symmetric $H_b$.
\item Sum to $H$; compare $\lambda_k(H)$ to Weyl sums of parts.
\item Check increase under adding $\alpha I$. 
\end{bullets}
}
\textbf{Implementation (End-to-End)}
\begin{codepy}
import numpy as np

def make_batches(d=10, B=3, seed=0):
    np.random.seed(seed)
    Hs = []
    for _ in range(B):
        G = np.random.randn(d, d)
        H = (G + G.T) / 2.0
        Hs.append(H)
    return Hs

def lam_sorted(X):
    return np.linalg.eigvalsh(X)[::-1]

def main():
    Hs = make_batches()
    H = sum(Hs)
    lamH = lam_sorted(H)
    lam1_sum = sum(lam_sorted(Hs[i])[0] for i in range(len(Hs)))
    print("lam1(H):", round(lamH[0], 4), "bound:", round(lam1_sum, 4))
    print("ok:", lamH[0] <= lam1_sum + 1e-8)

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{Top curvature vs. sum of top curvatures of batches.}
\INTERPRET{
Total curvature is controlled by curvatures of components.}
\NEXTSTEPS{
Use Ky Fan sums to bound total top-$k$ curvature.}

\DomainPage{Kaggle / Data Analytics}
\SCENARIO{
EDA spectrum stability under column removal and standardization.}
\ASSUMPTIONS{
\begin{bullets}
\item Numeric features; covariance spectra compared pre/post removal.
\end{bullets}
}
\WHICHFORMULA{
Cauchy interlacing (Formula 2).}
\varmapStart
\var{D}{DataFrame-like array $(n,d)$.}
\var{\Sigma}{Covariance matrix.}
\var{\Sigma_{-1}}{Covariance after removing first column.}
\var{\lambda_k}{Eigenvalues, decreasing.}
\varmapEnd
\PIPELINE{
\begin{bullets}
\item Simulate correlated features and standardize.
\item Compute covariance matrices and spectra.
\item Verify interlacing after dropping a column.
\end{bullets}
}
\textbf{Implementation (Complete EDA Pipeline)}
\begin{codepy}
import numpy as np

def create_df(n=300, d=4, seed=0):
    np.random.seed(seed)
    Z = np.random.randn(n, d)
    C = 0.3 * np.ones((d, d)) + 0.7 * np.eye(d)
    X = Z @ np.linalg.cholesky(C).T
    X -= X.mean(axis=0, keepdims=True)
    X /= X.std(axis=0, keepdims=True)
    return X

def cov(X):
    return (X.T @ X) / X.shape[0]

def main():
    X = create_df()
    S = cov(X)
    S2 = cov(X[:, 1:])
    lam = np.linalg.eigvalsh(S)[::-1]
    mu = np.linalg.eigvalsh(S2)[::-1]
    print("interlace:", lam[0] >= mu[0] >= lam[1],
          lam[1] >= mu[1] >= lam[2])

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{Interlacing checks for top two eigenvalues.}
\INTERPRET{
Dropping a feature reduces top eigenvalues within interlacing bounds.}
\NEXTSTEPS{
Assess robustness across different standardizations and subsets.}

\end{document}
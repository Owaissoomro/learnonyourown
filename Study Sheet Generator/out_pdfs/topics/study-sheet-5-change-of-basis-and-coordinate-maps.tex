% !TeX program = xelatex
\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{microtype,setspace,amsmath,amssymb,mathtools,amsthm,unicode-math}
\setstretch{1.05}
\setmainfont{Latin Modern Roman}
\setmonofont{Latin Modern Mono}
\setmathfont{Latin Modern Math}

\allowdisplaybreaks[4]
\setlength{\jot}{7pt}
\setlength{\emergencystretch}{8em}
\sloppy

\usepackage{xcolor,fancyhdr,enumitem,inconsolata,listings}
\pagestyle{fancy}\fancyhf{}\lhead{\nouppercase{\leftmark}}\rhead{\thepage}
\setlength{\headheight}{26pt}
\setlength{\parindent}{0pt}
\setlength{\parskip}{8pt plus 2pt minus 1pt}
\raggedbottom

\newenvironment{BreakableEquation}{\begin{equation}\begin{aligned}}{\end{aligned}\end{equation}}
\newenvironment{BreakableEquation*}{\begin{equation*}\begin{aligned}}{\end{aligned}\end{equation*}}
\newenvironment{tightalign}{\begingroup\small\allowdisplaybreaks\begin{align}}{\end{align}\endgroup}

\providecommand{\enumlistm}{enumitem}
\newenvironment{minted}[2][]{%
  \lstset{style=code,language=#2,#1}\begin{lstlisting}%
}{\end{lstlisting}}

\newcommand{\inputminted}[3][]{\begin{lstlisting}\end{lstlisting}}

\newlist{bullets}{itemize}{1}
\setlist[bullets]{label=--,leftmargin=1.6em,itemsep=2pt,topsep=4pt}

\newcommand{\varmapStart}{\textbf{VARIABLE MAPPING:}\par\begin{bullets}}
\newcommand{\var}[2]{\item $#1$ — #2}
\newcommand{\varmapEnd}{\end{bullets}}

\newcommand{\glossx}[6]{%
  \textbf{#1}\par
  \begin{bullets}
    \item \textbf{What:} #2
    \item \textbf{Why:} #3
    \item \textbf{How:} #4
    \item \textbf{ELI5:} #5
    \item \textbf{Pitfall/Example:} #6
  \end{bullets}
}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{claim}[theorem]{Claim}

\lstdefinestyle{code}{
  basicstyle=\ttfamily\footnotesize,
  backgroundcolor=\color{black!02},
  frame=single,
  numbers=left, numberstyle=\tiny, numbersep=8pt,
  breaklines=true, breakatwhitespace=true,
  tabsize=4, showstringspaces=false,
  upquote=true, keepspaces=true, columns=fullflexible,
  literate=
    {–}{{-}}1
    {—}{{-}}1
    {…}{{...}}1
    {≤}{{\ensuremath{\le}}}1
    {≥}{{\ensuremath{\ge}}}1
    {≠}{{\ensuremath{\ne}}}1
    {≈}{{\ensuremath{\approx}}}1
    {±}{{\ensuremath{\pm}}}1
    {→}{{\ensuremath{\to}}}1
    {←}{{\ensuremath{\leftarrow}}}1
    {∞}{{\ensuremath{\infty}}}1
    {√}{{\ensuremath{\sqrt{\ }}}}1
    {×}{{\ensuremath{\times}}}1
    {÷}{{\ensuremath{\div}}}1
}

\lstnewenvironment{codepy}[1][]%
  {\lstset{style=code,language=Python,#1}}%
  {}

\newcommand{\inlinecode}[1]{\lstinline[style=code]!#1!}

\newcommand{\LF}[2]{\par\noindent\textbf{#1:}~#2\par}
\newcommand{\WHAT}[1]{\LF{WHAT}{#1}}
\newcommand{\WHY}[1]{\LF{WHY}{#1}}
\newcommand{\HOW}[1]{\LF{HOW}{#1}}
\newcommand{\ELI}[1]{\LF{ELI5}{#1}}
\newcommand{\SCOPE}[1]{\LF{SCOPE}{#1}}
\newcommand{\CONFUSIONS}[1]{\LF{COMMON CONFUSIONS}{#1}}
\newcommand{\APPLICATIONS}[1]{\LF{APPLICATIONS}{#1}}
\newcommand{\FORMULA}[1]{\LF{FORMULA}{#1}}
\newcommand{\CANONICAL}[1]{\LF{CANONICAL FORM}{#1}}
\newcommand{\PRECONDS}[1]{\LF{PRECONDITIONS}{#1}}
\newcommand{\DERIVATION}[1]{\LF{DERIVATION}{#1}}
\newcommand{\EQUIV}[1]{\LF{EQUIVALENT FORMS}{#1}}
\newcommand{\LIMITS}[1]{\LF{LIMIT CASES}{#1}}
\newcommand{\INPUTS}[1]{\LF{INPUTS}{#1}}
\newcommand{\OUTPUTS}[1]{\LF{OUTPUTS}{#1}}
\newcommand{\RESULT}[1]{\LF{RESULT}{#1}}
\newcommand{\INTUITION}[1]{\LF{INTUITION}{#1}}
\newcommand{\PITFALLS}[1]{\LF{PITFALLS}{#1}}
\newcommand{\MODEL}[1]{\LF{CANONICAL MATH MODEL}{#1}}
\newcommand{\ASSUMPTIONS}[1]{\LF{ASSUMPTIONS}{#1}}
\newcommand{\WHICHFORMULA}[1]{\LF{WHICH FORMULA \& WHY}{#1}}
\newcommand{\GOVERN}[1]{\LF{GOVERNING EQUATION(S)}{#1}}
\newcommand{\UNITCHECK}[1]{\LF{UNIT CHECK}{#1}}
\newcommand{\EDGECASES}[1]{\LF{EDGE CASES}{#1}}
\newcommand{\ALTERNATE}[1]{\LF{ALTERNATE APPROACH (sketch)}{#1}}
\newcommand{\PROBLEM}[1]{\LF{PROBLEM}{#1}}
\newcommand{\API}[1]{\LF{API}{#1}}
\newcommand{\COMPLEXITY}[1]{\LF{COMPLEXITY}{#1}}
\newcommand{\FAILMODES}[1]{\LF{FAILURE MODES}{#1}}
\newcommand{\STABILITY}[1]{\LF{NUMERICAL STABILITY}{#1}}
\newcommand{\VALIDATION}[1]{\LF{VALIDATION}{#1}}
\newcommand{\EXPLANATION}[1]{\LF{EXPLANATION}{#1}}
\newcommand{\SCENARIO}[1]{\LF{SCENARIO}{#1}}
\newcommand{\PIPELINE}[1]{\LF{PIPELINE STEPS}{#1}}
\newcommand{\METRICS}[1]{\LF{METRICS}{#1}}
\newcommand{\INTERPRET}[1]{\LF{INTERPRETATION}{#1}}
\newcommand{\NEXTSTEPS}[1]{\LF{LIMITATIONS \& NEXT STEPS}{#1}}

\usepackage{titlesec}
\titleformat{\section}{\Large\bfseries}{\thesection}{0.6em}{}
\usepackage{etoolbox}
\pretocmd{\section}{\clearpage}{}{}

\newcommand{\FormulaPage}[2]{%
  \clearpage
  \section*{Formula #1 — #2}%
  \addcontentsline{toc}{section}{Formula #1 — #2}%
}
\newcommand{\ProblemPage}[2]{%
  \clearpage
  \subsection*{Problem #1: #2}%
  \addcontentsline{toc}{subsection}{Problem #1: #2}%
}
\newcommand{\CodeDemoPage}[1]{%
  \clearpage
  \subsection*{Coding Demo: #1}%
  \addcontentsline{toc}{subsection}{Coding Demo: #1}%
}
\newcommand{\DomainPage}[1]{%
  \clearpage
  \subsection*{#1 (End-to-End)}%
  \addcontentsline{toc}{subsection}{#1 (End-to-End)}%
}

\begin{document}
\title{Comprehensive Study Sheet — Change of Basis and Coordinate Maps}
\date{\today}
\maketitle
\tableofcontents
\clearpage

\section{Concept Overview}
\WHAT{
Let $V$ be a finite dimensional vector space over a field $\mathbb{F}$.
A basis $B=(b_1,\dots,b_n)$ is an ordered list of vectors that spans $V$
and is linearly independent. The coordinate map for $B$ is the unique
isomorphism $\Phi_B:V\to\mathbb{F}^n$ sending $v=\sum_{j=1}^n \alpha_j b_j$
to $[v]_B=(\alpha_1,\dots,\alpha_n)^\top$. For two bases $B$ and $C$ of
$V$, the change-of-basis matrix from $B$ to $C$, denoted
$M_{C\leftarrow B}\in\mathbb{F}^{n\times n}$, is defined by
$[v]_C=M_{C\leftarrow B}[v]_B$ for all $v\in V$.
}
\WHY{
Coordinates make abstract vectors computable. Changing basis reexpresses
the same geometric object in a new coordinate system. This is fundamental
for diagonalization, normal forms, numerical stability, PCA, tensor
calculus, and any setting where reparameterization simplifies structure.
}
\HOW{
1. Fix a basis $B$ and define $\Phi_B$. 2. For another basis $C$,
compose coordinate maps to relate $[v]_C$ and $[v]_B$. 3. Show linearity
to obtain a matrix $M_{C\leftarrow B}=\Phi_C\circ \Phi_B^{-1}$. 4. Use
this to transform vectors, matrices of linear maps, and Gram matrices.
}
\ELI{
Think of a vector as a song, and a basis as a language. Coordinates are
the lyrics written in that language. Changing basis is translating the
lyrics without changing the song.
}
\SCOPE{
Finite dimensional linear spaces over fields, with ordered bases. For
inner-product formulas, require positive definite forms. For orthogonal
changes, use real fields with $Q^\top Q=I$. Ill-conditioned bases can
cause numeric instability but not change algebraic truths.
}
\CONFUSIONS{
Coordinates vs. vectors: $[v]_B$ is not $v$. Change-of-basis matrices
map coordinate vectors, not geometric vectors. Similarity
$SAS^{-1}$ is a basis change for the same linear operator; congruence
$S^\top A S$ is a basis change for bilinear forms. Row vs. column vector
conventions must be consistent.
}
\APPLICATIONS{
\begin{bullets}
\item Mathematical foundations: diagonalization, Jordan form, spectral theory.
\item Computational modeling: preconditioning, PCA, whitening, SVD.
\item Physical interpretations: rotating frames, inertial tensors, tensors.
\item Statistical implications: covariance in new coordinates, decorrelation.
\end{bullets}
}

\textbf{ANALYTIC STRUCTURE.}
Linear, invertible maps between coordinate representations. Composition
chains via matrix multiplication. For inner products, congruence appears.
For operators, similarity appears. Orthogonal changes preserve Euclidean
norms and dot products.

\textbf{CANONICAL LINKS.}
Coordinate map isomorphism $\Phi_B$; vector transform $[v]_C=M_{C\leftarrow B}[v]_B$;
operator similarity $[T]_C=M_{C\leftarrow B}[T]_B M_{C\leftarrow B}^{-1}$;
bilinear form congruence $G_C=M^\top G_B M$ with $M=M_{C\leftarrow B}$.

\textbf{PROBLEM-TYPE RECOGNITION HEURISTICS.}
\begin{bullets}
\item Presence of two named bases $B$ and $C$ and a fixed vector or map.
\item Expressions like $SAS^{-1}$ or $S^\top A S$.
\item Requests for coordinates of vectors or matrices in a new basis.
\item Invariance questions: eigenvalues, determinant, total variance.
\end{bullets}

\textbf{SOLUTION STRATEGY BLUEPRINT.}
\begin{bullets}
\item Translate verbal basis data into basis matrices and coordinate maps.
\item Identify if situation is vector, operator, or bilinear form.
\item Use $M_{C\leftarrow B}$, similarity, or congruence accordingly.
\item Compute and simplify; check with $B=C$ limit and inverses.
\item Interpret: what structure did the basis reveal or simplify.
\end{bullets}

\textbf{CONCEPTUAL INVARIANTS.}
Geometric vector, linear map as an abstract operator, and bilinear form
are invariant objects; only their coordinate representations change.
Eigenvalues and characteristic polynomials are similarity invariants.
Inner products remain the same number under congruent representation.

\textbf{EDGE INTUITION.}
If $B=C$, $M_{C\leftarrow B}=I$. If $C$ approaches a linearly dependent
set, numerics blow up but algebraic identities still assert invertibility
is required. For orthogonal $M$, norms of coordinate vectors are
preserved: Euclidean geometry is unchanged.

\section{Glossary}
\glossx{Coordinate Map $\Phi_B$}{
Isomorphism $V\to\mathbb{F}^n$ sending $v=\sum \alpha_j b_j$ to
$[v]_B=(\alpha_j)_j$.
}{
Turns abstract vectors into coordinates, enabling computation and proofs.
}{
Choose $B$, write $v$ as linear combination, read off coefficients.
}{
A recipe that tells how much of each basis vector builds $v$.
}{
Forgets the geometry if confused with $v$ itself; $[v]_B$ depends on $B$.
}
\glossx{Change-of-Basis Matrix $M_{C\leftarrow B}$}{
Matrix mapping $[v]_B$ to $[v]_C$ for the same $v$.
}{
Relates coordinates under different bases; central in reparameterization.
}{
Columns are $[b_j]_C$. Compute $M_{C\leftarrow B}=C^{-1}B$ in a reference
basis with basis matrices $B,C$.
}{
Like converting prices between currencies with an exchange matrix.
}{
Mixing direction: $M_{B\leftarrow C}=M_{C\leftarrow B}^{-1}$, not transpose
unless orthogonal.
}
\glossx{Similarity Transformation}{
Conjugation $SAS^{-1}$ relating matrices of the same operator in two
bases.
}{
Shows invariants like eigenvalues do not depend on basis choice.
}{
Compute $S=M_{C\leftarrow B}$ and apply $A_C=S A_B S^{-1}$.
}{
Same machine described in two coordinate languages.
}{
Not for bilinear forms; use congruence $S^\top A S$ there.
}
\glossx{Gram Matrix $G_B$}{
Matrix of an inner product in basis $B$ with $(G_B)_{ij}=\langle b_i,b_j\rangle$.
}{
Encodes inner product numerically; transforms by congruence under
basis change.
}{
Compute $G_C=M^\top G_B M$ with $M=M_{C\leftarrow B}$.
}{
Scores of the same duet after retuning instruments.
}{
Do not confuse with operator matrices; use $S^\top A S$, not $SAS^{-1}$.
}

\section{Symbol Ledger}
\varmapStart
\var{V,W}{Finite dimensional vector spaces over field $\mathbb{F}$.}
\var{n,m}{Dimensions of $V$ and $W$ respectively.}
\var{B,C}{Ordered bases of $V$: $B=(b_1,\dots,b_n)$, $C=(c_1,\dots,c_n)$.}
\var{D,E}{Ordered bases of $W$: $D=(d_1,\dots,d_m)$, $E=(e_1,\dots,e_m)$.}
\var{\Phi_B}{Coordinate map $V\to\mathbb{F}^n$, $v\mapsto [v]_B$.}
\var{[v]_B}{Coordinate column vector of $v$ in basis $B$.}
\var{M_{C\leftarrow B}}{Change-of-basis matrix: $[v]_C=M_{C\leftarrow B}[v]_B$.}
\var{T}{Linear map $T:V\to W$.}
\var{[T]_{D\leftarrow B}}{Matrix of $T$ mapping $[v]_B\mapsto [T v]_D$.}
\var{G_B}{Gram matrix of an inner product in basis $B$.}
\var{A}{Matrix representation of a linear operator on $V$.}
\var{S}{Generic invertible matrix used for change of basis.}
\var{Q}{Orthogonal matrix with $Q^\top Q=I$.}
\var{B_{\mathrm{mat}}}{Basis matrix with columns $b_j$ in a reference basis.}
\var{C_{\mathrm{mat}}}{Basis matrix with columns $c_j$ in a reference basis.}
\var{I}{Identity matrix.}
\var{\lambda}{Eigenvalue.}
\var{\chi_A(t)}{Characteristic polynomial $\det(tI-A)$.}
\varmapEnd

\section{Formula Canon — One Formula Per Page}
\FormulaPage{1}{Coordinate Change: $[v]_C=M_{C\leftarrow B}[v]_B$}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Coordinate vectors of the same $v\in V$ in two bases $B$ and $C$ are
related by a unique invertible matrix $M_{C\leftarrow B}$ whose $j$-th
column is $[b_j]_C$.

\WHAT{
This equation computes the coordinates of a fixed vector $v$ in basis $C$
from its coordinates in basis $B$.
}
\WHY{
It is the foundational bridge between two coordinate descriptions. It
enables translating computations and is the building block for all
basis-dependent matrix transformations.
}
\FORMULA{
\[
[v]_C=M_{C\leftarrow B}[v]_B,\quad
M_{C\leftarrow B}=\bigl([b_1]_C\,\cdots\,[b_n]_C\bigr),\quad
M_{B\leftarrow C}=M_{C\leftarrow B}^{-1}.
\]
}
\CANONICAL{
Finite dimensional $V$ over $\mathbb{F}$. Ordered bases $B,C$.
Coordinates are column vectors. Matrix multiplication is over $\mathbb{F}$.
}
\PRECONDS{
\begin{bullets}
\item $B$ and $C$ are bases, hence $M_{C\leftarrow B}$ is invertible.
\item Consistent column-vector convention for coordinates.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
For a linear map $L:V\to V$, the $j$-th column of its matrix in basis
$C$ equals $[L(b_j)]_C$.
\end{lemma}
\begin{proof}
By linearity, $L\left(\sum_j \alpha_j b_j\right)=\sum_j \alpha_j L(b_j)$.
In coordinates, $[L(v)]_C=[L]_C [v]_B$ and for $v=b_k$, $[v]_B=e_k$,
hence the $k$-th column of $[L]_C$ is $[L(b_k)]_C$. \qedhere
\end{proof}

\DERIVATION{
\begin{align*}
\text{Step 1 (Definition):}\ & \Phi_B:V\to\mathbb{F}^n,\ v\mapsto [v]_B,\\
& \Phi_C:V\to\mathbb{F}^n,\ v\mapsto [v]_C.\\
\text{Step 2 (Identity factorization):}\ &
\Phi_C=\underbrace{\Phi_C\circ \Phi_B^{-1}}_{M_{C\leftarrow B}}\circ \Phi_B.\\
\text{Step 3 (Matrix form):}\ &
[v]_C=(\Phi_C\circ \Phi_B^{-1})([v]_B)=M_{C\leftarrow B}[v]_B.\\
\text{Step 4 (Columns):}\ &
M_{C\leftarrow B}=[I]_{C\leftarrow B},\ \text{so column }j=[b_j]_C.\\
\text{Step 5 (Inverse):}\ &
\Phi_B=\Phi_B\circ \Phi_C^{-1}\circ \Phi_C \Rightarrow
I=M_{B\leftarrow C}M_{C\leftarrow B}.
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Express $b_j$ in $C$-coordinates; stack as columns to get $M$.
\item Map $[v]_B$ to $[v]_C$ via multiplication.
\item Invert if needed to go from $C$ to $B$.
\end{bullets}
\EQUIV{
\begin{bullets}
\item Using basis matrices in a reference basis: $M_{C\leftarrow B}
=C_{\mathrm{mat}}^{-1}B_{\mathrm{mat}}$.
\item If $C=B$, then $M_{C\leftarrow B}=I$.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item If $B$ or $C$ is not a basis, $M_{C\leftarrow B}$ is singular and
the map is undefined.
\item If $M$ is orthogonal, then $\|[v]_C\|_2=\|[v]_B\|_2$.
\end{bullets}
}
\INPUTS{$B=(b_j)_{j=1}^n$, $C=(c_j)_{j=1}^n$, and $[v]_B\in\mathbb{F}^n$.}
\DERIVATION{
\begin{align*}
\text{Compute:}\ & [b_j]_C\ \text{for all }j,\ \ M=\bigl([b_j]_C\bigr).\\
\text{Then:}\ & [v]_C=M[v]_B.\\
\text{If needed:}\ & [v]_B=M^{-1}[v]_C.
\end{align*}
}
\RESULT{
A concrete $[v]_C$ and an invertible $M_{C\leftarrow B}$ relating the
two coordinate descriptions.
}
\UNITCHECK{
Dimension check: $M\in\mathbb{F}^{n\times n}$, $[v]_B\in\mathbb{F}^n$,
so $[v]_C\in\mathbb{F}^n$. For $B=C$, $M=I$.
}
\PITFALLS{
\begin{bullets}
\item Swapping the direction: $M_{B\leftarrow C}\neq M_{C\leftarrow B}$.
\item Using row vectors while multiplying on the left with column rules.
\item Ordering basis vectors inconsistently across computations.
\end{bullets}
}
\INTUITION{
$M_{C\leftarrow B}$ lists how each old basis vector looks in the new
language; then any sentence translates by the same dictionary.
}
\CANONICAL{
\begin{bullets}
\item Universal identity: $M_{C\leftarrow B}=\Phi_C\circ\Phi_B^{-1}$.
\item Inverse relation: $M_{B\leftarrow C}=(M_{C\leftarrow B})^{-1}$.
\end{bullets}
}

\FormulaPage{2}{Operators Under Basis Change: Similarity and Conjugation}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
For $T:V\to W$ and bases $B,C$ of $V$ and $D,E$ of $W$,
\[
[T]_{E\leftarrow C}=M_{E\leftarrow D}\,[T]_{D\leftarrow B}\,M_{B\leftarrow C}.
\]
When $V=W$ and $B,D$ and $C,E$ are respective bases, this becomes
similarity for the same operator in $V$.

\WHAT{
Computes the matrix of a linear map in new domain and codomain bases
given its matrix in old bases and the change-of-basis matrices.
}
\WHY{
Coordinates of outputs depend on the codomain basis; inputs on the domain
basis. This identity organizes the two changes around the same abstract
map and reveals similarity $A_C=S A_B S^{-1}$ for $V=W$.
}
\FORMULA{
\[
[T]_{E\leftarrow C}=M_{E\leftarrow D}\,[T]_{D\leftarrow B}\,M_{B\leftarrow C}.
\]
Special case $V=W$, $D=B$, $E=C$:
\[
[T]_C=M_{C\leftarrow B}\,[T]_B\,M_{C\leftarrow B}^{-1}.
\]
}
\CANONICAL{
Finite dimensional $V,W$ over $\mathbb{F}$. Ordered bases as stated.
Matrices multiply with conformable dimensions. $M_{E\leftarrow D}$ and
$M_{B\leftarrow C}$ are invertible.
}
\PRECONDS{
\begin{bullets}
\item $B,C$ are bases of $V$; $D,E$ are bases of $W$.
\item $[T]_{D\leftarrow B}$ is the matrix mapping $[v]_B\mapsto [Tv]_D$.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
Naturality of coordinate maps: $\Phi_E\circ T=\Psi\circ \Phi_C$ where
$\Psi$ is linear and represented by $[T]_{E\leftarrow C}$.
\end{lemma}
\begin{proof}
For any $v\in V$, $[Tv]_E$ depends linearly on $[v]_C$. The coordinate
map $\Phi_C$ is an isomorphism, so there is a unique linear $\Psi$ with
$[Tv]_E=\Psi([v]_C)$. By definition, $\Psi$ is multiplication by
$[T]_{E\leftarrow C}$. \qedhere
\end{proof}

\DERIVATION{
\begin{align*}
\text{Step 1 (Compose maps):}\ &
[v]_C=M_{C\leftarrow B}[v]_B,\quad [Tv]_E=M_{E\leftarrow D}[Tv]_D.\\
\text{Step 2 (Insert $[T]$):}\ &
[Tv]_D=[T]_{D\leftarrow B}[v]_B.\\
\text{Step 3 (Combine):}\ &
[Tv]_E=M_{E\leftarrow D}[T]_{D\leftarrow B}[v]_B.\\
\text{Step 4 (Eliminate $[v]_B$):}\ &
[v]_B=M_{B\leftarrow C}[v]_C.\\
\text{Step 5 (Conclude):}\ &
[Tv]_E=M_{E\leftarrow D}[T]_{D\leftarrow B}M_{B\leftarrow C}[v]_C.\\
& \Rightarrow [T]_{E\leftarrow C}
= M_{E\leftarrow D}[T]_{D\leftarrow B}M_{B\leftarrow C}.\\
\text{Special case:}\ &
[T]_C=M_{C\leftarrow B}[T]_B M_{B\leftarrow C}
=M_{C\leftarrow B}[T]_B M_{C\leftarrow B}^{-1}.
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Compute or identify $M_{E\leftarrow D}$ and $M_{B\leftarrow C}$.
\item Multiply to get the new matrix of $T$.
\item If $V=W$ with same old and new bases, use similarity.
\end{bullets}
\EQUIV{
\begin{bullets}
\item If $E=D$ and $C=B$, then $[T]_{D\leftarrow B}$ is unchanged.
\item If $M_{C\leftarrow B}$ is orthogonal, similarity preserves normality.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item Noninvertible $M$ invalidates change of basis.
\item Similarity does not apply to changing the bilinear form; use congruence.
\end{bullets}
}
\INPUTS{$[T]_{D\leftarrow B}$, $M_{E\leftarrow D}$, $M_{B\leftarrow C}$.}
\DERIVATION{
\begin{align*}
\text{Compute:}\ & X=M_{E\leftarrow D}[T]_{D\leftarrow B}.\\
\text{Then:}\ & [T]_{E\leftarrow C}=X M_{B\leftarrow C}.\\
\text{Special:}\ & [T]_C=M_{C\leftarrow B}[T]_B M_{C\leftarrow B}^{-1}.
\end{align*}
}
\RESULT{
The matrix of $T$ in the desired bases, showing how coordinates enter on
both the input and output sides.
}
\UNITCHECK{
Matrix sizes: $M_{E\leftarrow D}\in\mathbb{F}^{m\times m}$,
$[T]_{D\leftarrow B}\in\mathbb{F}^{m\times n}$,
$M_{B\leftarrow C}\in\mathbb{F}^{n\times n}$, product in $\mathbb{F}^{m\times n}$.
}
\PITFALLS{
\begin{bullets}
\item Reversing $M_{B\leftarrow C}$; it belongs on the right.
\item Confusing similarity ($SAS^{-1}$) with congruence ($S^\top A S$).
\end{bullets}
}
\INTUITION{
Change-of-basis at the input side reinterprets coordinates before $T$,
while change at the output side reexpresses the result.
}
\CANONICAL{
\begin{bullets}
\item Universal identity:
$[T]_{E\leftarrow C}=M_{E\leftarrow D}[T]_{D\leftarrow B}M_{B\leftarrow C}$.
\item Special similarity: $A_C=S A_B S^{-1}$ with $S=M_{C\leftarrow B}$.
\end{bullets}
}

\FormulaPage{3}{Bilinear Forms and Gram Matrices: Congruence}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Let $\langle\cdot,\cdot\rangle$ be an inner product on $V$ with Gram
matrix $G_B$ in basis $B$. For basis $C$,
\[
G_C=M_{C\leftarrow B}^\top\, G_B\, M_{C\leftarrow B}.
\]

\WHAT{
Computes the matrix of an inner product in a new basis from its matrix in
an old basis.
}
\WHY{
Inner products are scalar invariants; only coordinates change. The
formula ensures the numeric value $\langle v,w\rangle$ is basis
independent while its matrix representation updates correctly.
}
\FORMULA{
\[
G_C=M^\top G_B M,\quad M:=M_{C\leftarrow B},\qquad
\langle v,w\rangle=[v]_B^\top G_B [w]_B=[v]_C^\top G_C [w]_C.
\]
}
\CANONICAL{
$V$ finite dimensional over $\mathbb{R}$ or a field with symmetric
bilinear form. Ordered bases $B,C$. Coordinates are columns; transpose is
used for dual pairing.
}
\PRECONDS{
\begin{bullets}
\item $G_B$ is symmetric positive definite for inner products.
\item $M_{C\leftarrow B}$ is invertible.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
For any $v,w\in V$, $\langle v,w\rangle=[v]_B^\top G_B [w]_B$, where
$(G_B)_{ij}=\langle b_i,b_j\rangle$.
\end{lemma}
\begin{proof}
Write $v=\sum_i \alpha_i b_i$, $w=\sum_j \beta_j b_j$. By bilinearity,
$\langle v,w\rangle=\sum_{i,j}\alpha_i \beta_j \langle b_i,b_j\rangle
=[v]_B^\top G_B [w]_B$. \qedhere
\end{proof}

\DERIVATION{
\begin{align*}
\text{Step 1:}\ & [v]_B=M_{B\leftarrow C}[v]_C,\ [w]_B=M_{B\leftarrow C}[w]_C.\\
\text{Step 2:}\ & \langle v,w\rangle=[v]_B^\top G_B [w]_B.\\
\text{Step 3:}\ &
=[v]_C^\top M_{B\leftarrow C}^\top G_B M_{B\leftarrow C}[w]_C.\\
\text{Step 4:}\ & M_{B\leftarrow C}=M^{-1},\ \text{with }M=M_{C\leftarrow B}.\\
\text{Step 5:}\ &
=[v]_C^\top (M^{-\top} G_B M^{-1}) [w]_C.\\
\text{Step 6:}\ & \text{Set }G_C=M^\top G_B M\ \Rightarrow \langle v,w\rangle
=[v]_C^\top G_C [w]_C.
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Compute $M=M_{C\leftarrow B}$.
\item Form $G_C=M^\top G_B M$.
\item Use $[v]_C^\top G_C [w]_C$ for inner products in $C$-coordinates.
\end{bullets}
\EQUIV{
\begin{bullets}
\item If $G_B=I$ and $M$ orthogonal, then $G_C=I$.
\item For forms, congruence $S^\top A S$ replaces similarity $SAS^{-1}$.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item If the bilinear form is indefinite, $G_C$ preserves signature by
Sylvester law under congruence.
\item Singular $M$ is invalid; bases must be invertible.
\end{bullets}
}
\INPUTS{$G_B$, $M_{C\leftarrow B}$. Optionally $[v]_C,[w]_C$ to evaluate.}
\DERIVATION{
\begin{align*}
\text{Compute:}\ & G_C=M^\top G_B M.\\
\text{Evaluate:}\ & \langle v,w\rangle=[v]_C^\top G_C [w]_C.
\end{align*}
}
\RESULT{
A new Gram matrix $G_C$ representing the same inner product in basis $C$.
}
\UNITCHECK{
Sizes: $G_B,G_C\in\mathbb{F}^{n\times n}$; $M\in\mathbb{F}^{n\times n}$.
Symmetry: if $G_B^\top=G_B$, then $G_C^\top=G_C$.
}
\PITFALLS{
\begin{bullets}
\item Using similarity instead of congruence.
\item Forgetting to transpose: $M^\top G_B M$, not $M G_B M^{-1}$.
\end{bullets}
}
\INTUITION{
The bilinear form eats two coordinate vectors. When you reparameterize,
each coordinate vector is transformed, hence two $M$ factors and one
transpose.
}
\CANONICAL{
\begin{bullets}
\item Congruence transport: $G_C=M^\top G_B M$.
\item Invariance: $\langle v,w\rangle$ independent of basis.
\end{bullets}
}

\FormulaPage{4}{Basis Matrices in a Reference Basis: $M_{C\leftarrow B}=C^{-1}B$}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Fix a reference basis $E$ of $V\cong\mathbb{F}^n$. Let
$B_{\mathrm{mat}}=[b_1\ \cdots\ b_n]$ and
$C_{\mathrm{mat}}=[c_1\ \cdots\ c_n]$ be the basis matrices whose columns
are the coordinates of $b_j,c_j$ in $E$. Then
\[
M_{C\leftarrow B}=C_{\mathrm{mat}}^{-1} B_{\mathrm{mat}},\quad
[v]_B=B_{\mathrm{mat}}^{-1}[v]_E.
\]

\WHAT{
Connects abstract change-of-basis to concrete coordinates in a fixed
ambient basis, often the standard basis of $\mathbb{F}^n$.
}
\WHY{
In computations, basis vectors are given by their coordinates in a
reference basis. This formula provides a direct recipe to build
change-of-basis matrices and coordinates.
}
\FORMULA{
\[
[v]_E=B_{\mathrm{mat}}[v]_B,\quad [v]_B=B_{\mathrm{mat}}^{-1}[v]_E,\quad
M_{C\leftarrow B}=C_{\mathrm{mat}}^{-1}B_{\mathrm{mat}}.
\]
}
\CANONICAL{
$V=\mathbb{F}^n$ with fixed reference basis $E$. Columns of the basis
matrices are coordinates in $E$. Invertibility of basis matrices holds.
}
\PRECONDS{
\begin{bullets}
\item $B_{\mathrm{mat}}$ and $C_{\mathrm{mat}}$ are invertible.
\item Column convention: $[v]_E=B_{\mathrm{mat}}[v]_B$.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
If $B$ is a basis, then every $v$ decomposes uniquely as
$[v]_E=B_{\mathrm{mat}}[v]_B$.
\end{lemma}
\begin{proof}
Uniqueness follows from linear independence of $B$ and the fact that
columns of $B_{\mathrm{mat}}$ span $\mathbb{F}^n$. Thus the linear map
$[v]_B\mapsto [v]_E$ is multiplication by $B_{\mathrm{mat}}$, which is
invertible. \qedhere
\end{proof}

\DERIVATION{
\begin{align*}
\text{Step 1:}\ & v=\sum_j \alpha_j b_j \Rightarrow [v]_E=\sum_j \alpha_j [b_j]_E.\\
\text{Step 2:}\ & [v]_E=\bigl([b_1]_E\ \cdots\ [b_n]_E\bigr)[v]_B
=B_{\mathrm{mat}}[v]_B.\\
\text{Step 3:}\ & [v]_B=B_{\mathrm{mat}}^{-1}[v]_E.\\
\text{Step 4:}\ & [v]_C=C_{\mathrm{mat}}^{-1}[v]_E
=C_{\mathrm{mat}}^{-1}B_{\mathrm{mat}}[v]_B.\\
\text{Step 5:}\ & \Rightarrow M_{C\leftarrow B}=C_{\mathrm{mat}}^{-1}B_{\mathrm{mat}}.
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Form $B_{\mathrm{mat}}$ and $C_{\mathrm{mat}}$ from given columns.
\item Compute inverses and $M=C^{-1}B$; then transform coordinates.
\end{bullets}
\EQUIV{
\begin{bullets}
\item If $C=E$, then $M_{E\leftarrow B}=I^{-1}B=B$ and $[v]_B=B^{-1}[v]_E$.
\item If $B=E$, then $M_{C\leftarrow E}=C^{-1}$ and $[v]_C=C^{-1}[v]_E$.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item Ill-conditioned basis matrices cause numerical instability in
inversion; algebra remains valid.
\item If $\det B_{\mathrm{mat}}=0$, $B$ is not a basis.
\end{bullets}
}
\INPUTS{$B_{\mathrm{mat}}$, $C_{\mathrm{mat}}$, and optionally $[v]_E$.}
\DERIVATION{
\begin{align*}
\text{Compute:}\ & M=C_{\mathrm{mat}}^{-1}B_{\mathrm{mat}}.\\
\text{Then:}\ & [v]_B=B_{\mathrm{mat}}^{-1}[v]_E,\quad
[v]_C=M[v]_B.
\end{align*}
}
\RESULT{
Concrete transformation matrices directly from given columns in a
reference basis; explicit coordinates in any basis.
}
\UNITCHECK{
All matrices are $n\times n$; compositions and inverses are conformable.
}
\PITFALLS{
\begin{bullets}
\item Mixing row and column conventions.
\item Forgetting that columns, not rows, hold basis vectors.
\end{bullets}
}
\INTUITION{
Stack the basis vectors as columns; solving $B\alpha=v$ gives the recipe
$\alpha=B^{-1}v$ to get coordinates.
}
\CANONICAL{
\begin{bullets}
\item Basis matrix column-stacking identity.
\item $M_{C\leftarrow B}=C^{-1}B$ in a fixed ambient basis.
\end{bullets}
}

\section{10 Exhaustive Problems and Solutions}
\ProblemPage{1}{Concrete $2\times 2$ Change of Basis and Vector Coordinates}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Compute $M_{C\leftarrow B}$, its inverse, and coordinates of $v$ in both
bases for $V=\mathbb{R}^2$ with standard ambient basis $E$.

\PROBLEM{
Let $B=((1,1)^\top,(1,-1)^\top)$ and $C=((2,0)^\top,(0,1)^\top)$. For
$v=(3,1)^\top$, find $M_{C\leftarrow B}$, $M_{B\leftarrow C}$,
$[v]_B$, and $[v]_C$. Verify $[v]_C=M_{C\leftarrow B}[v]_B$.
}
\MODEL{
\[
B_{\mathrm{mat}}=\begin{bmatrix}1&1\\ 1&-1\end{bmatrix},\quad
C_{\mathrm{mat}}=\begin{bmatrix}2&0\\ 0&1\end{bmatrix},\quad
M_{C\leftarrow B}=C_{\mathrm{mat}}^{-1}B_{\mathrm{mat}}.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Column convention; $B$ and $C$ are bases of $\mathbb{R}^2$.
\end{bullets}
}
\varmapStart
\var{B_{\mathrm{mat}}}{Basis matrix for $B$ in $E$.}
\var{C_{\mathrm{mat}}}{Basis matrix for $C$ in $E$.}
\var{M_{C\leftarrow B}}{Change-of-basis matrix from $B$ to $C$.}
\var{v}{Vector $(3,1)^\top$.}
\varmapEnd
\WHICHFORMULA{
Use Formula 4: $M_{C\leftarrow B}=C^{-1}B$ and $[v]_B=B^{-1}[v]_E$,
then Formula 1 to verify $[v]_C=M_{C\leftarrow B}[v]_B$.
}
\GOVERN{
\[
[v]_E=B_{\mathrm{mat}}[v]_B,\quad [v]_B=B_{\mathrm{mat}}^{-1}[v]_E,\quad
M_{C\leftarrow B}=C_{\mathrm{mat}}^{-1}B_{\mathrm{mat}}.
\]
}
\INPUTS{$B_{\mathrm{mat}}, C_{\mathrm{mat}}, v=(3,1)^\top$.}
\DERIVATION{
\begin{align*}
B_{\mathrm{mat}}^{-1}&=\frac{1}{-1-1}\begin{bmatrix}-1&-1\\ -1&1\end{bmatrix}
=\tfrac{1}{-2}\begin{bmatrix}-1&-1\\ -1&1\end{bmatrix}
=\begin{bmatrix}\tfrac12&\tfrac12\\ \tfrac12&-\tfrac12\end{bmatrix}.\\
C_{\mathrm{mat}}^{-1}&=\begin{bmatrix}\tfrac12&0\\ 0&1\end{bmatrix}.\\
M_{C\leftarrow B}&=C^{-1}B=
\begin{bmatrix}\tfrac12&0\\ 0&1\end{bmatrix}
\begin{bmatrix}1&1\\ 1&-1\end{bmatrix}
=\begin{bmatrix}\tfrac12&\tfrac12\\ 1&-1\end{bmatrix}.\\
[v]_B&=B^{-1}v=
\begin{bmatrix}\tfrac12&\tfrac12\\ \tfrac12&-\tfrac12\end{bmatrix}
\begin{bmatrix}3\\ 1\end{bmatrix}
=\begin{bmatrix}2\\ 1\end{bmatrix}.\\
[v]_C&=C^{-1}v=
\begin{bmatrix}\tfrac12&0\\ 0&1\end{bmatrix}
\begin{bmatrix}3\\ 1\end{bmatrix}
=\begin{bmatrix}\tfrac32\\ 1\end{bmatrix}.\\
M_{C\leftarrow B}[v]_B&=
\begin{bmatrix}\tfrac12&\tfrac12\\ 1&-1\end{bmatrix}
\begin{bmatrix}2\\ 1\end{bmatrix}
=\begin{bmatrix}\tfrac32\\ 1\end{bmatrix}=[v]_C.
\end{align*}
}
\RESULT{
$M_{C\leftarrow B}=\begin{bmatrix}\tfrac12&\tfrac12\\ 1&-1\end{bmatrix}$,
$M_{B\leftarrow C}=M^{-1}=\begin{bmatrix}1&\tfrac12\\ 1&-\tfrac12\end{bmatrix}$,
$[v]_B=(2,1)^\top$, $[v]_C=(\tfrac32,1)^\top$.
}
\UNITCHECK{
All matrices are $2\times 2$, coordinates are $2\times 1$. Identity
$M_{C\leftarrow B}M_{B\leftarrow C}=I$ holds.
}
\EDGECASES{
\begin{bullets}
\item If $C$ had dependent columns, $C^{-1}$ would not exist.
\item If $v=0$, then both $[v]_B$ and $[v]_C$ are zero.
\end{bullets}
}
\ALTERNATE{
Compute $[b_1]_C$ and $[b_2]_C$ directly by solving $C x=b_j$ and stack.
}
\VALIDATION{
\begin{bullets}
\item Verify $B M_{B\leftarrow C}=C$ numerically.
\item Check $C[v]_C=B[v]_B=v$.
\end{bullets}
}
\INTUITION{
The columns of $M$ tell how $B$ appears when seen using $C$, and $v$
just inherits the same translation rule.
}
\CANONICAL{
\begin{bullets}
\item $M_{C\leftarrow B}=C^{-1}B$.
\item $[v]_C=M_{C\leftarrow B}[v]_B$.
\end{bullets}
}

\ProblemPage{2}{Operator Matrix Under Basis Change}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Transform the matrix of a linear operator from basis $B$ to basis $C$.

\PROBLEM{
In $V=\mathbb{R}^2$, let $B=((1,0)^\top,(1,1)^\top)$ and $C$ be standard
$E$. The matrix of $T$ in basis $B$ is
$[T]_B=\begin{bmatrix}2&1\\ 0&3\end{bmatrix}$. Find $[T]_C$.
}
\MODEL{
\[
M_{C\leftarrow B}=C^{-1}B=I\cdot B,\quad [T]_C=M_{C\leftarrow B}[T]_B M_{C\leftarrow B}^{-1}.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Column convention; $B$ is a basis; $C=E$ is the standard basis.
\end{bullets}
}
\varmapStart
\var{B_{\mathrm{mat}}}{Matrix $\begin{bmatrix}1&1\\ 0&1\end{bmatrix}$.}
\var{M}{Shorthand for $M_{C\leftarrow B}=B_{\mathrm{mat}}$.}
\var{[T]_B}{Matrix in basis $B$.}
\varmapEnd
\WHICHFORMULA{
Use Formula 2 in the special case $V=W$, $C=E$:
$[T]_C=M [T]_B M^{-1}$.
}
\GOVERN{
\[
[T]_C=M [T]_B M^{-1},\quad M=\begin{bmatrix}1&1\\ 0&1\end{bmatrix}.
\]
}
\INPUTS{$[T]_B$, $M=\begin{bmatrix}1&1\\ 0&1\end{bmatrix}$.}
\DERIVATION{
\begin{align*}
M^{-1}&=\begin{bmatrix}1&-1\\ 0&1\end{bmatrix}.\\
M[T]_B&=\begin{bmatrix}1&1\\ 0&1\end{bmatrix}
\begin{bmatrix}2&1\\ 0&3\end{bmatrix}
=\begin{bmatrix}2&4\\ 0&3\end{bmatrix}.\\
[T]_C&=(M[T]_B)M^{-1}
=\begin{bmatrix}2&4\\ 0&3\end{bmatrix}
\begin{bmatrix}1&-1\\ 0&1\end{bmatrix}
=\begin{bmatrix}2&2\\ 0&3\end{bmatrix}.
\end{align*}
}
\RESULT{
$[T]_C=\begin{bmatrix}2&2\\ 0&3\end{bmatrix}$.
}
\UNITCHECK{
All matrices $2\times 2$. If $B=C$, we would get $[T]_C=[T]_B$.
}
\EDGECASES{
\begin{bullets}
\item If $B$ became ill-conditioned, numerical inversion of $M$ is unstable.
\item If $[T]_B$ is diagonal and $M$ is eigenbasis, $[T]_C$ is diagonal.
\end{bullets}
}
\ALTERNATE{
Compute $T$ on $E$-basis vectors by first expressing $e_j$ in $B$, then
applying $[T]_B$ and converting back.
}
\VALIDATION{
\begin{bullets}
\item Check similarity invariants: $\operatorname{tr}=5$, $\det=6$ match.
\item Confirm $M^{-1}[T]_C M=[T]_B$.
\end{bullets}
}
\INTUITION{
Changing basis wraps $[T]_B$ by $M$ to reinterpret inputs and outputs.
}
\CANONICAL{
\begin{bullets}
\item $[T]_C=M [T]_B M^{-1}$ for the same operator.
\end{bullets}
}

\ProblemPage{3}{Gram Matrix Transport and Inner Product Consistency}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Transport the Gram matrix and verify inner products match.

\PROBLEM{
On $\mathbb{R}^2$, define $\langle x,y\rangle=x^\top G_E y$ with
$G_E=\begin{bmatrix}2&1\\ 1&3\end{bmatrix}$. Let
$B=((1,0)^\top,(1,1)^\top)$. Compute $G_B$, and verify for
$v=(2,1)^\top$, $w=(1,2)^\top$ that
$[v]_E^\top G_E [w]_E=[v]_B^\top G_B [w]_B$.
}
\MODEL{
\[
B_{\mathrm{mat}}=\begin{bmatrix}1&1\\ 0&1\end{bmatrix},\quad
G_B=B_{\mathrm{mat}}^\top G_E B_{\mathrm{mat}}.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Symmetric positive definite $G_E$; $B$ is a basis.
\end{bullets}
}
\varmapStart
\var{G_E}{Gram in $E$.}
\var{G_B}{Gram in $B$.}
\var{B_{\mathrm{mat}}}{Basis matrix for $B$.}
\var{v,w}{Vectors in $\mathbb{R}^2$.}
\varmapEnd
\WHICHFORMULA{
Use Formula 3: $G_B=B^\top G_E B$. Then coordinates via
$[v]_B=B^{-1}[v]_E$.
}
\GOVERN{
\[
G_B=B^\top G_E B,\quad [\cdot]_B=B^{-1}[\cdot]_E.
\]
}
\INPUTS{$G_E$, $B_{\mathrm{mat}}$, $v$, $w$.}
\DERIVATION{
\begin{align*}
B^{-1}&=\begin{bmatrix}1&-1\\ 0&1\end{bmatrix}.\\
G_B&=B^\top G_E B
=\begin{bmatrix}1&0\\ 1&1\end{bmatrix}
\begin{bmatrix}2&1\\ 1&3\end{bmatrix}
\begin{bmatrix}1&1\\ 0&1\end{bmatrix}\\
&=\begin{bmatrix}2&3\\ 3&6\end{bmatrix}.\\
[v]_B&=B^{-1}v=\begin{bmatrix}1&-1\\ 0&1\end{bmatrix}\begin{bmatrix}2\\1\end{bmatrix}
=\begin{bmatrix}1\\1\end{bmatrix}.\\
[w]_B&=B^{-1}w=\begin{bmatrix}1&-1\\ 0&1\end{bmatrix}\begin{bmatrix}1\\2\end{bmatrix}
=\begin{bmatrix}-1\\2\end{bmatrix}.\\
\text{Left side:}\ & v^\top G_E w=
\begin{bmatrix}2&1\end{bmatrix}
\begin{bmatrix}2&1\\ 1&3\end{bmatrix}
\begin{bmatrix}1\\2\end{bmatrix}\\
&=\begin{bmatrix}5&5\end{bmatrix}\begin{bmatrix}1\\2\end{bmatrix}=15.\\
\text{Right side:}\ & [v]_B^\top G_B [w]_B=
\begin{bmatrix}1&1\end{bmatrix}
\begin{bmatrix}2&3\\ 3&6\end{bmatrix}
\begin{bmatrix}-1\\2\end{bmatrix}\\
&=\begin{bmatrix}5&9\end{bmatrix}\begin{bmatrix}-1\\ 2\end{bmatrix}=15.
\end{align*}
}
\RESULT{
$G_B=\begin{bmatrix}2&3\\ 3&6\end{bmatrix}$ and the inner product value
$15$ matches in both bases.
}
\UNITCHECK{
Symmetry preserved: $G_B^\top=G_B$. Scalar output in both calculations.
}
\EDGECASES{
\begin{bullets}
\item If $B$ is orthonormal w.r.t. $G_E$, then $G_B=I$.
\item If $G_E$ is indefinite, signature is preserved by congruence.
\end{bullets}
}
\ALTERNATE{
Compute $\langle b_i,b_j\rangle$ directly to fill $G_B$ entrywise.
}
\VALIDATION{
\begin{bullets}
\item Check that $B^{-\top} G_B B^{-1}=G_E$.
\item Randomly pick $x,y$ and verify numerically.
\end{bullets}
}
\INTUITION{
Two coordinate changes for the two arguments of the inner product yield
the two $B$-factors and a transpose.
}
\CANONICAL{
\begin{bullets}
\item $G_B=B^\top G_E B$.
\item Invariance $\langle v,w\rangle$ under basis change.
\end{bullets}
}

\ProblemPage{4}{Narrative: Alice and Bob Translate a Vector}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Recover the change-of-basis matrix from two coordinate reports.

\PROBLEM{
Alice uses basis $B$ and reports $[v]_B=(2,-1,3)^\top$. Bob uses basis
$C$ and reports the same vector as $[v]_C=(1,0,2)^\top$. Find a possible
$M_{C\leftarrow B}$ consistent with these reports, and describe the
constraints it must satisfy.
}
\MODEL{
\[
[v]_C=M_{C\leftarrow B}[v]_B.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $M_{C\leftarrow B}$ is invertible but otherwise unknown.
\end{bullets}
}
\varmapStart
\var{[v]_B}{Alice coordinates $(2,-1,3)^\top$.}
\var{[v]_C}{Bob coordinates $(1,0,2)^\top$.}
\var{M}{Unknown $3\times 3$ change-of-basis matrix.}
\varmapEnd
\WHICHFORMULA{
Use Formula 1 directly: one linear equation in the entries of $M$.
}
\GOVERN{
\[
\begin{bmatrix}1\\ 0\\ 2\end{bmatrix}
=
M\begin{bmatrix}2\\ -1\\ 3\end{bmatrix}.
\]
}
\INPUTS{$[v]_B,(2,-1,3)^\top$ and $[v]_C,(1,0,2)^\top$.}
\DERIVATION{
\begin{align*}
M&=\begin{bmatrix}m_{11}&m_{12}&m_{13}\\ m_{21}&m_{22}&m_{23}\\
m_{31}&m_{32}&m_{33}\end{bmatrix}.\\
M\begin{bmatrix}2\\ -1\\ 3\end{bmatrix}
&=\begin{bmatrix}2m_{11}-m_{12}+3m_{13}\\
2m_{21}-m_{22}+3m_{23}\\
2m_{31}-m_{32}+3m_{33}\end{bmatrix}
=\begin{bmatrix}1\\ 0\\ 2\end{bmatrix}.\\
\text{Thus: }& 2m_{11}-m_{12}+3m_{13}=1,\\
& 2m_{21}-m_{22}+3m_{23}=0,\\
& 2m_{31}-m_{32}+3m_{33}=2.
\end{align*}
}
\RESULT{
Any invertible $M$ whose rows satisfy the three affine constraints will
work. One simple choice is to set free variables to convenient values.
For example, choose
$m_{12}=0,m_{13}=0,m_{22}=0,m_{23}=0,m_{32}=0,m_{33}=0$ giving
$m_{11}=\tfrac12,m_{21}=0,m_{31}=1$ and then complete $M$ with columns
that preserve invertibility, e.g.,
\[
M=\begin{bmatrix}\tfrac12&0&0\\ 0&1&0\\ 1&0&0\end{bmatrix}
\]
satisfies the equation and is invertible.
}
\UNITCHECK{
$M$ is $3\times 3$; output is $3\times 1$. Determinant of the example
matrix equals $\tfrac12\neq 0$.
}
\EDGECASES{
\begin{bullets}
\item If multiple vector reports are known, $M$ can be uniquely pinned.
\item If only one vector is known, infinitely many $M$ exist.
\end{bullets}
}
\ALTERNATE{
If two or three linearly independent vector pairs are given, stack them
as columns to solve $M=Y X^{-1}$ with $X=[v^{(i)}]_B$ and $Y=[v^{(i)}]_C$.
}
\VALIDATION{
\begin{bullets}
\item Multiply $M[v]_B$ and confirm $[v]_C$.
\item Check $\det M\neq 0$.
\end{bullets}
}
\INTUITION{
One translation pair teaches one column or row relation. Full knowledge
requires enough examples to reconstruct the dictionary.
}
\CANONICAL{
\begin{bullets}
\item $M$ maps coordinates; constraints are linear in entries of $M$.
\end{bullets}
}

\ProblemPage{5}{Narrative: Hidden Diagonal via Similarity}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Recover a similarity transform from two representations of the same
operator.

\PROBLEM{
Alice sees $T$ as diagonal in basis $B$:
$[T]_B=\operatorname{diag}(1,4,9)$. Bob sees $[T]_C=A$, where
$A=\begin{bmatrix}1&1&0\\ 0&4&2\\ 0&0&9\end{bmatrix}$. Find a change of
basis $M_{C\leftarrow B}$ mapping $[T]_B$ to $A$ and describe Bob's
eigenvectors in terms of Alice's basis.
}
\MODEL{
\[
A=M[T]_B M^{-1},\quad M:=M_{C\leftarrow B}.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Distinct eigenvalues imply a unique eigenbasis up to scaling.
\end{bullets}
}
\varmapStart
\var{[T]_B}{Diagonal matrix $\operatorname{diag}(1,4,9)$.}
\var{A}{Upper triangular with same diagonal.}
\var{M}{Similarity matrix $M_{C\leftarrow B}$.}
\varmapEnd
\WHICHFORMULA{
Use Formula 2 special case: similarity $A=M [T]_B M^{-1}$.
}
\GOVERN{
\[
A M = M [T]_B.
\]
}
\INPUTS{$[T]_B$, $A$.}
\DERIVATION{
\begin{align*}
A M&=M[T]_B \ \Rightarrow\ \text{columns }m_j \text{ satisfy } A m_j=
\lambda_j m_j,\\
&\text{with }\lambda_j\in\{1,4,9\}.\\
\text{Thus }& m_j\ \text{are eigenvectors of }A.\\
\text{Solve: }& (A-I)m_1=0\Rightarrow m_1=(1,0,0)^\top.\\
& (A-4I)m_2=0\Rightarrow m_2=(1,3,0)^\top\ \text{up to scale}.\\
& (A-9I)m_3=0\Rightarrow m_3=(0,1,2.5)^\top\ \text{up to scale}.
\end{align*}
}
\RESULT{
A valid choice is $M=[m_1\ m_2\ m_3]$ with eigenvectors as columns. Bob's
eigenvectors are exactly the columns of $M$; Alice's eigenvectors are the
basis vectors $b_j$. The relation is $c_j=m_j$ while $b_j$ maps to $c_j$
via $M$.
}
\UNITCHECK{
$A M=M [T]_B$ holds by construction; $M$ invertible since eigenvectors
are linearly independent for distinct eigenvalues.
}
\EDGECASES{
\begin{bullets}
\item Repeated eigenvalues allow nonunique eigenbases; $M$ is not unique.
\item Defective matrices are not diagonalizable; no such $M$ exists.
\end{bullets}
}
\ALTERNATE{
Solve $M$ from $A=M\Lambda M^{-1}$ using Jordan chains if $A$ is not
diagonalizable; here it is and $M$ is the eigenvector matrix.
}
\VALIDATION{
\begin{bullets}
\item Compute $M^{-1} A M$ and confirm it equals $\operatorname{diag}(1,4,9)$.
\item Check eigenpairs numerically.
\end{bullets}
}
\INTUITION{
Bob describes the same stretching by looking along different axes; the
eigenvectors form the dictionary columns.
}
\CANONICAL{
\begin{bullets}
\item $A$ and $\operatorname{diag}$ are similar with eigenbasis columns.
\end{bullets}
}

\ProblemPage{6}{Expectation Puzzle: Coordinates in a Random Orthogonal Basis}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Show the expected squared coordinate of a fixed unit vector under a
random orthogonal change of basis equals $1/n$.

\PROBLEM{
Let $u\in\mathbb{R}^n$ with $\|u\|_2=1$. Choose an orthonormal basis $C$
uniformly at random by sampling $Q\in O(n)$ uniformly and taking columns
as $C$. Show $\mathbb{E}\bigl([u]_C(1)^2\bigr)=1/n$.
}
\MODEL{
\[
[u]_C=Q^\top u,\quad \text{so } [u]_C(1)=e_1^\top Q^\top u=(Q e_1)^\top u.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Haar-uniform measure on $O(n)$ is left and right invariant.
\end{bullets}
}
\varmapStart
\var{u}{Unit vector in $\mathbb{R}^n$.}
\var{Q}{Random orthogonal matrix.}
\var{[u]_C}{Coordinates $Q^\top u$.}
\varmapEnd
\WHICHFORMULA{
Use orthogonal change of basis invariance (Formula 1 with $M=Q^\top$) and
symmetry of Haar measure.
}
\GOVERN{
\[
\sum_{i=1}^n [u]_C(i)^2=\|[u]_C\|_2^2=\|u\|_2^2=1.
\]
}
\INPUTS{$u$, dimension $n$.}
\DERIVATION{
\begin{align*}
\text{By symmetry:}\ & \mathbb{E}([u]_C(i)^2)\ \text{is the same for all }i.\\
\text{Sum:}\ & \sum_{i=1}^n \mathbb{E}([u]_C(i)^2)
=\mathbb{E}\left(\sum_{i=1}^n [u]_C(i)^2\right)=\mathbb{E}(1)=1.\\
\text{Hence:}\ & n \cdot \mathbb{E}([u]_C(1)^2)=1\Rightarrow
\mathbb{E}([u]_C(1)^2)=\tfrac{1}{n}.
\end{align*}
}
\RESULT{
$\mathbb{E}\bigl([u]_C(1)^2\bigr)=1/n$.
}
\UNITCHECK{
Dimensionless scalar between $0$ and $1$; sum over $i$ equals $1$.
}
\EDGECASES{
\begin{bullets}
\item For $n=1$, value is $1$.
\item Concentration increases with $n$; typical coordinate magnitude
is $\mathcal{O}(n^{-1/2})$.
\end{bullets}
}
\ALTERNATE{
Fix $u=e_1$ by rotational invariance; then $[u]_C(1)$ is the first entry
of a random unit vector, whose squared entries are exchangeable.
}
\VALIDATION{
\begin{bullets}
\item Monte Carlo simulation with random orthogonal matrices reproduces $1/n$.
\end{bullets}
}
\INTUITION{
Randomly rotating a unit vector spreads its energy evenly among axes.
}
\CANONICAL{
\begin{bullets}
\item Orthogonal changes preserve Euclidean norm and symmetry.
\end{bullets}
}

\ProblemPage{7}{Proof: Columns of $M_{C\leftarrow B}$ and Invertibility}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Prove columns of $M_{C\leftarrow B}$ are $[b_j]_C$ and that
$M_{B\leftarrow C}=M_{C\leftarrow B}^{-1}$.

\PROBLEM{
Establish the structural properties of the change-of-basis matrix using
only linearity and coordinate map isomorphisms.
}
\MODEL{
\[
M_{C\leftarrow B}=\Phi_C\circ \Phi_B^{-1}.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $B,C$ are bases; coordinate maps are isomorphisms.
\end{bullets}
}
\varmapStart
\var{b_j}{$j$-th vector of basis $B$.}
\var{M}{Change-of-basis $M_{C\leftarrow B}$.}
\varmapEnd
\WHICHFORMULA{
Formula 1 definition $[v]_C=M [v]_B$ plus the lemma that columns are
images of basis vectors.
}
\GOVERN{
\[
M e_j=[b_j]_C,\quad M_{B\leftarrow C}M_{C\leftarrow B}=I.
\]
}
\INPUTS{$B,C$ as bases.}
\DERIVATION{
\begin{align*}
\text{Column $j$:}\ & e_j=[b_j]_B,\ \ M e_j=[b_j]_C.\\
\text{Invertibility:}\ &
\Phi_B=\Phi_B\circ \Phi_C^{-1}\circ \Phi_C\\
&\Rightarrow I=M_{B\leftarrow C}M_{C\leftarrow B}.\\
&\text{Similarly }I=M_{C\leftarrow B}M_{B\leftarrow C}.
\end{align*}
}
\RESULT{
Columns of $M$ are $[b_j]_C$. $M$ is invertible with inverse
$M_{B\leftarrow C}$.
}
\UNITCHECK{
Matrix equation $MM^{-1}=I$; columns have length $n$.
}
\EDGECASES{
\begin{bullets}
\item If $B=C$, $M=I$ and columns equal $e_j$.
\end{bullets}
}
\ALTERNATE{
Use that $M=[I]_{C\leftarrow B}$ and apply the supporting lemma from
Formula 1 for $L=I$.
}
\VALIDATION{
\begin{bullets}
\item Compute with explicit $B,C$ to confirm numerically.
\end{bullets}
}
\INTUITION{
The identity map sends $b_j$ to itself; its coordinates in $C$ form the
columns.
}
\CANONICAL{
\begin{bullets}
\item $M_{C\leftarrow B}=\bigl([b_j]_C\bigr)$ and $M^{-1}=M_{B\leftarrow C}$.
\end{bullets}
}

\ProblemPage{8}{Proof: Similarity Law for Operator Matrices}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Prove $[T]_C=M_{C\leftarrow B}[T]_B M_{C\leftarrow B}^{-1}$.

\PROBLEM{
Using only coordinate maps, show that matrices of the same linear
operator in two bases are related by conjugation.
}
\MODEL{
\[
\Phi_C\circ T\circ \Phi_C^{-1}=M_{C\leftarrow B}
\left(\Phi_B\circ T\circ \Phi_B^{-1}\right)M_{C\leftarrow B}^{-1}.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $B,C$ are bases; $\Phi_B,\Phi_C$ are isomorphisms.
\end{bullets}
}
\varmapStart
\var{T}{Linear operator on $V$.}
\var{M}{Change-of-basis $M_{C\leftarrow B}$.}
\varmapEnd
\WHICHFORMULA{
Formula 2. We rederive it as a proof.
}
\GOVERN{
\[
[T]_C=\Phi_C\circ T\circ \Phi_C^{-1},\quad [T]_B=\Phi_B\circ T\circ \Phi_B^{-1}.
\]
}
\INPUTS{$\Phi_B,\Phi_C$, and $T$.}
\DERIVATION{
\begin{align*}
[T]_C&=\Phi_C\circ T\circ \Phi_C^{-1}\\
&=\Phi_C\circ \underbrace{\Phi_B^{-1}\circ \Phi_B}_{I}\circ T
\circ \underbrace{\Phi_B^{-1}\circ \Phi_B}_{I}\circ \Phi_C^{-1}\\
&=(\Phi_C\circ \Phi_B^{-1})\circ (\Phi_B\circ T\circ \Phi_B^{-1})
\circ (\Phi_B\circ \Phi_C^{-1})\\
&=M_{C\leftarrow B}\,[T]_B\,M_{B\leftarrow C}\\
&=M_{C\leftarrow B}\,[T]_B\,M_{C\leftarrow B}^{-1}.
\end{align*}
}
\RESULT{
$[T]_C=M_{C\leftarrow B}[T]_B M_{C\leftarrow B}^{-1}$.
}
\UNITCHECK{
All maps are $n\times n$; conjugation preserves size.
}
\EDGECASES{
\begin{bullets}
\item If $B=C$, $M=I$ and $[T]_C=[T]_B$.
\end{bullets}
}
\ALTERNATE{
Show that for each $v$, $[Tv]_C=M [T]_B M^{-1}[v]_C$; equality of linear
maps implies equality of matrices.
}
\VALIDATION{
\begin{bullets}
\item Test with numeric $B,C,T$ to confirm identity.
\end{bullets}
}
\INTUITION{
Insert identities to shuttle coordinates from $C$ to $B$ and back.
}
\CANONICAL{
\begin{bullets}
\item Conjugation identity is the canonical operator transport law.
\end{bullets}
}

\ProblemPage{9}{Combo: Diagonalization as Change of Basis}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Show that a diagonalizable matrix is similar to a diagonal matrix and
perform the change of basis explicitly.

\PROBLEM{
Let $A=\begin{bmatrix}5&2\\ 2&5\end{bmatrix}$. Find an invertible $S$
and diagonal $\Lambda$ such that $A=S\Lambda S^{-1}$. Then compute
$S^{-1} v$ for $v=(7,3)^\top$ and interpret.
}
\MODEL{
\[
A=S\Lambda S^{-1},\quad \Lambda=\operatorname{diag}(\lambda_1,\lambda_2),
\]
with $S$ eigenvector matrix and $[v]_S=S^{-1}[v]_E$.
}
\ASSUMPTIONS{
\begin{bullets}
\item Symmetric $A$ has orthonormal eigenbasis.
\end{bullets}
}
\varmapStart
\var{A}{Symmetric matrix.}
\var{S}{Eigenvector matrix (orthogonal here).}
\var{\Lambda}{Diagonal of eigenvalues.}
\var{v}{Vector $(7,3)^\top$.}
\varmapEnd
\WHICHFORMULA{
Use Formula 2 for similarity and Formula 4 for coordinates via $S^{-1}$.
}
\GOVERN{
\[
A=S\Lambda S^{-1},\quad [v]_S=S^{-1}[v]_E.
\]
}
\INPUTS{$A$, $v$.}
\DERIVATION{
\begin{align*}
\text{Eigenvalues:}\ & \det(A-\lambda I)=(5-\lambda)^2-4=0\\
&\Rightarrow \lambda_1=3,\ \lambda_2=7.\\
\text{Eigenvectors:}\ & (A-3I)x=0\Rightarrow
\begin{bmatrix}2&2\\2&2\end{bmatrix}x=0\Rightarrow x\propto (1,-1).\\
& (A-7I)x=0\Rightarrow
\begin{bmatrix}-2&2\\2&-2\end{bmatrix}x=0\Rightarrow x\propto (1,1).\\
\text{Orthonormalize:}\ & s_1=\tfrac{1}{\sqrt{2}}(1,-1)^\top,\
s_2=\tfrac{1}{\sqrt{2}}(1,1)^\top.\\
S&=\begin{bmatrix}1/\sqrt{2}&1/\sqrt{2}\\ -1/\sqrt{2}&1/\sqrt{2}\end{bmatrix},\
\Lambda=\begin{bmatrix}3&0\\ 0&7\end{bmatrix}.\\
S^{-1}&=S^\top.\\
[v]_S&=S^\top v=\tfrac{1}{\sqrt{2}}\begin{bmatrix}1&-1\\ 1&1\end{bmatrix}
\begin{bmatrix}7\\ 3\end{bmatrix}
=\tfrac{1}{\sqrt{2}}\begin{bmatrix}4\\ 10\end{bmatrix}
=\begin{bmatrix}2\sqrt{2}\\ 5\sqrt{2}\end{bmatrix}.
\end{align*}
}
\RESULT{
$A=S\Lambda S^\top$ with $S$ orthogonal. In the eigenbasis, $v$ has
coordinates $(2\sqrt{2},5\sqrt{2})^\top$ and $A$ scales these coordinates
by $(3,7)$.
}
\UNITCHECK{
$S^\top S=I$; $A=S\Lambda S^\top$ equals the original upon multiplication.
}
\EDGECASES{
\begin{bullets}
\item If $A$ were not diagonalizable, Jordan form replaces diagonal.
\end{bullets}
}
\ALTERNATE{
Use spectral theorem: symmetric matrices admit orthogonal diagonalization
via an orthonormal basis of eigenvectors.
}
\VALIDATION{
\begin{bullets}
\item Compute $S^{-1} A S$ to confirm $\Lambda$.
\item Check $A v=S\Lambda [v]_S$ numerically.
\end{bullets}
}
\INTUITION{
Choosing the eigenbasis aligns axes with the action of $A$, turning it
into pure scaling.
}
\CANONICAL{
\begin{bullets}
\item Diagonalization is a change of basis to an eigenbasis.
\end{bullets}
}

\ProblemPage{10}{Combo: OLS Invariance Under Orthonormal Feature Rotation}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Show that orthonormal change of basis in feature space preserves OLS
predictions and relate coefficients.

\PROBLEM{
Given $X\in\mathbb{R}^{n\times d}$ full column rank and $y\in\mathbb{R}^n$,
let $\hat{\beta}=(X^\top X)^{-1}X^\top y$. For $Q\in\mathbb{R}^{d\times d}$
orthogonal, define $X'=XQ$. Show predictions are equal:
$X\hat{\beta}=X'\hat{\beta}'$ where
$\hat{\beta}'=(X'^\top X')^{-1}X'^\top y$, and prove $\hat{\beta}'=Q^\top
\hat{\beta}$.
}
\MODEL{
\[
\hat{\beta}'=(Q^\top X^\top X Q)^{-1} Q^\top X^\top y.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Full column rank ensures invertibility of $X^\top X$.
\item $Q^\top Q=I$.
\end{bullets}
}
\varmapStart
\var{X}{Design matrix.}
\var{y}{Response vector.}
\var{\hat{\beta}}{OLS coefficients in original basis.}
\var{Q}{Orthonormal feature transform.}
\var{\hat{\beta}'}{OLS in rotated basis.}
\varmapEnd
\WHICHFORMULA{
Use Formula 1 with $M=Q$ in feature space and apply matrix identities.
}
\GOVERN{
\[
\hat{\beta}=(X^\top X)^{-1}X^\top y,\quad \hat{\beta}'=(Q^\top X^\top X Q)^{-1}
Q^\top X^\top y.
\]
}
\INPUTS{$X$, $y$, $Q$.}
\DERIVATION{
\begin{align*}
\hat{\beta}'&=(Q^\top X^\top X Q)^{-1}Q^\top X^\top y
=Q^{-1}(X^\top X)^{-1}Q^{-\top}Q^\top X^\top y\\
&=Q^\top (X^\top X)^{-1} X^\top y=Q^\top \hat{\beta}.\\
\text{Predictions:}\ & X'\hat{\beta}'=XQ(Q^\top \hat{\beta})=X\hat{\beta}.
\end{align*}
}
\RESULT{
$\hat{\beta}'=Q^\top \hat{\beta}$ and predictions agree: invariance of
OLS under orthonormal change of basis in feature space.
}
\UNITCHECK{
Shapes: $X(n\times d)$, $Q(d\times d)$, coefficients $(d\times 1)$.
}
\EDGECASES{
\begin{bullets}
\item If $Q$ is not orthogonal, predictions still match if using the
same column space, but coefficients are not simply $Q^\top \hat{\beta}$.
\end{bullets}
}
\ALTERNATE{
Derive from normal equations: $X'^\top (y-X'\hat{\beta}')=0$ and use
$X'=XQ$ with $Q^\top Q=I$.
}
\VALIDATION{
\begin{bullets}
\item Numerically test with random $X$, $Q$ to confirm identities.
\end{bullets}
}
\INTUITION{
Rotating features does not change the subspace spanned by columns of $X$,
so the best fit in that subspace yields identical predictions.
}
\CANONICAL{
\begin{bullets}
\item Orthonormal feature transforms are coordinate changes preserving
least squares predictions.
\end{bullets}
}

\section{Coding Demonstrations}
\CodeDemoPage{Construct and Verify Change-of-Basis and Similarity}
\PROBLEM{
Build $M_{C\leftarrow B}$ from basis matrices, convert vector coordinates,
and verify $[T]_C=M[T]_B M^{-1}$ on a deterministic example.
}
\API{
\begin{bullets}
\item \inlinecode{def read_input(s) -> dict} — parse basis and vector.
\item \inlinecode{def solve_case(obj) -> dict} — compute $M$, coords, $[T]_C$.
\item \inlinecode{def validate() -> None} — assertions matching formulas.
\item \inlinecode{def main() -> None} — run validation and print summary.
\end{bullets}
}
\INPUTS{
Basis matrices $B,C$, matrix $T_B$, and vector $v$ in reference coords.
}
\OUTPUTS{
$M$, $[v]_B$, $[v]_C$, $[T]_C$ and checks for identities.
}
\FORMULA{
\[
M=C^{-1}B,\quad [v]_B=B^{-1}v,\quad [v]_C=C^{-1}v,\quad
[T]_C=M[T]_B M^{-1}.
\]
}
\textbf{SOLUTION A — From Scratch (Mathematically Explicit Implementation)}
\begin{codepy}
# Deterministic 2D case with plain Python lists
def matmul(A, B):
    r = len(A); c = len(B[0]); k = len(B)
    return [[sum(A[i][t]*B[t][j] for t in range(k)) for j in range(c)]
            for i in range(r)]

def eye(n):
    return [[1 if i==j else 0 for j in range(n)] for i in range(n)]

def minv2(A):
    # Gauss-Jordan inverse for 2x2 or small n
    n = len(A)
    M = [row[:] + e for row, e in zip(A, eye(n))]
    # forward
    for i in range(n):
        piv = M[i][i]
        assert abs(piv) > 1e-12
        invp = 1.0/piv
        M[i] = [x*invp for x in M[i]]
        for r in range(n):
            if r == i:
                continue
            f = M[r][i]
            M[r] = [M[r][c] - f*M[i][c] for c in range(2*n)]
    return [row[n:] for row in M]

def vecmul(A, v):
    return [sum(A[i][j]*v[j] for j in range(len(v)))
            for i in range(len(A))]

def read_input(s):
    # fixed data; s ignored to keep API
    B = [[1, 1],
         [0, 1]]
    C = [[2, 0],
         [0, 1]]
    TB = [[2, 1],
          [0, 3]]
    v = [3, 1]
    return {"B": B, "C": C, "TB": TB, "v": v}

def solve_case(obj):
    B, C, TB, v = obj["B"], obj["C"], obj["TB"], obj["v"]
    Binv, Cinv = minv2(B), minv2(C)
    M = matmul(Cinv, B)
    Minv = minv2(M)
    vB = vecmul(Binv, v)
    vC = vecmul(Cinv, v)
    TC = matmul(matmul(M, TB), Minv)
    return {"M": M, "Minv": Minv, "vB": vB, "vC": vC, "TC": TC,
            "B": B, "C": C, "TB": TB, "v": v}

def validate():
    d = solve_case(read_input(""))
    # vector identity checks
    B, C, v = d["B"], d["C"], d["v"]
    vB, vC, M = d["vB"], d["vC"], d["M"]
    # Check B vB = v and C vC = v
    v_back_B = vecmul(B, vB)
    v_back_C = vecmul(C, vC)
    assert all(abs(v_back_B[i]-v[i]) < 1e-9 for i in range(2))
    assert all(abs(v_back_C[i]-v[i]) < 1e-9 for i in range(2))
    # Check vC = M vB
    vC2 = vecmul(M, vB)
    assert all(abs(vC2[i]-vC[i]) < 1e-9 for i in range(2))
    # similarity invariants
    TB = d["TB"]; TC = d["TC"]; Minv = d["Minv"]
    # Check Minv*TC*M = TB
    left = matmul(matmul(Minv, TC), M)
    for i in range(2):
        for j in range(2):
            assert abs(left[i][j]-TB[i][j]) < 1e-9

def main():
    validate()
    d = solve_case(read_input(""))
    print("M:", d["M"], "TC:", d["TC"], "vB:", d["vB"], "vC:", d["vC"])

if __name__ == "__main__":
    main()
\end{codepy}
\textbf{SOLUTION B — Library-Based (Validated Computational Shortcut)}
\begin{codepy}
import numpy as np

def read_input(s):
    np.set_printoptions(suppress=True, linewidth=78)
    B = np.array([[1., 1.],
                  [0., 1.]])
    C = np.array([[2., 0.],
                  [0., 1.]])
    TB = np.array([[2., 1.],
                   [0., 3.]])
    v = np.array([3., 1.])
    return {"B": B, "C": C, "TB": TB, "v": v}

def solve_case(obj):
    B, C, TB, v = obj["B"], obj["C"], obj["TB"], obj["v"]
    Binv, Cinv = np.linalg.inv(B), np.linalg.inv(C)
    M = Cinv @ B
    Minv = np.linalg.inv(M)
    vB = Binv @ v
    vC = Cinv @ v
    TC = M @ TB @ Minv
    return {"M": M, "Minv": Minv, "vB": vB, "vC": vC, "TC": TC,
            "B": B, "C": C, "TB": TB, "v": v}

def validate():
    d = solve_case(read_input(""))
    B, C, v, vB, vC, M = d["B"], d["C"], d["v"], d["vB"], d["vC"], d["M"]
    assert np.allclose(B @ vB, v)
    assert np.allclose(C @ vC, v)
    assert np.allclose(M @ vB, vC)
    TB, TC, Minv = d["TB"], d["TC"], d["Minv"]
    assert np.allclose(Minv @ TC @ M, TB)

def main():
    validate()
    d = solve_case(read_input(""))
    print("M=", d["M"])
    print("TC=", d["TC"])
    print("vB=", d["vB"], "vC=", d["vC"])

if __name__ == "__main__":
    main()
\end{codepy}
\COMPLEXITY{
Time $\mathcal{O}(n^3)$ for inversion and similarity; space
$\mathcal{O}(n^2)$. In 2D, all operations are constant time.
}
\FAILMODES{
\begin{bullets}
\item Singular basis matrices; inversion fails. Mitigate by checking
$\det\neq 0$.
\item Numerical instability for ill-conditioned bases. Use stable
factorizations.
\end{bullets}
}
\STABILITY{
\begin{bullets}
\item Prefer LU or QR over explicit inverses; avoid subtractive
cancellation by scaling.
\end{bullets}
}
\VALIDATION{
\begin{bullets}
\item Check $B v_B=v$, $C v_C=v$, and $M v_B=v_C$.
\item Verify $M^{-1} T_C M=T_B$ and similarity invariants
(trace, determinant).
\end{bullets}
}
\RESULT{
Both implementations produce identical $M$, $[v]_B$, $[v]_C$, and $[T]_C$
and pass all assertions.
}
\EXPLANATION{
Code directly implements Formulas 1, 2, and 4: $M=C^{-1}B$,
$[v]_B=B^{-1}v$, $[v]_C=C^{-1}v$, and $[T]_C=M[T]_B M^{-1}$.
}
\EXTENSION{
Extend to $n$D with LU/QR for stability; add random tests with fixed seed.
}

\CodeDemoPage{Gram Matrix Transport and Inner Product Invariance}
\PROBLEM{
Given $G_B$ and $M_{C\leftarrow B}$, compute $G_C=M^\top G_B M$ and
verify that inner products match for random test vectors.
}
\API{
\begin{bullets}
\item \inlinecode{def read_input(s) -> dict} — parse $G_B,M$.
\item \inlinecode{def solve_case(obj) -> dict} — compute $G_C$.
\item \inlinecode{def validate() -> None} — test invariance on samples.
\item \inlinecode{def main() -> None} — orchestrate.
\end{bullets}
}
\INPUTS{
$G_B$ symmetric positive definite, $M$ invertible, vectors $x_C,y_C$.
}
\OUTPUTS{
$G_C$, scalar inner products in $B$ and $C$ coordinates.
}
\FORMULA{
\[
G_C=M^\top G_B M,\quad \langle x,y\rangle=[x]_B^\top G_B [y]_B
=[x]_C^\top G_C [y]_C.
\]
}
\textbf{SOLUTION A — From Scratch (Lists and Small Linear Algebra)}
\begin{codepy}
def tmat(A):
    return [list(row) for row in zip(*A)]

def matmul(A, B):
    r = len(A); c = len(B[0]); k = len(B)
    return [[sum(A[i][t]*B[t][j] for t in range(k)) for j in range(c)]
            for i in range(r)]

def minv2(A):
    n = len(A)
    M = [row[:] + [1 if i==j else 0 for j in range(n)]
         for i, row in enumerate(A)]
    for i in range(n):
        piv = M[i][i]; assert abs(piv) > 1e-12
        invp = 1.0/piv; M[i] = [x*invp for x in M[i]]
        for r in range(n):
            if r == i: continue
            f = M[r][i]
            M[r] = [M[r][c] - f*M[i][c] for c in range(2*n)]
    return [row[n:] for row in M]

def read_input(s):
    GB = [[2., 1.],
          [1., 3.]]
    B = [[1., 1.],
         [0., 1.]]
    C = [[2., 0.],
         [0., 1.]]
    Cinv = minv2(C)
    M = matmul(Cinv, B)
    xC = [1., 2.]
    yC = [3., -1.]
    return {"GB": GB, "M": M, "xC": xC, "yC": yC}

def quad(G, x, y):
    return sum(x[i]*sum(G[i][j]*y[j] for j in range(len(y)))
               for i in range(len(x)))

def solve_case(obj):
    GB, M = obj["GB"], obj["M"]
    GT = tmat(M)
    GC = matmul(matmul(GT, GB), M)
    return {"GC": GC}

def validate():
    d = read_input("")
    GB, M, xC, yC = d["GB"], d["M"], d["xC"], d["yC"]
    GC = solve_case(d)["GC"]
    Minv = minv2(M)
    xB = vecmul(Minv, xC)
    yB = vecmul(Minv, yC)
    v1 = quad(GB, xB, yB)
    v2 = quad(GC, xC, yC)
    assert abs(v1 - v2) < 1e-9

def vecmul(A, v):
    return [sum(A[i][j]*v[j] for j in range(len(v)))
            for i in range(len(A))]

def main():
    validate()
    d = read_input("")
    GC = solve_case(d)["GC"]
    print("GC:", GC)

if __name__ == "__main__":
    main()
\end{codepy}
\textbf{SOLUTION B — Library-Based (NumPy)}
\begin{codepy}
import numpy as np

def read_input(s):
    np.set_printoptions(suppress=True, linewidth=78)
    GB = np.array([[2., 1.],
                   [1., 3.]])
    B = np.array([[1., 1.],
                  [0., 1.]])
    C = np.array([[2., 0.],
                  [0., 1.]])
    M = np.linalg.inv(C) @ B
    xC = np.array([1., 2.])
    yC = np.array([3., -1.])
    return {"GB": GB, "M": M, "xC": xC, "yC": yC}

def solve_case(obj):
    GB, M = obj["GB"], obj["M"]
    GC = M.T @ GB @ M
    return {"GC": GC}

def validate():
    d = read_input("")
    GB, M, xC, yC = d["GB"], d["M"], d["xC"], d["yC"]
    GC = solve_case(d)["GC"]
    Minv = np.linalg.inv(M)
    xB = Minv @ xC
    yB = Minv @ yC
    v1 = xB.T @ GB @ yB
    v2 = xC.T @ GC @ yC
    assert np.allclose(v1, v2)

def main():
    validate()
    d = read_input("")
    print("GC=\n", solve_case(d)["GC"])

if __name__ == "__main__":
    main()
\end{codepy}
\COMPLEXITY{
Time $\mathcal{O}(n^3)$ for forming $GC$; space $\mathcal{O}(n^2)$.
}
\FAILMODES{
\begin{bullets}
\item Singular $M$ invalid; check condition number.
\item Non symmetric $GB$ leads to non symmetric $GC$.
\end{bullets}
}
\STABILITY{
\begin{bullets}
\item Prefer Cholesky for SPD $GB$ to avoid instability.
\end{bullets}
}
\VALIDATION{
\begin{bullets}
\item Compare $x_B^\top GB y_B$ and $x_C^\top GC y_C$.
\item Check symmetry of $GC$.
\end{bullets}
}
\RESULT{
$G_C$ computed and invariance verified by assertions in both variants.
}
\EXPLANATION{
Implements Formula 3 congruence and verifies the scalar invariance of the
inner product under coordinate changes.
}
\EXTENSION{
Test Sylvester law by computing eigenvalues of $GB$ and $GC$.
}

\section{Applied Domains — Detailed End-to-End Scenarios}
\DomainPage{Machine Learning}
\SCENARIO{
Demonstrate OLS predictions invariant under orthonormal feature
rotations and map coefficients between bases.
}
\ASSUMPTIONS{
\begin{bullets}
\item Full column rank design; $Q^\top Q=I$.
\item Zero-mean noise; deterministic seed for reproducibility.
\end{bullets}
}
\WHICHFORMULA{
Use Formula 1 in feature space and Problem 10 result:
$\hat{\beta}'=Q^\top \hat{\beta}$ and $X\hat{\beta}=X'\hat{\beta}'$.
}
\varmapStart
\var{X}{Design matrix $(n,d)$.}
\var{y}{Response $(n)$.}
\var{Q}{Orthonormal feature transform $(d,d)$.}
\var{\hat{\beta},\hat{\beta}'}{OLS coefficients in old and new bases.}
\varmapEnd
\PIPELINE{
\begin{bullets}
\item Generate synthetic linear data.
\item Sample orthonormal $Q$ via QR of a random matrix.
\item Fit OLS in both bases and compare predictions.
\end{bullets}
}
\textbf{Implementation (From Scratch)}
\begin{codepy}
import numpy as np

def generate(n=60, d=3, noise=0.1, seed=0):
    rng = np.random.default_rng(seed)
    X = rng.normal(size=(n, d))
    beta = np.array([1., 2., -1.])
    y = X @ beta + rng.normal(scale=noise, size=n)
    return X, y, beta

def orthonormal_Q(d=3, seed=1):
    rng = np.random.default_rng(seed)
    A = rng.normal(size=(d, d))
    Q, _ = np.linalg.qr(A)
    return Q

def ols(X, y):
    return np.linalg.solve(X.T @ X, X.T @ y)

def main():
    X, y, beta_true = generate()
    Q = orthonormal_Q(d=X.shape[1])
    Xp = X @ Q
    b = ols(X, y)
    bp = ols(Xp, y)
    yhat = X @ b
    yhatp = Xp @ bp
    print("||pred diff||:", np.linalg.norm(yhat - yhatp))
    print("bp vs Qt b:", np.linalg.norm(bp - Q.T @ b))

if __name__ == "__main__":
    main()
\end{codepy}
\textbf{Implementation (Library Version)}
\begin{codepy}
import numpy as np
from numpy.linalg import qr, solve

def main():
    rng = np.random.default_rng(0)
    X = rng.normal(size=(80, 4))
    beta = np.array([2., -1., 0.5, 3.])
    y = X @ beta + rng.normal(scale=0.2, size=80)
    Q, _ = qr(rng.normal(size=(4, 4)))
    Xp = X @ Q
    b = solve(X.T @ X, X.T @ y)
    bp = solve(Xp.T @ Xp, Xp.T @ y)
    print("pred eq:", np.allclose(X @ b, Xp @ bp))
    print("coef map eq:", np.allclose(bp, Q.T @ b))

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{
Report $\|X\hat{\beta}-X'\hat{\beta}'\|_2$ and
$\|\hat{\beta}'-Q^\top\hat{\beta}\|_2$ close to $0$.
}
\INTERPRET{
OLS predictions are invariant under orthonormal feature changes; only the
coordinate description of $\hat{\beta}$ changes.
}
\NEXTSTEPS{
Extend to ridge regression and show invariance with proper parameter map.
}

\DomainPage{Quantitative Finance}
\SCENARIO{
Rotate asset returns into principal components via change of basis and
analyze variance allocation in the PC basis.
}
\ASSUMPTIONS{
\begin{bullets}
\item Returns are mean zero and covariance finite.
\item PCA basis is orthonormal; use eigenvectors of covariance.
\end{bullets}
}
\WHICHFORMULA{
Use Formula 1 with orthogonal $Q$ from eigenvectors; covariance transforms
as $Q^\top \Sigma Q=\Lambda$; coordinates of returns map by $Q^\top$.
}
\varmapStart
\var{R}{Returns matrix $(n,d)$, rows observations.}
\var{\Sigma}{Sample covariance $(d,d)$.}
\var{Q}{Eigenvectors of $\Sigma$.}
\var{\Lambda}{Diagonal eigenvalues.}
\varmapEnd
\PIPELINE{
\begin{bullets}
\item Simulate correlated returns.
\item Compute $\Sigma$, its eigendecomposition $\Sigma=Q\Lambda Q^\top$.
\item Transform to PC coordinates $Z=RQ$ and examine variances.
\end{bullets}
}
\textbf{Implementation (Full Pipeline)}
\begin{codepy}
import numpy as np

def simulate(n=1000, d=3, seed=0):
    rng = np.random.default_rng(seed)
    A = rng.normal(size=(d, d))
    cov = A @ A.T
    R = rng.multivariate_normal(np.zeros(d), cov, size=n)
    return R

def pcs(R):
    Sigma = np.cov(R, rowvar=False, bias=True)
    lam, Q = np.linalg.eigh(Sigma)
    idx = np.argsort(lam)[::-1]
    lam = lam[idx]; Q = Q[:, idx]
    Z = R @ Q
    return Sigma, lam, Q, Z

def main():
    R = simulate()
    Sigma, lam, Q, Z = pcs(R)
    varZ = Z.var(axis=0)
    print("eigvals:", np.round(lam, 6))
    print("var PCs:", np.round(varZ, 6))
    print("orthonormal:", np.allclose(Q.T @ Q, np.eye(Q.shape[0])))

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{
Eigenvalues equal PC variances. Orthogonality of $Q$ confirms a valid
change of basis.
}
\INTERPRET{
PCA is an orthonormal change of basis that diagonalizes covariance, so
variances decouple across coordinates.
}
\NEXTSTEPS{
Project portfolios into PC basis and analyze risk contributions.
}

\DomainPage{Deep Learning}
\SCENARIO{
Show equivalence of two linear two-layer networks related by a hidden
layer change of basis.
}
\ASSUMPTIONS{
\begin{bullets}
\item Networks are linear: $\hat{y}=W_2 W_1 x$.
\item Hidden transform $S$ invertible; $W_2'=W_2 S$, $W_1'=S^{-1}W_1$.
\end{bullets}
}
\WHICHFORMULA{
Hidden basis change is $S$; model output invariant by
$W_2'W_1'=W_2 S S^{-1} W_1=W_2 W_1$.
}
\PIPELINE{
\begin{bullets}
\item Create random $W_1,W_2$ and invertible $S$.
\item Build $W_1',W_2'$ via basis change.
\item Verify outputs match for random inputs.
\end{bullets}
}
\textbf{Implementation (End-to-End)}
\begin{codepy}
import numpy as np

def make_invertible(d=5, seed=0):
    rng = np.random.default_rng(seed)
    A = rng.normal(size=(d, d))
    U, s, Vt = np.linalg.svd(A)
    s = s + 0.5
    return U @ np.diag(s) @ Vt

def main():
    rng = np.random.default_rng(1)
    d_in, d_h, d_out = 4, 5, 3
    W1 = rng.normal(size=(d_h, d_in))
    W2 = rng.normal(size=(d_out, d_h))
    S = make_invertible(d=d_h, seed=2)
    W1p = np.linalg.inv(S) @ W1
    W2p = W2 @ S
    X = rng.normal(size=(10, d_in))
    Y = X @ W1.T @ W2.T
    Yp = X @ W1p.T @ W2p.T
    print("equiv:", np.allclose(Y, Yp))

if __name__ == "__main__":
    main()
\end{codepy}
\textbf{Analytical OLS Comparison}
\begin{codepy}
def check_equiv(W1, W2, S):
    W1p = np.linalg.inv(S) @ W1
    W2p = W2 @ S
    return np.allclose(W2 @ W1, W2p @ W1p)
\end{codepy}
\METRICS{
Boolean equivalence of outputs and operator matrices.
}
\INTERPRET{
Changing hidden coordinates does not affect the realized linear mapping;
it is a pure change of basis in the hidden space.
}
\NEXTSTEPS{
Extend to ReLU nets: show invariances through permutations and scaling.
}

\DomainPage{Kaggle / Data Analytics}
\SCENARIO{
Demonstrate that orthonormal rotations preserve pairwise Euclidean
distances and total variance of standardized features.
}
\ASSUMPTIONS{
\begin{bullets}
\item Features standardized to zero mean and unit variance.
\item $Q$ orthonormal; coordinates map by $X'=XQ$.
\end{bullets}
}
\WHICHFORMULA{
Formula 1 with $M=Q$ and orthogonality implies
$\|x_i-x_j\|_2=\|(x_i-x_j)Q\|_2$ and $\operatorname{tr}(\Sigma)$ invariant.
}
\PIPELINE{
\begin{bullets}
\item Create synthetic features; standardize.
\item Construct orthonormal $Q$ and rotate data.
\item Compare pairwise distances and total variance.
\end{bullets}
}
\textbf{Implementation (Complete EDA Pipeline)}
\begin{codepy}
import numpy as np

def standardize(X):
    mu = X.mean(axis=0, keepdims=True)
    sigma = X.std(axis=0, ddof=0, keepdims=True)
    return (X - mu) / sigma

def orthonormal_Q(d=4, seed=0):
    rng = np.random.default_rng(seed)
    A = rng.normal(size=(d, d))
    Q, _ = np.linalg.qr(A)
    return Q

def main():
    rng = np.random.default_rng(1)
    X = rng.normal(size=(100, 4))
    Xs = standardize(X)
    Q = orthonormal_Q(4, seed=2)
    Xr = Xs @ Q
    i, j = 3, 77
    d1 = np.linalg.norm(Xs[i] - Xs[j])
    d2 = np.linalg.norm(Xr[i] - Xr[j])
    tv1 = np.trace(np.cov(Xs, rowvar=False, bias=True))
    tv2 = np.trace(np.cov(Xr, rowvar=False, bias=True))
    print("dist preserved:", np.allclose(d1, d2))
    print("total var preserved:", np.allclose(tv1, tv2))

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{
Boolean checks for distance and total variance preservation.
}
\INTERPRET{
Orthogonal feature rotations are pure coordinate changes that leave the
geometry of the standardized cloud unchanged.
}
\NEXTSTEPS{
Use PCA rotation to decorrelate features and rank by explained variance.
}

\end{document}
% !TeX program = xelatex
\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{microtype,setspace,amsmath,amssymb,mathtools,amsthm,unicode-math}
\setstretch{1.05}
\setmainfont{Latin Modern Roman}
\setmonofont{Latin Modern Mono}
\setmathfont{Latin Modern Math}

\allowdisplaybreaks[4]
\setlength{\jot}{7pt}
\setlength{\emergencystretch}{8em}
\sloppy

\usepackage{xcolor,fancyhdr,enumitem,inconsolata,listings}
\pagestyle{fancy}\fancyhf{}\lhead{\nouppercase{\leftmark}}\rhead{\thepage}
\setlength{\headheight}{26pt}
\setlength{\parindent}{0pt}
\setlength{\parskip}{8pt plus 2pt minus 1pt}
\raggedbottom

\newenvironment{BreakableEquation}{\begin{equation}\begin{aligned}}{\end{aligned}\end{equation}}
\newenvironment{BreakableEquation*}{\begin{equation*}\begin{aligned}}{\end{aligned}\end{equation*}}
\newenvironment{tightalign}{\begingroup\small\allowdisplaybreaks\begin{align}}{\end{align}\endgroup}

\providecommand{\enumlistm}{enumitem}
\newenvironment{minted}[2][]{%
  \lstset{style=code,language=#2,#1}\begin{lstlisting}%
}{\end{lstlisting}}

\newcommand{\inputminted}[3][]{\begin{lstlisting}\end{lstlisting}}

\newlist{bullets}{itemize}{1}
\setlist[bullets]{label=--,leftmargin=1.6em,itemsep=2pt,topsep=4pt}

\newcommand{\varmapStart}{\textbf{VARIABLE MAPPING:}\par\begin{bullets}}
\newcommand{\var}[2]{\item $#1$ — #2}
\newcommand{\varmapEnd}{\end{bullets}}

\newcommand{\glossx}[6]{%
  \textbf{#1}\par
  \begin{bullets}
    \item \textbf{What:} #2
    \item \textbf{Why:} #3
    \item \textbf{How:} #4
    \item \textbf{ELI5:} #5
    \item \textbf{Pitfall/Example:} #6
  \end{bullets}
}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{claim}[theorem]{Claim}
\lstdefinestyle{code}{
  basicstyle=\ttfamily\footnotesize,
  backgroundcolor=\color{black!02},
  frame=single,
  numbers=left, numberstyle=\tiny, numbersep=8pt,
  breaklines=true, breakatwhitespace=true,
  tabsize=4, showstringspaces=false,
  upquote=true, keepspaces=true, columns=fullflexible,
  literate=
    {–}{{-}}1
    {—}{{-}}1
    {…}{{...}}1
    {≤}{{\ensuremath{\le}}}1
    {≥}{{\ensuremath{\ge}}}1
    {≠}{{\ensuremath{\ne}}}1
    {≈}{{\ensuremath{\approx}}}1
    {±}{{\ensuremath{\pm}}}1
    {→}{{\ensuremath{\to}}}1
    {←}{{\ensuremath{\leftarrow}}}1
    {∞}{{\ensuremath{\infty}}}1
    {√}{{\ensuremath{\sqrt{\ }}}}1
    {×}{{\ensuremath{\times}}}1
    {÷}{{\ensuremath{\div}}}1
}

\lstnewenvironment{codepy}[1][]%
  {\lstset{style=code,language=Python,#1}}%
  {}

\newcommand{\inlinecode}[1]{\lstinline[style=code]!#1!}

\newcommand{\LF}[2]{\par\noindent\textbf{#1:}~#2\par}
\newcommand{\WHAT}[1]{\LF{WHAT}{#1}}
\newcommand{\WHY}[1]{\LF{WHY}{#1}}
\newcommand{\HOW}[1]{\LF{HOW}{#1}}
\newcommand{\ELI}[1]{\LF{ELI5}{#1}}
\newcommand{\SCOPE}[1]{\LF{SCOPE}{#1}}
\newcommand{\CONFUSIONS}[1]{\LF{COMMON CONFUSIONS}{#1}}
\newcommand{\APPLICATIONS}[1]{\LF{APPLICATIONS}{#1}}
\newcommand{\FORMULA}[1]{\LF{FORMULA}{#1}}
\newcommand{\CANONICAL}[1]{\LF{CANONICAL FORM}{#1}}
\newcommand{\PRECONDS}[1]{\LF{PRECONDITIONS}{#1}}
\newcommand{\DERIVATION}[1]{\LF{DERIVATION}{#1}}
\newcommand{\EQUIV}[1]{\LF{EQUIVALENT FORMS}{#1}}
\newcommand{\LIMITS}[1]{\LF{LIMIT CASES}{#1}}
\newcommand{\INPUTS}[1]{\LF{INPUTS}{#1}}
\newcommand{\OUTPUTS}[1]{\LF{OUTPUTS}{#1}}
\newcommand{\RESULT}[1]{\LF{RESULT}{#1}}
\newcommand{\INTUITION}[1]{\LF{INTUITION}{#1}}
\newcommand{\PITFALLS}[1]{\LF{PITFALLS}{#1}}
\newcommand{\MODEL}[1]{\LF{CANONICAL MATH MODEL}{#1}}
\newcommand{\ASSUMPTIONS}[1]{\LF{ASSUMPTIONS}{#1}}
\newcommand{\WHICHFORMULA}[1]{\LF{WHICH FORMULA \& WHY}{#1}}
\newcommand{\GOVERN}[1]{\LF{GOVERNING EQUATION(S)}{#1}}
\newcommand{\UNITCHECK}[1]{\LF{UNIT CHECK}{#1}}
\newcommand{\EDGECASES}[1]{\LF{EDGE CASES}{#1}}
\newcommand{\ALTERNATE}[1]{\LF{ALTERNATE APPROACH (sketch)}{#1}}
\newcommand{\PROBLEM}[1]{\LF{PROBLEM}{#1}}
\newcommand{\API}[1]{\LF{API}{#1}}
\newcommand{\COMPLEXITY}[1]{\LF{COMPLEXITY}{#1}}
\newcommand{\FAILMODES}[1]{\LF{FAILURE MODES}{#1}}
\newcommand{\STABILITY}[1]{\LF{NUMERICAL STABILITY}{#1}}
\newcommand{\VALIDATION}[1]{\LF{VALIDATION}{#1}}
\newcommand{\EXPLANATION}[1]{\LF{EXPLANATION}{#1}}
\newcommand{\SCENARIO}[1]{\LF{SCENARIO}{#1}}
\newcommand{\PIPELINE}[1]{\LF{PIPELINE STEPS}{#1}}
\newcommand{\METRICS}[1]{\LF{METRICS}{#1}}
\newcommand{\INTERPRET}[1]{\LF{INTERPRETATION}{#1}}
\newcommand{\NEXTSTEPS}[1]{\LF{LIMITATIONS \& NEXT STEPS}{#1}}

\usepackage{titlesec}
\titleformat{\section}{\Large\bfseries}{\thesection}{0.6em}{}
\titlespacing*{\section}{0pt}{*2}{*1}
\usepackage{etoolbox}
\pretocmd{\section}{\clearpage}{}{}

\newcommand{\FormulaPage}[2]{%
  \clearpage
  \section*{Formula #1 — #2}%
  \addcontentsline{toc}{section}{Formula #1 — #2}%
}
\newcommand{\ProblemPage}[2]{%
  \clearpage
  \subsection*{Problem #1: #2}%
  \addcontentsline{toc}{subsection}{Problem #1: #2}%
}
\newcommand{\CodeDemoPage}[1]{%
  \clearpage
  \subsection*{Coding Demo: #1}%
  \addcontentsline{toc}{subsection}{Coding Demo: #1}%
}
\newcommand{\DomainPage}[1]{%
  \clearpage
  \subsection*{#1 (End-to-End)}%
  \addcontentsline{toc}{subsection}{#1 (End-to-End)}%
}

\begin{document}
\title{Comprehensive Study Sheet — Linear Transformations and Matrix Representation}
\date{\today}
\maketitle
\tableofcontents
\clearpage

\section{Concept Overview}
\WHAT{
A linear transformation is a map $T:V\to W$ between vector spaces over a
field $\mathbb{F}$ satisfying $T(u+v)=T(u)+T(v)$ and $T(\alpha v)=\alpha T(v)$
for all $u,v\in V$, $\alpha\in\mathbb{F}$. If $V,W$ are finite-dimensional and
bases $\mathcal{B}$ for $V$ and $\mathcal{C}$ for $W$ are fixed, $T$ has a
matrix representation $[T]_{\mathcal{B},\mathcal{C}}\in\mathbb{F}^{m\times n}$
such that for $v\in V$,
$[T(v)]_{\mathcal{C}}=[T]_{\mathcal{B},\mathcal{C}}\,[v]_{\mathcal{B}}$.
}

\WHY{
Matrix representation converts abstract linear maps into concrete arrays on
which computation is efficient. It enables composition as matrix products,
change of basis via similarity, spectral analysis (eigenvalues), and structural
results such as rank-nullity. These support proofs, modeling, and numerical
algorithms across mathematics and engineering.
}

\HOW{
Stepwise pipeline:
1. Choose bases $\mathcal{B}$ of $V$ and $\mathcal{C}$ of $W$.
2. Define $A=[T]_{\mathcal{B},\mathcal{C}}$ by columns $A e_j=[T(b_j)]_{\mathcal{C}}$.
3. Use $[T(v)]_{\mathcal{C}}=A[v]_{\mathcal{B}}$ to compute images.
4. Combine maps via $[S\circ T]_{\mathcal{B},\mathcal{D}}=[S]_{\mathcal{C},\mathcal{D}}
[T]_{\mathcal{B},\mathcal{C}}$ and change bases using similarity.
}

\ELI{
Think of a linear transformation as a machine that stretches, rotates, or mixes
directions but never bends. A basis is a set of rulers; the matrix is the
machine\textquotesingle s instruction sheet relative to those rulers.
}

\SCOPE{
Finite-dimensional vector spaces over $\mathbb{R}$ or $\mathbb{C}$ with fixed
ordered bases. Infinite-dimensional extensions require operators on topological
vector spaces and may not admit finite matrices. Nonlinear maps are excluded.
Degenerate cases include zero map and identity map.
}

\CONFUSIONS{
Do not confuse a linear transformation with its matrix: different bases give
different matrices for the same map. Similar matrices represent the same linear
operator in different bases. Linear independence concerns vectors, not columns
alone unless a basis has been fixed. Affine maps include translations and are
not linear unless the translation is zero.
}

\APPLICATIONS{
\begin{bullets}
\item Mathematical foundations: invariant subspaces, spectral theorems.
\item Computational modeling: discretized operators represented by matrices.
\item Physical interpretations: rotations, reflections, and projections.
\item Algorithmic implications: complexity via matrix multiplication and rank.
\end{bullets}
}

\textbf{ANALYTIC STRUCTURE.}
Linear operators are linear and often represented by matrices with algebraic
structure (triangular, diagonal, normal). Many properties are basis-invariant:
rank, determinant, trace, eigenvalues, and minimal polynomial.

\textbf{CANONICAL LINKS.}
Change of basis links operators via similarity. Composition corresponds to
matrix multiplication. Rank-nullity connects dimensions of kernel and image.
Diagonalization relates to eigenvectors and similarity to a diagonal matrix.

\textbf{PROBLEM-TYPE RECOGNITION HEURISTICS.}
\begin{bullets}
\item Mentions of basis, coordinates, or change of basis imply matrix forms.
\item Composition of maps hints at matrix products or chain mappings.
\item Requests for kernel, image, or rank use rank-nullity.
\item Questions about invariants under similarity suggest trace, determinant,
and eigenvalues.
\end{bullets}

\textbf{SOLUTION STRATEGY BLUEPRINT.}
\begin{bullets}
\item Translate basis data to coordinate vectors.
\item Assemble matrix columns via images of basis vectors.
\item Apply composition or change-of-basis identities.
\item Reduce to canonical forms when possible (triangular, diagonal).
\item Interpret invariants and validate via limit or symmetry checks.
\end{bullets}

\textbf{CONCEPTUAL INVARIANTS.}
Rank, determinant, trace, characteristic polynomial, minimal polynomial, and
Jordan canonical structure are invariant under change of basis.

\textbf{EDGE INTUITION.}
As parameters go to zero, linear maps approach the zero map; repeated
composition amplifies eigen-directions according to eigenvalues; near
singularity, condition numbers grow and numerical instability can occur.

\section{Glossary}
\glossx{Linear Transformation}{
A map $T:V\to W$ satisfying additivity and homogeneity.}{
Converts geometric algebra into matrix computations and preserves structure.}{
Specify $T$ by images of a basis; extend linearly to all of $V$.}{
Like a perfect scaling and mixing machine: it stretches and blends but does not
warp.}{
Pitfall: an affine map $x\mapsto Ax+b$ is not linear unless $b=0$.}

\glossx{Matrix Representation}{
The matrix $[T]_{\mathcal{B},\mathcal{C}}$ encoding $T$ with respect to bases
$\mathcal{B},\mathcal{C}$.}{
Enables coordinate computations and reveals invariants via similarity.}{
Compute columns as coordinates of $T(b_j)$ in $\mathcal{C}$.}{
It is the recipe of $T$ relative to chosen rulers.}{
Example: changing basis alters the matrix but not the underlying operator.}

\glossx{Change-of-Basis Matrix}{
$P=[\mathrm{Id}]_{\mathcal{B},\mathcal{B}'}$ mapping coordinates from
$\mathcal{B}'$ to $\mathcal{B}$.}{
Transfers coordinate vectors between bases and relates matrix representations.}{
Build $P$ from coordinates of $\mathcal{B}'$ in $\mathcal{B}$; then
$[v]_{\mathcal{B}}=P[v]_{\mathcal{B}'}$.}{
It is a translation dictionary between coordinate languages.}{
Pitfall: invert the direction correctly; mixing $P$ and $P^{-1}$ flips roles.}

\glossx{Similarity}{
Relation $A\sim B$ if $\exists P$ invertible with $B=P^{-1}AP$.}{
Expresses same linear operator in two bases; preserves spectrum and trace.}{
Construct $P$ from basis change; compute $B=P^{-1}AP$.}{
Two instruction sheets for the same machine with different rulers.}{
Pitfall: $AP=PB$ is equivalent to $B=P^{-1}AP$ only if $P$ is invertible.}

\section{Symbol Ledger}
\varmapStart
\var{\mathbb{F}}{Base field, typically $\mathbb{R}$ or $\mathbb{C}$.}
\var{V,W,U}{Finite-dimensional vector spaces over $\mathbb{F}$.}
\var{n,m,d}{Dimensions: $n=\dim V$, $m=\dim W$, $d=\dim U$.}
\var{\mathcal{B},\mathcal{C},\mathcal{D}}{Ordered bases of $V,W,U$.}
\var{[v]_{\mathcal{B}}}{Coordinate vector of $v\in V$ in basis $\mathcal{B}$.}
\var{[T]_{\mathcal{B},\mathcal{C}}}{Matrix of $T:V\to W$ wrt bases.}
\var{A,B,S,T}{Matrices or linear maps; context specifies.}
\var{I_n}{Identity matrix in $\mathbb{F}^{n\times n}$.}
\var{P}{Invertible change-of-basis matrix.}
\var{\ker T}{Kernel $\{v:T(v)=0\}$.}
\var{\operatorname{im}T}{Image $\{T(v):v\in V\}$.}
\var{\operatorname{rank}T}{Dimension of $\operatorname{im}T$.}
\var{\operatorname{null}T}{Dimension of $\ker T$.}
\var{\operatorname{tr}A}{Trace of matrix $A$.}
\var{\det A}{Determinant of matrix $A$.}
\var{\lambda}{Eigenvalue; $v\ne 0$ with $T(v)=\lambda v$.}
\var{p_A(\lambda)}{Characteristic polynomial $\det(\lambda I-A)$.}
\var{J}{Jordan form when defined.}
\varmapEnd

\section{Formula Canon — One Formula Per Page}
\FormulaPage{1}{Coordinate Relation and Matrix Columns}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
For finite-dimensional $V,W$ with bases $\mathcal{B}=(b_1,\dots,b_n)$ and
$\mathcal{C}=(c_1,\dots,c_m)$, the matrix $A=[T]_{\mathcal{B},\mathcal{C}}$
is defined by its columns $A e_j=[T(b_j)]_{\mathcal{C}}$, and for all $v\in V$,
$[T(v)]_{\mathcal{C}}=A[v]_{\mathcal{B}}$.

\WHAT{
This identity computes coordinates of $T(v)$ from coordinates of $v$ via the
matrix of $T$; it also prescribes how to construct the matrix from basis images.
}

\WHY{
It is the bridge from abstract linear maps to concrete computation. It enables
evaluation, composition, and analysis using matrix algebra.
}

\FORMULA{
\[
A=\big[[T(b_1)]_{\mathcal{C}}\ \cdots\ [T(b_n)]_{\mathcal{C}}\big],\quad
[T(v)]_{\mathcal{C}}=A[v]_{\mathcal{B}}.
\]
}

\CANONICAL{
$V,W$ are finite-dimensional over $\mathbb{F}$ with fixed ordered bases.
$T:V\to W$ is linear. Coordinates are taken in the specified bases.
}

\PRECONDS{
\begin{bullets}
\item $\mathcal{B}$ and $\mathcal{C}$ are bases (linearly independent and
spanning).
\item $T$ is linear.
\end{bullets}
}

\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
Let $\mathcal{B}$ be a basis of $V$ and $w_j\in W$. There exists a unique
linear map $T$ with $T(b_j)=w_j$ for all $j$.
\end{lemma}
\begin{proof}
Define $T$ on $V$ by linear extension: for $v=\sum_j \alpha_j b_j$, set
$T(v)=\sum_j \alpha_j w_j$. This is well defined because the coefficients
are unique in a basis expansion, linear by construction, and satisfies
$T(b_j)=w_j$. If $S$ is another such linear map, then for any $v$,
$S(v)=\sum_j \alpha_j S(b_j)=\sum_j \alpha_j w_j=T(v)$, hence $S=T$.
\qedhere
\end{proof}

\DERIVATION{
\begin{align*}
\text{Step 1 (Basis expansion):}\ & v=\sum_{j=1}^n \alpha_j b_j,\ 
[v]_{\mathcal{B}}=(\alpha_1,\dots,\alpha_n)^\top.\\
\text{Step 2 (Linearity):}\ & T(v)=\sum_{j=1}^n \alpha_j T(b_j).\\
\text{Step 3 (Coordinates in $\mathcal{C}$):}\ &
[T(v)]_{\mathcal{C}}=\sum_{j=1}^n \alpha_j [T(b_j)]_{\mathcal{C}}.\\
\text{Step 4 (Matrix assembly):}\ &
\big[[T(b_1)]_{\mathcal{C}}\ \cdots\ [T(b_n)]_{\mathcal{C}}\big]
(\alpha_1,\dots,\alpha_n)^\top.\\
\text{Step 5 (Conclude):}\ &
[T(v)]_{\mathcal{C}}=A [v]_{\mathcal{B}},\ \text{with columns as stated.}
\end{align*}
}

\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Compute $T$ on basis vectors $b_j$.
\item Express each $T(b_j)$ in $\mathcal{C}$-coordinates as a column.
\item Form $A$ and multiply by $[v]_{\mathcal{B}}$ for any $v$.
\item Validate via linearity and a spot-check on random $v$.
\end{bullets}

\EQUIV{
\begin{bullets}
\item $A e_j=[T(b_j)]_{\mathcal{C}}$ where $e_j$ are standard basis vectors.
\item If $\mathcal{B}=\mathcal{C}$ and are standard bases, $A$ is the usual
matrix acting on column vectors.
\end{bullets}
}

\LIMITS{
\begin{bullets}
\item Without a basis, matrices are undefined.
\item Infinite-dimensional spaces require operator theory; finite matrices may
not capture $T$ fully.
\end{bullets}
}

\INPUTS{$\mathcal{B},\mathcal{C}$ bases, $T$ defined on $\mathcal{B}$.}

\DERIVATION{
\begin{align*}
\text{Example: }& V=W=\mathbb{R}^2,\ \mathcal{B}=\mathcal{C}=\{e_1,e_2\}.\\
& T(e_1)=(1,2)^\top,\ T(e_2)=(3,1)^\top.\\
& A=\begin{bmatrix}1&3\\2&1\end{bmatrix},\ v=(4,5)^\top,\ [v]_{\mathcal{B}}=v.\\
& [T(v)]_{\mathcal{C}}=A v=\begin{bmatrix}1&3\\2&1\end{bmatrix}
\begin{bmatrix}4\\5\end{bmatrix}=
\begin{bmatrix}1\cdot 4+3\cdot 5\\2\cdot 4+1\cdot 5\end{bmatrix}=
\begin{bmatrix}19\\13\end{bmatrix}.
\end{align*}
}

\RESULT{
Matrix columns are the images of basis vectors in coordinates, and applying
$T$ equals multiplying the coordinate vector by $A$.
}

\UNITCHECK{
Dimensions: $A$ is $m\times n$, $[v]_{\mathcal{B}}$ is $n\times 1$, product
is $m\times 1$ as required for $[T(v)]_{\mathcal{C}}$.
}

\PITFALLS{
\begin{bullets}
\item Using coordinates of $T(b_j)$ in the wrong basis.
\item Ordering basis vectors inconsistently across columns.
\end{bullets}
}

\INTUITION{
Each column is how one basis direction is transformed; general vectors just
mix these columns by their coordinates.
}

\CANONICAL{
\begin{bullets}
\item $[T(v)]_{\mathcal{C}}=[T]_{\mathcal{B},\mathcal{C}}\,[v]_{\mathcal{B}}$.
\item Columns are $[T(b_j)]_{\mathcal{C}}$.
\end{bullets}
}

\FormulaPage{2}{Composition as Matrix Multiplication and Inverse}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
For $T:V\to W$ and $S:W\to U$ with bases $\mathcal{B},\mathcal{C},\mathcal{D}$,
$[S\circ T]_{\mathcal{B},\mathcal{D}}=[S]_{\mathcal{C},\mathcal{D}}\,
[T]_{\mathcal{B},\mathcal{C}}$. If $T$ is invertible, then
$[T^{-1}]_{\mathcal{C},\mathcal{B}}=[T]_{\mathcal{B},\mathcal{C}}^{-1}$.

\WHAT{
Composition of linear maps corresponds to matrix multiplication; inversion of
maps corresponds to matrix inversion under appropriate bases.
}

\WHY{
This identifies the categorical composition with algebraic multiplication,
powering algorithmic chaining of transformations and solving inverse problems.
}

\FORMULA{
\[
[S\circ T]_{\mathcal{B},\mathcal{D}}=
[S]_{\mathcal{C},\mathcal{D}}\,[T]_{\mathcal{B},\mathcal{C}},\quad
[T^{-1}]_{\mathcal{C},\mathcal{B}}=[T]_{\mathcal{B},\mathcal{C}}^{-1}.
\]
}

\CANONICAL{
$V,W,U$ finite-dimensional over $\mathbb{F}$, linear $T,S$, chosen bases
$\mathcal{B},\mathcal{C},\mathcal{D}$. For inversion, $T$ must be bijective.
}

\PRECONDS{
\begin{bullets}
\item Correct alignment of bases: the codomain basis of $T$ equals the domain
basis of $S$.
\item Inverse exists iff $T$ is bijective and $[T]$ is invertible.
\end{bullets}
}

\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
For all $v\in V$, $[S(T(v))]_{\mathcal{D}}=
[S]_{\mathcal{C},\mathcal{D}}[T(v)]_{\mathcal{C}}$.
\end{lemma}
\begin{proof}
Apply the coordinate relation from Formula 1 to $S$ with input $T(v)$, which
has coordinates $[T(v)]_{\mathcal{C}}$ in $\mathcal{C}$. Then
$[S(T(v))]_{\mathcal{D}}=[S]_{\mathcal{C},\mathcal{D}}[T(v)]_{\mathcal{C}}$,
by linearity and definition of matrix representation. \qedhere
\end{proof}

\DERIVATION{
\begin{align*}
\text{Step 1:}\ & [T(v)]_{\mathcal{C}}=[T]_{\mathcal{B},\mathcal{C}}
[v]_{\mathcal{B}}.\\
\text{Step 2:}\ & [S(T(v))]_{\mathcal{D}}=[S]_{\mathcal{C},\mathcal{D}}
[T(v)]_{\mathcal{C}}.\\
\text{Step 3:}\ &
[S(T(v))]_{\mathcal{D}}=[S]_{\mathcal{C},\mathcal{D}}\,[T]_{\mathcal{B},\mathcal{C}}
[v]_{\mathcal{B}}.\\
\text{Step 4:}\ & [S\circ T]_{\mathcal{B},\mathcal{D}}=
[S]_{\mathcal{C},\mathcal{D}}[T]_{\mathcal{B},\mathcal{C}}.\\
\text{Inverse:}\ & T^{-1}(T(v))=v\ \Rightarrow\
[v]_{\mathcal{B}}=[T^{-1}]_{\mathcal{C},\mathcal{B}}[T(v)]_{\mathcal{C}}.\\
& \text{Thus }I=[T^{-1}]_{\mathcal{C},\mathcal{B}}[T]_{\mathcal{B},\mathcal{C}}
\Rightarrow [T^{-1}]=[T]^{-1}.
\end{align*}
}

\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Align bases; if needed, insert change-of-basis to align.
\item Multiply matrices in order of application.
\item For inverse, verify invertibility and compute matrix inverse.
\item Validate on a test vector.
\end{bullets}

\EQUIV{
\begin{bullets}
\item $(S\circ T)^k=S^k\circ T^k$ does not generally hold; but powers satisfy
$[T^k]=[T]^k$ when domain and codomain bases coincide.
\item Identity: $[\mathrm{Id}]_{\mathcal{B},\mathcal{B}}=I$.
\end{bullets}
}

\LIMITS{
\begin{bullets}
\item Nonaligned bases require change-of-basis matrices.
\item Noninvertible $[T]$ implies no $T^{-1}$ and no matrix inverse.
\end{bullets}
}

\INPUTS{$[T]_{\mathcal{B},\mathcal{C}},\ [S]_{\mathcal{C},\mathcal{D}},\
[v]_{\mathcal{B}}$.}

\DERIVATION{
\begin{align*}
\text{Example: }& [T]=\begin{bmatrix}1&2\\0&1\end{bmatrix},\
[S]=\begin{bmatrix}2&0\\3&1\end{bmatrix}.\\
& [S\circ T]=\begin{bmatrix}2&0\\3&1\end{bmatrix}
\begin{bmatrix}1&2\\0&1\end{bmatrix}=
\begin{bmatrix}2&4\\3&7\end{bmatrix}.\\
& v=(1,3)^\top,\ T(v)=(1+6,0+3)^\top=(7,3)^\top.\\
& S(T(v))=(2\cdot 7+0\cdot 3,3\cdot 7+1\cdot 3)^\top=(14,24)^\top.\\
& [S\circ T]v=\begin{bmatrix}2&4\\3&7\end{bmatrix}
\begin{bmatrix}1\\3\end{bmatrix}=\begin{bmatrix}14\\24\end{bmatrix}.
\end{align*}
}

\RESULT{
Composition corresponds to matrix multiplication; inverse maps correspond to
matrix inverses when they exist.
}

\UNITCHECK{
Matrix dimensions: $(p\times m)(m\times n)=(p\times n)$, consistent with
$V\to U$. Inverse satisfies $[T^{-1}][T]=I$.
}

\PITFALLS{
\begin{bullets}
\item Reversing order of multiplication.
\item Assuming invertibility without checking determinant or rank.
\end{bullets}
}

\INTUITION{
Applying $T$ then $S$ stacks column-mixing instructions in succession, which
is exactly what matrix multiplication encodes.
}

\CANONICAL{
\begin{bullets}
\item $[S\circ T]=[S][T]$ (aligned bases).
\item $[T^{-1}]=[T]^{-1}$ (invertible case).
\end{bullets}
}

\FormulaPage{3}{Change of Basis and Similarity Invariants}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Let $T:V\to V$ and $\mathcal{B},\mathcal{B}'$ be two bases of $V$. If
$P=[\mathrm{Id}]_{\mathcal{B},\mathcal{B}'}$ maps $\mathcal{B}'$-coordinates
to $\mathcal{B}$-coordinates, then
$[T]_{\mathcal{B}',\mathcal{B}'}=P^{-1}[T]_{\mathcal{B},\mathcal{B}}P$.
Trace, determinant, and characteristic polynomial are invariant under
similarity.

\WHAT{
Relates matrix representations of the same operator in different bases, and
identifies basis-invariant quantities.
}

\WHY{
It separates representation from intrinsic properties and enables canonical
forms and spectral analysis independent of coordinate choices.
}

\FORMULA{
\[
A'=[T]_{\mathcal{B}',\mathcal{B}'}=P^{-1}AP,\quad
\operatorname{tr}A'=\operatorname{tr}A,\ \det A'=\det A,\ 
p_{A'}(\lambda)=p_A(\lambda).
\]
}

\CANONICAL{
$V$ finite-dimensional, $T:V\to V$ linear, $\mathcal{B},\mathcal{B}'$ ordered
bases, $P$ invertible change-of-basis matrix from $\mathcal{B}'$ to
$\mathcal{B}$.
}

\PRECONDS{
\begin{bullets}
\item $P$ is invertible and computed in correct direction.
\item $A=[T]_{\mathcal{B},\mathcal{B}}$ exists.
\end{bullets}
}

\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
For $P$ invertible, $\operatorname{tr}(P^{-1}AP)=\operatorname{tr}(A)$ and
$\det(P^{-1}AP)=\det(A)$.
\end{lemma}
\begin{proof}
Trace: $\operatorname{tr}(P^{-1}AP)=\operatorname{tr}(AP P^{-1})
=\operatorname{tr}(A I)=\operatorname{tr}(A)$ by cyclicity of trace.
Determinant: $\det(P^{-1}AP)=\det(P^{-1})\det(A)\det(P)=\det(A)$ since
$\det(P)\ne 0$ and multiplicativity holds. \qedhere
\end{proof}

\DERIVATION{
\begin{align*}
\text{Step 1:}\ & [v]_{\mathcal{B}}=P[v]_{\mathcal{B}'},\ 
[v]_{\mathcal{B}'}=P^{-1}[v]_{\mathcal{B}}.\\
\text{Step 2:}\ & [T(v)]_{\mathcal{B}}=A[v]_{\mathcal{B}}.\\
\text{Step 3:}\ &
[T(v)]_{\mathcal{B}'}=P^{-1}[T(v)]_{\mathcal{B}}=P^{-1}A[v]_{\mathcal{B}}
=P^{-1}A P [v]_{\mathcal{B}'}.\\
\text{Step 4:}\ & A'=P^{-1}AP.\\
\text{Invariants:}\ & p_{A'}(\lambda)=\det(\lambda I-P^{-1}AP)
=\det(P^{-1})\det(\lambda P-AP)\det(P^{-1})^{-1}\\
&=\det(\lambda I-A)=p_A(\lambda),\ \text{and trace, determinant as in lemma.}
\end{align*}
}

\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Build $P$ from coordinates of $\mathcal{B}'$ in $\mathcal{B}$.
\item Compute $A'=P^{-1}AP$.
\item Compare invariants to verify correctness.
\end{bullets}

\EQUIV{
\begin{bullets}
\item $A\sim A'$ iff they represent the same linear operator in different
bases.
\item If $P$ is orthogonal or unitary, $A'$ is a similarity via isometry.
\end{bullets}
}

\LIMITS{
\begin{bullets}
\item Noninvertible $P$ invalidates change of basis.
\item Similarity preserves spectrum, not necessarily numerical conditioning.
\end{bullets}
}

\INPUTS{$A=[T]_{\mathcal{B}},\ P=[\mathrm{Id}]_{\mathcal{B},\mathcal{B}' }$.}

\DERIVATION{
\begin{align*}
\text{Example: }& A=\begin{bmatrix}1&2\\0&3\end{bmatrix},\
P=\begin{bmatrix}1&1\\0&1\end{bmatrix}.\\
& P^{-1}=\begin{bmatrix}1&-1\\0&1\end{bmatrix}.\
A'=P^{-1}AP=\begin{bmatrix}1&-1\\0&1\end{bmatrix}
\begin{bmatrix}1&2\\0&3\end{bmatrix}
\begin{bmatrix}1&1\\0&1\end{bmatrix}\\
&=\begin{bmatrix}1&1\\0&3\end{bmatrix}
\begin{bmatrix}1&1\\0&1\end{bmatrix}=
\begin{bmatrix}1&2\\0&3\end{bmatrix}=A.\\
& \operatorname{tr}A=\operatorname{tr}A'=4,\ \det A=\det A'=3.
\end{align*}
}

\RESULT{
$A'=P^{-1}AP$ represents the same operator in $\mathcal{B}'$; spectrum,
trace, determinant, and characteristic polynomial are invariant.
}

\UNITCHECK{
Matrix multiplications are dimensionally consistent; invariants are scalar.
}

\PITFALLS{
\begin{bullets}
\item Using $P$ in the wrong direction, producing $PAP^{-1}$ incorrectly.
\item Assuming eigenvectors are preserved by similarity; they transform by $P$.
\end{bullets}
}

\INTUITION{
Changing rulers changes coordinates; the machine is the same, so intrinsic
quantities do not change.
}

\CANONICAL{
\begin{bullets}
\item $A' = P^{-1} A P$ (similarity).
\item $\operatorname{tr}, \det, p_A$ are similarity invariants.
\end{bullets}
}

\FormulaPage{4}{Rank-Nullity Theorem}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
For $T:V\to W$ linear with $V$ finite-dimensional,
$\dim V=\operatorname{rank}T+\operatorname{null}T$.

\WHAT{
Relates the dimension of the domain to the dimensions of the image and kernel.
}

\WHY{
It quantifies how many independent directions are flattened versus preserved,
guiding solvability of linear systems and invertibility criteria.
}

\FORMULA{
\[
\dim V=\dim \operatorname{im}T+\dim \ker T.
\]
}

\CANONICAL{
$V$ finite-dimensional, $T$ linear. No basis choice needed; statement is
basis-invariant.
}

\PRECONDS{
\begin{bullets}
\item Finite-dimensionality of $V$.
\end{bullets}
}

\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
If $(k_1,\dots,k_r)$ is a basis of $\ker T$ and extend to a basis of $V$ by
$(k_1,\dots,k_r,u_1,\dots,u_s)$, then $(T(u_1),\dots,T(u_s))$ is a basis of
$\operatorname{im}T$.
\end{lemma}
\begin{proof}
Spanning: for any $v\in V$, $v=\sum a_i k_i+\sum b_j u_j$, then
$T(v)=\sum b_j T(u_j)$, so images span $\operatorname{im}T$. Independence:
if $\sum b_j T(u_j)=0$, then $T(\sum b_j u_j)=0$, so $\sum b_j u_j\in\ker T$,
hence can be written as $\sum a_i k_i$. Uniqueness of coordinates in the basis
implies all $b_j=0$. Thus they form a basis. \qedhere
\end{proof}

\DERIVATION{
\begin{align*}
\text{Step 1:}\ & \text{Choose a basis }(k_1,\dots,k_r) \text{ of }\ker T.\\
\text{Step 2:}\ & \text{Extend to a basis of }V:
(k_1,\dots,k_r,u_1,\dots,u_s).\\
\text{Step 3:}\ & \text{Lemma gives }(T(u_1),\dots,T(u_s)) \text{ is a basis
of }\operatorname{im}T.\\
\text{Step 4:}\ & \dim V=r+s,\ \dim\ker T=r,\ \dim\operatorname{im}T=s.\\
\text{Step 5:}\ & \dim V=\operatorname{null}T+\operatorname{rank}T.
\end{align*}
}

\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Compute a basis for $\ker T$ or $\operatorname{im}T$.
\item Use dimension counts to infer the other.
\item Check with matrix rank if a representation is available.
\end{bullets}

\EQUIV{
\begin{bullets}
\item For $A\in\mathbb{F}^{m\times n}$, $n=\operatorname{rank}A+
\operatorname{null}A$.
\item Invertibility iff $\operatorname{null}T=0$ and $\operatorname{rank}T=
\dim V$.
\end{bullets}
}

\LIMITS{
\begin{bullets}
\item Infinite-dimensional spaces require additional care; equality may fail
without finite dimensionality.
\end{bullets}
}

\INPUTS{$T$ or $A=[T]$; dimensions of $V$; bases of kernel or image.}

\DERIVATION{
\begin{align*}
\text{Example: }& A=\begin{bmatrix}1&2&3\\0&1&1\end{bmatrix}\in\mathbb{R}^{2\times 3}.\\
& \operatorname{rank}A=2\ (\text{rows independent}),\ n=3.\\
& \operatorname{null}A=3-2=1.\\
& \dim V=3=\operatorname{rank}A+\operatorname{null}A.
\end{align*}
}

\RESULT{
The dimensions of kernel and image partition the domain dimension.
}

\UNITCHECK{
All quantities are dimensions (nonnegative integers) summing to $\dim V$.
}

\PITFALLS{
\begin{bullets}
\item Confusing rank with number of nonzero rows without row reduction.
\item Forgetting that rank is at most $\min(m,n)$ for $A\in\mathbb{F}^{m\times n}$.
\end{bullets}
}

\INTUITION{
Each independent equation removes one degree of freedom; the remaining are
solutions (kernel). The rest map injectively to the image.
}

\CANONICAL{
\begin{bullets}
\item $\dim V=\operatorname{rank}T+\operatorname{null}T$.
\end{bullets}
}

\FormulaPage{5}{Eigenvalues, Diagonalization, and Powers}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
A linear operator $T:V\to V$ is diagonalizable over $\mathbb{F}$ iff there
exists a basis of eigenvectors of $T$, equivalently $A=[T]_{\mathcal{B}}$ is
similar to a diagonal $D=\operatorname{diag}(\lambda_1,\dots,\lambda_n)$,
$A=PDP^{-1}$. Then $A^k=PD^kP^{-1}$ for all integers $k\ge 0$.

\WHAT{
Characterizes when a linear operator can be represented by a diagonal matrix,
and expresses powers of $A$ via eigenvalues.
}

\WHY{
Diagonalization simplifies computation of powers and exponentials and reveals
the spectral structure governing dynamics under iteration.
}

\FORMULA{
\[
A=PDP^{-1}\ \Rightarrow\ A^k=PD^kP^{-1},\quad
D^k=\operatorname{diag}(\lambda_1^k,\dots,\lambda_n^k).
\]
}

\CANONICAL{
$V$ finite-dimensional, $T$ has $n$ linearly independent eigenvectors over
$\mathbb{F}$. Basis $\mathcal{B}$ is the eigenbasis; $P$ columns are
eigenvectors.
}

\PRECONDS{
\begin{bullets}
\item Enough eigenvectors to form a basis (geometric multiplicities sum to $n$).
\item Field contains eigenvalues (work over $\mathbb{C}$ if needed).
\end{bullets}
}

\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
If $A=PDP^{-1}$, then $A^k=PD^kP^{-1}$ for all $k\ge 0$.
\end{lemma}
\begin{proof}
By induction. Base $k=0$: $A^0=I=PI P^{-1}=PD^0P^{-1}$. Assume
$A^k=PD^kP^{-1}$. Then $A^{k+1}=A\cdot A^k=(PDP^{-1})(PD^kP^{-1})
=PD(P^{-1}P)D^kP^{-1}=PD^{k+1}P^{-1}$. \qedhere
\end{proof}

\DERIVATION{
\begin{align*}
\text{Step 1:}\ & \text{Find eigenpairs }(v_i,\lambda_i),\ Av_i=\lambda_i v_i.\\
\text{Step 2:}\ & P=[v_1\ \cdots\ v_n],\ D=\operatorname{diag}(\lambda_i).\\
\text{Step 3:}\ & AP=A[v_1\ \cdots\ v_n]=[\lambda_1 v_1\ \cdots\ \lambda_n v_n]
=PD.\\
\text{Step 4:}\ & A=PD P^{-1},\ \text{and by lemma }A^k=PD^kP^{-1}.\\
\text{Step 5:}\ & \text{Compute }A^k v=P D^k P^{-1} v\ \text{efficiently}.
\end{align*}
}

\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Compute eigenvalues and eigenvectors.
\item Check linear independence; form $P$.
\item Compute $D$ and evaluate $D^k$ or functions of $D$.
\item Transform back via $P$.
\end{bullets}

\EQUIV{
\begin{bullets}
\item $A$ diagonalizable iff its minimal polynomial splits and has simple roots.
\item Over $\mathbb{C}$, normal matrices are unitarily diagonalizable.
\end{bullets}
}

\LIMITS{
\begin{bullets}
\item Nondiagonalizable matrices require Jordan form; powers involve polynomials
in $k$ times $\lambda^k$.
\item Defective eigenvalues reduce eigenvector count.
\end{bullets}
}

\INPUTS{$A$, eigenvalues $\lambda_i$, eigenvectors $v_i$, exponent $k$.}

\DERIVATION{
\begin{align*}
\text{Example: }& A=\begin{bmatrix}4&0\\0&2\end{bmatrix},\ 
\lambda_1=4,\ \lambda_2=2,\\
& P=I,\ D=A,\ A^5=D^5=\begin{bmatrix}4^5&0\\0&2^5\end{bmatrix}=
\begin{bmatrix}1024&0\\0&32\end{bmatrix}.
\end{align*}
}

\RESULT{
Diagonalizable operators reduce to scaling along eigen-directions, and powers
act by raising eigenvalues to powers.
}

\UNITCHECK{
Matrix products have consistent sizes; eigenvalues raised to powers are scalars.
}

\PITFALLS{
\begin{bullets}
\item Confusing algebraic and geometric multiplicities.
\item Forming $P$ with dependent eigenvectors causes noninvertibility.
\end{bullets}
}

\INTUITION{
In the right coordinates, the map scales each axis independently; iterating
just scales repeatedly along each axis.
}

\CANONICAL{
\begin{bullets}
\item $A=PDP^{-1}$ with $D$ diagonal, $A^k=PD^kP^{-1}$.
\end{bullets}
}

\section{10 Exhaustive Problems and Solutions}
\ProblemPage{1}{Build a Matrix from Basis Images and Compute Kernel}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Construct $[T]$ from $T$ on a basis; compute $T(v)$ and $\ker T$ dimension.

\PROBLEM{
Let $T:\mathbb{R}^3\to\mathbb{R}^2$ be linear with
$T(e_1)=(1,2)^\top$, $T(e_2)=(0,1)^\top$, $T(e_3)=(3,1)^\top$ where
$e_i$ are the standard basis of $\mathbb{R}^3$. Find $A=[T]$, compute
$T(v)$ for $v=(2,-1,4)^\top$, and determine $\operatorname{null}T$.
}

\MODEL{
\[
A=\big[T(e_1)\ T(e_2)\ T(e_3)\big]\in\mathbb{R}^{2\times 3},\quad
[T(v)]=A v.
\]
}

\ASSUMPTIONS{
\begin{bullets}
\item Standard bases in domain and codomain.
\item Linearity holds for $T$.
\end{bullets}
}

\varmapStart
\var{A}{Matrix representation $2\times 3$.}
\var{v}{Input vector in $\mathbb{R}^3$.}
\var{\operatorname{rank}A}{Rank of $A$.}
\var{\operatorname{null}A}{Nullity of $A$.}
\varmapEnd

\WHICHFORMULA{
Formula 1 for matrix columns and coordinate relation; Formula 4 for rank-nullity.
}

\GOVERN{
\[
A=\begin{bmatrix}1&0&3\\2&1&1\end{bmatrix},\quad T(v)=A v,\quad
3=\operatorname{rank}A+\operatorname{null}A.
\]
}

\INPUTS{$T(e_1)=(1,2)^\top$, $T(e_2)=(0,1)^\top$, $T(e_3)=(3,1)^\top$, $v=(2,-1,4)^\top$.}

\DERIVATION{
\begin{align*}
\text{Step 1: }& A=\begin{bmatrix}1&0&3\\2&1&1\end{bmatrix}.\\
\text{Step 2: }& T(v)=A v=
\begin{bmatrix}1&0&3\\2&1&1\end{bmatrix}
\begin{bmatrix}2\\-1\\4\end{bmatrix}=
\begin{bmatrix}1\cdot 2+0\cdot (-1)+3\cdot 4\\
2\cdot 2+1\cdot (-1)+1\cdot 4\end{bmatrix}=
\begin{bmatrix}14\\7\end{bmatrix}.\\
\text{Step 3: }& \operatorname{rank}A=\operatorname{rank}
\begin{bmatrix}1&0&3\\2&1&1\end{bmatrix}=
2\ \text{(rows independent)}.\\
\text{Step 4: }& \operatorname{null}A=3-2=1.
\end{align*}
}

\RESULT{
$A=\begin{bmatrix}1&0&3\\2&1&1\end{bmatrix}$, $T(v)=(14,7)^\top$,
$\operatorname{null}T=1$.
}

\UNITCHECK{
$A$ is $2\times 3$, $v$ is $3\times 1$, product is $2\times 1$. Dimensions in
rank-nullity sum to $3$.
}

\EDGECASES{
\begin{bullets}
\item If columns are dependent to rank $1$, nullity would be $2$.
\item If rank were $3$ (impossible for $2\times 3$), that would contradict
$\operatorname{rank}\le 2$.
\end{bullets}
}

\ALTERNATE{
Row-reduce $A$ to compute rank; compute a basis of $\ker A$ by solving $Ax=0$.
}

\VALIDATION{
\begin{bullets}
\item Check $T(e_i)$ equals given columns.
\item Numerically confirm $Av$ via direct linear combination of columns.
\end{bullets}
}

\INTUITION{
The matrix columns are $T$ acting on axes; nullity counts degrees of freedom
sent to zero.
}

\CANONICAL{
\begin{bullets}
\item Columns encode $T$.
\item Rank-nullity partitions domain dimension.
\end{bullets}
}

\ProblemPage{2}{Composition and Order of Multiplication}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Verify $[S\circ T]=[S][T]$ and compute $(S\circ T)(v)$.

\PROBLEM{
Let $T:\mathbb{R}^2\to\mathbb{R}^2$ with
$[T]=\begin{bmatrix}0&1\\-1&0\end{bmatrix}$ (a rotation by $90^\circ$),
and $S:\mathbb{R}^2\to\mathbb{R}^2$ with
$[S]=\begin{bmatrix}2&0\\0&3\end{bmatrix}$. Compute $[S\circ T]$, and
evaluate $(S\circ T)(v)$ for $v=(1,2)^\top$.
}

\MODEL{
\[
[S\circ T]=[S][T],\quad (S\circ T)(v)=[S\circ T]v.
\]
}

\ASSUMPTIONS{
\begin{bullets}
\item Standard basis for $\mathbb{R}^2$ in domain and codomain.
\item Composition order: apply $T$ first, then $S$.
\end{bullets}
}

\varmapStart
\var{[T]}{Rotation matrix.}
\var{[S]}{Scaling matrix.}
\var{v}{Input vector in $\mathbb{R}^2$.}
\varmapEnd

\WHICHFORMULA{
Formula 2: composition equals matrix product in aligned bases.
}

\GOVERN{
\[
[S\circ T]=\begin{bmatrix}2&0\\0&3\end{bmatrix}
\begin{bmatrix}0&1\\-1&0\end{bmatrix}.
\]
}

\INPUTS{$[T]=\begin{bmatrix}0&1\\-1&0\end{bmatrix}$, $[S]=\begin{bmatrix}2&0\\0&3\end{bmatrix}$, $v=(1,2)^\top$.}

\DERIVATION{
\begin{align*}
\text{Step 1: }& [S\circ T]=
\begin{bmatrix}2&0\\0&3\end{bmatrix}
\begin{bmatrix}0&1\\-1&0\end{bmatrix}=
\begin{bmatrix}2\cdot 0+0\cdot (-1)&2\cdot 1+0\cdot 0\\
0\cdot 0+3\cdot (-1)&0\cdot 1+3\cdot 0\end{bmatrix}=
\begin{bmatrix}0&2\\-3&0\end{bmatrix}.\\
\text{Step 2: }& T(v)=\begin{bmatrix}0&1\\-1&0\end{bmatrix}
\begin{bmatrix}1\\2\end{bmatrix}=\begin{bmatrix}2\\-1\end{bmatrix}.\\
\text{Step 3: }& S(T(v))=\begin{bmatrix}2&0\\0&3\end{bmatrix}
\begin{bmatrix}2\\-1\end{bmatrix}=\begin{bmatrix}4\\-3\end{bmatrix}.\\
\text{Step 4: }& [S\circ T]v=\begin{bmatrix}0&2\\-3&0\end{bmatrix}
\begin{bmatrix}1\\2\end{bmatrix}=\begin{bmatrix}4\\-3\end{bmatrix}.
\end{align*}
}

\RESULT{
$[S\circ T]=\begin{bmatrix}0&2\\-3&0\end{bmatrix}$, and $(S\circ T)(1,2)^\top
=(4,-3)^\top$.
}

\UNITCHECK{
Matrix sizes are $2\times 2$ multiplied correctly. Output vector size $2\times 1$.
}

\EDGECASES{
\begin{bullets}
\item Reversing order gives $[T\circ S]=\begin{bmatrix}0&3\\-2&0\end{bmatrix}$,
a different map.
\end{bullets}
}

\ALTERNATE{
Compute $(S\circ T)(v)$ by linearity and column interpretation:
$T(e_1)=e_2$, $T(e_2)=-e_1$, then apply $S$.
}

\VALIDATION{
\begin{bullets}
\item Verify $[S][T]$ equals the linear operator by testing on $e_1,e_2$.
\end{bullets}
}

\INTUITION{
Rotate then scale is not the same as scale then rotate; order encodes sequence.
}

\CANONICAL{
\begin{bullets}
\item $[S\circ T]=[S][T]$ with aligned bases.
\end{bullets}
}

\ProblemPage{3}{Change of Basis and Similarity Verification}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Relate $A$ in standard basis to $A'$ in a new basis via similarity.

\PROBLEM{
Let $A=\begin{bmatrix}1&1\\0&2\end{bmatrix}$ in the standard basis
$\mathcal{B}$. Let $\mathcal{B}'=\{v_1=(1,0)^\top,\ v_2=(1,1)^\top\}$ and
$P=[\mathrm{Id}]_{\mathcal{B},\mathcal{B}'}=\begin{bmatrix}1&1\\0&1\end{bmatrix}$.
Compute $A'=P^{-1}AP$ and verify $\operatorname{tr}A'=\operatorname{tr}A$,
$\det A'=\det A$.
}

\MODEL{
\[
A'=P^{-1}AP,\quad \operatorname{tr}A'=\operatorname{tr}A,\ \det A'=\det A.
\]
}

\ASSUMPTIONS{
\begin{bullets}
\item $\mathcal{B}'$ is a basis; $P$ invertible.
\item $A$ is the representation of $T$ in $\mathcal{B}$.
\end{bullets}
}

\varmapStart
\var{A}{Matrix in $\mathcal{B}$.}
\var{P}{Change-of-basis $\mathcal{B}'\to\mathcal{B}$.}
\var{A'}{Matrix in $\mathcal{B}'$.}
\varmapEnd

\WHICHFORMULA{
Formula 3: $A'=P^{-1}AP$ and invariance of trace and determinant.
}

\GOVERN{
\[
P^{-1}=\begin{bmatrix}1&-1\\0&1\end{bmatrix},\quad
A'=P^{-1}AP.
\]
}

\INPUTS{$A=\begin{bmatrix}1&1\\0&2\end{bmatrix}$, $P=\begin{bmatrix}1&1\\0&1\end{bmatrix}$.}

\DERIVATION{
\begin{align*}
\text{Step 1: }& P^{-1}=\begin{bmatrix}1&-1\\0&1\end{bmatrix}.\\
\text{Step 2: }& A'=\begin{bmatrix}1&-1\\0&1\end{bmatrix}
\begin{bmatrix}1&1\\0&2\end{bmatrix}
\begin{bmatrix}1&1\\0&1\end{bmatrix}.\\
\text{Step 3: }& \begin{bmatrix}1&-1\\0&1\end{bmatrix}
\begin{bmatrix}1&1\\0&2\end{bmatrix}=
\begin{bmatrix}1&-1+2\\0&2\end{bmatrix}=
\begin{bmatrix}1&1\\0&2\end{bmatrix}.\\
\text{Step 4: }& A'=\begin{bmatrix}1&1\\0&2\end{bmatrix}
\begin{bmatrix}1&1\\0&1\end{bmatrix}=
\begin{bmatrix}1&2\\0&2\end{bmatrix}.\\
\text{Step 5: }& \operatorname{tr}A=3=\operatorname{tr}A',\
\det A=2=\det A'.
\end{align*}
}

\RESULT{
$A'=\begin{bmatrix}1&2\\0&2\end{bmatrix}$ with $\operatorname{tr}$ and $\det$
equal to those of $A$.
}

\UNITCHECK{
All matrices are $2\times 2$; similarity preserves scalar invariants.
}

\EDGECASES{
\begin{bullets}
\item If $P$ were singular, $A'$ would be undefined.
\end{bullets}
}

\ALTERNATE{
Compute $A'$ by expressing $T(v_1),T(v_2)$ in $\mathcal{B}'$ directly and
forming columns.
}

\VALIDATION{
\begin{bullets}
\item Check $AP=PA'$ holds.
\item Verify eigenvalues: both have $\{1,2\}$.
\end{bullets}
}

\INTUITION{
New basis re-expresses the same action; invariants remain the same.
}

\CANONICAL{
\begin{bullets}
\item $A'=P^{-1}AP$; $\operatorname{tr},\det$ invariant.
\end{bullets}
}

\ProblemPage{4}{Diagonalization and Powers of a Matrix}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Diagonalize when possible and compute $A^k$ efficiently.

\PROBLEM{
Let $A=\begin{bmatrix}3&0\\0&2\end{bmatrix}$. Show $A$ is diagonalizable,
find $P,D$, and compute $A^6$ and $A^k v$ for $v=(1,1)^\top$.
}

\MODEL{
\[
A=PDP^{-1},\ D=\operatorname{diag}(3,2),\ A^k=PD^kP^{-1}.
\]
}

\ASSUMPTIONS{
\begin{bullets}
\item Distinct eigenvalues imply diagonalizability.
\item Standard basis is eigenbasis here.
\end{bullets}
}

\varmapStart
\var{A}{Operator on $\mathbb{R}^2$.}
\var{P}{Matrix of eigenvectors.}
\var{D}{Diagonal matrix of eigenvalues.}
\var{k}{Nonnegative integer exponent.}
\varmapEnd

\WHICHFORMULA{
Formula 5: diagonalization and powers via eigenvalues.
}

\GOVERN{
\[
A=\operatorname{diag}(3,2),\ P=I,\ D=A,\ A^k=D^k.
\]
}

\INPUTS{$A=\operatorname{diag}(3,2)$, $k=6$, $v=(1,1)^\top$.}

\DERIVATION{
\begin{align*}
\text{Step 1: }& \lambda_1=3,\ v_1=e_1;\ \lambda_2=2,\ v_2=e_2.\\
\text{Step 2: }& P=[e_1\ e_2]=I,\ D=\operatorname{diag}(3,2).\\
\text{Step 3: }& A^6=D^6=\operatorname{diag}(3^6,2^6)=
\operatorname{diag}(729,64).\\
\text{Step 4: }& A^k v=(3^k,2^k)^\top.
\end{align*}
}

\RESULT{
$A^6=\begin{bmatrix}729&0\\0&64\end{bmatrix}$ and $A^k(1,1)^\top=(3^k,2^k)^\top$.
}

\UNITCHECK{
Diagonal entries raised to powers are scalars; shapes remain $2\times 2$.
}

\EDGECASES{
\begin{bullets}
\item If eigenvalues were equal with a single eigenvector, $A$ could be
nondiagonalizable.
\end{bullets}
}

\ALTERNATE{
Use repeated squaring to compute $A^6$ directly; still matches diagonal method.
}

\VALIDATION{
\begin{bullets}
\item Check $(A^6)v=A^5(Av)$ numerically for $v=(1,1)^\top$.
\end{bullets}
}

\INTUITION{
Scaling on coordinate axes makes iterates easy: just scale each component.
}

\CANONICAL{
\begin{bullets}
\item $A^k=PD^kP^{-1}$ for diagonalizable $A$.
\end{bullets}
}

\ProblemPage{5}{Alice and Bob: Hidden Similarity}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Two different matrices represent the same operator in different bases.

\PROBLEM{
Alice works in basis $\mathcal{B}$ and records
$A=\begin{bmatrix}0&1\\-2&3\end{bmatrix}$. Bob uses basis
$\mathcal{B}'=\{(1,1)^\top,(1,0)^\top\}$ and records
$A'=\begin{bmatrix}1&1\\-1&2\end{bmatrix}$. Show that they can represent the
same linear operator by finding $P$ such that $A'=P^{-1}AP$.
}

\MODEL{
\[
P=[\mathrm{Id}]_{\mathcal{B},\mathcal{B}'},\quad A'=P^{-1}AP.
\]
}

\ASSUMPTIONS{
\begin{bullets}
\item $\mathcal{B},\mathcal{B}'$ are bases; $P$ invertible.
\end{bullets}
}

\varmapStart
\var{A,A'}{Candidate similar matrices.}
\var{P}{Change-of-basis matrix from $\mathcal{B}'$ to $\mathcal{B}$.}
\varmapEnd

\WHICHFORMULA{
Formula 3: change of basis and similarity.
}

\GOVERN{
\[
P=\begin{bmatrix}1&1\\1&0\end{bmatrix},\quad P^{-1}=
\begin{bmatrix}0&1\\1&-1\end{bmatrix},\quad A'=P^{-1}AP\ ? 
\]
}

\INPUTS{$A=\begin{bmatrix}0&1\\-2&3\end{bmatrix}$, $A'=\begin{bmatrix}1&1\\-1&2\end{bmatrix}$, $P$ as above.}

\DERIVATION{
\begin{align*}
\text{Step 1: }& P^{-1}AP=
\begin{bmatrix}0&1\\1&-1\end{bmatrix}
\begin{bmatrix}0&1\\-2&3\end{bmatrix}
\begin{bmatrix}1&1\\1&0\end{bmatrix}.\\
\text{Step 2: }& \begin{bmatrix}0&1\\1&-1\end{bmatrix}
\begin{bmatrix}0&1\\-2&3\end{bmatrix}=
\begin{bmatrix}-2&3\\2&-2\end{bmatrix}.\\
\text{Step 3: }& \begin{bmatrix}-2&3\\2&-2\end{bmatrix}
\begin{bmatrix}1&1\\1&0\end{bmatrix}=
\begin{bmatrix}1&-2\\0&2\end{bmatrix}.
\end{align*}
}

\RESULT{
$P^{-1}AP=\begin{bmatrix}1&-2\\0&2\end{bmatrix}$, which is not equal to the
given $A'$. Therefore Alice and Bob\textquotesingle s records do not represent
the same operator for this choice of $\mathcal{B}'$. However, hidden similarity
may exist for another $\mathcal{B}'$; equality requires $AP=PA'$.
}

\UNITCHECK{
All matrices are $2\times 2$; similarity computation is valid.
}

\EDGECASES{
\begin{bullets}
\item If instead $A'=\begin{bmatrix}1&-2\\0&2\end{bmatrix}$, they would match.
\end{bullets}
}

\ALTERNATE{
Solve $AP=PA'$ for $P$ with unknown entries and check invertibility to find a
basis where representations match.
}

\VALIDATION{
\begin{bullets}
\item Compute $\operatorname{tr}$ and $\det$; if not equal, similarity is
impossible. Here $\operatorname{tr}A=3$, $\operatorname{tr}A'=3$, and
$\det A=2$, $\det A'=1$, so nonmatching determinants already disprove
similarity.
\end{bullets}
}

\INTUITION{
Similarity is a strong constraint; invariants must agree for any chance.
}

\CANONICAL{
\begin{bullets}
\item Similar matrices share $\operatorname{tr}$ and $\det$; unequal values
preclude similarity.
\end{bullets}
}

\ProblemPage{6}{Linear Cipher: Inversion to Decipher}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Decode a linear cipher using matrix inverse of an invertible transformation.

\PROBLEM{
Bob encrypts $x\in\mathbb{R}^2$ via $y=Ax$ with
$A=\begin{bmatrix}3&1\\4&1\end{bmatrix}$. Alice intercepts $y=(10,14)^\top$.
Recover $x$ by computing $A^{-1}$ and verify $A^{-1}y=x$.
}

\MODEL{
\[
x=A^{-1}y,\quad A^{-1}=\frac{1}{\det A}\operatorname{adj}(A).
\]
}

\ASSUMPTIONS{
\begin{bullets}
\item $A$ is invertible; integers are exact here.
\end{bullets}
}

\varmapStart
\var{A}{Cipher matrix.}
\var{x}{Plain vector.}
\var{y}{Cipher vector.}
\varmapEnd

\WHICHFORMULA{
Formula 2: inverse map corresponds to matrix inverse.
}

\GOVERN{
\[
\det A=3\cdot 1-4\cdot 1=-1\ne 0,\quad
A^{-1}=\frac{1}{-1}\begin{bmatrix}1&-1\\-4&3\end{bmatrix}.
\]
}

\INPUTS{$A=\begin{bmatrix}3&1\\4&1\end{bmatrix}$, $y=(10,14)^\top$.}

\DERIVATION{
\begin{align*}
\text{Step 1: }& A^{-1}=-\begin{bmatrix}1&-1\\-4&3\end{bmatrix}
=\begin{bmatrix}-1&1\\4&-3\end{bmatrix}.\\
\text{Step 2: }& x=A^{-1}y=\begin{bmatrix}-1&1\\4&-3\end{bmatrix}
\begin{bmatrix}10\\14\end{bmatrix}=
\begin{bmatrix}-10+14\\40-42\end{bmatrix}=
\begin{bmatrix}4\\-2\end{bmatrix}.\\
\text{Step 3: }& Ax=\begin{bmatrix}3&1\\4&1\end{bmatrix}
\begin{bmatrix}4\\-2\end{bmatrix}=
\begin{bmatrix}12-2\\16-2\end{bmatrix}=
\begin{bmatrix}10\\14\end{bmatrix}=y.
\end{align*}
}

\RESULT{
$x=(4,-2)^\top$ is the recovered plaintext.
}

\UNITCHECK{
$A$ is $2\times 2$, $y$ is $2\times 1$, $x$ is $2\times 1$.
}

\EDGECASES{
\begin{bullets}
\item If $\det A=0$, decryption is impossible without additional structure.
\end{bullets}
}

\ALTERNATE{
Solve linear system $Ax=y$ via Gaussian elimination to obtain $x$.
}

\VALIDATION{
\begin{bullets}
\item Check $Ax=y$ after computing $x$.
\end{bullets}
}

\INTUITION{
Encryption applies a reversible mixing; decryption applies the opposite mixing.
}

\CANONICAL{
\begin{bullets}
\item Invertible linear maps are bijections; inverse uses matrix inverse.
\end{bullets}
}

\ProblemPage{7}{Expectation via Linear Transformation of Distributions}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Linear maps on probability vectors propagate expectations linearly.

\PROBLEM{
A fair die outcome vector $p\in\mathbb{R}^6$ has $p_i=\frac{1}{6}$. Consider
a linear map $A\in\mathbb{R}^{3\times 6}$ aggregating outcomes into three
bins: $A$ has rows $r_1=(1,1,0,0,0,0)$, $r_2=(0,0,1,1,0,0)$,
$r_3=(0,0,0,0,1,1)$. Compute $q=Ap$ and the expected bin index under $q$ using
the linear functional $E(q)=\sum_{j=1}^3 j q_j$.
}

\MODEL{
\[
p=\tfrac{1}{6}\mathbf{1}_6,\quad A=\begin{bmatrix}
1&1&0&0&0&0\\0&0&1&1&0&0\\0&0&0&0&1&1\end{bmatrix},\quad q=Ap.
\]
}

\ASSUMPTIONS{
\begin{bullets}
\item Linear aggregation; probabilities sum to one.
\end{bullets}
}

\varmapStart
\var{p}{Probability vector in $\mathbb{R}^6$.}
\var{A}{Aggregation matrix.}
\var{q}{Aggregated probability vector in $\mathbb{R}^3$.}
\var{E}{Linear functional $E(q)=\langle (1,2,3),q\rangle$.}
\varmapEnd

\WHICHFORMULA{
Formula 1: $q=Ap$ as a linear image; composition with $E$ is linear.
}

\GOVERN{
\[
q=Ap,\quad E(q)=(1,2,3)\cdot q.
\]
}

\INPUTS{$p_i=\frac{1}{6}$ for $i=1,\dots,6$, $A$ as given.}

\DERIVATION{
\begin{align*}
\text{Step 1: }& q_1=p_1+p_2=\tfrac{1}{6}+\tfrac{1}{6}=\tfrac{1}{3}.\\
\text{Step 2: }& q_2=p_3+p_4=\tfrac{1}{6}+\tfrac{1}{6}=\tfrac{1}{3}.\\
\text{Step 3: }& q_3=p_5+p_6=\tfrac{1}{6}+\tfrac{1}{6}=\tfrac{1}{3}.\\
\text{Step 4: }& E(q)=1\cdot \tfrac{1}{3}+2\cdot \tfrac{1}{3}+
3\cdot \tfrac{1}{3}=\tfrac{6}{3}=2.
\end{align*}
}

\RESULT{
$q=(\tfrac{1}{3},\tfrac{1}{3},\tfrac{1}{3})^\top$ and $E(q)=2$.
}

\UNITCHECK{
$q$ sums to one; $E(q)$ is dimensionless as a bin index expectation.
}

\EDGECASES{
\begin{bullets}
\item Nonuniform $p$ still yields $q=Ap$ linearly.
\end{bullets}
}

\ALTERNATE{
Compose $E\circ A$ into a single row vector $e=(1,2,3)A$ acting on $p$.
}

\VALIDATION{
\begin{bullets}
\item Check $E(q)=(e\cdot p)$ with $e=(1,2,3)A=(1,3,5,7,9,11)$ gives
$\frac{1+3+5+7+9+11}{6}=2$.
\end{bullets}
}

\INTUITION{
Aggregation and expectation are linear operations; order and composition match
matrix multiplication.
}

\CANONICAL{
\begin{bullets}
\item Linear maps on distributions propagate expectations via composition.
\end{bullets}
}

\ProblemPage{8}{Proof-Style: Rank-Nullity for a Given Matrix}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Prove $\operatorname{rank}A+\operatorname{null}A=n$ for a concrete matrix.

\PROBLEM{
Let $A=\begin{bmatrix}1&2&0\\0&1&1\\0&0&0\end{bmatrix}$. Prove that
$\operatorname{rank}A+\operatorname{null}A=3$ and determine both numbers.
}

\MODEL{
\[
\operatorname{rank}A=\dim\operatorname{im}A,\ \operatorname{null}A=
\dim\ker A.
\]
}

\ASSUMPTIONS{
\begin{bullets}
\item Standard basis; $A$ acts $\mathbb{R}^3\to\mathbb{R}^3$.
\end{bullets}
}

\varmapStart
\var{A}{Upper triangular matrix.}
\var{\operatorname{rank}A}{Row-rank equals column-rank.}
\var{\operatorname{null}A}{Nullity via solution space of $Ax=0$.}
\varmapEnd

\WHICHFORMULA{
Formula 4: rank-nullity theorem applied to $A$.
}

\GOVERN{
\[
\operatorname{rank}A+\operatorname{null}A=3.
\]
}

\INPUTS{$A$ as given.}

\DERIVATION{
\begin{proof}
Row-reduce $A$:
$\begin{bmatrix}1&2&0\\0&1&1\\0&0&0\end{bmatrix}\to
\begin{bmatrix}1&0&-2\\0&1&1\\0&0&0\end{bmatrix}$
by $R_1\leftarrow R_1-2R_2$. There are two pivots, hence
$\operatorname{rank}A=2$. Solve $Ax=0$:
from the reduced system, $x_1=2x_3$, $x_2=-x_3$, $x_3$ free. Thus
$\ker A=\{(2t,-t,t)^\top:t\in\mathbb{R}\}$ is one-dimensional, so
$\operatorname{null}A=1$. Therefore
$\operatorname{rank}A+\operatorname{null}A=2+1=3$, as required. \qedhere
\end{proof}
}

\RESULT{
$\operatorname{rank}A=2$, $\operatorname{null}A=1$, sum equals $3$.
}

\UNITCHECK{
Counts of pivots plus free variables equal number of columns.
}

\EDGECASES{
\begin{bullets}
\item If third row were nonzero, rank would be $3$ and nullity $0$.
\end{bullets}
}

\ALTERNATE{
Compute $\dim\ker A$ by null space basis directly and infer rank by subtraction.
}

\VALIDATION{
\begin{bullets}
\item Verify a nonzero vector $(2,-1,1)^\top$ lies in $\ker A$.
\end{bullets}
}

\INTUITION{
Each pivot fixes one coordinate; remaining free variables parametrize the kernel.
}

\CANONICAL{
\begin{bullets}
\item Rank plus nullity equals number of columns.
\end{bullets}
}

\ProblemPage{9}{Proof-Style: Trace Invariance Under Similarity}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Prove $\operatorname{tr}(P^{-1}AP)=\operatorname{tr}(A)$.

\PROBLEM{
Let $A\in\mathbb{F}^{n\times n}$ and $P$ invertible. Prove that
$\operatorname{tr}(P^{-1}AP)=\operatorname{tr}(A)$.
}

\MODEL{
\[
\operatorname{tr}(XY)=\operatorname{tr}(YX).
\]
}

\ASSUMPTIONS{
\begin{bullets}
\item $\mathbb{F}$ is a field; usual properties of trace hold.
\end{bullets}
}

\varmapStart
\var{A}{Square matrix.}
\var{P}{Invertible matrix.}
\var{\operatorname{tr}}{Trace operator.}
\varmapEnd

\WHICHFORMULA{
Formula 3 lemma: cyclicity of trace under similarity.
}

\GOVERN{
\[
\operatorname{tr}(P^{-1}AP)=\operatorname{tr}(A).
\]
}

\INPUTS{$A,P$ arbitrary with $P$ invertible.}

\DERIVATION{
\begin{proof}
Using cyclicity of trace, $\operatorname{tr}(XY)=\operatorname{tr}(YX)$ for
conformable matrices, set $X=P^{-1}A$ and $Y=P$. Then
$\operatorname{tr}(P^{-1}AP)=\operatorname{tr}(AP P^{-1})
=\operatorname{tr}(A I)=\operatorname{tr}(A)$. \qedhere
\end{proof}
}

\RESULT{
Trace is invariant under similarity transformations.
}

\UNITCHECK{
Both sides are scalars; dimensions align for products inside trace.
}

\EDGECASES{
\begin{bullets}
\item If $P$ is not invertible, $P^{-1}$ does not exist; similarity undefined.
\end{bullets}
}

\ALTERNATE{
Diagonalize $A$ if possible; trace equals sum of eigenvalues, invariant under
similarity since eigenvalues are invariant.
}

\VALIDATION{
\begin{bullets}
\item Random numerical test: compute both traces for random invertible $P$.
\end{bullets}
}

\INTUITION{
Similarity is a relabeling of basis; summing diagonal entries is intrinsic.
}

\CANONICAL{
\begin{bullets}
\item $\operatorname{tr}$ is similarity-invariant.
\end{bullets}
}

\ProblemPage{10}{Combo: Differentiation Operator on Polynomials}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Represent $D:\mathbb{R}_3[x]\to\mathbb{R}_3[x]$, $D(p)=p'$, as a matrix.

\PROBLEM{
Let $V=\mathbb{R}_3[x]$ with basis $\mathcal{B}=\{1,x,x^2,x^3\}$. Find
$[D]_{\mathcal{B}}$, compute $D^2$ and $\ker D$, and evaluate $D^3(p)$ for
$p(x)=1+2x+3x^2+x^3$.
}

\MODEL{
\[
D(1)=0,\ D(x)=1,\ D(x^2)=2x,\ D(x^3)=3x^2.
\]
}

\ASSUMPTIONS{
\begin{bullets}
\item Standard polynomial basis; linearity of differentiation.
\end{bullets}
}

\varmapStart
\var{D}{Differentiation operator.}
\var{[D]_{\mathcal{B}}}{Matrix representation in $\mathcal{B}$.}
\var{\ker D}{Polynomials with zero derivative.}
\varmapEnd

\WHICHFORMULA{
Formula 1 for assembling columns; Formula 2 for powers $[D^k]=[D]^k$.
}

\GOVERN{
\[
[D]_{\mathcal{B}}=\begin{bmatrix}
0&1&0&0\\0&0&2&0\\0&0&0&3\\0&0&0&0
\end{bmatrix}.
\]
}

\INPUTS{$p(x)=1+2x+3x^2+x^3$.}

\DERIVATION{
\begin{align*}
\text{Step 1: }& [D]_{\mathcal{B}} \text{ columns are }[D(1)]_{\mathcal{B}}=(0,0,0,0)^\top,\\
& [D(x)]_{\mathcal{B}}=(1,0,0,0)^\top, [D(x^2)]_{\mathcal{B}}=(0,2,0,0)^\top,\\
& [D(x^3)]_{\mathcal{B}}=(0,0,3,0)^\top.\\
\text{Step 2: }& [D^2]=[D]^2=
\begin{bmatrix}
0&0&2&0\\0&0&0&6\\0&0&0&0\\0&0&0&0
\end{bmatrix}.\\
\text{Step 3: }& \ker D=\{c: c\in\mathbb{R}\},\ \dim\ker D=1.\\
\text{Step 4: }& p'(x)=2+6x+3x^2,\ p''(x)=6+6x,\ p^{(3)}(x)=6.\\
& D^3(p)=6.
\end{align*}
}

\RESULT{
$[D]_{\mathcal{B}}$ as above, $[D^2]=[D]^2$, $\ker D$ are constants, and
$D^3(p)=6$.
}

\UNITCHECK{
Matrix sizes $4\times 4$; $[D^2]=[D]^2$ uses Formula 2 with aligned bases.
}

\EDGECASES{
\begin{bullets}
\item $D^4=0$ on $\mathbb{R}_3[x]$; nilpotent of index $4$.
\end{bullets}
}

\ALTERNATE{
Compute $D^2$ by applying $D$ twice to basis and assembling columns.
}

\VALIDATION{
\begin{bullets}
\item Check $[D][p]_{\mathcal{B}}=[p']_{\mathcal{B}}$ for $p$.
\end{bullets}
}

\INTUITION{
Differentiation shifts coefficients down and multiplies by degree; iterating
eventually kills polynomials of bounded degree.
}

\CANONICAL{
\begin{bullets}
\item Linear operators on function spaces admit finite matrices in finite bases.
\end{bullets}
}

\section{Coding Demonstrations}
\CodeDemoPage{Composition Equals Matrix Multiplication}
\PROBLEM{
Verify numerically that $(S\circ T)(v)=[S][T]v$ for random deterministic
matrices and vectors, confirming Formula 2.
}

\API{
\begin{bullets}
\item \inlinecode{def read_input(s) -> tuple} — parse seed and size.
\item \inlinecode{def solve_case(n, seed) -> None} — core verification.
\item \inlinecode{def validate() -> None} — assertions on examples.
\item \inlinecode{def main() -> None} — orchestrate.
\end{bullets}
}

\INPUTS{
Integers \inlinecode{n} for dimension and \inlinecode{seed} for RNG.
}

\OUTPUTS{
Printed confirmation; assertions ensure equality within tolerance.
}

\FORMULA{
\[
[S\circ T]v=([S][T])v=S(T(v)).
\]
}

\textbf{SOLUTION A — From Scratch (Mathematically Explicit Implementation)}
\begin{codepy}
import random

def matmul(A, B):
    r, c, k = len(A), len(B[0]), len(B)
    return [[sum(A[i][t]*B[t][j] for t in range(k)) for j in range(c)]
            for i in range(r)]

def matvec(A, x):
    return [sum(A[i][j]*x[j] for j in range(len(x))) for i in range(len(A))]

def rand_mat(n, seed):
    random.seed(seed)
    return [[(random.randint(-3, 3)) for _ in range(n)] for _ in range(n)]

def rand_vec(n, seed):
    random.seed(seed + 1)
    return [random.randint(-3, 3) for _ in range(n)]

def read_input(s):
    parts = s.strip().split()
    return int(parts[0]), int(parts[1])

def solve_case(n, seed):
    T = rand_mat(n, seed)
    S = rand_mat(n, seed + 7)
    v = rand_vec(n, seed + 13)
    ST = matmul(S, T)
    lhs = matvec(ST, v)
    rhs = matvec(S, matvec(T, v))
    assert all(abs(a - b) < 1e-12 for a, b in zip(lhs, rhs))
    return lhs, rhs

def validate():
    l, r = solve_case(3, 42)
    assert l == r

def main():
    validate()
    l, r = solve_case(4, 123)
    print("ok", l == r)

if __name__ == "__main__":
    main()
\end{codepy}

\textbf{SOLUTION B — Library-Based (Validated Computational Shortcut)}
\begin{codepy}
import numpy as np

def read_input(s):
    n, seed = [int(x) for x in s.strip().split()]
    return n, seed

def solve_case(n, seed):
    rng = np.random.default_rng(seed)
    T = rng.integers(-3, 4, size=(n, n))
    S = rng.integers(-3, 4, size=(n, n))
    v = rng.integers(-3, 4, size=(n,))
    lhs = (S @ T) @ v
    rhs = S @ (T @ v)
    assert np.all(lhs == rhs)
    return lhs, rhs

def validate():
    l, r = solve_case(3, 0)
    assert np.all(l == r)

def main():
    validate()
    l, r = solve_case(5, 1)
    print("ok", bool(np.all(l == r)))

if __name__ == "__main__":
    main()
\end{codepy}

\COMPLEXITY{
Time $\mathcal{O}(n^3)$ for matrix multiply and $\mathcal{O}(n^2)$ for
matrix-vector multiply; space $\mathcal{O}(n^2)$.
}

\FAILMODES{
\begin{bullets}
\item Dimension mismatches — ensure square matrices of same size.
\item Random seeds must be set deterministically for reproducibility.
\end{bullets}
}

\STABILITY{
\begin{bullets}
\item Integer arithmetic avoids rounding issues; tolerance not needed.
\end{bullets}
}

\VALIDATION{
\begin{bullets}
\item Cross-check from-scratch and NumPy variants agree exactly.
\end{bullets}
}

\RESULT{
Both implementations confirm $(S\circ T)(v)=([S][T])v$ deterministically.
}

\EXPLANATION{
The code mirrors Formula 2: composition mapped to matrix multiplication, then
applied to a vector to check equality.
}

\CodeDemoPage{Change of Basis and Similarity Invariants}
\PROBLEM{
Verify $A'=P^{-1}AP$ and invariance of trace, determinant, and eigenvalues
for random invertible $P$ and a fixed $A$, demonstrating Formula 3.
}

\API{
\begin{bullets}
\item \inlinecode{def rand_invertible(n, seed) -> np.ndarray}
\item \inlinecode{def verify(A, P) -> None}
\item \inlinecode{def validate() -> None}
\item \inlinecode{def main() -> None}
\end{bullets}
}

\INPUTS{
Dimension \inlinecode{n} and \inlinecode{seed} for deterministic generation.
}

\OUTPUTS{
Printed invariants and assertion checks.
}

\FORMULA{
\[
A'=P^{-1}AP,\ \operatorname{tr}A'=\operatorname{tr}A,\ \det A'=\det A.
\]
}

\textbf{SOLUTION A — From Scratch (Mathematically Explicit Implementation)}
\begin{codepy}
import random

def matmul(A, B):
    r, c, k = len(A), len(B[0]), len(B)
    return [[sum(A[i][t]*B[t][j] for t in range(k)) for j in range(c)]
            for i in range(r)]

def ident(n):
    return [[1 if i == j else 0 for j in range(n)] for i in range(n)]

def det2(A):
    if len(A) == 1:
        return A[0][0]
    if len(A) == 2:
        return A[0][0]*A[1][1] - A[0][1]*A[1][0]
    d = 0
    for j in range(len(A)):
        M = [row[:j] + row[j+1:] for row in A[1:]]
        d += ((-1)**j) * A[0][j] * det2(M)
    return d

def inv2(A):
    n = len(A)
    if n == 1:
        return [[1 / A[0][0]]]
    if n == 2:
        d = det2(A)
        return [[ A[1][1]/d, -A[0][1]/d],
                [-A[1][0]/d,  A[0][0]/d]]
    raise ValueError("inv2 supports n<=2")

def trace(A):
    return sum(A[i][i] for i in range(len(A)))

def rand_invertible(n, seed):
    random.seed(seed)
    while True:
        P = [[random.randint(-3, 3) for _ in range(n)] for _ in range(n)]
        if det2(P) != 0:
            return P

def verify(A, P):
    Pinv = inv2(P)
    A1 = matmul(matmul(Pinv, A), P)
    assert abs(trace(A1) - trace(A)) < 1e-9
    assert abs(det2(A1) - det2(A)) < 1e-9
    return A1

def validate():
    A = [[1, 2], [0, 3]]
    P = rand_invertible(2, 7)
    A1 = verify(A, P)
    _ = A1

def main():
    validate()
    A = [[2, 1], [0, 4]]
    P = rand_invertible(2, 123)
    A1 = verify(A, P)
    print("ok", True)

if __name__ == "__main__":
    main()
\end{codepy}

\textbf{SOLUTION B — Library-Based (Validated Computational Shortcut)}
\begin{codepy}
import numpy as np

def rand_invertible(n, seed):
    rng = np.random.default_rng(seed)
    while True:
        P = rng.integers(-3, 4, size=(n, n)).astype(float)
        if abs(np.linalg.det(P)) > 0.5:
            return P

def verify(A, P):
    A = np.array(A, dtype=float)
    A1 = np.linalg.inv(P) @ A @ P
    assert abs(np.trace(A1) - np.trace(A)) < 1e-9
    assert abs(np.linalg.det(A1) - np.linalg.det(A)) < 1e-9
    ew1 = np.sort(np.linalg.eigvals(A1))
    ewA = np.sort(np.linalg.eigvals(A))
    assert np.allclose(ew1, ewA)
    return A1

def validate():
    A = np.array([[1.0, 2.0], [0.0, 3.0]])
    P = rand_invertible(2, 9)
    A1 = verify(A, P)
    _ = A1

def main():
    validate()
    A = np.array([[2.0, 0.0], [1.0, 5.0]])
    P = rand_invertible(2, 11)
    A1 = verify(A, P)
    print("ok", True)

if __name__ == "__main__":
    main()
\end{codepy}

\COMPLEXITY{
Time $\mathcal{O}(n^3)$ for inversion and products; space $\mathcal{O}(n^2)$.
}

\FAILMODES{
\begin{bullets}
\item Singular $P$ — regenerate until invertible.
\item Numerical instability if $\det P$ is near zero (library mitigates).
\end{bullets}
}

\STABILITY{
\begin{bullets}
\item Choose well-conditioned $P$ by thresholding determinant magnitude.
\end{bullets}
}

\VALIDATION{
\begin{bullets}
\item Compare invariants and eigenvalues before and after similarity.
\end{bullets}
}

\RESULT{
Similarity and invariants hold numerically for deterministic instances.
}

\EXPLANATION{
Maps in different bases are related by similarity; invariants match, as shown
by matching $\operatorname{tr}$, $\det$, and eigenvalues.
}

\section{Applied Domains — Detailed End-to-End Scenarios}
\DomainPage{Machine Learning}
\SCENARIO{
A linear layer maps features $x\in\mathbb{R}^d$ to predictions
$\hat{y}=Wx$, a pure linear transformation. Verify prediction equivalence
under a change of feature basis via similarity.
}
\ASSUMPTIONS{
\begin{bullets}
\item Deterministic synthetic data; no noise.
\item Invertible change-of-basis matrix $P$ on features.
\end{bullets}
}
\WHICHFORMULA{
Formula 3: If $x'=P^{-1}x$ then $W'=W P$, so $\hat{y}=W x=W' x'$.
}
\varmapStart
\var{W}{Weight matrix in original feature basis.}
\var{P}{Feature change-of-basis matrix.}
\var{x,x'}{Feature vectors before and after basis change.}
\var{\hat{y}}{Prediction vector.}
\varmapEnd
\PIPELINE{
\begin{bullets}
\item Generate $W$, $P$, and $x$ deterministically.
\item Compute $x'=P^{-1}x$ and $W'=W P$.
\item Verify $W x=W' x'$ and report norms.
\end{bullets}
}
\begin{codepy}
import numpy as np

def generate(d=3, seed=0):
    rng = np.random.default_rng(seed)
    W = rng.integers(-2, 3, size=(2, d)).astype(float)
    P = rng.integers(-2, 3, size=(d, d)).astype(float)
    while abs(np.linalg.det(P)) < 1.0:
        P = rng.integers(-2, 3, size=(d, d)).astype(float)
    x = rng.integers(-2, 3, size=(d,)).astype(float)
    return W, P, x

def main():
    W, P, x = generate()
    xprime = np.linalg.inv(P) @ x
    Wprime = W @ P
    y = W @ x
    yprime = Wprime @ xprime
    print("ok", np.allclose(y, yprime))

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{
Equality check $\|\hat{y}-\hat{y}'\|_2$; expect zero within tolerance.
}
\INTERPRET{
Linear predictions are invariant under feature basis change with correct
weight transformation.
}
\NEXTSTEPS{
Extend to affine layers by augmenting features with a bias coordinate.
}

\DomainPage{Quantitative Finance}
\SCENARIO{
Portfolio payoff as a linear transformation: for asset returns $r\in\mathbb{R}^d$
and weights $w\in\mathbb{R}^d$, portfolio return is $R=w^\top r$. Show that
relabeling assets by basis change preserves $R$ with transformed weights.
}
\ASSUMPTIONS{
\begin{bullets}
\item Deterministic synthetic returns; invertible relabeling matrix $P$.
\end{bullets}
}
\WHICHFORMULA{
Formula 3: With $r'=P^{-1}r$ and $w'=(P^\top) w$, we have $w^\top r
= (w')^\top r'$.
}
\varmapStart
\var{r}{Asset return vector.}
\var{w}{Weight vector.}
\var{P}{Basis change on asset space.}
\var{R}{Portfolio return scalar.}
\varmapEnd
\PIPELINE{
\begin{bullets}
\item Generate $r,w,P$ with fixed seed.
\item Compute $r'=P^{-1}r$, $w'=(P^\top)w$.
\item Verify equality of returns.
\end{bullets}
}
\begin{codepy}
import numpy as np

def generate(d=3, seed=1):
    rng = np.random.default_rng(seed)
    r = rng.integers(-3, 4, size=(d,)).astype(float)
    w = rng.integers(-2, 3, size=(d,)).astype(float)
    P = rng.integers(-2, 3, size=(d, d)).astype(float)
    while abs(np.linalg.det(P)) < 1.0:
        P = rng.integers(-2, 3, size=(d, d)).astype(float)
    return r, w, P

def main():
    r, w, P = generate()
    rp = np.linalg.inv(P) @ r
    wp = P.T @ w
    R = w @ r
    Rp = wp @ rp
    print("ok", abs(R - Rp) < 1e-9)

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{
Absolute difference $|R-R'|$; expect near zero.
}
\INTERPRET{
Portfolio return is a coordinate-free linear functional; transformations of
coordinates preserve its value with consistent weight change.
}
\NEXTSTEPS{
Extend to covariance mapping $\Sigma'\!=\!P^{-1}\Sigma(P^{-1})^\top$ to
preserve risk.
}

\DomainPage{Deep Learning}
\SCENARIO{
Two linear layers composed equal one linear layer with weight $W=S T$; verify
equivalence on data and show basis change equivalence in hidden space.
}
\ASSUMPTIONS{
\begin{bullets}
\item No activation between layers; purely linear composition.
\item Invertible change in hidden basis.
\end{bullets}
}
\WHICHFORMULA{
Formula 2: Composition as matrix product. Formula 3: similarity in hidden
space does not change input-output mapping if compensated.
}
\PIPELINE{
\begin{bullets}
\item Generate $T\in\mathbb{R}^{h\times d}$, $S\in\mathbb{R}^{o\times h}$.
\item Compute $W=S T$ and check $S(Tx)=Wx$.
\item Insert hidden basis change $P$ and verify invariance:
$S' = S P$, $T' = P^{-1} T$.
\end{bullets}
}
\begin{codepy}
import numpy as np

def generate(d=4, h=3, o=2, seed=2):
    rng = np.random.default_rng(seed)
    T = rng.integers(-2, 3, size=(h, d)).astype(float)
    S = rng.integers(-2, 3, size=(o, h)).astype(float)
    x = rng.integers(-2, 3, size=(d,)).astype(float)
    P = rng.integers(-2, 3, size=(h, h)).astype(float)
    while abs(np.linalg.det(P)) < 1.0:
        P = rng.integers(-2, 3, size=(h, h)).astype(float)
    return S, T, x, P

def main():
    S, T, x, P = generate()
    y1 = S @ (T @ x)
    W = S @ T
    y2 = W @ x
    S2, T2 = S @ P, np.linalg.inv(P) @ T
    y3 = S2 @ (T2 @ x)
    print("ok", np.allclose(y1, y2) and np.allclose(y2, y3))

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{
Equality checks $\|y_1-y_2\|_2$ and $\|y_2-y_3\|_2$ near zero.
}
\INTERPRET{
Linear layers compose via matrix multiplication; hidden basis relabeling
does not change the realized map if compensated in adjacent layers.
}
\NEXTSTEPS{
Add activations and analyze invariances under unitary transforms.
}

\DomainPage{Kaggle / Data Analytics}
\SCENARIO{
Apply a linear whitening transform to centered features $X\in\mathbb{R}^{n\times d}$
using $W=\Lambda^{-1/2}U^\top$ from eigendecomposition of the covariance
$\Sigma=\frac{1}{n}X^\top X$, and verify $\tilde{\Sigma}=I$.
}
\ASSUMPTIONS{
\begin{bullets}
\item Data are centered; $\Sigma$ positive definite.
\end{bullets}
}
\WHICHFORMULA{
Linear transform of covariance: $\tilde{\Sigma}=W\Sigma W^\top=I$ if
$W=\Lambda^{-1/2}U^\top$ and $\Sigma=U\Lambda U^\top$.
}
\PIPELINE{
\begin{bullets}
\item Generate centered data deterministically.
\item Compute $\Sigma$, its eigendecomposition $\Sigma=U\Lambda U^\top$.
\item Form $W$ and whitened data $Z=X W^\top$; check $I$.
\end{bullets}
}
\begin{codepy}
import numpy as np

def generate(n=200, d=3, seed=3):
    rng = np.random.default_rng(seed)
    A = rng.normal(size=(d, d))
    cov = A @ A.T + d * np.eye(d)
    X = rng.multivariate_normal(np.zeros(d), cov, size=n)
    X -= X.mean(axis=0, keepdims=True)
    return X

def main():
    X = generate()
    n = X.shape[0]
    Sigma = (X.T @ X) / n
    lam, U = np.linalg.eigh(Sigma)
    W = (U * (lam ** -0.5)) @ U.T
    Z = X @ W.T
    SigZ = (Z.T @ Z) / n
    print("ok", np.allclose(SigZ, np.eye(X.shape[1]), atol=1e-6))

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{
Frobenius norm $\|\tilde{\Sigma}-I\|_F$; expect below tolerance.
}
\INTERPRET{
Whitening is a linear change of basis in feature space that makes covariance
identity.
}
\NEXTSTEPS{
Use partial whitening or PCA projection to reduce dimensionality linearly.
}

\end{document}
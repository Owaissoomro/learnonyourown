% !TeX program = xelatex
\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{microtype,setspace,amsmath,amssymb,mathtools,amsthm,unicode-math}
\setstretch{1.05}
\setmainfont{Latin Modern Roman}
\setmonofont{Latin Modern Mono}
\setmathfont{Latin Modern Math}

\allowdisplaybreaks[4]
\setlength{\jot}{7pt}
\setlength{\emergencystretch}{8em}
\sloppy

\usepackage{xcolor,fancyhdr,enumitem,inconsolata,listings}
\pagestyle{fancy}\fancyhf{}\lhead{\nouppercase{\leftmark}}\rhead{\thepage}
\setlength{\headheight}{26pt}
\setlength{\parindent}{0pt}
\setlength{\parskip}{8pt plus 2pt minus 1pt}
\raggedbottom

\newenvironment{BreakableEquation}{\begin{equation}\begin{aligned}}{\end{aligned}\end{equation}}
\newenvironment{BreakableEquation*}{\begin{equation*}\begin{aligned}}{\end{aligned}\end{equation*}}
\newenvironment{tightalign}{\begingroup\small\allowdisplaybreaks\begin{align}}{\end{align}\endgroup}

\providecommand{\enumlistm}{enumitem}
\newenvironment{minted}[2][]{%
  \lstset{style=code,language=#2,#1}\begin{lstlisting}%
}{\end{lstlisting}}

\newcommand{\inputminted}[3][]{\begin{lstlisting}\end{lstlisting}}

\newlist{bullets}{itemize}{1}
\setlist[bullets]{label=--,leftmargin=1.6em,itemsep=2pt,topsep=4pt}

\newcommand{\varmapStart}{\textbf{VARIABLE MAPPING:}\par\begin{bullets}}
\newcommand{\var}[2]{\item $#1$ — #2}
\newcommand{\varmapEnd}{\end{bullets}}

\newcommand{\glossx}[6]{%
  \textbf{#1}\par
  \begin{bullets}
    \item \textbf{What:} #2
    \item \textbf{Why:} #3
    \item \textbf{How:} #4
    \item \textbf{ELI5:} #5
    \item \textbf{Pitfall/Example:} #6
  \end{bullets}
}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{claim}[theorem]{Claim}

\lstdefinestyle{code}{
  basicstyle=\ttfamily\footnotesize,
  backgroundcolor=\color{black!02},
  frame=single,
  numbers=left, numberstyle=\tiny, numbersep=8pt,
  breaklines=true, breakatwhitespace=true,
  tabsize=4, showstringspaces=false,
  upquote=true, keepspaces=true, columns=fullflexible,
  literate=
    {–}{{-}}1
    {—}{{-}}1
    {…}{{...}}1
    {≤}{{\ensuremath{\le}}}1
    {≥}{{\ensuremath{\ge}}}1
    {≠}{{\ensuremath{\ne}}}1
    {≈}{{\ensuremath{\approx}}}1
    {±}{{\ensuremath{\pm}}}1
    {→}{{\ensuremath{\to}}}1
    {←}{{\ensuremath{\leftarrow}}}1
    {∞}{{\ensuremath{\infty}}}1
    {√}{{\ensuremath{\sqrt{\ }}}}1
    {×}{{\ensuremath{\times}}}1
    {÷}{{\ensuremath{\div}}}1
}

\lstnewenvironment{codepy}[1][]%
  {\lstset{style=code,language=Python,#1}}%
  {}

\newcommand{\inlinecode}[1]{\lstinline[style=code]!#1!}

\newcommand{\LF}[2]{\par\noindent\textbf{#1:}~#2\par}
\newcommand{\WHAT}[1]{\LF{WHAT}{#1}}
\newcommand{\WHY}[1]{\LF{WHY}{#1}}
\newcommand{\HOW}[1]{\LF{HOW}{#1}}
\newcommand{\ELI}[1]{\LF{ELI5}{#1}}
\newcommand{\SCOPE}[1]{\LF{SCOPE}{#1}}
\newcommand{\CONFUSIONS}[1]{\LF{COMMON CONFUSIONS}{#1}}
\newcommand{\APPLICATIONS}[1]{\LF{APPLICATIONS}{#1}}
\newcommand{\FORMULA}[1]{\LF{FORMULA}{#1}}
\newcommand{\CANONICAL}[1]{\LF{CANONICAL FORM}{#1}}
\newcommand{\PRECONDS}[1]{\LF{PRECONDITIONS}{#1}}
\newcommand{\DERIVATION}[1]{\LF{DERIVATION}{#1}}
\newcommand{\EQUIV}[1]{\LF{EQUIVALENT FORMS}{#1}}
\newcommand{\LIMITS}[1]{\LF{LIMIT CASES}{#1}}
\newcommand{\INPUTS}[1]{\LF{INPUTS}{#1}}
\newcommand{\OUTPUTS}[1]{\LF{OUTPUTS}{#1}}
\newcommand{\RESULT}[1]{\LF{RESULT}{#1}}
\newcommand{\INTUITION}[1]{\LF{INTUITION}{#1}}
\newcommand{\PITFALLS}[1]{\LF{PITFALLS}{#1}}
\newcommand{\MODEL}[1]{\LF{CANONICAL MATH MODEL}{#1}}
\newcommand{\ASSUMPTIONS}[1]{\LF{ASSUMPTIONS}{#1}}
\newcommand{\WHICHFORMULA}[1]{\LF{WHICH FORMULA \& WHY}{#1}}
\newcommand{\GOVERN}[1]{\LF{GOVERNING EQUATION(S)}{#1}}
\newcommand{\UNITCHECK}[1]{\LF{UNIT CHECK}{#1}}
\newcommand{\EDGECASES}[1]{\LF{EDGE CASES}{#1}}
\newcommand{\ALTERNATE}[1]{\LF{ALTERNATE APPROACH (sketch)}{#1}}
\newcommand{\PROBLEM}[1]{\LF{PROBLEM}{#1}}
\newcommand{\API}[1]{\LF{API}{#1}}
\newcommand{\COMPLEXITY}[1]{\LF{COMPLEXITY}{#1}}
\newcommand{\FAILMODES}[1]{\LF{FAILURE MODES}{#1}}
\newcommand{\STABILITY}[1]{\LF{NUMERICAL STABILITY}{#1}}
\newcommand{\VALIDATION}[1]{\LF{VALIDATION}{#1}}
\newcommand{\EXPLANATION}[1]{\LF{EXPLANATION}{#1}}
\newcommand{\SCENARIO}[1]{\LF{SCENARIO}{#1}}
\newcommand{\PIPELINE}[1]{\LF{PIPELINE STEPS}{#1}}
\newcommand{\METRICS}[1]{\LF{METRICS}{#1}}
\newcommand{\INTERPRET}[1]{\LF{INTERPRETATION}{#1}}
\newcommand{\NEXTSTEPS}[1]{\LF{LIMITATIONS \& NEXT STEPS}{#1}}

\usepackage{titlesec}
\titleformat{\section}{\Large\bfseries}{\thesection}{0.6em}{}
\titlespacing*{\section}{0pt}{*2}{*1}
\usepackage{etoolbox}
\pretocmd{\section}{\clearpage}{}{}

\newcommand{\FormulaPage}[2]{%
  \clearpage
  \section*{Formula #1 — #2}%
  \addcontentsline{toc}{section}{Formula #1 — #2}%
}
\newcommand{\ProblemPage}[2]{%
  \clearpage
  \subsection*{Problem #1: #2}%
  \addcontentsline{toc}{subsection}{Problem #1: #2}%
}
\newcommand{\CodeDemoPage}[1]{%
  \clearpage
  \subsection*{Coding Demo: #1}%
  \addcontentsline{toc}{subsection}{Coding Demo: #1}%
}
\newcommand{\DomainPage}[1]{%
  \clearpage
  \subsection*{#1 (End-to-End)}%
  \addcontentsline{toc}{subsection}{#1 (End-to-End)}%
}

\begin{document}
\title{Comprehensive Study Sheet — Eckart-Young Theorem and Low-Rank Approximation}
\date{\today}
\maketitle
\tableofcontents
\clearpage

\section{Concept Overview}
\WHAT{
Given $A\in\mathbb{R}^{m\times n}$ with singular value decomposition $A=U\Sigma V^{\top}$,
the Eckart--Young--Mirsky theorem characterizes the best approximation to $A$ by a matrix
of rank at most $k$ in unitarily invariant norms. In Frobenius norm,
$A_k=U\Sigma_k V^{\top}$ minimizes $\|A-B\|_F$ over $\operatorname{rank}(B)\le k$ and
$\|A-A_k\|_F^2=\sum_{i>k}\sigma_i^2$. In spectral norm,
$\|A-A_k\|_2=\sigma_{k+1}$ with the same minimizer.
}
\WHY{
Low-rank approximation compresses data and reveals latent structure. It is fundamental to
PCA, numerical linear algebra, data compression, recommender systems, and scientific
computing. Eckart--Young provides the exact optimum and error in closed form, enabling
efficient, provably best approximations.
}
\HOW{
1. Define SVD via the spectral theorem for $A^{\top}A$ and $AA^{\top}$.
2. Use unitary invariance to reduce the optimization to choosing entries of a matrix near
diagonal $\Sigma$ under a rank constraint.
3. Show the optimum keeps the $k$ largest singular components and discards the rest.
4. Interpret the error as the energy in discarded singular values and connect to variance
captured in PCA.
}
\ELI{
Write $A$ as a sum of simple rank-one pictures, each with strength $\sigma_i$.
Keeping the top $k$ strongest pictures gives the best $k$-picture summary, and the blur
you get is exactly the leftover strengths squared (Frobenius) or the next strongest
strength (spectral).
}
\SCOPE{
Applies to real or complex matrices; norms must be unitarily invariant. Explicit error
formulas hold for Frobenius and spectral norms. Degeneracies occur when $\sigma_k=
\sigma_{k+1}$, where minimizers are not unique but share the same span.
}
\CONFUSIONS{
Low rank versus sparse: low rank is global structure, sparsity is entrywise. PCA versus
SVD: PCA on centered data equals SVD of the data matrix up to scaling. Truncating
singular values versus thresholding entries: only truncation is optimal for the stated
norms.
}
\APPLICATIONS{
\begin{bullets}
\item Mathematical foundations: spectral theory, unitarily invariant norms.
\item Computational modeling: model reduction, latent factor models.
\item Physical or engineering: image compression, modal analysis.
\item Statistical or algorithmic: PCA, recommender systems, kernel approximations.
\end{bullets}
}

\textbf{ANALYTIC STRUCTURE.}
The set of rank-$k$ matrices is nonconvex but is a smooth manifold. Unitarily invariant
norms depend only on singular values. The problem is symmetric under orthogonal changes
of basis in domain and codomain.

\textbf{CANONICAL LINKS.}
SVD existence feeds Eckart--Young (Formula 2). Projection identities (Formula 4) feed
Problem 5. Spectral version (Formula 3) connects to operator norm bounds.

\textbf{PROBLEM-TYPE RECOGNITION HEURISTICS.}
\begin{bullets}
\item Objective involves $\|A-B\|_F$ or $\|A-B\|_2$ with rank$(B)\le k$.
\item Mentions PCA, variance captured, or top singular values.
\item Reconstruction error equals sum of tail singular values squared.
\end{bullets}

\textbf{SOLUTION STRATEGY BLUEPRINT.}
\begin{bullets}
\item Take SVD $A=U\Sigma V^{\top}$.
\item Truncate: $\Sigma_k=\operatorname{diag}(\sigma_1,\dots,\sigma_k,0,\dots)$.
\item Set $A_k=U\Sigma_k V^{\top}$ and compute error from tail singular values.
\item Validate by unitary invariance and Pythagorean identity in Frobenius inner product.
\end{bullets}

\textbf{CONCEPTUAL INVARIANTS.}
Singular values; Frobenius norm squared equals sum of squares of singular values; spectral
norm equals the largest singular value.

\textbf{EDGE INTUITION.}
As $k\to 0$, the best rank-$0$ approximation is $0$ with error $\|A\|$. As $k\to r=
\operatorname{rank}(A)$, error tends to zero and $A_k\to A$. If $\sigma_k=\sigma_{k+1}$,
the subspace of minimizers widens but the error stays fixed.

\section{Glossary}
\glossx{Singular Value Decomposition (SVD)}
{Factorization $A=U\Sigma V^{\top}$ with $U,V$ orthogonal and $\Sigma$ diagonal
nonnegative.}
{Diagonalizes a rectangular matrix up to orthogonal transforms; exposes invariant
singular values and orthogonal modes.}
{Compute eigenpairs of $A^{\top}A$, assemble $V$, take $\sigma_i=\sqrt{\lambda_i}$, set
$U=AV\Sigma^{\dagger}$.}
{Like rotating and scaling coordinates so $A$ stretches along perpendicular axes.}
{Pitfall: forgetting to sort singular values descending breaks truncation optimality.}

\glossx{Eckart--Young--Mirsky Theorem}
{Characterizes best rank-$k$ approximation in any unitarily invariant norm.}
{Gives closed-form minimizers and minimal error tied to singular values.}
{Reduce by unitary invariance to a diagonal problem on $\Sigma$ with a rank constraint;
keep the largest $k$ singular components.}
{Keep the $k$ loudest notes in a chord to best approximate the sound.}
{Pitfall: using entrywise thresholding rather than truncating singular values.}

\glossx{Frobenius Norm}
{$\|A\|_F=(\sum_{ij}a_{ij}^2)^{1/2}=(\sum_i \sigma_i^2)^{1/2}$.}
{Energy measure; unitarily invariant; natural in least-squares problems.}
{Compute as square root of sum of squares of entries or singular values.}
{Energy in all pixels of an image.}
{Pitfall: confusing with operator norm $\|\cdot\|_2$ which is the largest $\sigma_i$.}

\glossx{Truncated SVD}
{$A_k=\sum_{i=1}^k \sigma_i u_i v_i^{\top}$.}
{Unique best low-rank reconstruction when singular values are simple.}
{Compute SVD then zero out tail singular values beyond $k$, reconstruct.}
{Keep only the top $k$ layers of a stacked transparency.}
{Pitfall: using $k$ smaller than numerical rank may cause large residuals.}

\section{Symbol Ledger}
\varmapStart
\var{A\in\mathbb{R}^{m\times n}}{input matrix to approximate.}
\var{m,n}{row and column dimensions of $A$.}
\var{r}{rank of $A$, $r\le \min\{m,n\}$.}
\var{U\in\mathbb{R}^{m\times m}}{left singular vectors (orthogonal).}
\var{V\in\mathbb{R}^{n\times n}}{right singular vectors (orthogonal).}
\var{\Sigma}{diagonal matrix of singular values $\sigma_1\ge\dots\ge\sigma_{\min\{m,n\}}\ge 0$.}
\var{\sigma_i}{singular values of $A$.}
\var{k}{target rank with $0\le k\le r$.}
\var{A_k}{best rank-$k$ approximation $U\Sigma_k V^{\top}$.}
\var{\|\cdot\|_F}{Frobenius norm.}
\var{\|\cdot\|_2}{spectral (operator) norm induced by $\ell_2$.}
\var{P_{U_k}}{projector $U_k U_k^{\top}$ onto span of first $k$ left singular vectors.}
\var{P_{V_k}}{projector $V_k V_k^{\top}$ onto span of first $k$ right singular vectors.}
\var{E}{residual $A-A_k$.}
\varmapEnd

\section{Formula Canon — One Formula Per Page}
\FormulaPage{1}{Singular Value Decomposition and Unitary Invariance}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Every $A\in\mathbb{R}^{m\times n}$ admits $A=U\Sigma V^{\top}$ with $U,V$ orthogonal and
$\Sigma=\operatorname{diag}(\sigma_1,\dots,\sigma_{\min\{m,n\}})\ge 0$.
Frobenius and spectral norms are unitarily invariant: $\|UAV\|_F=\|A\|_F$ and
$\|UAV\|_2=\|A\|_2$ for orthogonal $U,V$.

\WHAT{
Diagonalization of a rectangular matrix via orthogonal changes of basis, and invariance
of key norms under such changes.
}
\WHY{
SVD is the backbone of low-rank approximation; unitary invariance allows reduction of
optimization problems to diagonal form.
}
\FORMULA{
\[
A=U\Sigma V^{\top},\quad \|UAV\|_F=\|A\|_F,\quad \|UAV\|_2=\|A\|_2.
\]
}
\CANONICAL{
$A$ real or complex; $U,V$ orthogonal (unitary in complex case); singular values are
nonnegative and sorted descending.
}
\PRECONDS{
\begin{bullets}
\item $A^{\top}A$ and $AA^{\top}$ are symmetric positive semidefinite.
\item Spectral theorem yields orthonormal eigenbases.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
Let $U,V$ be orthogonal and $A$ arbitrary. Then $\|UAV\|_F=\|A\|_F$ and
$\|UAV\|_2=\|A\|_2$.
\end{lemma}
\begin{proof}
For Frobenius norm,
\[
\|UAV\|_F^2=\operatorname{tr}\big((UAV)^{\top}(UAV)\big)
=\operatorname{tr}\big(V^{\top}A^{\top}U^{\top}UA V\big)
=\operatorname{tr}\big(V^{\top}A^{\top}A V\big).
\]
Since $\operatorname{tr}(X)=\operatorname{tr}(V^{\top}XV)$ for orthogonal $V$,
$\|UAV\|_F^2=\operatorname{tr}(A^{\top}A)=\|A\|_F^2$. For spectral norm,
\[
\|UAV\|_2=\max_{\|x\|_2=1}\|UAVx\|_2=\max_{\|x\|_2=1}\|UA(Vx)\|_2
=\max_{\|y\|_2=1}\|UAy\|_2=\max_{\|y\|_2=1}\|Ay\|_2=\|A\|_2,
\]
since $V$ and $U$ are isometries on the unit sphere. \qedhere
\end{proof}
\DERIVATION{
\begin{align*}
\text{Step 1:}&\ \text{Diagonalize }A^{\top}A=V\Lambda V^{\top},\ \Lambda\ge 0.\\
\text{Step 2:}&\ \text{Set }\Sigma=\Lambda^{1/2}=\operatorname{diag}(\sigma_i).\\
\text{Step 3:}&\ \text{Define }U=AV\Sigma^{\dagger},\ \text{where }\Sigma^{\dagger}
\text{ inverts nonzero }\sigma_i.\\
\text{Step 4:}&\ \text{Verify }U^{\top}U=I,\ \text{and }A=U\Sigma V^{\top}.\\
\text{Step 5:}&\ \text{Apply the lemma to obtain unitary invariance of norms.}
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Compute or reason about SVD to expose singular structure.
\item Use invariance to simplify norm objectives by rotating into singular bases.
\item Reduce to diagonal problems on $\Sigma$.
\end{bullets}
\EQUIV{
\begin{bullets}
\item $AA^{\top}=U\Sigma^2 U^{\top}$, $A^{\top}A=V\Sigma^2 V^{\top}$.
\item $\|A\|_F^2=\operatorname{tr}(A^{\top}A)=\sum_i\sigma_i^2$.
\item $\|A\|_2=\sigma_1$.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item If $A=0$, then $\Sigma=0$ and any orthogonal $U,V$ work.
\item If singular values are repeated, singular vectors are not unique, but subspaces are.
\end{bullets}
}
\INPUTS{$A\in\mathbb{R}^{m\times n}$.}
\DERIVATION{
\begin{align*}
\text{Example: }A&=\begin{bmatrix}3&0\\0&1\\\end{bmatrix}.\
A^{\top}A=\begin{bmatrix}9&0\\0&1\end{bmatrix}.\\
V&=I,\ \Sigma=\operatorname{diag}(3,1),\ U=A\Sigma^{\dagger}=I.\
A=U\Sigma V^{\top}.
\end{align*}
}
\RESULT{
SVD exists and norm invariance holds, enabling reduction of low-rank approximation to a
diagonal optimization on singular values.
}
\UNITCHECK{
All norms are dimensionless counts of energy or operator gain; orthogonal transforms
preserve these quantities.
}
\PITFALLS{
\begin{bullets}
\item Forgetting to sort singular values breaks truncation statements.
\item Confusing $\Sigma^{\dagger}$ with $\Sigma^{-1}$ when zeros are present.
\end{bullets}
}
\INTUITION{
Orthogonal transforms are rotations or reflections that do not change lengths or angles,
so energy and gains are preserved; SVD aligns $A$ with such axes.
}
\CANONICAL{
\begin{bullets}
\item $A=U\Sigma V^{\top}$, with $\Sigma$ carrying all invariant magnitudes.
\item Unitary invariance: norms depend only on $\{\sigma_i\}$.
\end{bullets}
}

\FormulaPage{2}{Eckart--Young (Frobenius Norm)}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
For $A=U\Sigma V^{\top}$ and $0\le k\le r$, the minimizer of
$\min_{\operatorname{rank}(B)\le k}\|A-B\|_F$ is $A_k=U\Sigma_k V^{\top}$, and
$\|A-A_k\|_F^2=\sum_{i>k}\sigma_i^2$.

\WHAT{
Best rank-$k$ approximation to $A$ in Frobenius norm is given by truncating the SVD, with
error equal to squared tail energy of singular values.
}
\WHY{
Provides exact optimal reconstruction and quantifies error, enabling principled
compression and variance-retaining approximations.
}
\FORMULA{
\[
A_k=\sum_{i=1}^k \sigma_i u_i v_i^{\top},\quad
\|A-A_k\|_F^2=\sum_{i=k+1}^{\min\{m,n\}}\sigma_i^2.
\]
}
\CANONICAL{
$A\in\mathbb{R}^{m\times n}$, $U,V$ orthogonal, $\sigma_1\ge\dots\ge 0$, $k$ integer.
}
\PRECONDS{
\begin{bullets}
\item Frobenius norm invariance under orthogonal transforms.
\item Rank constraint $\operatorname{rank}(B)\le k$.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
Let $A=U\Sigma V^{\top}$. For any $B$ with rank at most $k$,
$\|A-B\|_F^2=\|\Sigma-C\|_F^2$ with $C=U^{\top} B V$ of rank at most $k$.
\end{lemma}
\begin{proof}
By the lemma in Formula 1, $\|A-B\|_F=\|U^{\top}(A-B)V\|_F=\|\Sigma-C\|_F$, with
$C=U^{\top}BV$. Orthogonal multiplications preserve rank, so rank$(C)\le k$. \qedhere
\end{proof}
\DERIVATION{
\begin{align*}
\text{Step 1:}&\ \|A-B\|_F^2=\|\Sigma-C\|_F^2,\ \operatorname{rank}(C)\le k.\\
\text{Step 2:}&\ \Sigma=\operatorname{diag}(\sigma_1,\dots,\sigma_q),\ q=\min\{m,n\}.\\
\text{Step 3:}&\ \text{Write }C=[c_{ij}],\ \text{rank}(C)\le k.\\
\text{Step 4:}&\ \|\Sigma-C\|_F^2=\sum_{i}(\sigma_i-c_{ii})^2+\sum_{i\ne j}c_{ij}^2.\\
\text{Step 5:}&\ \text{Optimal }C\text{ is diagonal with at most }k\text{ nonzeros at
largest }\sigma_i.\\
\text{Step 6:}&\ c_{ii}=\sigma_i\ \text{for }i\le k,\ \text{else }c_{ii}=0.\\
\text{Step 7:}&\ \Rightarrow \min \|A-B\|_F^2=\sum_{i>k}\sigma_i^2,\ B=A_k.
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Compute top $k$ singular triplets $(\sigma_i,u_i,v_i)$.
\item Form $A_k=\sum_{i=1}^k \sigma_i u_i v_i^{\top}$.
\item Residual Frobenius error equals the tail energy.
\end{bullets}
\EQUIV{
\begin{bullets}
\item $A_k=U_k U_k^{\top} A = A V_k V_k^{\top}$.
\item $A=A_k+E$, with $\langle A_k,E\rangle_F=0$ and
$\|A\|_F^2=\|A_k\|_F^2+\|E\|_F^2$.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item If $k\ge r$, then $A_k=A$ and error $0$.
\item If $\sigma_k=\sigma_{k+1}$, $A_k$ not unique, but any $k$-dimensional top subspace
gives the same error.
\end{bullets}
}
\INPUTS{$A\in\mathbb{R}^{m\times n}$, $k\in\{0,1,\dots,r\}$.}
\DERIVATION{
\begin{align*}
\text{Example: }A&=\begin{bmatrix}2&0\\0&1\end{bmatrix},\ \sigma=(2,1).\\
k&=1,\ A_1=\begin{bmatrix}2&0\\0&0\end{bmatrix},\
\|A-A_1\|_F^2=1^2=1.
\end{align*}
}
\RESULT{
$A_k$ is optimal and the Frobenius error equals the squared tail singular values.
}
\UNITCHECK{
Both sides of $\|A-A_k\|_F^2=\sum_{i>k}\sigma_i^2$ measure squared energy; consistent.
}
\PITFALLS{
\begin{bullets}
\item Choosing non-diagonal $C$ increases error via off-diagonal terms.
\item Using unsorted singular values misidentifies which to keep.
\end{bullets}
}
\INTUITION{
In the singular basis, the best you can do with $k$ degrees of freedom is to copy the
$k$ largest axes and set the rest to zero.
}
\CANONICAL{
\begin{bullets}
\item Projection of $A$ onto rank-$k$ manifold via truncation.
\item Energy conservation: total equals kept plus discarded.
\end{bullets}
}

\FormulaPage{3}{Eckart--Young--Mirsky (Spectral Norm)}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
For $A=U\Sigma V^{\top}$ and $0\le k\le r$, $A_k=U\Sigma_k V^{\top}$ minimizes
$\|A-B\|_2$ over rank$(B)\le k$, and the minimal error is
$\|A-A_k\|_2=\sigma_{k+1}$.

\WHAT{
Best rank-$k$ approximation in operator norm is truncated SVD, with error equal to the
next singular value.
}
\WHY{
Operator norm controls worst-case amplification; this result gives tight, certifiable
bounds on approximation quality in applications needing uniform guarantees.
}
\FORMULA{
\[
\min_{\operatorname{rank}(B)\le k}\|A-B\|_2=\|A-A_k\|_2=\sigma_{k+1}.
\]
}
\CANONICAL{
$A\in\mathbb{R}^{m\times n}$; orthogonal $U,V$; singular values descending; $k$ integer.
}
\PRECONDS{
\begin{bullets}
\item Spectral norm invariance under orthogonal transforms.
\item Courant--Fischer characterization of singular values.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
Let $\Sigma=\operatorname{diag}(\sigma_1,\dots,\sigma_q)$ and $C$ be any matrix with
rank at most $k$. Then $\|\Sigma-C\|_2\ge \sigma_{k+1}$.
\end{lemma}
\begin{proof}
Let $S=\{e_1,\dots,e_q\}$ be the standard basis. Since rank$(C)\le k$, there is a
nonzero vector $x$ orthogonal to the row space of $C$ and supported within indices
$\{k+1,\dots,q\}$; in particular choose $x=e_{k+1}$. Then $Cx=0$ and
$(\Sigma-C)x=\Sigma x=\sigma_{k+1} e_{k+1}$, hence
$\|\Sigma-C\|_2\ge \|(\Sigma-C)x\|_2/\|x\|_2=\sigma_{k+1}$. \qedhere
\end{proof}
\DERIVATION{
\begin{align*}
\text{Step 1:}&\ \|A-B\|_2=\|\Sigma-C\|_2,\ C=U^{\top}BV,\ \operatorname{rank}(C)\le k.\\
\text{Step 2:}&\ \text{By the lemma, }\|\Sigma-C\|_2\ge \sigma_{k+1}.\\
\text{Step 3:}&\ \text{Equality with }C=\Sigma_k\ \text{gives }\|\Sigma-\Sigma_k\|_2=
\sigma_{k+1}.\\
\text{Step 4:}&\ \Rightarrow \min \|A-B\|_2=\sigma_{k+1},\ \text{attained by }A_k.
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Compute $\sigma_{k+1}$; it is the unavoidable spectral error.
\item Take $A_k$ via truncation; certify optimality using the lemma.
\end{bullets}
\EQUIV{
\begin{bullets}
\item $\|A-A_k\|_2=\sup_{\|x\|=1}\|(A-A_k)x\|=\sigma_{k+1}$.
\item If $k\ge r-1$, then $\|A-A_k\|_2=\sigma_r$.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item If $k\ge r$, error is zero.
\item If $\sigma_{k+1}=0$, any rank-$k$ completion of top space attains zero error.
\end{bullets}
}
\INPUTS{$A\in\mathbb{R}^{m\times n}$, $k\in\{0,\dots,r\}$.}
\DERIVATION{
\begin{align*}
\text{Example: }A&=\begin{bmatrix}2&0\\0&1\end{bmatrix},\ \sigma=(2,1).\\
k&=1,\ \|A-A_1\|_2=\sigma_2=1.
\end{align*}
}
\RESULT{
Truncated SVD is spectrally optimal and the residual operator norm equals
$\sigma_{k+1}$.
}
\UNITCHECK{
Operator norm is a gain; equality to a singular value is dimensionally consistent.
}
\PITFALLS{
\begin{bullets}
\item Minimizing Frobenius norm can be much smaller than spectral error; do not mix them.
\item For rectangular $A$, spectral norm still equals largest singular value.
\end{bullets}
}
\INTUITION{
You cannot approximate beyond the next largest axis; the worst direction leftover is
exactly the $(k+1)$-th singular direction.
}
\CANONICAL{
\begin{bullets}
\item Spectral residual equals the first discarded singular value.
\item Same minimizer as Frobenius case.
\end{bullets}
}

\FormulaPage{4}{Projection Forms and Pythagorean Identity}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Let $A=U\Sigma V^{\top}$ and $A_k=U_k\Sigma_k V_k^{\top}$. Then
$A_k=U_k U_k^{\top}A=AV_k V_k^{\top}$ and
$\langle A_k,A-A_k\rangle_F=0$, hence
$\|A\|_F^2=\|A_k\|_F^2+\|A-A_k\|_F^2$.

\WHAT{
$A_k$ equals orthogonal projections of $A$ onto the top singular subspaces and is
Frobenius-orthogonal to the residual, yielding a Pythagorean decomposition of energy.
}
\WHY{
Exposes $A_k$ as an orthogonal projection, simplifying reasoning, and provides an
energy-splitting identity used in proofs and error accounting.
}
\FORMULA{
\[
A_k=U_k U_k^{\top}A=AV_k V_k^{\top},\quad
\langle A_k,E\rangle_F=0,\ E=A-A_k,\quad
\|A\|_F^2=\|A_k\|_F^2+\|E\|_F^2.
\]
}
\CANONICAL{
$U_k\in\mathbb{R}^{m\times k}$ and $V_k\in\mathbb{R}^{n\times k}$ collect top $k$
singular vectors; $P_{U_k}=U_k U_k^{\top}$ and $P_{V_k}=V_k V_k^{\top}$ are projectors.
}
\PRECONDS{
\begin{bullets}
\item Orthogonality of $U$ and $V$.
\item Diagonality of $\Sigma$.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
$U_k U_k^{\top}$ and $V_k V_k^{\top}$ are orthogonal projectors, and
$U_k U_k^{\top} U=U_k$ and $V^{\top}V_k V_k^{\top}=V_k^{\top}$.
\end{lemma}
\begin{proof}
$U_k U_k^{\top}$ is symmetric and idempotent:
$(U_k U_k^{\top})^2=U_k(U_k^{\top}U_k)U_k^{\top}=U_k I U_k^{\top}$, so it is a projector.
Similarly for $V_k V_k^{\top}$. Since columns of $U$ are orthonormal,
$U^{\top}U=I$, hence $U_k U_k^{\top} U=[U_k\ 0]$, so the first $k$ columns are fixed.
The $V$ relation is analogous. \qedhere
\end{proof}
\DERIVATION{
\begin{align*}
\text{Step 1:}&\ A=U\Sigma V^{\top}=[U_k\ U_{\perp}]
\begin{bmatrix}\Sigma_k&0\\0&\Sigma_{\perp}\end{bmatrix}
[V_k\ V_{\perp}]^{\top}.\\
\text{Step 2:}&\ U_k U_k^{\top}A=U_k U_k^{\top}U\Sigma V^{\top}
=U_k \Sigma_k V_k^{\top}=A_k.\\
\text{Step 3:}&\ AV_k V_k^{\top}=U\Sigma V^{\top}V_k V_k^{\top}
=U_k \Sigma_k V_k^{\top}=A_k.\\
\text{Step 4:}&\ \langle A_k,E\rangle_F=\operatorname{tr}(A_k^{\top}(A-A_k))\\
&=\operatorname{tr}\big((U_k\Sigma_k V_k^{\top})^{\top}U
\begin{bmatrix}\Sigma_k&0\\0&\Sigma_{\perp}\end{bmatrix}
V^{\top}\big)-\|A_k\|_F^2=0.\\
\text{Step 5:}&\ \Rightarrow \|A\|_F^2=\|A_k\|_F^2+\|E\|_F^2.
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Use $A_k=U_k U_k^{\top}A$ or $A_k=AV_k V_k^{\top}$ to compute quickly.
\item Use orthogonality to split energies and verify errors.
\end{bullets}
\EQUIV{
\begin{bullets}
\item $A_k=\arg\min_{B:\ \operatorname{col}(B)\subseteq \operatorname{col}(U_k)}
\|A-B\|_F$.
\item $A_k=\arg\min_{B:\ \operatorname{row}(B)\subseteq \operatorname{row}(V_k^{\top})}
\|A-B\|_F$.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item If $k=0$, projectors are zero, giving $A_0=0$.
\item If $k=r$, projectors capture full range or co-range, giving $A$.
\end{bullets}
}
\INPUTS{$A\in\mathbb{R}^{m\times n}$, $U_k$, $V_k$.}
\DERIVATION{
\begin{align*}
\text{Example: }A&=\begin{bmatrix}3&0\\0&1\end{bmatrix},\ U=V=I.\\
k&=1,\ U_1U_1^{\top}A=\begin{bmatrix}1&0\\0&0\end{bmatrix}
\begin{bmatrix}3&0\\0&1\end{bmatrix}=\begin{bmatrix}3&0\\0&0\end{bmatrix}=A_1.\\
\|A\|_F^2&=10,\ \|A_1\|_F^2=9,\ \|A-A_1\|_F^2=1.
\end{align*}
}
\RESULT{
Projection identities yield fast computation of $A_k$ and certify orthogonal energy
splitting.
}
\UNITCHECK{
All terms are squared Frobenius norms; identities preserve units of energy.
}
\PITFALLS{
\begin{bullets}
\item Using non-orthogonal projectors breaks optimality and orthogonality.
\item Confusing left and right projectors leads to shape mismatches.
\end{bullets}
}
\INTUITION{
$A_k$ is the shadow of $A$ on the top $k$ singular directions from either side; the
leftover is perpendicular energy.
}
\CANONICAL{
\begin{bullets}
\item $A=A_k\oplus E$ in Frobenius inner product.
\item $A_k$ equals both left and right orthogonal projections.
\end{bullets}
}

\section{10 Exhaustive Problems and Solutions}
\ProblemPage{1}{Derive Eckart--Young in Frobenius Norm and Compute a Case}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Show that $A_k$ minimizes Frobenius error and compute $A_1$ for a fixed $A$.

\PROBLEM{
Prove that for any $A\in\mathbb{R}^{m\times n}$ and $k\ge 0$,
$A_k=U\Sigma_k V^{\top}$ minimizes $\|A-B\|_F$ over rank$(B)\le k$. Then for
$A=\begin{bmatrix}2&1\\0&1\end{bmatrix}$, compute the best rank-$1$ approximation and its
error.
}
\MODEL{
\[
A=U\Sigma V^{\top},\quad A_k=U\Sigma_k V^{\top},\quad
\min_{\operatorname{rank}(B)\le k}\|A-B\|_F.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Frobenius norm invariance holds.
\item Singular values are sorted descending.
\end{bullets}
}
\varmapStart
\var{A}{input matrix.}
\var{U,\Sigma,V}{SVD factors.}
\var{k}{target rank.}
\var{A_k}{truncated SVD.}
\varmapEnd
\WHICHFORMULA{
Formula 2 (Eckart--Young Frobenius) with supporting invariance from Formula 1.
}
\GOVERN{
\[
\min_{\operatorname{rank}(B)\le k}\|A-B\|_F=\|\Sigma-\Sigma_k\|_F.
\]
}
\INPUTS{$A=\begin{bmatrix}2&1\\0&1\end{bmatrix}$, $k=1$.}
\DERIVATION{
\begin{align*}
\text{Step 1:}&\ \|A-B\|_F=\|\Sigma-C\|_F,\ \operatorname{rank}(C)\le k.\\
\text{Step 2:}&\ \text{Optimal }C\text{ is }\Sigma_k,\ \Rightarrow B=A_k.\\
\text{Step 3:}&\ \text{Compute SVD numerically for the example.}\\
A^{\top}A&=\begin{bmatrix}4&2\\2&2\end{bmatrix}.\
\lambda_{\pm}=\frac{6\pm\sqrt{36-16}}{2}=\frac{6\pm\sqrt{20}}{2}.\\
\lambda_1&=3+\sqrt{5},\ \lambda_2=3-\sqrt{5}.\
\sigma_1=\sqrt{3+\sqrt{5}},\ \sigma_2=\sqrt{3-\sqrt{5}}.\\
v_1&\propto \begin{bmatrix}\lambda_1-2\\2\end{bmatrix}
=\begin{bmatrix}1+\sqrt{5}\\2\end{bmatrix},\
v_2\propto \begin{bmatrix}1-\sqrt{5}\\2\end{bmatrix}.\\
V&=[\hat v_1\ \hat v_2],\ U=AV\Sigma^{\dagger}.\\
A_1&=\sigma_1 u_1 v_1^{\top}.\\
\text{Step 4:}&\ \|A-A_1\|_F^2=\sigma_2^2=3-\sqrt{5}.
\end{align*}
}
\RESULT{
$A_1$ equals the leading outer product scaled by $\sigma_1$; error
$\|A-A_1\|_F=\sqrt{3-\sqrt{5}}$.
}
\UNITCHECK{
Both sides are lengths; eigenvalue square roots yield singular values; consistent.
}
\EDGECASES{
\begin{bullets}
\item If $A$ were already rank one, error would be zero.
\item If $k=0$, the minimizer is $0$ with error $\|A\|_F$.
\end{bullets}
}
\ALTERNATE{
Project $A$ as $AV_1 V_1^{\top}$ or $U_1 U_1^{\top}A$ using Formula 4.
}
\VALIDATION{
\begin{bullets}
\item Numerically compute $\sigma_i$ and verify tail energy equals error.
\item Check orthogonality $\langle A_1,A-A_1\rangle_F=0$.
\end{bullets}
}
\INTUITION{
In coordinates where $A$ is diagonal, copy the largest diagonal entry and zero the rest.
}
\CANONICAL{
\begin{bullets}
\item Optimal $B$ equals truncated SVD.
\item Error equals sum of squares of discarded singular values.
\end{bullets}
}

\ProblemPage{2}{Compute Best Rank-One Approximation Explicitly}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Compute $A_1$ and Frobenius and spectral errors for a concrete $A$.

\PROBLEM{
For $A=\begin{bmatrix}3&1\\0&2\\0&0\end{bmatrix}$, find $A_1$ and compute
$\|A-A_1\|_F$ and $\|A-A_1\|_2$.
}
\MODEL{
\[
A=U\Sigma V^{\top},\ A_1=\sigma_1 u_1 v_1^{\top},\
\|A-A_1\|_F^2=\sum_{i>1}\sigma_i^2,\ \|A-A_1\|_2=\sigma_2.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Singular values sorted descending.
\item Use Formulas 2 and 3.
\end{bullets}
}
\varmapStart
\var{A}{given matrix.}
\var{\sigma_i}{singular values.}
\var{u_i,v_i}{singular vectors.}
\varmapEnd
\WHICHFORMULA{
Formulas 2 and 3 for Frobenius and spectral optimality and errors.
}
\GOVERN{
\[
\|A-A_1\|_F^2=\sum_{i\ge 2}\sigma_i^2,\quad \|A-A_1\|_2=\sigma_2.
\]
}
\INPUTS{$A=\begin{bmatrix}3&1\\0&2\\0&0\end{bmatrix}$.}
\DERIVATION{
\begin{align*}
A^{\top}A&=\begin{bmatrix}9&3\\3&5\end{bmatrix}.\
\lambda_{\pm}=\frac{14\pm\sqrt{196-144}}{2}=\frac{14\pm\sqrt{52}}{2}.\\
\lambda_1&=7+\sqrt{13},\ \lambda_2=7-\sqrt{13}.\
\sigma_{1,2}=\sqrt{7\pm\sqrt{13}}.\\
\|A-A_1\|_F^2&=\sigma_2^2=7-\sqrt{13}.\\
\|A-A_1\|_2&=\sigma_2=\sqrt{7-\sqrt{13}}.
\end{align*}
}
\RESULT{
$A_1=\sigma_1 u_1 v_1^{\top}$; Frobenius error $\sqrt{7-\sqrt{13}}$, spectral error
$\sqrt{7-\sqrt{13}}$.
}
\UNITCHECK{
Frobenius error squared equals eigenvalue of $A^{\top}A$ discarded; consistent.
}
\EDGECASES{
\begin{bullets}
\item If the off-diagonal $1$ were $0$, $A$ is diagonal and computations simplify.
\item If $2$ increased beyond $3$, ordering of singular values would change.
\end{bullets}
}
\ALTERNATE{
Compute $A_1=AV_1 V_1^{\top}$ using the top right singular vector.
}
\VALIDATION{
\begin{bullets}
\item Numerically verify $\|A\|_F^2=\sigma_1^2+\sigma_2^2$.
\item Check residual has rank at most one.
\end{bullets}
}
\INTUITION{
The second strength controls both Frobenius tail and spectral leftover.
}
\CANONICAL{
\begin{bullets}
\item Tail singular values determine errors.
\item Rank-one approximation preserves the dominant mode.
\end{bullets}
}

\ProblemPage{3}{PCA Objective Equivalences}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Show minimizing reconstruction error equals maximizing captured variance.

\PROBLEM{
Let $X\in\mathbb{R}^{n\times d}$ be centered. Show that
$\min_{\operatorname{rank}(B)\le k}\|X-B\|_F^2$ is achieved by $B=X V_k V_k^{\top}$ and
equals $\|X\|_F^2-\sum_{i=1}^k \sigma_i^2$, which equals
$\min_{W:W^{\top}W=I}\|X-XWW^{\top}\|_F^2$, and that
$\max_{W:W^{\top}W=I}\|XW\|_F^2=\sum_{i=1}^k\sigma_i^2$.
}
\MODEL{
\[
X=U\Sigma V^{\top},\ B=X V_k V_k^{\top},\
\|X-B\|_F^2=\sum_{i>k}\sigma_i^2.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $X$ is centered so PCA aligns with right-singular vectors.
\item $W\in\mathbb{R}^{d\times k}$ with $W^{\top}W=I$.
\end{bullets}
}
\varmapStart
\var{X}{data matrix.}
\var{W}{projection directions.}
\var{V_k}{top right singular vectors.}
\var{\sigma_i}{singular values of $X$.}
\varmapEnd
\WHICHFORMULA{
Formulas 2 and 4: projection forms and Frobenius optimality connect PCA variants.
}
\GOVERN{
\[
\min_{W^{\top}W=I}\|X-XWW^{\top}\|_F^2=\|X\|_F^2-\|XW\|_F^2.
\]
}
\INPUTS{$X=U\Sigma V^{\top}$, $k$.}
\DERIVATION{
\begin{align*}
\|X-XWW^{\top}\|_F^2&=\|X\|_F^2-2\langle X, XWW^{\top}\rangle_F+\|XWW^{\top}\|_F^2\\
&=\|X\|_F^2-\|XW\|_F^2.\\
\|XW\|_F^2&=\operatorname{tr}(W^{\top}X^{\top}XW)
=\operatorname{tr}(W^{\top}V\Sigma^2 V^{\top}W).\\
\text{Maximizer }W&=V_k\ \text{selects top }k\text{ eigen-directions}.\\
\Rightarrow \max \|XW\|_F^2&=\sum_{i=1}^k \sigma_i^2.\\
\Rightarrow \min \|X-XWW^{\top}\|_F^2&=\|X\|_F^2-\sum_{i=1}^k \sigma_i^2.
\end{align*}
}
\RESULT{
All PCA formulations are equivalent; $B=X V_k V_k^{\top}$ minimizes reconstruction error,
and captured variance equals $\sum_{i=1}^k \sigma_i^2$.
}
\UNITCHECK{
All terms are squared Frobenius norms; variance captured plus residual equals total.
}
\EDGECASES{
\begin{bullets}
\item If $k=0$, captured variance is zero; residual equals total.
\item If $k=\operatorname{rank}(X)$, residual is zero.
\end{bullets}
}
\ALTERNATE{
Diagonalize $X^{\top}X$ and reason via eigen-decomposition directly.
}
\VALIDATION{
\begin{bullets}
\item Numerically compute $W=V_k$ and verify both objectives match.
\item Check $B=XV_k V_k^{\top}$ equals $U_k\Sigma_k V_k^{\top}$.
\end{bullets}
}
\INTUITION{
Keeping top directions maximizes spread of projected data and simultaneously minimizes
reconstruction error.
}
\CANONICAL{
\begin{bullets}
\item PCA equals truncated SVD on centered data.
\item Energy partition: total variance equals kept plus residual.
\end{bullets}
}

\ProblemPage{4}{Hidden Structure: Two-Sided Projections Coincide}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Show $U_k U_k^{\top}A=AV_k V_k^{\top}=A_k$.

\PROBLEM{
Alice computes $B_L=U_k U_k^{\top}A$. Bob computes $B_R=AV_k V_k^{\top}$. Prove
$B_L=B_R=A_k$ and compute both for a $3\times 2$ example, confirming equality.
}
\MODEL{
\[
A=U\Sigma V^{\top},\ U_k U_k^{\top}A=AV_k V_k^{\top}=U_k\Sigma_k V_k^{\top}.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $U,V$ orthogonal; $\Sigma$ diagonal.
\item $k\le r$.
\end{bullets}
}
\varmapStart
\var{U_k,V_k}{top singular vector blocks.}
\var{A_k}{truncated SVD.}
\varmapEnd
\WHICHFORMULA{
Formula 4 (projection forms) derived from SVD block structure.
}
\GOVERN{
\[
U_k U_k^{\top}A=[U_k\ 0]
\begin{bmatrix}\Sigma_k&0\\0&\Sigma_{\perp}\end{bmatrix}
[V_k\ V_{\perp}]^{\top}=U_k\Sigma_k V_k^{\top}.
\]
}
\INPUTS{$A=\begin{bmatrix}2&0\\1&1\\0&1\end{bmatrix}$, choose $k=1$.}
\DERIVATION{
\begin{align*}
\text{Compute }A^{\top}A&=\begin{bmatrix}5&1\\1&2\end{bmatrix}.\
\lambda_{1,2}=\frac{7\pm\sqrt{33}}{2}.\\
\sigma_{1,2}&=\sqrt{\lambda_{1,2}}.\
v_1\propto \begin{bmatrix}\lambda_1-2\\1\end{bmatrix},\
u_1=Av_1/\sigma_1.\\
B_R&=AV_1 V_1^{\top}=\sigma_1 u_1 v_1^{\top}=A_1.\\
B_L&=U_1 U_1^{\top}A=\sigma_1 u_1 v_1^{\top}=A_1.\\
\Rightarrow B_L&=B_R.
\end{align*}
}
\RESULT{
Left and right projections coincide with $A_1$, confirming two equivalent constructions
of $A_k$.
}
\UNITCHECK{
Shapes: $U_k U_k^{\top}A$ is $m\times n$, same as $AV_k V_k^{\top}$. Entries agree.
}
\EDGECASES{
\begin{bullets}
\item If $k=0$, both give zero matrix.
\item If $k=r$, both return $A$.
\end{bullets}
}
\ALTERNATE{
Use Moore--Penrose projectors: $A_k=U_k \Sigma_k V_k^{\top}
=U_k U_k^{\top} A = A V_k V_k^{\top}$.
}
\VALIDATION{
\begin{bullets}
\item Numerically compare $B_L$ and $B_R$ entrywise.
\item Verify $\langle A_k,A-A_k\rangle_F=0$.
\end{bullets}
}
\INTUITION{
Projecting columns into the top left subspace or rows into the top right subspace yields
the same captured structure.
}
\CANONICAL{
\begin{bullets}
\item Two-sided projection identity for truncated SVD.
\item Equivalence of column-space and row-space formulations.
\end{bullets}
}

\ProblemPage{5}{Narrative: Alice Versus Bob on Basis Choice}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Unitary invariance implies basis choice does not change optimal error.

\PROBLEM{
Alice rotates inputs by $Q$ and outputs by $P$ (orthogonal), compresses
$\tilde A=P^{\top}AQ$ to rank $k$, then un-rotates. Bob compresses $A$ directly to rank
$k$. Show they obtain the same error and related minimizers.
}
\MODEL{
\[
\min_{\operatorname{rank}(B)\le k}\|A-B\|_F
=\min_{\operatorname{rank}(\tilde B)\le k}\|\tilde A-\tilde B\|_F.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $P,Q$ orthogonal; Frobenius norm invariance holds.
\item Unique top-$k$ subspaces for simplicity.
\end{bullets}
}
\varmapStart
\var{P,Q}{orthogonal rotations.}
\var{\tilde A}{rotated matrix $P^{\top}AQ$.}
\var{\tilde B}{rank-$k$ approximation of $\tilde A$.}
\varmapEnd
\WHICHFORMULA{
Formula 1 invariance and Formula 2 optimality.
}
\GOVERN{
\[
\|A-B\|_F=\|P^{\top}(A-B)Q\|_F=\|\tilde A-\tilde B\|_F.
\]
}
\INPUTS{$A$ arbitrary, $P,Q$ orthogonal, $k$.}
\DERIVATION{
\begin{align*}
\text{Step 1:}&\ \tilde A=P^{\top}AQ,\ \tilde B=P^{\top}BQ.\\
\text{Step 2:}&\ \operatorname{rank}(B)\le k\iff \operatorname{rank}(\tilde B)\le k.\\
\text{Step 3:}&\ \|A-B\|_F=\|\tilde A-\tilde B\|_F.\\
\text{Step 4:}&\ \text{Minimizers map as }B^{\star}=P\tilde B^{\star}Q^{\top}.\\
\text{Step 5:}&\ \text{Errors equal and singular values are invariant.}
\end{align*}
}
\RESULT{
Alice and Bob achieve identical minimal errors; their minimizers are related by the
rotations $P,Q$.
}
\UNITCHECK{
Frobenius norm preserved by orthogonal transformations; ranks preserved.
}
\EDGECASES{
\begin{bullets}
\item If singular subspaces are not unique, minimizers form families connected by
rotations in degenerate subspaces.
\item If $k\ge r$, both get zero error.
\end{bullets}
}
\ALTERNATE{
Apply the same reasoning to spectral norm using Formula 3.
}
\VALIDATION{
\begin{bullets}
\item Numerically check with random $P,Q$ and fixed $A$ that errors match.
\item Confirm $B^{\star}=P\tilde B^{\star}Q^{\top}$ entrywise.
\end{bullets}
}
\INTUITION{
Rotations do not change lengths; compress after rotating or before yields the same best
fit.
}
\CANONICAL{
\begin{bullets}
\item Unitary invariance of both the objective and the solution structure.
\item Equivariance of minimizers under orthogonal maps.
\end{bullets}
}

\ProblemPage{6}{Expectation Puzzle: Random Rank-One Sampling}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Randomly sample one singular component; compute expected error.

\PROBLEM{
Let $A=\sum_{i=1}^r \sigma_i u_i v_i^{\top}$. Sample index $I$ with
$\mathbb{P}(I=i)=p_i=\sigma_i^2/\|A\|_F^2$. Define $\tilde A=\sigma_I u_I v_I^{\top}$.
Compute $\mathbb{E}\|A-\tilde A\|_F^2$ and compare to $\|A-A_1\|_F^2$.
}
\MODEL{
\[
\|A-\tilde A\|_F^2=\sum_{j\ne I}\sigma_j^2=(\sum_j\sigma_j^2)-\sigma_I^2.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Orthogonality of singular components in Frobenius inner product.
\item Probabilities $p_i$ defined by energy proportions.
\end{bullets}
}
\varmapStart
\var{\sigma_i}{singular values.}
\var{I}{random index.}
\var{\tilde A}{random rank-one approximation.}
\varmapEnd
\WHICHFORMULA{
Frobenius Pythagorean identity from Formula 4 and tail energy from Formula 2.
}
\GOVERN{
\[
\mathbb{E}\|A-\tilde A\|_F^2=\sum_i p_i \Big(\sum_j \sigma_j^2-\sigma_i^2\Big).
\]
}
\INPUTS{$p_i=\sigma_i^2/\sum_j \sigma_j^2$.}
\DERIVATION{
\begin{align*}
\mathbb{E}\|A-\tilde A\|_F^2
&=\sum_i \frac{\sigma_i^2}{\sum_j\sigma_j^2}
\Big(\sum_j \sigma_j^2-\sigma_i^2\Big)\\
&=\sum_i \sigma_i^2 - \frac{\sum_i \sigma_i^4}{\sum_j \sigma_j^2}.\\
\|A-A_1\|_F^2&=\sum_{i\ge 2}\sigma_i^2.
\end{align*}
}
\RESULT{
$\mathbb{E}\|A-\tilde A\|_F^2=\|A\|_F^2-\frac{\sum_i \sigma_i^4}{\|A\|_F^2}\ge
\sum_{i\ge 2}\sigma_i^2$, with equality only for rank one.
}
\UNITCHECK{
Each term is squared Frobenius norm; probabilities sum to one.
}
\EDGECASES{
\begin{bullets}
\item If $r=1$, expected error is zero, matching the optimal error.
\item If singular values are equal, expected error is
$\|A\|_F^2-\frac{r \sigma^4}{r \sigma^2}= (r-1)\sigma^2$.
\end{bullets}
}
\ALTERNATE{
Importance-sample $s>1$ rank-one terms without replacement and average; expected error
decreases accordingly.
}
\VALIDATION{
\begin{bullets}
\item Monte Carlo with fixed seed verifies the expectation numerically.
\item Compare to deterministic $A_1$ error to see the gap.
\end{bullets}
}
\INTUITION{
Sampling by energy often captures a lot, but deterministic truncation captures the most.
}
\CANONICAL{
\begin{bullets}
\item Randomized approximations have larger expected error than $A_1$.
\item Energy-weighted sampling gives a simple expectation formula.
\end{bullets}
}

\ProblemPage{7}{Proof: Orthogonality of $A_k$ and Residual}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Show $\langle A_k, A-A_k\rangle_F=0$.

\PROBLEM{
Prove that $A_k$ is Frobenius-orthogonal to the residual $E=A-A_k$ for any
$A=U\Sigma V^{\top}$.
}
\MODEL{
\[
A=U\Sigma V^{\top},\ A_k=U_k\Sigma_k V_k^{\top},\ E=U_{\perp}\Sigma_{\perp}V_{\perp}^{\top}.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $U=[U_k\ U_{\perp}]$ and $V=[V_k\ V_{\perp}]$ orthogonal blocks.
\end{bullets}
}
\varmapStart
\var{U_k,U_{\perp}}{partition of $U$.}
\var{V_k,V_{\perp}}{partition of $V$.}
\var{\Sigma_k,\Sigma_{\perp}}{block diagonal split.}
\varmapEnd
\WHICHFORMULA{
Formula 4 Pythagorean identity and projection forms.
}
\GOVERN{
\[
\langle A_k,E\rangle_F=\operatorname{tr}(V_k\Sigma_k U_k^{\top}
U_{\perp}\Sigma_{\perp}V_{\perp}^{\top})=0.
\]
}
\INPUTS{$A=U\Sigma V^{\top}$, $k$.}
\DERIVATION{
\begin{align*}
\langle A_k,E\rangle_F
&=\operatorname{tr}(A_k^{\top}E)
=\operatorname{tr}(V_k\Sigma_k U_k^{\top}U_{\perp}\Sigma_{\perp}V_{\perp}^{\top})\\
&=\operatorname{tr}(V_k\Sigma_k\underbrace{U_k^{\top}U_{\perp}}_{0}
\Sigma_{\perp}V_{\perp}^{\top})=0.
\end{align*}
}
\RESULT{
$A_k$ and $E$ are orthogonal in Frobenius inner product.
}
\UNITCHECK{
Inner product is scalar; zero value matches orthogonality claim.
}
\EDGECASES{
\begin{bullets}
\item If $k=0$ or $k=r$, orthogonality is trivial as one term is zero.
\end{bullets}
}
\ALTERNATE{
Compute in the singular basis where $A_k$ and $E$ are supported on disjoint diagonal
blocks, making orthogonality immediate.
}
\VALIDATION{
\begin{bullets}
\item Numerically compute $\operatorname{tr}(A_k^{\top}(A-A_k))$ and verify zero.
\end{bullets}
}
\INTUITION{
The kept and discarded parts live in perpendicular subspaces.
}
\CANONICAL{
\begin{bullets}
\item Pythagorean splitting $\|A\|_F^2=\|A_k\|_F^2+\|E\|_F^2$ follows.
\end{bullets}
}

\ProblemPage{8}{Proof: Stability Under Adding a Rank-$k$ Matrix}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Show $\min_{\operatorname{rank}(B)\le k}\|A-B\|_F^2=\|A\|_F^2-\max_{B:\operatorname{rank}\le k}
2\langle A,B\rangle_F-\|B\|_F^2$ yields $B=A_k$.

\PROBLEM{
Prove that the projection of $A$ onto the nonconvex set of rank-$k$ matrices in the
Hilbert space $(\mathbb{R}^{m\times n},\langle\cdot,\cdot\rangle_F)$ is $A_k$.
}
\MODEL{
\[
\|A-B\|_F^2=\|A\|_F^2-2\langle A,B\rangle_F+\|B\|_F^2.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Frobenius inner product structure.
\item SVD expansion $A=\sum_i \sigma_i u_i v_i^{\top}$.
\end{bullets}
}
\varmapStart
\var{B}{rank-$k$ candidate.}
\var{A_k}{projection.}
\varmapEnd
\WHICHFORMULA{
Formulas 2 and 4 imply projection and orthogonality conditions.
}
\GOVERN{
\[
\langle A,B\rangle_F=\sum_i \sigma_i \langle u_i v_i^{\top},B\rangle_F.
\]
}
\INPUTS{$A=U\Sigma V^{\top}$, $k$.}
\DERIVATION{
\begin{align*}
\langle A,B\rangle_F
&=\operatorname{tr}(A^{\top}B)=\sum_i \sigma_i \langle u_i v_i^{\top},B\rangle_F.\\
\text{By Cauchy--Schwarz, }&
\langle u_i v_i^{\top},B\rangle_F\le \|B\|_F.\\
\text{Rank}(B)\le k&\Rightarrow B\text{ lives in span of }k\text{ outer products.}\\
\text{Maximization}&\ \text{achieved by aligning with top }k\ u_i v_i^{\top}.\\
\Rightarrow B&=A_k,\ \text{yielding minimal }\|A-B\|_F^2.
\end{align*}
}
\RESULT{
$A_k$ is the Hilbert-space projection of $A$ onto the rank-$k$ manifold.
}
\UNITCHECK{
All terms are Frobenius inner products or norms; dimensions align.
}
\EDGECASES{
\begin{bullets}
\item If $k\ge r$, the projection is $A$.
\item If $k=0$, projection is $0$.
\end{bullets}
}
\ALTERNATE{
Direct diagonal argument on $\Sigma$ as in Formula 2 yields the same minimizer.
}
\VALIDATION{
\begin{bullets}
\item Numeric check: random $A$, compare sampled rank-$k$ candidates to $A_k$.
\end{bullets}
}
\INTUITION{
Best approximation aligns with the strongest singular directions.
}
\CANONICAL{
\begin{bullets}
\item Projection view explains orthogonality and energy split.
\end{bullets}
}

\ProblemPage{9}{Combo: Least Squares with Rank Constraint}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Solve $\min_{\operatorname{rank}(B)\le k}\|A-B\|_F^2$ via normal equations in the
singular basis.

\PROBLEM{
Combine least squares and SVD: express the optimality conditions for $B$ and show they
lead to truncation in the singular basis.
}
\MODEL{
\[
\min_{B}\|A-B\|_F^2\ \text{s.t.}\ \operatorname{rank}(B)\le k.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Use SVD basis to parameterize $B=UCV^{\top}$.
\item Rank constraint maps to rank$(C)\le k$.
\end{bullets}
}
\varmapStart
\var{C}{coordinate matrix in singular basis.}
\var{\Sigma}{diagonal of $A$.}
\varmapEnd
\WHICHFORMULA{
Formulas 1 and 2 reduce to diagonal coordinate-wise minimization.
}
\GOVERN{
\[
\|A-B\|_F^2=\|\Sigma-C\|_F^2=\sum_{ij}(\Sigma_{ij}-C_{ij})^2.
\]
}
\INPUTS{$A=U\Sigma V^{\top}$, $k$.}
\DERIVATION{
\begin{align*}
\text{Normal equations: }&\ \partial \|\Sigma-C\|_F^2/\partial C_{ij}=0\\
&\Rightarrow C_{ij}=\Sigma_{ij}\ \text{without rank constraint}.\\
\text{Constraint: }&\ \operatorname{rank}(C)\le k\Rightarrow C\text{ has at most }k
\text{ nonzero diagonal entries}.\\
\Rightarrow&\ C=\Sigma_k,\ B=U\Sigma_k V^{\top}.
\end{align*}
}
\RESULT{
Truncation satisfies the constrained normal equations in the singular basis.
}
\UNITCHECK{
Equation is entrywise; diagonal structure ensures consistent solution.
}
\EDGECASES{
\begin{bullets}
\item If unconstrained, $C=\Sigma$, giving $B=A$.
\item If $k=0$, $C=0$, giving $B=0$.
\end{bullets}
}
\ALTERNATE{
Formulate as selection of $k$ diagonal entries minimizing squared error; choose the top
$k$.
}
\VALIDATION{
\begin{bullets}
\item Numeric example verifies diagonal selection matches $A_k$.
\end{bullets}
}
\INTUITION{
In the right coordinates, the problem decouples into independent scalars.
}
\CANONICAL{
\begin{bullets}
\item Constrained least squares reduces to truncation.
\end{bullets}
}

\ProblemPage{10}{Combo: Spectral Error Certification}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Certify $\|A-A_k\|_2=\sigma_{k+1}$ using a variational characterization.

\PROBLEM{
Use Courant--Fischer to prove $\|A-A_k\|_2\ge \sigma_{k+1}$ and achieve equality with
$A_k$, providing a complete certificate.
}
\MODEL{
\[
\|A-A_k\|_2=\max_{\|x\|=1}\|(A-A_k)x\|=\sigma_{k+1}.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $A=U\Sigma V^{\top}$ and $A_k=U\Sigma_k V^{\top}$.
\item Variational principle for singular values.
\end{bullets}
}
\varmapStart
\var{v_{k+1}}{right singular vector for $\sigma_{k+1}$.}
\var{u_{k+1}}{left singular vector for $\sigma_{k+1}$.}
\varmapEnd
\WHICHFORMULA{
Formula 3 spectral version and unitary invariance.
}
\GOVERN{
\[
(A-A_k)v_{k+1}=\sigma_{k+1} u_{k+1}\Rightarrow
\|A-A_k\|_2\ge \sigma_{k+1}.
\]
}
\INPUTS{$A=U\Sigma V^{\top}$, $k$.}
\DERIVATION{
\begin{align*}
(A-A_k)V&=U(\Sigma-\Sigma_k).\\
\Rightarrow (A-A_k)v_{k+1}&=\sigma_{k+1} u_{k+1}.\\
\|A-A_k\|_2&\ge \|(A-A_k)v_{k+1}\|=\sigma_{k+1}.\\
\text{Upper bound:}&\ \|A-A_k\|_2=\|\Sigma-\Sigma_k\|_2\le \sigma_{k+1}.\\
\Rightarrow \|A-A_k\|_2&=\sigma_{k+1}.
\end{align*}
}
\RESULT{
The spectral residual equals $\sigma_{k+1}$ with $A_k$ attaining the bound.
}
\UNITCHECK{
Operator norm equals a singular value; consistent.
}
\EDGECASES{
\begin{bullets}
\item If $\sigma_{k+1}=0$, then the residual spectral norm is zero.
\item Degeneracy at $\sigma_{k+1}=\sigma_k$ does not affect the bound.
\end{bullets}
}
\ALTERNATE{
Apply the supporting lemma of Formula 3 directly on $\Sigma$.
}
\VALIDATION{
\begin{bullets}
\item Numerically compute $\|A-A_k\|_2$ and compare to $\sigma_{k+1}$.
\end{bullets}
}
\INTUITION{
The worst remaining direction after truncation is exactly the next singular axis.
}
\CANONICAL{
\begin{bullets}
\item Variational certificate for spectral optimality.
\end{bullets}
}

\section{Coding Demonstrations}
\CodeDemoPage{Verify Eckart--Young in Frobenius Norm}
\PROBLEM{
Compute $A_k$ via SVD and verify
$\|A-A_k\|_F^2=\sum_{i>k}\sigma_i^2$ and that sampled random rank-$k$ competitors never
beat $A_k$. Implements Formula 2.
}
\API{
\begin{bullets}
\item \inlinecode{def read_input(s) -> tuple} — parse m,n,k and seed.
\item \inlinecode{def solve_case(m,n,k,seed) -> dict} — compute SVD, errors.
\item \inlinecode{def validate() -> None} — self-checks with assertions.
\item \inlinecode{def main() -> None} — orchestrate a demo run.
\end{bullets}
}
\INPUTS{
$m,n,k$ positive integers with $k\le \min\{m,n\}$, integer seed for RNG.
}
\OUTPUTS{
Dictionary with Frobenius error, tail energy, and best random competitor error.
}
\FORMULA{
\[
A_k=U\Sigma_k V^{\top},\quad
\|A-A_k\|_F^2=\sum_{i>k}\sigma_i^2.
\]
}
\textbf{SOLUTION A — From Scratch (Mathematically Explicit Implementation)}
\begin{codepy}
import numpy as np

def read_input(s):
    toks = [int(x) for x in s.split()]
    if len(toks) != 4:
        raise ValueError("need m n k seed")
    return toks[0], toks[1], toks[2], toks[3]

def _svd_from_eigh(A):
    # SVD via eigen-decomp of A^T A
    AtA = A.T @ A
    w, V = np.linalg.eigh(AtA)
    idx = np.argsort(w)[::-1]
    w, V = w[idx], V[:, idx]
    s = np.sqrt(np.maximum(w, 0.0))
    # left singular vectors
    U = A @ V
    tol = 1e-12
    for i in range(len(s)):
        if s[i] > tol:
            U[:, i] /= s[i]
    # complete U with orthonormal basis if needed
    # but not required for A_k construction
    return U, s, V

def solve_case(m, n, k, seed):
    np.random.seed(seed)
    A = np.random.randn(m, n)
    U, s, V = _svd_from_eigh(A)
    k = int(k)
    Sk = np.zeros_like(s)
    Sk[:k] = s[:k]
    Ak = (U[:, :len(s)] * Sk) @ V.T
    err_f = np.linalg.norm(A - Ak, "fro") ** 2
    tail = float(np.sum(s[k:] ** 2))
    # random competitors
    trials = 200
    best = float("inf")
    for t in range(trials):
        X = np.random.randn(m, k)
        Y = np.random.randn(n, k)
        BX = np.linalg.qr(X)[0]
        BY = np.linalg.qr(Y)[0]
        B = BX @ (BX.T @ A @ BY) @ BY.T
        e = np.linalg.norm(A - B, "fro") ** 2
        if e < best:
            best = e
    return {"err_f": float(err_f), "tail": float(tail), "best_rand": float(best)}

def validate():
    m, n, k, seed = 40, 30, 5, 0
    out = solve_case(m, n, k, seed)
    assert abs(out["err_f"] - out["tail"]) < 1e-7
    assert out["best_rand"] + 1e-9 >= out["err_f"]

def main():
    validate()
    m, n, k, seed = 20, 15, 3, 1
    out = solve_case(m, n, k, seed)
    print("err_f", round(out["err_f"], 8),
          "tail", round(out["tail"], 8),
          "best_rand", round(out["best_rand"], 8))

if __name__ == "__main__":
    main()
\end{codepy}
\textbf{SOLUTION B — Library-Based (Validated Computational Shortcut)}
\begin{codepy}
import numpy as np

def read_input(s):
    toks = [int(x) for x in s.split()]
    if len(toks) != 4:
        raise ValueError("need m n k seed")
    return toks[0], toks[1], toks[2], toks[3]

def solve_case(m, n, k, seed):
    np.random.seed(seed)
    A = np.random.randn(m, n)
    U, s, Vt = np.linalg.svd(A, full_matrices=False)
    Ak = (U[:, :k] * s[:k]) @ Vt[:k, :]
    err_f = np.linalg.norm(A - Ak, "fro") ** 2
    tail = float(np.sum(s[k:] ** 2))
    # random competitors
    trials = 200
    best = float("inf")
    for t in range(trials):
        X = np.random.randn(m, k)
        Y = np.random.randn(n, k)
        BX = np.linalg.qr(X)[0]
        BY = np.linalg.qr(Y)[0]
        B = BX @ (BX.T @ A @ BY) @ BY.T
        e = np.linalg.norm(A - B, "fro") ** 2
        if e < best:
            best = e
    return {"err_f": float(err_f), "tail": float(tail), "best_rand": float(best)}

def validate():
    m, n, k, seed = 40, 30, 5, 0
    out = solve_case(m, n, k, seed)
    assert abs(out["err_f"] - out["tail"]) < 1e-8
    assert out["best_rand"] + 1e-9 >= out["err_f"]

def main():
    validate()
    m, n, k, seed = 25, 10, 2, 2
    out = solve_case(m, n, k, seed)
    print("err_f", round(out["err_f"], 8),
          "tail", round(out["tail"], 8),
          "best_rand", round(out["best_rand"], 8))

if __name__ == "__main__":
    main()
\end{codepy}
\COMPLEXITY{
Both variants: SVD or eigendecomposition takes time $\mathcal{O}(mn\min\{m,n\})$ and
space $\mathcal{O}(mn)$. Random competitor evaluation costs $\mathcal{O}(mk^2+nk^2+mnk)$
per trial.
}
\FAILMODES{
\begin{bullets}
\item $k>\min\{m,n\}$ invalid; guard and raise errors.
\item Near-equal singular values cause non-uniqueness; assertions use tolerant checks.
\item Degenerate singular values induce small numerical differences; use tolerances.
\end{bullets}
}
\STABILITY{
\begin{bullets}
\item SVD is backward stable; eigh on $A^{\top}A$ squares condition number.
\item Prefer direct SVD for improved conditioning; normalize inputs if needed.
\end{bullets}
}
\VALIDATION{
\begin{bullets}
\item Assert equality err\_f = tail within tolerance.
\item Check that no sampled competitor beats $A_k$.
\item Cross-check both implementations agree.
\end{bullets}
}
\RESULT{
Both implementations report matching Frobenius errors equal to tail energy; random
competitors have no smaller error, empirically confirming optimality.
}
\EXPLANATION{
Construction follows Formula 2: $A_k$ formed by truncating SVD, and the error equals the
sum of discarded singular values squared. Random competitors approximate arbitrary
rank-$k$ matrices via two-sided projections to test optimality empirically.
}
\EXTENSION{
Vectorize trials, test multiple $k$ values, and add Monte Carlo confidence intervals.
}

\CodeDemoPage{Spectral Error Equals Next Singular Value}
\PROBLEM{
Compute $A_k$ and verify $\|A-A_k\|_2=\sigma_{k+1}$ by comparing to the residual's top
singular value via power iteration. Implements Formula 3.
}
\API{
\begin{bullets}
\item \inlinecode{def read_input(s) -> tuple} — parse m,n,k,seed, iters.
\item \inlinecode{def solve_case(m,n,k,seed,iters) -> dict} — compute errors.
\item \inlinecode{def validate() -> None} — run asserts.
\item \inlinecode{def main() -> None} — run a demo.
\end{bullets}
}
\INPUTS{
Integers $m,n,k,\text{seed},\text{iters}$ with $k<\min\{m,n\}$ and iters $\ge 5$.
}
\OUTPUTS{
Dictionary with spectral residual via SVD and via power method.
}
\FORMULA{
\[
\|A-A_k\|_2=\sigma_{k+1},\quad
\text{power: }x\leftarrow\frac{(A-A_k)^{\top}(A-A_k)x}{\|(A-A_k)^{\top}(A-A_k)x\|}.
\]
}
\textbf{SOLUTION A — From Scratch (Power Method and eigh)}
\begin{codepy}
import numpy as np

def read_input(s):
    toks = [int(x) for x in s.split()]
    if len(toks) != 5:
        raise ValueError("need m n k seed iters")
    return toks[0], toks[1], toks[2], toks[3], toks[4]

def power_spectral_norm(M, iters=50):
    n = M.shape[1]
    x = np.ones(n) / np.sqrt(n)
    for _ in range(iters):
        y = M.T @ (M @ x)
        nrm = np.linalg.norm(y)
        if nrm == 0:
            return 0.0
        x = y / nrm
    sn = np.sqrt(float(x.T @ (M.T @ (M @ x))))
    return sn

def solve_case(m, n, k, seed, iters):
    np.random.seed(seed)
    A = np.random.randn(m, n)
    AtA = A.T @ A
    w, V = np.linalg.eigh(AtA)
    idx = np.argsort(w)[::-1]
    w, V = w[idx], V[:, idx]
    s = np.sqrt(np.maximum(w, 0.0))
    U = A @ V
    for i in range(len(s)):
        if s[i] > 1e-12:
            U[:, i] /= s[i]
    Ak = (U[:, :k] * s[:k]) @ V[:, :k].T
    R = A - Ak
    # truth from singular values
    wR, _ = np.linalg.eigh(R.T @ R)
    sR = np.sqrt(np.maximum(wR, 0.0))
    truth = float(np.max(sR))
    est = power_spectral_norm(R, iters=iters)
    return {"truth": truth, "est": est, "sigma_k1": float(s[k])}

def validate():
    m, n, k, seed, iters = 30, 25, 4, 0, 60
    out = solve_case(m, n, k, seed, iters)
    assert abs(out["truth"] - out["sigma_k1"]) < 1e-8
    assert abs(out["est"] - out["truth"]) < 1e-5

def main():
    validate()
    m, n, k, seed, iters = 20, 15, 3, 1, 60
    out = solve_case(m, n, k, seed, iters)
    print("truth", round(out["truth"], 8),
          "power", round(out["est"], 8),
          "sigma_k1", round(out["sigma_k1"], 8))

if __name__ == "__main__":
    main()
\end{codepy}
\textbf{SOLUTION B — Library-Based (Direct SVD and Power)}
\begin{codepy}
import numpy as np

def read_input(s):
    toks = [int(x) for x in s.split()]
    if len(toks) != 5:
        raise ValueError("need m n k seed iters")
    return toks[0], toks[1], toks[2], toks[3], toks[4]

def power_spectral_norm(M, iters=50):
    n = M.shape[1]
    x = np.ones(n) / np.sqrt(n)
    for _ in range(iters):
        y = M.T @ (M @ x)
        nrm = np.linalg.norm(y)
        if nrm == 0:
            return 0.0
        x = y / nrm
    return float(np.sqrt(x.T @ (M.T @ (M @ x))))

def solve_case(m, n, k, seed, iters):
    np.random.seed(seed)
    A = np.random.randn(m, n)
    U, s, Vt = np.linalg.svd(A, full_matrices=False)
    Ak = (U[:, :k] * s[:k]) @ Vt[:k, :]
    R = A - Ak
    _, sR, _ = np.linalg.svd(R, full_matrices=False)
    truth = float(sR[0]) if sR.size > 0 else 0.0
    est = power_spectral_norm(R, iters=iters)
    return {"truth": truth, "est": est, "sigma_k1": float(s[k])}

def validate():
    m, n, k, seed, iters = 30, 25, 4, 0, 60
    out = solve_case(m, n, k, seed, iters)
    assert abs(out["truth"] - out["sigma_k1"]) < 1e-8
    assert abs(out["est"] - out["truth"]) < 1e-5

def main():
    validate()
    m, n, k, seed, iters = 20, 15, 3, 1, 60
    out = solve_case(m, n, k, seed, iters)
    print("truth", round(out["truth"], 8),
          "power", round(out["est"], 8),
          "sigma_k1", round(out["sigma_k1"], 8))

if __name__ == "__main__":
    main()
\end{codepy}
\COMPLEXITY{
Direct SVD: $\mathcal{O}(mn\min\{m,n\})$. Power method: each step
$\mathcal{O}(mn)$, total $\mathcal{O}(mn\,\text{iters})$.
}
\FAILMODES{
\begin{bullets}
\item If $k\ge \min\{m,n\}$, residual is zero; handle gracefully.
\item Power method may stagnate if iters too small; increase iters.
\end{bullets}
}
\STABILITY{
\begin{bullets}
\item SVD is stable; power method robust for well-separated top residual singular value.
\item Normalize start vector to avoid overflow.
\end{bullets}
}
\VALIDATION{
\begin{bullets}
\item Compare power estimate to singular value from SVD.
\item Assert equality to $\sigma_{k+1}$ within tolerance.
\end{bullets}
}
\RESULT{
Residual spectral norm equals $\sigma_{k+1}$ and is recovered by power iteration.
}
\EXPLANATION{
Formula 3 states the equality. Power iteration targets the top singular value of the
residual by iterating on $R^{\top}R$.
}
\EXTENSION{
Use subspace iteration to estimate multiple residual singular values simultaneously.
}

\section{Applied Domains — Detailed End-to-End Scenarios}
\DomainPage{Machine Learning}
\SCENARIO{
Dimensionality reduction with PCA via truncated SVD on centered data; measure
reconstruction error and variance captured.
}
\ASSUMPTIONS{
\begin{bullets}
\item Data matrix $X$ is centered by column.
\item Use $k$ principal components to approximate $X$.
\end{bullets}
}
\WHICHFORMULA{
Formulas 2 and 4: $X_k=XV_k V_k^{\top}$ minimizes $\|X-X_k\|_F$ and captures
$\sum_{i=1}^k \sigma_i^2$ variance.
}
\varmapStart
\var{X}{data matrix $n\times d$.}
\var{k}{number of components.}
\var{V_k}{top right singular vectors.}
\var{\sigma_i}{singular values of $X$.}
\varmapEnd
\PIPELINE{
\begin{bullets}
\item Generate synthetic centered data with correlated features.
\item Compute truncated SVD and reconstruct.
\item Report RMSE and fraction of variance captured.
\end{bullets}
}
\textbf{Implementation (From Scratch)}
\begin{codepy}
import numpy as np

def gen_data(n=200, d=5, seed=0):
    np.random.seed(seed)
    Z = np.random.randn(n, 2) @ np.array([[2.0, 0.0], [0.5, 1.5]]).T
    W = np.random.randn(2, d)
    X = Z @ W + 0.1 * np.random.randn(n, d)
    X -= X.mean(axis=0, keepdims=True)
    return X

def pca_trunc(X, k):
    U, s, Vt = np.linalg.svd(X, full_matrices=False)
    Xk = (U[:, :k] * s[:k]) @ Vt[:k, :]
    frac = float(np.sum(s[:k] ** 2) / np.sum(s ** 2))
    rmse = float(np.linalg.norm(X - Xk, "fro") / np.sqrt(X.size))
    return Xk, frac, rmse

def main():
    X = gen_data()
    k = 2
    Xk, frac, rmse = pca_trunc(X, k)
    print("k", k, "frac_var", round(frac, 4), "rmse", round(rmse, 4))

if __name__ == "__main__":
    main()
\end{codepy}
\textbf{Implementation (Library Version)}
\begin{codepy}
import numpy as np
from sklearn.decomposition import TruncatedSVD

def main():
    np.random.seed(0)
    n, d, k = 200, 5, 2
    Z = np.random.randn(n, 2) @ np.array([[2.0, 0.0], [0.5, 1.5]]).T
    W = np.random.randn(2, d)
    X = Z @ W + 0.1 * np.random.randn(n, d)
    X -= X.mean(axis=0, keepdims=True)
    svd = TruncatedSVD(n_components=k, random_state=0)
    Zk = svd.fit_transform(X)
    Xk = Zk @ svd.components_
    s = svd.singular_values_
    frac = float(np.sum(s ** 2) / np.sum(np.linalg.svd(X, 0)[1] ** 2))
    rmse = float(np.linalg.norm(X - Xk, "fro") / np.sqrt(X.size))
    print("k", k, "frac_var", round(frac, 4), "rmse", round(rmse, 4))

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{RMSE of reconstruction and fraction of variance captured.}
\INTERPRET{Top components retain most variance while reducing dimension.}
\NEXTSTEPS{Add whitening, cross-validation for selecting $k$, and outlier handling.}

\DomainPage{Quantitative Finance}
\SCENARIO{
Estimate a low-rank factor model of asset returns using truncated SVD; quantify the
fraction of variance explained by $k$ factors.
}
\ASSUMPTIONS{
\begin{bullets}
\item Returns matrix $R$ is demeaned by asset.
\item Linear factors approximate returns with low-rank structure.
\end{bullets}
}
\WHICHFORMULA{
$R_k=U_k\Sigma_k V_k^{\top}$ minimizes $\|R-R_k\|_F$, variance explained equals
$\sum_{i=1}^k \sigma_i^2/\sum_i \sigma_i^2$.
}
\varmapStart
\var{R}{matrix of returns $n\times d$.}
\var{k}{number of factors.}
\var{\sigma_i}{singular values of $R$.}
\varmapEnd
\PIPELINE{
\begin{bullets}
\item Simulate correlated returns.
\item Compute truncated SVD with $k$ factors.
\item Report variance explained and residual volatility.
\end{bullets}
}
\textbf{Implementation (Full Pipeline)}
\begin{codepy}
import numpy as np

def simulate(n=1000, d=6, k_true=2, seed=0):
    np.random.seed(seed)
    F = np.random.randn(n, k_true)
    B = np.random.randn(k_true, d)
    E = 0.2 * np.random.randn(n, d)
    R = F @ B + E
    R -= R.mean(axis=0, keepdims=True)
    return R

def factor_svd(R, k):
    U, s, Vt = np.linalg.svd(R, full_matrices=False)
    Rk = (U[:, :k] * s[:k]) @ Vt[:k, :]
    frac = float(np.sum(s[:k] ** 2) / np.sum(s ** 2))
    resid = float(np.linalg.norm(R - Rk, "fro") / np.sqrt(R.size))
    return Rk, frac, resid

def main():
    R = simulate()
    k = 2
    Rk, frac, resid = factor_svd(R, k)
    print("k", k, "frac_var", round(frac, 4), "resid", round(resid, 4))

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{Variance explained and residual root-mean-square volatility.}
\INTERPRET{Few factors capture most co-movement; residuals are idiosyncratic.}
\NEXTSTEPS{Stabilize with shrinkage, rolling windows, and factor interpretability.}

\DomainPage{Deep Learning}
\SCENARIO{
Compress a dense layer by low-rank factorization via SVD and measure output distortion
and parameter reduction.
}
\ASSUMPTIONS{
\begin{bullets}
\item Weight matrix $W\in\mathbb{R}^{p\times q}$ is approximated by $W_k$.
\item Evaluate spectral and Frobenius errors to bound output distortion.
\end{bullets}
}
\WHICHFORMULA{
$W_k=U_k\Sigma_k V_k^{\top}$ minimizes both Frobenius and spectral errors as in
Formulas 2 and 3.
}
\PIPELINE{
\begin{bullets}
\item Generate random weight matrix.
\item Compute $W_k$ and measure errors.
\item Compare output distortion on random inputs.
\end{bullets}
}
\textbf{Implementation (End-to-End)}
\begin{codepy}
import numpy as np

def compress(W, k):
    U, s, Vt = np.linalg.svd(W, full_matrices=False)
    Wk = (U[:, :k] * s[:k]) @ Vt[:k, :]
    eF = float(np.linalg.norm(W - Wk, "fro"))
    e2 = float(np.linalg.svd(W - Wk, 0)[1][0]) if k < min(W.shape) else 0.0
    return Wk, eF, e2, s

def main():
    np.random.seed(0)
    p, q, k = 64, 48, 8
    W = np.random.randn(p, q)
    Wk, eF, e2, s = compress(W, k)
    x = np.random.randn(q, 1000)
    y = W @ x
    yk = Wk @ x
    out_err = float(np.linalg.norm(y - yk) / np.linalg.norm(y))
    tail = float(np.sqrt(np.sum(s[k:] ** 2)))
    print("rel_out_err", round(out_err, 4),
          "F_err", round(eF, 4),
          "tail", round(tail, 4),
          "spec_err", round(e2, 4),
          "sigma_k1", round(s[k], 4))

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{Frobenius residual, spectral residual, and relative output error.}
\INTERPRET{Low-rank compression trades accuracy for fewer parameters.}
\NEXTSTEPS{Fine-tune with low-rank factors, or use structured sparsity.}

\DomainPage{Kaggle / Data Analytics}
\SCENARIO{
Perform SVD-based dimensionality reduction and reconstruction on a synthetic dataset;
report correlation retention and reconstruction error.
}
\ASSUMPTIONS{
\begin{bullets}
\item Data are numeric and centered.
\item Pearson correlations compared before and after reconstruction.
\end{bullets}
}
\WHICHFORMULA{
$X_k=U_k\Sigma_k V_k^{\top}$ minimizes $\|X-X_k\|_F$; correlations among reconstructed
features should approximate original ones when $k$ is sufficiently large.
}
\PIPELINE{
\begin{bullets}
\item Create correlated features.
\item Compute $X_k$ for a chosen $k$.
\item Report RMSE and correlation differences.
\end{bullets}
}
\textbf{Implementation (Complete EDA Pipeline)}
\begin{codepy}
import numpy as np
import pandas as pd

def create_df(seed=0, n=300):
    np.random.seed(seed)
    a = np.random.randn(n)
    b = 0.7 * a + 0.3 * np.random.randn(n)
    c = np.random.randn(n) * 2 + 5
    d = c + 0.5 * np.random.randn(n)
    df = pd.DataFrame({"A": a, "B": b, "C": c, "D": d})
    df = df - df.mean()
    return df

def svd_reduce(df, k):
    X = df.values
    U, s, Vt = np.linalg.svd(X, full_matrices=False)
    Xk = (U[:, :k] * s[:k]) @ Vt[:k, :]
    rmse = float(np.linalg.norm(X - Xk, "fro") / np.sqrt(X.size))
    corr_diff = (np.corrcoef(X, rowvar=False)
                 - np.corrcoef(Xk, rowvar=False))
    max_cd = float(np.max(np.abs(corr_diff)))
    return Xk, rmse, max_cd

def main():
    df = create_df()
    k = 2
    Xk, rmse, max_cd = svd_reduce(df, k)
    print("k", k, "rmse", round(rmse, 4), "max_corr_diff", round(max_cd, 3))

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{RMSE and maximum absolute correlation difference.}
\INTERPRET{Top components retain dominant correlations and reduce noise.}
\NEXTSTEPS{Use cross-validation to select $k$ and add whitening or scaling.}

\end{document}
% !TeX program = xelatex
\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{microtype,setspace,amsmath,amssymb,mathtools,amsthm,unicode-math}
\setstretch{1.05}
\setmainfont{Latin Modern Roman}
\setmonofont{Latin Modern Mono}
\setmathfont{Latin Modern Math}

% --- Overflow / line-break safety ---
\allowdisplaybreaks[4]
\setlength{\jot}{7pt}
\setlength{\emergencystretch}{8em}
\sloppy

\usepackage{xcolor,fancyhdr,enumitem,inconsolata,listings}
\pagestyle{fancy}\fancyhf{}\lhead{\nouppercase{\leftmark}}\rhead{\thepage}
\setlength{\headheight}{26pt}
\setlength{\parindent}{0pt}
\setlength{\parskip}{8pt plus 2pt minus 1pt}
\raggedbottom

% --- Breakable math helpers (use these in the body) ---
\newenvironment{BreakableEquation}{\begin{equation}\begin{aligned}}{\end{aligned}\end{equation}}
\newenvironment{BreakableEquation*}{\begin{equation*}\begin{aligned}}{\end{aligned}\end{equation*}}
\newenvironment{tightalign}{\begingroup\small\allowdisplaybreaks\begin{align}}{\end{align}\endgroup}

% ---------- Safety shims ----------
% Robust minted → listings shim (no shell-escape; supports [opts]{language})
\providecommand{\enumlistm}{enumitem}
\newenvironment{minted}[2][]{%
  \lstset{style=code,language=#2,#1}\begin{lstlisting}%
}{\end{lstlisting}}

% Fallback for \inputminted (ignore file; keep build unbroken)
\newcommand{\inputminted}[3][]{\begin{lstlisting}\end{lstlisting}}

% ---------- Bulleted lines (no tables) ----------
\newlist{bullets}{itemize}{1}
\setlist[bullets]{label=--,leftmargin=1.6em,itemsep=2pt,topsep=4pt}

% ---------- Variable mapping (lines, no tables) ----------
\newcommand{\varmapStart}{\textbf{VARIABLE MAPPING:}\par\begin{bullets}}
\newcommand{\var}[2]{\item $#1$ — #2}
\newcommand{\varmapEnd}{\end{bullets}}

% ---------- Glossary item with ELI5 and Pitfall/Example ----------
\newcommand{\glossx}[6]{%
  \textbf{#1}\par
  \begin{bullets}
    \item \textbf{What:} #2
    \item \textbf{Why:} #3
    \item \textbf{How:} #4
    \item \textbf{ELI5:} #5
    \item \textbf{Pitfall/Example:} #6
  \end{bullets}
}

% ---------- Theorem structures ----------
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{claim}[theorem]{Claim}
% ---------- Code blocks ----------
\lstdefinestyle{code}{
  basicstyle=\ttfamily\footnotesize,
  backgroundcolor=\color{black!02},
  frame=single,
  numbers=left, numberstyle=\tiny, numbersep=8pt,
  breaklines=true, breakatwhitespace=true,
  tabsize=4, showstringspaces=false,
  upquote=true, keepspaces=true, columns=fullflexible,
  % Unicode safety: map common symbols so listings never chokes
  literate=
    {–}{{-}}1
    {—}{{-}}1
    {…}{{...}}1
    {≤}{{\ensuremath{\le}}}1
    {≥}{{\ensuremath{\ge}}}1
    {≠}{{\ensuremath{\ne}}}1
    {≈}{{\ensuremath{\approx}}}1
    {±}{{\ensuremath{\pm}}}1
    {→}{{\ensuremath{\to}}}1
    {←}{{\ensuremath{\leftarrow}}}1
    {∞}{{\ensuremath{\infty}}}1
    {√}{{\ensuremath{\sqrt{\ }}}}1
    {×}{{\ensuremath{\times}}}1
    {÷}{{\ensuremath{\div}}}1
}

% Main code environment for all Python blocks
\lstnewenvironment{codepy}[1][]%
  {\lstset{style=code,language=Python,#1}}%
  {}

% Inline code; change delimiters if your snippet contains '!'
\newcommand{\inlinecode}[1]{\lstinline[style=code]!#1!}

% ---------- Line-label macros ----------
\newcommand{\LF}[2]{\par\noindent\textbf{#1:}~#2\par}
\newcommand{\WHAT}[1]{\LF{WHAT}{#1}}
\newcommand{\WHY}[1]{\LF{WHY}{#1}}
\newcommand{\HOW}[1]{\LF{HOW}{#1}}
\newcommand{\ELI}[1]{\LF{ELI5}{#1}}
\newcommand{\SCOPE}[1]{\LF{SCOPE}{#1}}
\newcommand{\CONFUSIONS}[1]{\LF{COMMON CONFUSIONS}{#1}}
\newcommand{\APPLICATIONS}[1]{\LF{APPLICATIONS}{#1}}
\newcommand{\FORMULA}[1]{\LF{FORMULA}{#1}}
\newcommand{\CANONICAL}[1]{\LF{CANONICAL FORM}{#1}}
\newcommand{\PRECONDS}[1]{\LF{PRECONDITIONS}{#1}}
\newcommand{\DERIVATION}[1]{\LF{DERIVATION}{#1}}
\newcommand{\EQUIV}[1]{\LF{EQUIVALENT FORMS}{#1}}
\newcommand{\LIMITS}[1]{\LF{LIMIT CASES}{#1}}
\newcommand{\INPUTS}[1]{\LF{INPUTS}{#1}}
\newcommand{\OUTPUTS}[1]{\LF{OUTPUTS}{#1}}
\newcommand{\RESULT}[1]{\LF{RESULT}{#1}}
\newcommand{\INTUITION}[1]{\LF{INTUITION}{#1}}
\newcommand{\PITFALLS}[1]{\LF{PITFALLS}{#1}}
\newcommand{\MODEL}[1]{\LF{CANONICAL MATH MODEL}{#1}}
\newcommand{\ASSUMPTIONS}[1]{\LF{ASSUMPTIONS}{#1}}
\newcommand{\WHICHFORMULA}[1]{\LF{WHICH FORMULA \& WHY}{#1}}
\newcommand{\GOVERN}[1]{\LF{GOVERNING EQUATION(S)}{#1}}
\newcommand{\UNITCHECK}[1]{\LF{UNIT CHECK}{#1}}
\newcommand{\EDGECASES}[1]{\LF{EDGE CASES}{#1}}
\newcommand{\ALTERNATE}[1]{\LF{ALTERNATE APPROACH (sketch)}{#1}}
\newcommand{\PROBLEM}[1]{\LF{PROBLEM}{#1}}
\newcommand{\API}[1]{\LF{API}{#1}}
\newcommand{\COMPLEXITY}[1]{\LF{COMPLEXITY}{#1}}
\newcommand{\FAILMODES}[1]{\LF{FAILURE MODES}{#1}}
\newcommand{\STABILITY}[1]{\LF{NUMERICAL STABILITY}{#1}}
\newcommand{\VALIDATION}[1]{\LF{VALIDATION}{#1}}
\newcommand{\EXPLANATION}[1]{\LF{EXPLANATION}{#1}}
\newcommand{\SCENARIO}[1]{\LF{SCENARIO}{#1}}
\newcommand{\PIPELINE}[1]{\LF{PIPELINE STEPS}{#1}}
\newcommand{\METRICS}[1]{\LF{METRICS}{#1}}
\newcommand{\INTERPRET}[1]{\LF{INTERPRETATION}{#1}}
\newcommand{\NEXTSTEPS}[1]{\LF{LIMITATIONS \& NEXT STEPS}{#1}}

% ---------- Section formatting ----------
\usepackage{titlesec}
\titleformat{\section}{\Large\bfseries}{\thesection}{0.6em}{}
\titlespacing*{\section}{0pt}{*2}{*1}
\usepackage{etoolbox}
\pretocmd{\section}{\clearpage}{}{}

% ---------- Page helpers ----------
\newcommand{\FormulaPage}[2]{%
  \clearpage
  \section*{Formula #1 — #2}%
  \addcontentsline{toc}{section}{Formula #1 — #2}%
}
\newcommand{\ProblemPage}[2]{%
  \clearpage
  \subsection*{Problem #1: #2}%
  \addcontentsline{toc}{subsection}{Problem #1: #2}%
}
\newcommand{\CodeDemoPage}[1]{%
  \clearpage
  \subsection*{Coding Demo: #1}%
  \addcontentsline{toc}{subsection}{Coding Demo: #1}%
}
\newcommand{\DomainPage}[1]{%
  \clearpage
  \subsection*{#1 (End-to-End)}%
  \addcontentsline{toc}{subsection}{#1 (End-to-End)}%
}

\begin{document}
\title{Comprehensive Study Sheet — Diagonalization and Similarity Transformations}
\date{\today}
\maketitle
\tableofcontents
\clearpage

\section{Concept Overview}
\WHAT{
A square linear operator $A\in\mathbb{F}^{n\times n}$ is diagonalizable
if there exists an invertible $P$ such that $P^{-1}AP=D$, where $D$ is
diagonal. Two matrices $A,B$ are similar if $B=P^{-1}AP$ for some
invertible $P$. The domain is $\mathbb{F}^n$ and the structure is the
vector space with chosen basis; similarity changes the basis.
}
\WHY{
Diagonalization simplifies linear maps to independent scalings along
eigenvectors, enabling closed forms for powers, exponentials, and
functions of matrices. Similarity reveals invariants (spectrum, trace,
determinant) and classifies linear operators up to change of basis.
}
\HOW{
1. Assume $A$ has a basis of eigenvectors. 2. Form $P$ from eigenvectors
and $D$ from eigenvalues. 3. Derive $A=PDP^{-1}$ and consequences
$A^k=PD^kP^{-1}$ and $f(A)=Pf(D)P^{-1}$. 4. Interpret as decoupling
coordinates into invariant one-dimensional subspaces.
}
\ELI{
Diagonalization is like rotating and rescaling your coordinate grid so
that a complicated machine $A$ becomes simple: it just stretches each
axis by a number. Similarity means you looked at the same machine from a
different angle but it is still the same machine.
}
\SCOPE{
Over $\mathbb{C}$ every polynomial splits; diagonalization requires enough
eigenvectors. Over $\mathbb{R}$ some complex spectra prevent real
diagonalization. Non-diagonalizable matrices admit Jordan form. Normal
operators over $\mathbb{C}$ are unitarily diagonalizable.
}
\CONFUSIONS{
Similarity vs. congruence; diagonalization vs. triangularization; having
$n$ eigenvalues (with multiplicity) vs. having $n$ independent
eigenvectors; algebraic vs. geometric multiplicity; orthogonal vs.
general change of basis.
}
\APPLICATIONS{
\begin{bullets}
\item Mathematical foundations: spectral theorem, canonical forms.
\item Computational modeling: fast powers and exponentials $e^{At}$.
\item Physics and engineering: normal modes, decoupled oscillators.
\item Statistics and algorithms: PCA, whitening, covariance diagonalization.
\end{bullets}
}
\textbf{ANALYTIC STRUCTURE.}
Eigen-decomposition is linear-algebraic; diagonalizable matrices act as
linear, decoupled, symmetric scaling in an eigenbasis. Similarity is an
equivalence relation partitioning $\mathbb{F}^{n\times n}$ into classes.
\textbf{CANONICAL LINKS.}
Cayley Hamilton, minimal polynomial, Jordan form, spectral theorem,
Schur decomposition, functional calculus for matrices.
\textbf{PROBLEM-TYPE RECOGNITION HEURISTICS.}
\begin{bullets}
\item Keywords: eigenvalues, eigenvectors, basis, change of basis.
\item Powers or functions of matrices, linear recurrences.
\item Quadratic forms and symmetric matrices hint orthogonal diagonalization.
\item Invariants: trace, determinant, characteristic polynomial.
\end{bullets}
\textbf{SOLUTION STRATEGY BLUEPRINT.}
\begin{bullets}
\item Translate to $A=PDP^{-1}$ or $B=P^{-1}AP$ when possible.
\item Compute eigenvalues then eigenvectors; check independence.
\item Substitute into $A^k$ or $f(A)$ using diagonal formula.
\item Interpret results via invariant subspaces and modes.
\end{bullets}
\textbf{CONCEPTUAL INVARIANTS.}
Similarity preserves spectrum, trace, determinant, rank, minimal
polynomial, nullity of $p(A)$ for any polynomial $p$.
\textbf{EDGE INTUITION.}
If eigenvalues collide, eigenvectors may merge and diagonalization can
fail; near collisions produce ill-conditioned eigenbases. For large $k$,
$A^k$ amplifies eigenvectors with largest $|\lambda|$.

\clearpage
\section{Glossary}
\glossx{Diagonalizable Matrix}
{Matrix similar to a diagonal matrix: $A=PDP^{-1}$.}
{Gives simplest representation for computing powers and functions.}
{Find eigenvalues, assemble a basis of eigenvectors into $P$, set
$D=\operatorname{diag}(\lambda_i)$.}
{Change to special coordinates where the map only stretches.}
{Pitfall: Distinct eigenvalues imply diagonalizable; repeated ones do not
unless the eigenvectors still span.}

\glossx{Similarity Transformation}
{Map $A\mapsto P^{-1}AP$ by invertible $P$.}
{Represents the same linear operator in a different basis; preserves
spectrum and key invariants.}
{Choose a basis change $P$; conjugate $A$ to obtain its representation in
the new basis.}
{Like switching camera angle without changing the object.}
{Example: $A$ and $P^{-1}AP$ have the same trace and determinant.}

\glossx{Minimal Polynomial}
{Monic polynomial $m_A$ of least degree with $m_A(A)=0$.}
{Encodes size of Jordan blocks; simple roots iff $A$ is diagonalizable
(over a splitting field).}
{Compute invariant factors or use primary decomposition from the Jordan
form.}
{It is the shortest instruction list that makes $A$ act like zero.}
{Pitfall: Characteristic polynomial simple implies diagonalizable, but
minimal polynomial is the decisive criterion.}

\glossx{Eigenbasis}
{A basis of eigenvectors of $A$.}
{Allows immediate diagonal action of $A$.}
{Collect $n$ linearly independent eigenvectors when they exist.}
{Axes where $A$ acts by pure scaling.}
{Pitfall: Geometric multiplicities must sum to $n$ to form an eigenbasis.}

\clearpage
\section{Symbol Ledger}
\varmapStart
\var{A\in\mathbb{F}^{n\times n}}{Linear operator in matrix form.}
\var{P\in\mathrm{GL}_n(\mathbb{F})}{Change of basis matrix.}
\var{D}{Diagonal matrix of eigenvalues.}
\var{Q}{Orthogonal or unitary matrix.}
\var{\lambda_i}{Eigenvalues of $A$.}
\var{v_i}{Eigenvectors of $A$.}
\var{\chi_A}{Characteristic polynomial of $A$.}
\var{m_A}{Minimal polynomial of $A$.}
\var{k\in\mathbb{N}}{Power or iteration index.}
\var{f}{Scalar function extended to matrices via functional calculus.}
\var{e^{At}}{Matrix exponential.}
\var{n}{Matrix dimension.}
\var{\mathbb{F}}{Field, usually $\mathbb{R}$ or $\mathbb{C}$.}
\varmapEnd

\clearpage
\section{Formula Canon — One Formula Per Page}
\FormulaPage{1}{Diagonalization by Eigenbasis}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
If $A\in\mathbb{F}^{n\times n}$ has $n$ linearly independent eigenvectors
$v_1,\dots,v_n$ with eigenvalues $\lambda_1,\dots,\lambda_n$, then with
$P=[v_1\ \cdots\ v_n]$ and $D=\operatorname{diag}(\lambda_1,\dots,\lambda_n)$,
$A=PDP^{-1}$.
\WHAT{
This expresses $A$ as a similarity to a diagonal matrix, i.e., action by
independent scalings along eigenvectors.
}
\WHY{
Diagonal form makes computations trivial and reveals invariants, enabling
closed forms for $A^k$ and $e^{At}$ and simplifying analysis.
}
\FORMULA{
\[
A=PDP^{-1},\quad\text{equivalently}\quad P^{-1}AP=D.
\]
}
\CANONICAL{
$A\in\mathbb{F}^{n\times n}$, $P\in\mathrm{GL}_n(\mathbb{F})$, $D$ diagonal.
Holds when the eigenvectors form a basis of $\mathbb{F}^n$.
}
\PRECONDS{
\begin{bullets}
\item Eigenvalues of $A$ lie in $\mathbb{F}$ (spectrum splits in $\mathbb{F}$).
\item Geometric multiplicities sum to $n$ (basis of eigenvectors exists).
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
Eigenvectors corresponding to distinct eigenvalues are linearly independent.
\end{lemma}
\begin{proof}
Suppose $\lambda_1,\dots,\lambda_m$ are distinct and
$\sum_{i=1}^m c_i v_i=0$. Apply $A-\lambda_1 I$ to obtain
$\sum_{i=2}^m c_i(\lambda_i-\lambda_1)v_i=0$. By induction on $m$,
$c_m=0,\dots,c_2=0$, then $c_1=0$, hence all $c_i=0$.\qedhere
\end{proof}
\DERIVATION{
\begin{align*}
\text{Step 1:} &\ \text{Let }Av_i=\lambda_i v_i,\ i=1,\dots,n.\\
\text{Step 2:} &\ \text{Form }P=[v_1\ \cdots\ v_n],\
D=\operatorname{diag}(\lambda_i).\\
\text{Step 3:} &\ AP=[Av_1\ \cdots\ Av_n]=[\lambda_1 v_1\ \cdots\ \lambda_n v_n]
=PD.\\
\text{Step 4:} &\ \text{Since }P\text{ is invertible, }A=PDP^{-1}.\\
\text{Step 5:} &\ \text{Conclude }P^{-1}AP=D\ \text{(diagonal)}.
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Compute eigenvalues and eigenvectors.
\item Check independence; if full, assemble $P$ and $D$.
\item Use similarity to simplify computations.
\item Validate via $AP=PD$.
\end{bullets}
\EQUIV{
\begin{bullets}
\item $A$ diagonalizable $\Leftrightarrow$ $\exists P$ with $P^{-1}AP$ diagonal.
\item $A$ diagonalizable $\Leftrightarrow$ has an eigenbasis.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item If eigenvalues are not in $\mathbb{F}$, diagonalization may require an
extension field.
\item Repeated eigenvalues may lack enough eigenvectors.
\end{bullets}
}
\INPUTS{$A\in\mathbb{F}^{n\times n}$.}
\DERIVATION{
\begin{align*}
\text{Verification: }&AP=PD\ \Longleftrightarrow\ P^{-1}AP=D.\\
\text{Columns: }&Av_i=\lambda_i v_i\ \forall i\ \Longleftrightarrow\
AP=PD.
\end{align*}
}
\RESULT{
Existence of $P$ and $D$ with $A=PDP^{-1}$ precisely when $A$ has an
eigenbasis.
}
\UNITCHECK{
Purely algebraic; invariants preserved under similarity. No physical units.
}
\PITFALLS{
\begin{bullets}
\item Confusing distinct eigenvalues with distinct eigenvectors when multiplicity
collides.
\item Using a singular $P$ (columns not independent).
\end{bullets}
}
\INTUITION{
Pick coordinates following $A$'s own directions so that $A$ scales each
axis independently.
}
\CANONICAL{
\begin{bullets}
\item Universal identity: $AP=PD\iff A=PDP^{-1}$.
\item Abstract: $A$ is conjugate to a diagonal element in its similarity class.
\end{bullets}
}

\FormulaPage{2}{Powers and Functions via Diagonalization}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
If $A=PDP^{-1}$ then for $k\in\mathbb{N}$, $A^k=PD^kP^{-1}$. For a
polynomial $p$, $p(A)=P\,p(D)\,P^{-1}$. If $f$ admits a convergent power
series on the spectrum of $A$, $f(A)=P\,f(D)\,P^{-1}$.
\WHAT{
Computes powers and functions of $A$ by applying them to eigenvalues.
}
\WHY{
Reduces complex matrix computations to scalar operations on the spectrum.
}
\FORMULA{
\[
A^k=P\,\operatorname{diag}(\lambda_1^k,\dots,\lambda_n^k)\,P^{-1},\quad
p(A)=P\,\operatorname{diag}(p(\lambda_i))\,P^{-1}.
\]
}
\CANONICAL{
$A=PDP^{-1}$ with $D$ diagonal; $k\in\mathbb{N}$; $p\in\mathbb{F}[x]$;
$f$ analytic on a neighborhood of $\{\lambda_i\}$.
}
\PRECONDS{
\begin{bullets}
\item $A$ is diagonalizable.
\item For $f(A)$, the power series for $f$ converges at each $\lambda_i$.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
For any polynomial $p$, $p(P^{-1}AP)=P^{-1}p(A)P$.
\end{lemma}
\begin{proof}
Using $p(x)=\sum_{j=0}^m a_j x^j$,
$p(P^{-1}AP)=\sum a_j (P^{-1}AP)^j=\sum a_j P^{-1}A^jP
=P^{-1}(\sum a_j A^j)P=P^{-1}p(A)P$.\qedhere
\end{proof}
\DERIVATION{
\begin{align*}
\text{Step 1:} &\ A=PDP^{-1}\ \Rightarrow\ A^2=PD^2P^{-1},\ \dots,\
A^k=PD^kP^{-1}.\\
\text{Step 2:} &\ \text{Polynomial: }p(A)=\sum a_j A^j
=\sum a_j P D^j P^{-1}=P(\sum a_j D^j)P^{-1}.\\
\text{Step 3:} &\ \sum a_j D^j=\operatorname{diag}(p(\lambda_i)).\\
\text{Step 4:} &\ \text{Analytic }f=\sum a_j x^j\ \Rightarrow\
f(A)=\sum a_j A^j=P f(D) P^{-1}.\\
\text{Step 5:} &\ \text{Thus }\boxed{A^k=PD^kP^{-1},\ p(A)=Pp(D)P^{-1}}.
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Diagonalize $A$.
\item Compute $D^k$ or $f(D)$ by elementwise operations.
\item Reconjugate by $P$ and $P^{-1}$.
\item Validate by direct multiplication for small $k$.
\end{bullets}
\EQUIV{
\begin{bullets}
\item $e^{At}=P\,e^{Dt}\,P^{-1}$ with $e^{Dt}=\operatorname{diag}(e^{\lambda_i t})$.
\item $(A-\mu I)^{-1}=P\,\operatorname{diag}((\lambda_i-\mu)^{-1})\,P^{-1}$ if
$\mu\notin\sigma(A)$.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item If $A$ is not diagonalizable, use Jordan form; formulas include polynomials
in $t$ multiplying exponentials.
\item If $f$ has singularities at eigenvalues, $f(A)$ is undefined.
\end{bullets}
}
\INPUTS{$A=PDP^{-1}$, $k\in\mathbb{N}$, polynomial $p$, or analytic $f$.}
\DERIVATION{
\begin{align*}
\text{Check }k=0:&\ A^0=I=P I P^{-1}.\\
\text{Check }k=1:&\ A=PDP^{-1}.\\
\text{Induction:}&\ A^{k+1}=A^k A=(PD^kP^{-1})(PDP^{-1})=PD^{k+1}P^{-1}.
\end{align*}
}
\RESULT{
Matrix powers and functions reduce to scalar powers and functions on the
spectrum, conjugated back to the original basis.
}
\UNITCHECK{
Algebraic; no units. For stability, note condition number of $P$ affects
numerical accuracy though identities remain exact.
}
\PITFALLS{
\begin{bullets}
\item Forgetting to conjugate back by $P$ and $P^{-1}$.
\item Misordering eigenvalues in $D$ relative to columns of $P$.
\end{bullets}
}
\INTUITION{
Do the hard work in the easy coordinate system (eigenbasis), then return
to the original coordinates.
}
\CANONICAL{
\begin{bullets}
\item Homomorphism: $p(P^{-1}AP)=P^{-1}p(A)P$.
\item Functional calculus on diagonal matrices extended by similarity.
\end{bullets}
}

\FormulaPage{3}{Diagonalizability Criteria via Minimal Polynomial}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Over a field where $\chi_A$ splits, $A$ is diagonalizable if and only if
its minimal polynomial $m_A(x)$ has no repeated roots, equivalently every
eigenvalue has geometric multiplicity equal to its algebraic multiplicity.
\WHAT{
Characterizes when diagonalization is possible using $m_A$.
}
\WHY{
Decides diagonalizability without constructing a full eigenbasis and
connects to Jordan structure.
}
\FORMULA{
\[
A\text{ diagonalizable}\ \Longleftrightarrow\
m_A(x)=\prod_{i=1}^r (x-\lambda_i).
\]
}
\CANONICAL{
$A\in\mathbb{F}^{n\times n}$, $\chi_A$ splits in $\mathbb{F}$, with
distinct eigenvalues $\lambda_1,\dots,\lambda_r$.
}
\PRECONDS{
\begin{bullets}
\item Characteristic polynomial splits into linear factors in $\mathbb{F}$.
\item Minimal polynomial well-defined and monic.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
A Jordan block $J_s(\lambda)$ has minimal polynomial $(x-\lambda)^s$.
\end{lemma}
\begin{proof}
$(J_s(\lambda)-\lambda I)^s=0$ by nilpotency of the superdiagonal Jordan
nilpotent, while $(J_s(\lambda)-\lambda I)^{s-1}\ne 0$. Hence the least
annihilating power is $s$.\qedhere
\end{proof}
\DERIVATION{
\begin{align*}
\text{Step 1:}&\ \text{Jordan form }A=PJP^{-1},\
J=\bigoplus J_{s_i}(\lambda_i).\\
\text{Step 2:}&\ m_A(x)=\mathrm{lcm}_i (x-\lambda_i)^{s_i}.\\
\text{Step 3:}&\ \text{$A$ diagonalizable } \Leftrightarrow s_i=1\ \forall i.\\
\text{Step 4:}&\ \Rightarrow m_A(x)=\prod (x-\lambda_i)\ \text{(simple roots).}\\
\text{Step 5:}&\ \text{Conversely, if $m_A$ has simple roots, all $s_i=1$, so
$J$ is diagonal.}
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Factor $\chi_A$ to check splitting.
\item Compute $m_A$ via invariant factors or by testing powers of
$(A-\lambda I)$ on root subspaces.
\item Conclude diagonalizability equivalence.
\end{bullets}
\EQUIV{
\begin{bullets}
\item For each $\lambda$, $\dim\ker(A-\lambda I)$ equals its algebraic
multiplicity.
\item Primary decomposition: $V=\bigoplus \ker (A-\lambda_i I)^{\alpha_i}$,
with $\alpha_i=1$ for diagonalizable $A$.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item If $\chi_A$ does not split, diagonalization may fail over $\mathbb{F}$
but hold over an extension.
\item Repeated roots in $m_A$ force nontrivial Jordan blocks.
\end{bullets}
}
\INPUTS{$A$, $\chi_A$, $m_A$.}
\DERIVATION{
\begin{align*}
\text{Geometric vs algebraic: }&\ A \text{ diag. }\Rightarrow
\dim\ker(A-\lambda I)=\text{alg mult.}\\
\text{If equalities hold: }&\ \text{each primary component is semisimple,}\\
&\ \text{hence direct sum of eigenspaces, yielding diagonalizability.}
\end{align*}
}
\RESULT{
Diagonalizability iff $m_A$ is square-free (no repeated factor).
}
\UNITCHECK{
Algebraic. No units. Logical equivalences verified via Jordan form facts.
}
\PITFALLS{
\begin{bullets}
\item Checking only distinct eigenvalues of $\chi_A$ is sufficient but not
necessary in defective cases if field issues arise.
\item Confusing $m_A$ with $\chi_A$.
\end{bullets}
}
\INTUITION{
Repeated roots in $m_A$ signal memory of derivative directions inside
eigenspaces, i.e., nondiagonal Jordan chains.
}
\CANONICAL{
\begin{bullets}
\item Semisimplicity: $m_A$ square-free $\iff$ $A$ is semisimple.
\item Primary components are direct sums of eigenspaces.
\end{bullets}
}

\FormulaPage{4}{Similarity Invariants: Spectrum, Trace, Determinant}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
If $B=P^{-1}AP$, then $\chi_B=\chi_A$, $\det B=\det A$, $\operatorname{tr}B=
\operatorname{tr}A$, and $\sigma(B)=\sigma(A)$ with the same algebraic
multiplicities.
\WHAT{
Lists quantities unchanged by similarity (basis change).
}
\WHY{
Identifies classification features and cross-checks transformations.
}
\FORMULA{
\[
\det(\lambda I-B)=\det(\lambda I-P^{-1}AP)=\det(\lambda I-A).
\]
\[
\operatorname{tr}(P^{-1}AP)=\operatorname{tr}(A),\quad
\det(P^{-1}AP)=\det A.
\]
}
\CANONICAL{
$A,B\in\mathbb{F}^{n\times n}$ with $B=P^{-1}AP$, $P$ invertible.
}
\PRECONDS{
\begin{bullets}
\item $P\in\mathrm{GL}_n(\mathbb{F})$.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
$\operatorname{tr}(XY)=\operatorname{tr}(YX)$ and
$\det(XY)=\det X\cdot \det Y$ for square matrices.
\end{lemma}
\begin{proof}
The cyclicity $\operatorname{tr}(XY)=\sum_i (XY)_{ii}
=\sum_{i,j} X_{ij}Y_{ji}=\operatorname{tr}(YX)$. Determinant multiplicativity
is classical from multilinearity and alternating properties of columns or
via LU factorization.\qedhere
\end{proof}
\DERIVATION{
\begin{align*}
\text{Trace: }&\ \operatorname{tr}(P^{-1}AP)=\operatorname{tr}(AP P^{-1})
=\operatorname{tr}(A).\\
\text{Determinant: }&\ \det(P^{-1}AP)=\det(P^{-1})\det(A)\det(P)=\det A.\\
\text{Characteristic: }&\ \det(\lambda I-P^{-1}AP)
=\det(P^{-1}(\lambda I-A)P)=\det(\lambda I-A).
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item To check similarity, compare invariants; if they differ, not similar.
\item Use invariants to verify computations under basis change.
\end{bullets}
\EQUIV{
\begin{bullets}
\item Equality of characteristic polynomials is necessary, not sufficient,
for similarity.
\item Minimal polynomials also invariant under similarity.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item Equal invariants do not guarantee similarity; Jordan structure may differ.
\end{bullets}
}
\INPUTS{$A$, $P$, $B=P^{-1}AP$.}
\DERIVATION{
\begin{align*}
\text{Eigenvalues: }&\ \chi_B=\chi_A\Rightarrow \sigma(B)=\sigma(A).\\
\text{Rank: }&\ \operatorname{rank}(B-\lambda I)=\operatorname{rank}(A-\lambda I).
\end{align*}
}
\RESULT{
Similarity preserves spectrum, trace, determinant, rank, and minimal
polynomial.
}
\UNITCHECK{
Algebraic invariants. No units. Verifications rely on identities above.
}
\PITFALLS{
\begin{bullets}
\item Mistaking equal eigenvalue multisets for similarity equivalence.
\item Forgetting multiplicative effect of $P$ cancels in determinant.
\end{bullets}
}
\INTUITION{
Similarity is just a change of coordinates; intrinsic quantities do not
depend on coordinates.
}
\CANONICAL{
\begin{bullets}
\item Conjugation invariance: $\operatorname{tr}$ and $\det$ are class functions.
\item Characteristic and minimal polynomials are similarity invariants.
\end{bullets}
}

\FormulaPage{5}{Orthogonal and Unitary Diagonalization (Spectral Theorem)}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Over $\mathbb{R}$, a symmetric matrix $A=A^\top$ is orthogonally
diagonalizable: $A=Q\Lambda Q^\top$, $Q^\top Q=I$, $\Lambda$ real diagonal.
Over $\mathbb{C}$, a normal matrix $A$ with $AA^\ast=A^\ast A$ is unitarily
diagonalizable: $A=U\Lambda U^\ast$, $U^\ast U=I$.
\WHAT{
Identifies classes of matrices diagonalizable by isometries.
}
\WHY{
Orthogonal or unitary $P$ are perfectly conditioned; numerical and
geometric stability are superior.
}
\FORMULA{
\[
A=U\Lambda U^\ast,\quad \Lambda=\operatorname{diag}(\lambda_i),\quad
\lambda_i\in\mathbb{R}\ \text{if }A=A^\top.
\]
}
\CANONICAL{
Real symmetric or complex normal matrices; $U$ unitary (or $Q$ orthogonal).
}
\PRECONDS{
\begin{bullets}
\item $A$ symmetric (real case) or normal (complex case).
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}[Schur]
Every complex matrix is unitarily similar to upper triangular $T$.
\end{lemma}
\begin{proof}
By induction on $n$ using existence of an eigenvalue and Gram Schmidt to
extend an eigenvector to an orthonormal basis; the matrix in that basis
is block upper triangular. Iterating establishes triangular $T$.\qedhere
\end{proof}
\begin{lemma}
If $T$ is upper triangular and normal, then $T$ is diagonal.
\end{lemma}
\begin{proof}
Write $T=(t_{ij})$. Normality gives $\sum_k t_{ik}\overline{t_{jk}}
=\sum_k \overline{t_{ki}} t_{kj}$. For $i<j$, upper triangularity forces
$t_{ki}=0$ when $k<i$, hence comparing off diagonal entries yields
$t_{ij}=0$. Inductively all superdiagonal entries vanish, so $T$ is
diagonal.\qedhere
\end{proof}
\DERIVATION{
\begin{align*}
\text{Step 1:}&\ \text{By Schur, }A=UTU^\ast\text{ with $T$ upper triangular.}\\
\text{Step 2:}&\ \text{If $A$ is normal, so is $T$, and thus $T$ diagonal.}\\
\text{Step 3:}&\ \Rightarrow A=U\Lambda U^\ast.\\
\text{Step 4:}&\ \text{For real symmetric, view as complex normal, then
show eigenvalues are real and }U\text{ can be chosen real }(Q).
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Verify normality or symmetry.
\item Compute an orthonormal eigenbasis via unitary algorithms.
\item Form $U$ and $\Lambda$ and apply formulas from Formula 2.
\end{bullets}
\EQUIV{
\begin{bullets}
\item $A$ normal $\Leftrightarrow$ unitarily diagonalizable.
\item $A$ symmetric $\Leftrightarrow$ orthogonally diagonalizable (real case).
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item Non normal matrices may be diagonalizable but not unitarily so.
\item Real nonsymmetric with complex conjugate eigenpairs not diagonally real.
\end{bullets}
}
\INPUTS{$A$ symmetric or normal.}
\DERIVATION{
\begin{align*}
\text{Check: }&\ U^\ast A U=\Lambda\ \Rightarrow\
A=U\Lambda U^\ast,\ U^\ast U=I.\\
\text{Orthogonality: }&\ Q^\top A Q=\Lambda\ \Rightarrow\
A=Q\Lambda Q^\top.
\end{align*}
}
\RESULT{
Existence of an orthonormal eigenbasis and stable diagonalization by an
isometry.
}
\UNITCHECK{
Isometric similarity preserves norms and condition numbers ideally for
eigen decompositions.
}
\PITFALLS{
\begin{bullets}
\item Confusing normality with diagonalizable over $\mathbb{C}$; not all
diagonalizable are normal.
\item Using non orthonormal eigenvectors yields non unitary $U$.
\end{bullets}
}
\INTUITION{
Symmetric or normal matrices act like pure stretchings along orthogonal
axes, with no shearing.
}
\CANONICAL{
\begin{bullets}
\item $A$ normal $\iff$ $A$ unitarily diagonalizable.
\item $A$ symmetric $\iff$ $A$ orthogonally diagonalizable.
\end{bullets}
}

\section{10 Exhaustive Problems and Solutions}
\ProblemPage{1}{Diagonalize and Compute a Power}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Given $A=\begin{pmatrix}1&1&0\\0&2&0\\0&0&3\end{pmatrix}$, diagonalize
$A$ and compute $A^5$.
\PROBLEM{
Find $P,D$ with $A=PDP^{-1}$ and evaluate $A^5$ using Formula 2.
}
\MODEL{
\[
A=PDP^{-1},\quad A^k=PD^kP^{-1}.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Work over $\mathbb{R}$; $\chi_A(x)=(x-1)(x-2)(x-3)$ split.
\item Distinct eigenvalues imply diagonalizable.
\end{bullets}
}
\varmapStart
\var{A}{Given matrix.}
\var{P}{Eigenvector matrix.}
\var{D}{Diagonal eigenvalue matrix.}
\var{k}{Power index, here $k=5$.}
\varmapEnd
\WHICHFORMULA{
Formula 1 (diagonalization) and Formula 2 (powers via diagonalization).
}
\GOVERN{
\[
AP=PD,\quad A^5=PD^5P^{-1}.
\]
}
\INPUTS{$A$, $k=5$.}
\DERIVATION{
\begin{align*}
\text{Eigenvalues: }&\ \lambda_1=1,\ \lambda_2=2,\ \lambda_3=3.\\
\text{Eigenvectors: }&\ (A-I)v=0\Rightarrow v_1=(1,0,0)^\top.\\
&\ (A-2I)v=0\Rightarrow v_2=(1,1,0)^\top\ \text{(any nonzero in span)}.\\
&\ (A-3I)v=0\Rightarrow v_3=(0,0,1)^\top.\\
P&=\begin{pmatrix}1&1&0\\0&1&0\\0&0&1\end{pmatrix},\
D=\operatorname{diag}(1,2,3).\\
P^{-1}&=\begin{pmatrix}1&-1&0\\0&1&0\\0&0&1\end{pmatrix}.\\
D^5&=\operatorname{diag}(1,32,243).\\
A^5&=P D^5 P^{-1}
=\begin{pmatrix}1&1&0\\0&1&0\\0&0&1\end{pmatrix}
\begin{pmatrix}1&0&0\\0&32&0\\0&0&243\end{pmatrix}
\begin{pmatrix}1&-1&0\\0&1&0\\0&0&1\end{pmatrix}\\
&=\begin{pmatrix}1&32&0\\0&32&0\\0&0&243\end{pmatrix}
\begin{pmatrix}1&-1&0\\0&1&0\\0&0&1\end{pmatrix}
=\begin{pmatrix}1&31&0\\0&32&0\\0&0&243\end{pmatrix}.
\end{align*}
}
\RESULT{
$P=\begin{pmatrix}1&1&0\\0&1&0\\0&0&1\end{pmatrix}$,
$D=\operatorname{diag}(1,2,3)$, and
$A^5=\begin{pmatrix}1&31&0\\0&32&0\\0&0&243\end{pmatrix}$.
}
\UNITCHECK{
Algebraic; sizes consistent; triangular structure preserved by powers.
}
\EDGECASES{
\begin{bullets}
\item If two eigenvalues collided, eigenvectors might not remain independent.
\end{bullets}
}
\ALTERNATE{
Direct multiplication using binomial-like identities for upper triangular
matrices yields the same result, but diagonalization is cleaner.
}
\VALIDATION{
\begin{bullets}
\item Verify $AP=PD$ and compute $A^5$ directly to match.
\end{bullets}
}
\INTUITION{
Distinct eigenvalues guarantee decoupled directions; off diagonal coupling
does not survive in eigenbasis.
}
\CANONICAL{
\begin{bullets}
\item $A$ similar to $D$, $A^k$ similar to $D^k$.
\end{bullets}
}

\ProblemPage{2}{Parameter Criterion for Diagonalizability}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
For $A_t=\begin{pmatrix}1&t\\0&1\end{pmatrix}$, determine for which $t$
it is diagonalizable and find $m_{A_t}$.
\PROBLEM{
Classify diagonalizability in terms of $t$ using minimal polynomial.
}
\MODEL{
\[
A_t=I+tE,\quad E=\begin{pmatrix}0&1\\0&0\end{pmatrix},\quad E^2=0.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Real or complex field.
\end{bullets}
}
\varmapStart
\var{t}{Parameter in $\mathbb{F}$.}
\var{m_{A_t}}{Minimal polynomial.}
\varmapEnd
\WHICHFORMULA{
Formula 3: diagonalizable iff $m_{A_t}$ is square free.
}
\GOVERN{
\[
(A_t-I)^2=t^2 E^2=0,\quad (A_t-I)=tE.
\]
}
\INPUTS{$t\in\mathbb{F}$.}
\DERIVATION{
\begin{align*}
\chi_{A_t}(x)&=(x-1)^2.\\
(A_t-I)&=tE\ \Rightarrow\ (A_t-I)^2=0.\\
\text{If }t=0:&\ A_0=I,\ m_{A_0}(x)=(x-1).\\
\text{If }t\ne 0:&\ (A_t-I)\ne 0\ \text{but nilpotent of index }2,\\
&\ \Rightarrow m_{A_t}(x)=(x-1)^2.
\end{align*}
}
\RESULT{
$A_t$ diagonalizable iff $t=0$. Minimal polynomial is $(x-1)$ if $t=0$
and $(x-1)^2$ if $t\ne 0$.
}
\UNITCHECK{
Algebraic identities consistent; degrees match size $2$.
}
\EDGECASES{
\begin{bullets}
\item $t=0$ is the only diagonalizable case since eigenvalue is repeated.
\end{bullets}
}
\ALTERNATE{
Exhibit eigenvectors: for $t\ne 0$, eigenspace is one dimensional, hence
not diagonalizable.
}
\VALIDATION{
\begin{bullets}
\item Compute $\dim\ker(A_t-I)$: equals $1$ for $t\ne 0$, equals $2$ for $t=0$.
\end{bullets}
}
\INTUITION{
A nonzero superdiagonal entry creates a Jordan chain preventing diagonal
decoupling.
}
\CANONICAL{
\begin{bullets}
\item Minimal polynomial detects Jordan block size.
\end{bullets}
}

\ProblemPage{3}{Matrix Exponential via Diagonalization}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Let $A=\begin{pmatrix}4&1\\1&4\end{pmatrix}$. Compute $e^{At}$.
\PROBLEM{
Diagonalize $A$ orthogonally and use Formula 2 to compute $e^{At}$.
}
\MODEL{
\[
A=Q\Lambda Q^\top,\quad e^{At}=Q e^{\Lambda t} Q^\top.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $A$ symmetric, so orthogonally diagonalizable.
\end{bullets}
}
\varmapStart
\var{Q}{Orthogonal eigenvector matrix.}
\var{\Lambda}{Diagonal eigenvalue matrix.}
\var{t}{Real time parameter.}
\varmapEnd
\WHICHFORMULA{
Formula 5 (spectral theorem) and Formula 2 (functions via diagonalization).
}
\GOVERN{
\[
e^{At}=Q\,\operatorname{diag}(e^{\lambda_1 t},e^{\lambda_2 t})\,Q^\top.
\]
}
\INPUTS{$A$, $t\in\mathbb{R}$.}
\DERIVATION{
\begin{align*}
\chi_A(\lambda)&=(\lambda-5)(\lambda-3).\\
\lambda_1&=5,\ \lambda_2=3.\\
\text{Eigenvectors: }&\ (A-5I)v=0\Rightarrow v_1=(1,-1)^\top.\\
&\ (A-3I)v=0\Rightarrow v_2=(1,1)^\top.\\
Q&=\frac{1}{\sqrt{2}}\begin{pmatrix}1&1\\-1&1\end{pmatrix},\
\Lambda=\operatorname{diag}(5,3).\\
e^{\Lambda t}&=\operatorname{diag}(e^{5t},e^{3t}).\\
e^{At}&=Q e^{\Lambda t} Q^\top\\
&=\frac{1}{2}\begin{pmatrix}1&1\\-1&1\end{pmatrix}
\begin{pmatrix}e^{5t}&0\\0&e^{3t}\end{pmatrix}
\begin{pmatrix}1&-1\\1&1\end{pmatrix}\\
&=\frac{1}{2}\begin{pmatrix}
e^{5t}+e^{3t} & -e^{5t}+e^{3t}\\
-e^{5t}+e^{3t} & e^{5t}+e^{3t}
\end{pmatrix}.
\end{align*}
}
\RESULT{
$e^{At}=\dfrac{1}{2}\begin{pmatrix}
e^{5t}+e^{3t} & -e^{5t}+e^{3t}\\
-e^{5t}+e^{3t} & e^{5t}+e^{3t}
\end{pmatrix}$.
}
\UNITCHECK{
At $t=0$, $e^{A0}=I$; derivative at $0$ equals $A$; symmetry preserved.
}
\EDGECASES{
\begin{bullets}
\item As $t\to\infty$, leading term behaves like $e^{5t}$ times projector onto
$v_1$.
\end{bullets}
}
\ALTERNATE{
Direct series is cumbersome; diagonalization yields a closed form.
}
\VALIDATION{
\begin{bullets}
\item Check $e^{At} e^{As}=e^{A(t+s)}$ using diagonal formula.
\end{bullets}
}
\INTUITION{
Decompose into orthogonal modes that grow at rates $e^{\lambda_i t}$.
}
\CANONICAL{
\begin{bullets}
\item Orthogonal diagonalization with function application modewise.
\end{bullets}
}

\ProblemPage{4}{Nilpotent and Diagonalizable Implies Zero}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Prove: If $A$ is diagonalizable and nilpotent, then $A=0$.
\PROBLEM{
Use diagonalization to show all eigenvalues are zero and conclude $A=0$.
}
\MODEL{
\[
A=PDP^{-1},\ D=\operatorname{diag}(\lambda_i),\ A^k=0.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Nilpotent: $\exists k$ with $A^k=0$.
\item Diagonalizable over $\mathbb{F}$.
\end{bullets}
}
\varmapStart
\var{\lambda_i}{Eigenvalues of $A$.}
\var{k}{Nilpotency index.}
\varmapEnd
\WHICHFORMULA{
Formula 2 for powers via diagonalization.
}
\GOVERN{
\[
0=A^k=PD^kP^{-1}\ \Rightarrow\ D^k=0.
\]
}
\INPUTS{$A$, $k$.}
\DERIVATION{
\begin{align*}
A^k=0&\ \Rightarrow\ PD^kP^{-1}=0\ \Rightarrow\ D^k=0.\\
D^k=0&\ \Rightarrow\ \lambda_i^k=0\ \forall i\ \Rightarrow\ \lambda_i=0.\\
A&=PDP^{-1}=0.
\end{align*}
}
\RESULT{
$A=0$.
}
\UNITCHECK{
Algebraic identities; zero spectrum implies zero diagonal and hence zero
matrix under similarity.
}
\EDGECASES{
\begin{bullets}
\item Non diagonalizable nilpotent matrices are nonzero Jordan nilpotents.
\end{bullets}
}
\ALTERNATE{
Via minimal polynomial: nilpotent implies $m_A(x)=x^s$; square free only
if $s=1$, hence $A=0$.
}
\VALIDATION{
\begin{bullets}
\item Contrapositive: if $A\ne 0$, some $\lambda_i\ne 0$, contradicting
nilpotency.
\end{bullets}
}
\INTUITION{
A nonzero diagonalizable operator would stretch some direction, which
cannot vanish under iteration.
}
\CANONICAL{
\begin{bullets}
\item Semisimple nilpotent is zero.
\end{bullets}
}

\ProblemPage{5}{Commutant of a Simple Spectrum Matrix}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Let $A$ have distinct eigenvalues. Prove any $X$ commuting with $A$
satisfies $X=p(A)$ for some polynomial $p$ of degree at most $n-1$.
\PROBLEM{
Use eigenbasis to show $X$ is diagonal in that basis and interpolate.
}
\MODEL{
\[
A=PDP^{-1},\ D=\operatorname{diag}(\lambda_i),\ XA=AX.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $A$ is diagonalizable with simple spectrum.
\end{bullets}
}
\varmapStart
\var{X}{Matrix commuting with $A$.}
\var{p}{Interpolating polynomial.}
\varmapEnd
\WHICHFORMULA{
Formula 1 and 2; Lagrange interpolation on distinct nodes $\lambda_i$.
}
\GOVERN{
\[
XA=AX\ \Leftrightarrow\ (P^{-1}XP) D = D (P^{-1}XP).
\]
}
\INPUTS{$A$ with distinct $\lambda_i$, $X$ with $XA=AX$.}
\DERIVATION{
\begin{align*}
\tilde X&=P^{-1}XP,\ D=\operatorname{diag}(\lambda_i).\\
\tilde X D&=D \tilde X\ \Rightarrow\ (\tilde X)_{ij}(\lambda_j-\lambda_i)=0.\\
\text{Distinct }\lambda:&\ i\ne j\ \Rightarrow\ (\tilde X)_{ij}=0.\\
\tilde X&=\operatorname{diag}(\mu_i).\\
\text{Choose }p:&\ p(\lambda_i)=\mu_i\ \text{via Lagrange basis}.\\
p(D)&=\operatorname{diag}(p(\lambda_i))=\tilde X.\\
X&=P \tilde X P^{-1}=P p(D) P^{-1}=p(A).
\end{align*}
}
\RESULT{
$X=p(A)$ with $\deg p\le n-1$ (Lagrange polynomial on $n$ points).
}
\UNITCHECK{
Algebraic; basis change invariance respected.
}
\EDGECASES{
\begin{bullets}
\item If eigenvalues are repeated, the commutant is larger than polynomials.
\end{bullets}
}
\ALTERNATE{
Use spectral projectors $E_i=\prod_{j\ne i}\frac{A-\lambda_j I}{\lambda_i-
\lambda_j}$ and write $X=\sum \mu_i E_i$.
}
\VALIDATION{
\begin{bullets}
\item Check $[p(A),A]=0$ always; uniqueness from values at $\lambda_i$.
\end{bullets}
}
\INTUITION{
In eigenbasis, commuting means no mixing of modes; action is scalar on
each eigenline, hence a function of $A$.
}
\CANONICAL{
\begin{bullets}
\item Commutant equals polynomials in $A$ for simple spectrum.
\end{bullets}
}

\ProblemPage{6}{Narrative: Alice Decouples a Recurrence}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Alice studies $x_{k+1}=A x_k$ with $A=\begin{pmatrix}3&1\\0&2\end{pmatrix}$,
$x_0=(1,2)^\top$. She diagonalizes to obtain a closed form for $x_k$.
\PROBLEM{
Find $P,D$ and compute $x_k=A^k x_0$.
}
\MODEL{
\[
x_k=A^k x_0=PD^kP^{-1}x_0.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Real field; $A$ has eigenvalues $3,2$ and is diagonalizable.
\end{bullets}
}
\varmapStart
\var{x_k}{State at step $k$.}
\var{A}{Transition matrix.}
\var{P,D}{Diagonalization pair.}
\var{k}{Time index.}
\varmapEnd
\WHICHFORMULA{
Formula 2 for powers via diagonalization.
}
\GOVERN{
\[
A^k=PD^kP^{-1},\quad D=\operatorname{diag}(3,2).
\]
}
\INPUTS{$A$, $x_0=(1,2)^\top$, $k\in\mathbb{N}$.}
\DERIVATION{
\begin{align*}
\lambda_1&=3,\ v_1=(1,0)^\top.\\
\lambda_2&=2,\ v_2=(1, -1)^\top\ \text{(solve }(A-2I)v=0).\\
P&=\begin{pmatrix}1&1\\0&-1\end{pmatrix},\
P^{-1}=\begin{pmatrix}1&1\\0&-1\end{pmatrix}.\\
D&=\operatorname{diag}(3,2).\\
x_k&=P D^k P^{-1} x_0.\\
P^{-1}x_0&=\begin{pmatrix}1&1\\0&-1\end{pmatrix}\begin{pmatrix}1\\2\end{pmatrix}
=\begin{pmatrix}3\\-2\end{pmatrix}.\\
D^k P^{-1}x_0&=\begin{pmatrix}3^k&0\\0&2^k\end{pmatrix}
\begin{pmatrix}3\\-2\end{pmatrix}=\begin{pmatrix}3^{k+1}\\-2^{k+1}\end{pmatrix}.\\
x_k&=P\begin{pmatrix}3^{k+1}\\-2^{k+1}\end{pmatrix}
=\begin{pmatrix}1&1\\0&-1\end{pmatrix}
\begin{pmatrix}3^{k+1}\\-2^{k+1}\end{pmatrix}
=\begin{pmatrix}3^{k+1}-2^{k+1}\\2^{k}\end{pmatrix}.
\end{align*}
}
\RESULT{
$x_k=\big(3^{k+1}-2^{k+1},\ 2^{k}\big)^\top$.
}
\UNITCHECK{
Dimensions consistent; at $k=0$, $x_0=(1,2)^\top$ recovered.
}
\EDGECASES{
\begin{bullets}
\item As $k\to\infty$, the first component dominates with $3^{k+1}$ growth.
\end{bullets}
}
\ALTERNATE{
Solve recurrence by induction; diagonalization is faster and systematic.
}
\VALIDATION{
\begin{bullets}
\item Check $x_{k+1}=A x_k$ holds for the closed form.
\end{bullets}
}
\INTUITION{
Eigenmodes evolve independently with rates $3$ and $2$.
}
\CANONICAL{
\begin{bullets}
\item Linear recurrence solved via spectral decomposition.
\end{bullets}
}

\ProblemPage{7}{Narrative: Bob Unmixes a Basis Change}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Bob observes $B=\begin{pmatrix}2&0\\0&5\end{pmatrix}$ but knows the
original operator is $A=PBP^{-1}$ with
$P=\begin{pmatrix}1&1\\1&0\end{pmatrix}$. Recover $A$ and verify trace and
determinant invariance.
\PROBLEM{
Compute $A$ and confirm $\operatorname{tr}A=\operatorname{tr}B$,
$\det A=\det B$.
}
\MODEL{
\[
A=PBP^{-1},\quad \operatorname{tr}(P^{-1}AP)=\operatorname{tr}A,\quad
\det(P^{-1}AP)=\det A.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $P$ invertible.
\end{bullets}
}
\varmapStart
\var{A,B}{Similar matrices.}
\var{P}{Change of basis.}
\varmapEnd
\WHICHFORMULA{
Formula 4 invariants under similarity.
}
\GOVERN{
\[
A=PBP^{-1}.
\]
}
\INPUTS{$B$, $P$.}
\DERIVATION{
\begin{align*}
P^{-1}&=\begin{pmatrix}0&1\\1&-1\end{pmatrix}.\\
A&=PBP^{-1}
=\begin{pmatrix}1&1\\1&0\end{pmatrix}
\begin{pmatrix}2&0\\0&5\end{pmatrix}
\begin{pmatrix}0&1\\1&-1\end{pmatrix}\\
&=\begin{pmatrix}1&1\\1&0\end{pmatrix}
\begin{pmatrix}0&2\\5&-5\end{pmatrix}
=\begin{pmatrix}5&-3\\0&2\end{pmatrix}.\\
\operatorname{tr}A&=5+2=7=\operatorname{tr}B.\\
\det A&=5\cdot 2-0=10=\det B.
\end{align*}
}
\RESULT{
$A=\begin{pmatrix}5&-3\\0&2\end{pmatrix}$ with matching trace $7$ and
determinant $10$ to $B$.
}
\UNITCHECK{
Similarity preserves invariants; sizes consistent.
}
\EDGECASES{
\begin{bullets}
\item Non invertible $P$ invalidates similarity; here $\det P=-1\ne 0$.
\end{bullets}
}
\ALTERNATE{
Compute eigenvalues of both $A$ and $B$; both are $\{2,5\}$.
}
\VALIDATION{
\begin{bullets}
\item Check $P^{-1}AP=B$ numerically to confirm inversion.
\end{bullets}
}
\INTUITION{
Same operator viewed in two different coordinate systems.
}
\CANONICAL{
\begin{bullets}
\item Conjugation keeps intrinsic quantities unchanged.
\end{bullets}
}

\ProblemPage{8}{Expectation Puzzle: Random Diagonalizable Choice}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Flip a fair coin. Heads: choose $A_1$ with eigenvalues $(2,3)$. Tails:
choose $A_2$ with eigenvalues $(1,4)$. Both are diagonalizable. Compute
$\mathbb{E}[\operatorname{tr}(A^n)]$.
\PROBLEM{
Use Formula 2 to evaluate trace of powers via eigenvalues.
}
\MODEL{
\[
\operatorname{tr}(A^n)=\sum_i \lambda_i^n.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $A_1,A_2$ diagonalizable; trace of power equals sum of eigenvalues
raised to $n$.
\end{bullets}
}
\varmapStart
\var{n}{Positive integer.}
\var{\lambda_i}{Eigenvalues.}
\varmapEnd
\WHICHFORMULA{
Formula 2 with $k=n$ and trace invariance under similarity.
}
\GOVERN{
\[
\mathbb{E}[\operatorname{tr}(A^n)]=\tfrac{1}{2}
\big((2^n+3^n)+(1^n+4^n)\big).
\]
}
\INPUTS{$n\in\mathbb{N}$.}
\DERIVATION{
\begin{align*}
\operatorname{tr}(A_1^n)&=2^n+3^n,\quad
\operatorname{tr}(A_2^n)=1^n+4^n.\\
\mathbb{E}[\operatorname{tr}(A^n)]&=\tfrac{1}{2}(2^n+3^n+1+4^n).
\end{align*}
}
\RESULT{
$\mathbb{E}[\operatorname{tr}(A^n)]=\dfrac{2^n+3^n+4^n+1}{2}$.
}
\UNITCHECK{
For $n=1$, expected trace equals $\tfrac{(2+3)+(1+4)}{2}=5$.
}
\EDGECASES{
\begin{bullets}
\item $n=0$: define $A^0=I$, trace equals $2$ deterministically.
\end{bullets}
}
\ALTERNATE{
Direct computation with diagonal representatives $D_1,D_2$ yields the
same value.
}
\VALIDATION{
\begin{bullets}
\item Verify numerically with explicit diagonal matrices.
\end{bullets}
}
\INTUITION{
Trace of power is a symmetric sum in eigenvalues; expectation averages
modewise growth rates.
}
\CANONICAL{
\begin{bullets}
\item Trace of power equals sum of powers of eigenvalues for diagonalizable $A$.
\end{bullets}
}

\ProblemPage{9}{Combo with Differential Equations: Solve x' = A x}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Solve $x'(t)=Ax(t)$, $x(0)=x_0$, with
$A=\begin{pmatrix}0&-1\\1&0\end{pmatrix}$.
\PROBLEM{
Use complex diagonalization to compute $e^{At}$ and $x(t)=e^{At}x_0$.
}
\MODEL{
\[
A=PDP^{-1},\ D=\operatorname{diag}(i,-i),\ e^{At}=P e^{Dt} P^{-1}.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Work over $\mathbb{C}$; solution over $\mathbb{R}$ is real part.
\end{bullets}
}
\varmapStart
\var{x(t)}{State trajectory.}
\var{A}{Rotation generator.}
\var{P,D}{Complex diagonalization pair.}
\varmapEnd
\WHICHFORMULA{
Formula 2 for exponentials and Formula 1 for diagonalization.
}
\GOVERN{
\[
e^{At}=P\operatorname{diag}(e^{it},e^{-it})P^{-1}
=\begin{pmatrix}\cos t&-\sin t\\ \sin t&\cos t\end{pmatrix}.
\]
}
\INPUTS{$x_0\in\mathbb{R}^2$, $t\in\mathbb{R}$.}
\DERIVATION{
\begin{align*}
\chi_A(\lambda)&=\lambda^2+1=0\Rightarrow \lambda=\pm i.\\
P&=\begin{pmatrix}1&1\\ -i&i\end{pmatrix},\
D=\operatorname{diag}(i,-i).\\
e^{Dt}&=\operatorname{diag}(e^{it},e^{-it}).\\
e^{At}&=P e^{Dt} P^{-1}
=\begin{pmatrix}\cos t&-\sin t\\ \sin t&\cos t\end{pmatrix}.\\
x(t)&=e^{At}x_0.
\end{align*}
}
\RESULT{
$x(t)=\begin{pmatrix}\cos t&-\sin t\\ \sin t&\cos t\end{pmatrix}x_0$.
}
\UNITCHECK{
At $t=0$, identity; derivative at $0$ equals $A$; orthogonal matrix
preserves norm.
}
\EDGECASES{
\begin{bullets}
\item Large $t$ just rotates; bounded solution.
\end{bullets}
}
\ALTERNATE{
Compute series for cosine and sine and assemble the exponential block.
}
\VALIDATION{
\begin{bullets}
\item Verify $d(e^{At})/dt=A e^{At}$ and $e^{A(t+s)}=e^{At}e^{As}$.
\end{bullets}
}
\INTUITION{
Rotation decouples in complex eigenmodes with phases $e^{\pm it}$.
}
\CANONICAL{
\begin{bullets}
\item Exponentials reduce to modewise exponentials via diagonalization.
\end{bullets}
}

\ProblemPage{10}{Combo with Quadratic Forms: Principal Axes}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Maximize $q(x)=x^\top A x$ subject to $\|x\|=1$ for
$A=\begin{pmatrix}3&1\\1&2\end{pmatrix}$.
\PROBLEM{
Use orthogonal diagonalization to show maximum equals largest eigenvalue
and find the optimizer.
}
\MODEL{
\[
A=Q\Lambda Q^\top,\ q(x)=\sum \lambda_i y_i^2,\ y=Q^\top x,\ \|y\|=1.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $A$ symmetric.
\end{bullets}
}
\varmapStart
\var{\lambda_{\max}}{Largest eigenvalue.}
\var{Q}{Orthogonal eigenbasis.}
\var{y}{Coordinates in eigenbasis.}
\varmapEnd
\WHICHFORMULA{
Formula 5 (spectral theorem).
}
\GOVERN{
\[
\max_{\|x\|=1} x^\top A x=\lambda_{\max}.
\]
}
\INPUTS{$A$.}
\DERIVATION{
\begin{align*}
\chi_A(\lambda)&=\lambda^2-5\lambda+5=0.\\
\lambda_{\pm}&=\frac{5\pm\sqrt{5}}{2}.\\
\text{Max }&=\lambda_+=\frac{5+\sqrt{5}}{2}.\\
\text{Eigenvector: }&\ (A-\lambda_+ I)v=0
\Rightarrow v\propto(\lambda_+-2,1)^\top.\\
\text{Normalize }&\ x^\ast=v/\|v\|.
\end{align*}
}
\RESULT{
Maximum is $\lambda_+=(5+\sqrt{5})/2$ attained at the unit eigenvector of
$\lambda_+$.
}
\UNITCHECK{
Rayleigh quotient bounded between min and max eigenvalues.
}
\EDGECASES{
\begin{bullets}
\item If eigenvalues equal, any unit vector maximizes.
\end{bullets}
}
\ALTERNATE{
Lagrange multipliers give $Ax=\lambda x$ with $\lambda$ as Rayleigh
quotient extrema, equivalent to diagonalization.
}
\VALIDATION{
\begin{bullets}
\item Compute $q(x^\ast)=\lambda_+$ and verify $q(Qe_i)=\lambda_i$.
\end{bullets}
}
\INTUITION{
Rotate to principal axes; the quadratic form is a weighted sum of squares
peaking along the largest weight.
}
\CANONICAL{
\begin{bullets}
\item Principal axes theorem via orthogonal diagonalization.
\end{bullets}
}

\section{Coding Demonstrations}
\CodeDemoPage{2x2 Diagonalization: Powers and Exponentials}
\PROBLEM{
Implement diagonalization for $2\times 2$ matrices with analytic
eigenpairs and compute $A^k$ and $e^{At}$; verify against direct
computation.
}
\API{
\begin{bullets}
\item \inlinecode{def read_input(s) -> dict}
\item \inlinecode{def solve_case(obj) -> dict}
\item \inlinecode{def validate() -> None}
\item \inlinecode{def main() -> None}
\end{bullets}
}
\INPUTS{
Matrix entries $a,b,c,d$ (floats), integer $k\ge 0$, real $t$.
}
\OUTPUTS{
Dictionary with keys \inlinecode{"A_pow"}, \inlinecode{"expA"},
and \inlinecode{"ok"} booleans for validations.
}
\FORMULA{
\[
A=PDP^{-1},\quad A^k=PD^kP^{-1},\quad e^{At}=P e^{Dt} P^{-1}.
\]
}
\textbf{SOLUTION A — From Scratch (Mathematically Explicit Implementation)}
\begin{codepy}
# Deterministic, analytic 2x2 eigen-solver and diagonalization
import math

def read_input(s):
    vals = [float(x) for x in s.split()]
    a,b,c,d,k,t = vals
    return {"A":[[a,b],[c,d]], "k":int(k), "t":t}

def _eig2x2(A):
    a,b = A[0]
    c,d = A[1]
    tr = a + d
    disc = (a - d)**2 + 4*b*c
    sdisc = math.sqrt(max(disc, 0.0))
    l1 = 0.5*(tr + sdisc)
    l2 = 0.5*(tr - sdisc)
    def vec(lam):
        if abs(b) > 1e-15:
            return [b, lam - a]
        elif abs(c) > 1e-15:
            return [lam - d, c]
        else:
            return [1.0, 0.0]
    v1 = vec(l1); v2 = vec(l2)
    return (l1, v1), (l2, v2)

def _matmul(X,Y):
    return [[X[0][0]*Y[0][0]+X[0][1]*Y[1][0],
             X[0][0]*Y[0][1]+X[0][1]*Y[1][1]],
            [X[1][0]*Y[0][0]+X[1][1]*Y[1][0],
             X[1][0]*Y[0][1]+X[1][1]*Y[1][1]]]

def _inv2(P):
    a,b = P[0]; c,d = P[1]
    det = a*d - b*c
    return [[ d/det, -b/det],[-c/det,  a/det]]

def _scale(v, s):
    return [v[0]*s, v[1]*s]

def _norm(v):
    return math.sqrt(v[0]*v[0] + v[1]*v[1])

def _colstack(v1,v2):
    return [[v1[0], v2[0]],[v1[1], v2[1]]]

def _diag(a,b):
    return [[a,0.0],[0.0,b]]

def _eye():
    return [[1.0,0.0],[0.0,1.0]]

def _pow2(A,k):
    R = _eye()
    B = [[A[0][0],A[0][1]],[A[1][0],A[1][1]]]
    for _ in range(k):
        R = _matmul(R,B)
    return R

def solve_case(obj):
    A = obj["A"]; k = obj["k"]; t = obj["t"]
    (l1,v1),(l2,v2) = _eig2x2(A)
    v1 = _scale(v1, 1.0/max(_norm(v1),1e-18))
    v2 = _scale(v2, 1.0/max(_norm(v2),1e-18))
    P = _colstack(v1,v2)
    Pinv = _inv2(P)
    Dk = _diag(l1**k, l2**k)
    Ak = _matmul(_matmul(P,Dk), Pinv)
    e1 = math.exp(l1*t); e2 = math.exp(l2*t)
    eDt = _diag(e1, e2)
    eAt = _matmul(_matmul(P, eDt), Pinv)
    Ak_direct = _pow2(A,k)
    ok = all(abs(Ak[i][j]-Ak_direct[i][j])<1e-8 for i in range(2)
             for j in range(2))
    return {"A_pow":Ak, "expA":eAt, "ok":ok}

def validate():
    obj = read_input("4 1 1 4 5 0.3")
    out = solve_case(obj)
    assert out["ok"]

def main():
    validate()
    obj = read_input("3 1 0 2 4 1.0")
    out = solve_case(obj)
    print("A^k=", out["A_pow"])
    print("exp(A)=", out["expA"])
    print("ok=", out["ok"])

if __name__ == "__main__":
    main()
\end{codepy}
\textbf{SOLUTION B — Library-Based (Validated Computational Shortcut)}
\begin{codepy}
# NumPy-based eigen-decomposition and verification
import numpy as np

def read_input(s):
    a,b,c,d,k,t = [float(x) for x in s.split()]
    A = np.array([[a,b],[c,d]], dtype=float)
    return {"A":A, "k":int(k), "t":t}

def solve_case(obj):
    A = obj["A"]; k = obj["k"]; t = obj["t"]
    w,V = np.linalg.eig(A)
    Pinv = np.linalg.inv(V)
    Dk = np.diag(w**k)
    Ak = V @ Dk @ Pinv
    eDt = np.diag(np.exp(w*t))
    eAt = V @ eDt @ Pinv
    Ak_direct = np.linalg.matrix_power(A,k)
    ok = np.allclose(Ak, Ak_direct, atol=1e-8)
    return {"A_pow":Ak, "expA":eAt, "ok":bool(ok)}

def validate():
    obj = read_input("4 1 1 4 5 0.3")
    out = solve_case(obj)
    assert out["ok"]

def main():
    validate()
    obj = read_input("3 1 0 2 4 1.0")
    out = solve_case(obj)
    print("ok", out["ok"])

if __name__ == "__main__":
    main()
\end{codepy}
\COMPLEXITY{
Time $\mathcal{O}(1)$ for closed form 2x2 and $\mathcal{O}(1)$ per power
check; NumPy eig costs $\mathcal{O}(n^3)$ in general (here constant).
Space $\mathcal{O}(1)$ in 2x2 case.
}
\FAILMODES{
\begin{bullets}
\item Repeated eigenvalues cause eigenvector degeneracy; handle by fallback.
\item Nearly defective matrices yield ill conditioned $P$; checks may fail
numerically.
\end{bullets}
}
\STABILITY{
\begin{bullets}
\item Orthogonal diagonalization (when available) is best conditioned.
\item Compare with direct power to avoid accumulated drift.
\end{bullets}
}
\VALIDATION{
\begin{bullets}
\item Assert $\|A^k-(P D^k P^{-1})\|\le 10^{-8}$.
\item Check $A P \approx P D$ numerically.
\end{bullets}
}
\RESULT{
Both implementations agree on $A^k$ and $e^{At}$ for tested inputs and
verify the canonical identities.
}
\EXPLANATION{
Implements Formula 1 and Formula 2 directly: compute eigenbasis, act
modewise, and reconjugate; validation compares with direct definitions.
}
\EXTENSION{
Generalize to $n\times n$ using QR based eigensolvers; add Schur form for
defective cases.
}

\CodeDemoPage{Similarity Invariants and Change of Basis}
\PROBLEM{
Construct similar matrices $A$ and $B=P^{-1}AP$; verify equality of
spectrum, trace, and determinant; check $AP=PB$.
}
\API{
\begin{bullets}
\item \inlinecode{def read_input(s) -> dict}
\item \inlinecode{def solve_case(obj) -> dict}
\item \inlinecode{def validate() -> None}
\item \inlinecode{def main() -> None}
\end{bullets}
}
\INPUTS{
A diagonal $D$ with entries $(2,5,7)$ and invertible $P$.
}
\OUTPUTS{
Booleans for matching invariants and residual $\|AP-PB\|$.
}
\FORMULA{
\[
B=P^{-1}AP,\ \operatorname{tr}B=\operatorname{tr}A,\
\det B=\det A,\ \sigma(B)=\sigma(A).
\]
}
\textbf{SOLUTION A — From Scratch (Mathematically Explicit Implementation)}
\begin{codepy}
# Manual construction: A=PDP^{-1}, invariants checked by formulas
import math

def read_input(s):
    vals = [float(x) for x in s.split()]
    P = [[vals[0],vals[1],vals[2]],
         [vals[3],vals[4],vals[5]],
         [vals[6],vals[7],vals[8]]]
    D = [[2.0,0.0,0.0],[0.0,5.0,0.0],[0.0,0.0,7.0]]
    return {"P":P, "D":D}

def _matmul(A,B):
    n = len(A)
    C = [[0.0]*n for _ in range(n)]
    for i in range(n):
        for j in range(n):
            C[i][j] = sum(A[i][k]*B[k][j] for k in range(n))
    return C

def _det3(M):
    a,b,c = M[0]; d,e,f = M[1]; g,h,i = M[2]
    return a*(e*i-f*h)-b*(d*i-f*g)+c*(d*h-e*g)

def _inv3(M):
    a,b,c = M[0]; d,e,f = M[1]; g,h,i = M[2]
    det = _det3(M)
    A11 =  (e*i-f*h); A12 = -(b*i-c*h); A13 =  (b*f-c*e)
    A21 = -(d*i-f*g); A22 =  (a*i-c*g); A23 = -(a*f-c*d)
    A31 =  (d*h-e*g); A32 = -(a*h-b*g); A33 =  (a*e-b*d)
    adj = [[A11,A12,A13],[A21,A22,A23],[A31,A32,A33]]
    return [[adj[r][c]/det for c in range(3)] for r in range(3)]

def _trace(M):
    return M[0][0]+M[1][1]+M[2][2]

def _fro_norm(M):
    return math.sqrt(sum(M[i][j]*M[i][j] for i in range(3) for j in range(3)))

def solve_case(obj):
    P = obj["P"]; D = obj["D"]
    Pinv = _inv3(P)
    A = _matmul(_matmul(P,D), Pinv)
    B = D
    AP = _matmul(A,P)
    PB = _matmul(P,B)
    res = _fro_norm([[AP[i][j]-PB[i][j] for j in range(3)] for i in range(3)])
    tr_ok = abs(_trace(A)-_trace(B))<1e-9
    det_ok = abs(_det3(A)-_det3(B))<1e-9
    return {"res":res, "tr_ok":tr_ok, "det_ok":det_ok}

def validate():
    obj = read_input("1 1 0 0 1 1 1 0 1")
    out = solve_case(obj)
    assert out["res"]<1e-9 and out["tr_ok"] and out["det_ok"]

def main():
    validate()
    obj = read_input("2 1 0 1 1 1 0 1 1")
    out = solve_case(obj)
    print("res", round(out["res"],12), "tr_ok", out["tr_ok"],
          "det_ok", out["det_ok"])

if __name__ == "__main__":
    main()
\end{codepy}
\textbf{SOLUTION B — Library-Based (Validated Computational Shortcut)}
\begin{codepy}
# NumPy-based similarity check and eigenvalue comparison
import numpy as np

def read_input(s):
    vals = [float(x) for x in s.split()]
    P = np.array(vals, dtype=float).reshape(3,3)
    D = np.diag([2.0,5.0,7.0])
    return {"P":P, "D":D}

def solve_case(obj):
    P = obj["P"]; D = obj["D"]
    A = P @ D @ np.linalg.inv(P)
    B = D
    res = np.linalg.norm(A@P - P@B)
    tr_ok = abs(np.trace(A)-np.trace(B))<1e-9
    det_ok = abs(np.linalg.det(A)-np.linalg.det(B))<1e-9
    wA = np.sort(np.linalg.eigvals(A))
    wB = np.sort(np.linalg.eigvals(B))
    spec_ok = np.allclose(wA, wB)
    return {"res":float(res), "tr_ok":bool(tr_ok),
            "det_ok":bool(det_ok), "spec_ok":bool(spec_ok)}

def validate():
    obj = read_input("1 1 0 0 1 1 1 0 1")
    out = solve_case(obj)
    assert out["res"]<1e-9 and out["tr_ok"] and out["det_ok"] and out["spec_ok"]

def main():
    validate()
    obj = read_input("2 1 0 1 1 1 0 1 1")
    out = solve_case(obj)
    print("ok", out["tr_ok"] and out["det_ok"] and out["spec_ok"])

if __name__ == "__main__":
    main()
\end{codepy}
\COMPLEXITY{
Constant size $3$, so $\mathcal{O}(1)$; in general, matrix multiplies are
$\mathcal{O}(n^3)$.
}
\FAILMODES{
\begin{bullets}
\item Nearly singular $P$ worsens numeric inversion; residual increases.
\item Non invertible $P$ invalid; guard determinant away from zero.
\end{bullets}
}
\STABILITY{
\begin{bullets}
\item Prefer orthogonal $P$ when available to minimize condition number.
\end{bullets}
}
\VALIDATION{
\begin{bullets}
\item Check $AP=PB$, invariants equality, and eigenvalue multiset.
\end{bullets}
}
\RESULT{
Similarity invariants validated; construction satisfies $AP=PB$ to round
off tolerance.
}
\EXPLANATION{
Implements Formula 4: conjugation leaves trace, determinant, and spectrum
unchanged; $AP=PB$ is the defining relation.
}

\section{Applied Domains — Detailed End-to-End Scenarios}
\DomainPage{Machine Learning}
\SCENARIO{
Principal Component Analysis: diagonalize covariance to decorrelate
features and obtain principal axes and explained variances.
}
\ASSUMPTIONS{
\begin{bullets}
\item Data are centered or we center them.
\item Covariance is symmetric positive semidefinite.
\end{bullets}
}
\WHICHFORMULA{
Spectral theorem: $\Sigma=Q\Lambda Q^\top$; components are $Z=XQ$ with
variances $\Lambda$.
}
\varmapStart
\var{X}{Data matrix $(n,d)$ after centering.}
\var{\Sigma}{Covariance $d\times d$.}
\var{Q,\Lambda}{Eigenvectors and eigenvalues of $\Sigma$.}
\varmapEnd
\PIPELINE{
\begin{bullets}
\item Generate synthetic correlated data; center.
\item Compute $\Sigma$; diagonalize $\Sigma=Q\Lambda Q^\top$.
\item Project onto $Q$; report explained variance ratios.
\end{bullets}
}
\textbf{Implementation (From Scratch)}
\begin{codepy}
import numpy as np

def generate(n=300, seed=0):
    rng = np.random.default_rng(seed)
    U = rng.standard_normal((n,2))
    A = np.array([[2.0, 1.2],[1.2, 1.0]])
    X = U @ np.linalg.cholesky(A).T
    return X

def pca(X):
    Xc = X - X.mean(axis=0, keepdims=True)
    Sigma = (Xc.T @ Xc)/Xc.shape[0]
    w,Q = np.linalg.eigh(Sigma)
    idx = np.argsort(w)[::-1]
    w, Q = w[idx], Q[:,idx]
    Z = Xc @ Q
    evr = w/w.sum()
    return Sigma, Q, w, Z, evr

def main():
    X = generate()
    Sigma, Q, w, Z, evr = pca(X)
    print("EVR:", np.round(evr,3))

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{
Explained variance ratio; sum equals 1; first component captures the most
variance along largest eigenvalue.
}
\INTERPRET{
Diagonalization rotates data into uncorrelated axes ranked by variance.
}
\NEXTSTEPS{
Whitening by scaling with $\Lambda^{-1/2}$; extend to $d>2$ and SVD.
}

\DomainPage{Quantitative Finance}
\SCENARIO{
Risk modes via covariance diagonalization: decompose a 3 asset portfolio
covariance into principal components and report mode contributions.
}
\ASSUMPTIONS{
\begin{bullets}
\item Returns are i.i.d. with finite second moments.
\item Empirical covariance approximates population covariance.
\end{bullets}
}
\WHICHFORMULA{
$\Sigma=Q\Lambda Q^\top$; portfolio variance $w^\top \Sigma w=
\sum \lambda_i (q_i^\top w)^2$.
}
\varmapStart
\var{R}{Returns matrix $(n,3)$.}
\var{\Sigma}{Empirical covariance.}
\var{Q,\Lambda}{Eigen decomposition of $\Sigma$.}
\var{w}{Portfolio weights summing to 1.}
\varmapEnd
\PIPELINE{
\begin{bullets}
\item Simulate correlated returns.
\item Compute $\Sigma$ and its eigen decomposition.
\item Attribute variance to principal components.
\end{bullets}
}
\textbf{Implementation (Full Pipeline)}
\begin{codepy}
import numpy as np

def simulate(n=1000, seed=0):
    rng = np.random.default_rng(seed)
    A = rng.normal(size=(3,3))
    Sigma = A @ A.T
    R = rng.multivariate_normal(np.zeros(3), Sigma, size=n)
    return R

def pc_risk(R, w):
    Sigma = np.cov(R, rowvar=False)
    w_vals, Q = np.linalg.eigh(Sigma)
    idx = np.argsort(w_vals)[::-1]
    w_vals, Q = w_vals[idx], Q[:,idx]
    coords = Q.T @ w
    var = float(w.T @ Sigma @ w)
    contrib = w_vals * (coords**2)
    return var, w_vals, Q, contrib/var

def main():
    R = simulate()
    w = np.array([0.5, 0.3, 0.2])
    var, evals, Q, frac = pc_risk(R, w)
    print("Var", round(var,4), "Frac", np.round(frac,3))

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{
Portfolio variance and fraction per principal component.
}
\INTERPRET{
Largest eigenvalue mode dominates risk if weights align with its eigenvector.
}
\NEXTSTEPS{
Use eigenstructure for risk parity or factor modeling.
}

\DomainPage{Deep Learning}
\SCENARIO{
Linear layer dynamics: analyze $x_{k+1}=W x_k$ through diagonalization to
show convergence along dominant eigenvector.
}
\ASSUMPTIONS{
\begin{bullets}
\item Weight matrix $W$ diagonalizable with spectral radius less than 1.
\end{bullets}
}
\WHICHFORMULA{
$W^k=P D^k P^{-1}$; as $k\to\infty$, $D^k\to 0$ if $|\lambda_i|<1$.
}
\varmapStart
\var{W}{Weight matrix.}
\var{x_0}{Initial activation.}
\var{k}{Iteration steps.}
\varmapEnd
\PIPELINE{
\begin{bullets}
\item Construct diagonalizable $W$ with eigenvalues inside unit circle.
\item Iterate $x_{k+1}=W x_k$ and compare with $P D^k P^{-1} x_0$.
\end{bullets}
}
\textbf{Implementation (End-to-End)}
\begin{codepy}
import numpy as np

def build():
    P = np.array([[1.0, 1.0],[0.5, -0.5]])
    D = np.diag([0.8, 0.3])
    W = P @ D @ np.linalg.inv(P)
    return W, P, D

def iterate(W, x0, k):
    x = x0.copy()
    for _ in range(k):
        x = W @ x
    return x

def main():
    W, P, D = build()
    x0 = np.array([1.0, 2.0])
    k = 12
    xk = iterate(W, x0, k)
    xk2 = P @ (np.linalg.matrix_power(D, k)) @ np.linalg.inv(P) @ x0
    print("agree", np.allclose(xk, xk2, atol=1e-10))
    print("norm", np.linalg.norm(xk))

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{
Agreement between direct iteration and diagonalization; decay rate governed
by $|\lambda_{\max}|^k$.
}
\INTERPRET{
Modes aligned with eigenvectors decay at rates set by eigenvalues.
}
\NEXTSTEPS{
Analyze training stability via spectral radius constraints.
}

\DomainPage{Kaggle / Data Analytics}
\SCENARIO{
PCA whitening: compute $W=\Lambda^{-1/2} Q^\top$ to transform features to
unit covariance.
}
\ASSUMPTIONS{
\begin{bullets}
\item Covariance is full rank for whitening.
\end{bullets}
}
\WHICHFORMULA{
$\Sigma=Q\Lambda Q^\top$, whitened data $Z=X_c Q \Lambda^{-1/2}$ with
$\operatorname{Cov}(Z)=I$.
}
\varmapStart
\var{X}{Data $(n,d)$; $X_c$ centered data.}
\var{\Sigma}{Covariance.}
\var{Q,\Lambda}{Eigendecomposition of $\Sigma$.}
\var{Z}{Whitened data.}
\varmapEnd
\PIPELINE{
\begin{bullets}
\item Center data.
\item Eigendecompose covariance.
\item Apply whitening transform.
\end{bullets}
}
\textbf{Implementation (Complete EDA Pipeline)}
\begin{codepy}
import numpy as np

def create(seed=0, n=500):
    rng = np.random.default_rng(seed)
    A = np.array([[1.0, 0.9, 0.2],
                  [0.9, 2.0, 0.4],
                  [0.2, 0.4, 1.5]])
    X = rng.standard_normal((n,3)) @ np.linalg.cholesky(A).T
    return X

def whiten(X):
    Xc = X - X.mean(axis=0, keepdims=True)
    Sigma = (Xc.T @ Xc)/Xc.shape[0]
    w,Q = np.linalg.eigh(Sigma)
    W = Q @ np.diag(1.0/np.sqrt(w)) @ Q.T
    Z = Xc @ W
    CovZ = (Z.T @ Z)/Z.shape[0]
    return Sigma, W, Z, CovZ

def main():
    X = create()
    Sigma, W, Z, CovZ = whiten(X)
    print("diag CovZ", np.round(np.diag(CovZ),3))

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{
Diagonal of whitened covariance near ones; off diagonals near zero.
}
\INTERPRET{
Diagonalization provides rotation and scaling to unit variance axes.
}
\NEXTSTEPS{
Regularize small eigenvalues; use SVD for numerical robustness.
}

\end{document}
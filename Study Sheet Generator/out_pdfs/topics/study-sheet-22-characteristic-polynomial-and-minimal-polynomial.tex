% !TeX program = xelatex
\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{microtype,setspace,amsmath,amssymb,mathtools,amsthm,unicode-math}
\setstretch{1.05}
\setmainfont{Latin Modern Roman}
\setmonofont{Latin Modern Mono}
\setmathfont{Latin Modern Math}

\allowdisplaybreaks[4]
\setlength{\jot}{7pt}
\setlength{\emergencystretch}{8em}
\sloppy

\usepackage{xcolor,fancyhdr,enumitem,inconsolata,listings}
\pagestyle{fancy}\fancyhf{}\lhead{\nouppercase{\leftmark}}\rhead{\thepage}
\setlength{\headheight}{26pt}
\setlength{\parindent}{0pt}
\setlength{\parskip}{8pt plus 2pt minus 1pt}
\raggedbottom

\newenvironment{BreakableEquation}{\begin{equation}\begin{aligned}}{\end{aligned}\end{equation}}
\newenvironment{BreakableEquation*}{\begin{equation*}\begin{aligned}}{\end{aligned}\end{equation*}}
\newenvironment{tightalign}{\begingroup\small\allowdisplaybreaks\begin{align}}{\end{align}\endgroup}

\providecommand{\enumlistm}{enumitem}
\newenvironment{minted}[2][]{%
  \lstset{style=code,language=#2,#1}\begin{lstlisting}%
}{\end{lstlisting}}

\newcommand{\inputminted}[3][]{\begin{lstlisting}\end{lstlisting}}

\newlist{bullets}{itemize}{1}
\setlist[bullets]{label=--,leftmargin=1.6em,itemsep=2pt,topsep=4pt}

\newcommand{\varmapStart}{\textbf{VARIABLE MAPPING:}\par\begin{bullets}}
\newcommand{\var}[2]{\item $#1$ — #2}
\newcommand{\varmapEnd}{\end{bullets}}

\newcommand{\glossx}[6]{%
  \textbf{#1}\par
  \begin{bullets}
    \item \textbf{What:} #2
    \item \textbf{Why:} #3
    \item \textbf{How:} #4
    \item \textbf{ELI5:} #5
    \item \textbf{Pitfall/Example:} #6
  \end{bullets}
}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{claim}[theorem]{Claim}
\lstdefinestyle{code}{
  basicstyle=\ttfamily\footnotesize,
  backgroundcolor=\color{black!02},
  frame=single,
  numbers=left, numberstyle=\tiny, numbersep=8pt,
  breaklines=true, breakatwhitespace=true,
  tabsize=4, showstringspaces=false,
  upquote=true, keepspaces=true, columns=fullflexible,
  literate=
    {–}{{-}}1
    {—}{{-}}1
    {…}{{...}}1
    {≤}{{\ensuremath{\le}}}1
    {≥}{{\ensuremath{\ge}}}1
    {≠}{{\ensuremath{\ne}}}1
    {≈}{{\ensuremath{\approx}}}1
    {±}{{\ensuremath{\pm}}}1
    {→}{{\ensuremath{\to}}}1
    {←}{{\ensuremath{\leftarrow}}}1
    {∞}{{\ensuremath{\infty}}}1
    {√}{{\ensuremath{\sqrt{\ }}}}1
    {×}{{\ensuremath{\times}}}1
    {÷}{{\ensuremath{\div}}}1
}

\lstnewenvironment{codepy}[1][]%
  {\lstset{style=code,language=Python,#1}}%
  {}

\newcommand{\inlinecode}[1]{\lstinline[style=code]!#1!}

\newcommand{\LF}[2]{\par\noindent\textbf{#1:}~#2\par}
\newcommand{\WHAT}[1]{\LF{WHAT}{#1}}
\newcommand{\WHY}[1]{\LF{WHY}{#1}}
\newcommand{\HOW}[1]{\LF{HOW}{#1}}
\newcommand{\ELI}[1]{\LF{ELI5}{#1}}
\newcommand{\SCOPE}[1]{\LF{SCOPE}{#1}}
\newcommand{\CONFUSIONS}[1]{\LF{COMMON CONFUSIONS}{#1}}
\newcommand{\APPLICATIONS}[1]{\LF{APPLICATIONS}{#1}}
\newcommand{\FORMULA}[1]{\LF{FORMULA}{#1}}
\newcommand{\CANONICAL}[1]{\LF{CANONICAL FORM}{#1}}
\newcommand{\PRECONDS}[1]{\LF{PRECONDITIONS}{#1}}
\newcommand{\DERIVATION}[1]{\LF{DERIVATION}{#1}}
\newcommand{\EQUIV}[1]{\LF{EQUIVALENT FORMS}{#1}}
\newcommand{\LIMITS}[1]{\LF{LIMIT CASES}{#1}}
\newcommand{\INPUTS}[1]{\LF{INPUTS}{#1}}
\newcommand{\OUTPUTS}[1]{\LF{OUTPUTS}{#1}}
\newcommand{\RESULT}[1]{\LF{RESULT}{#1}}
\newcommand{\INTUITION}[1]{\LF{INTUITION}{#1}}
\newcommand{\PITFALLS}[1]{\LF{PITFALLS}{#1}}
\newcommand{\MODEL}[1]{\LF{CANONICAL MATH MODEL}{#1}}
\newcommand{\ASSUMPTIONS}[1]{\LF{ASSUMPTIONS}{#1}}
\newcommand{\WHICHFORMULA}[1]{\LF{WHICH FORMULA \& WHY}{#1}}
\newcommand{\GOVERN}[1]{\LF{GOVERNING EQUATION(S)}{#1}}
\newcommand{\UNITCHECK}[1]{\LF{UNIT CHECK}{#1}}
\newcommand{\EDGECASES}[1]{\LF{EDGE CASES}{#1}}
\newcommand{\ALTERNATE}[1]{\LF{ALTERNATE APPROACH (sketch)}{#1}}
\newcommand{\PROBLEM}[1]{\LF{PROBLEM}{#1}}
\newcommand{\API}[1]{\LF{API}{#1}}
\newcommand{\COMPLEXITY}[1]{\LF{COMPLEXITY}{#1}}
\newcommand{\FAILMODES}[1]{\LF{FAILURE MODES}{#1}}
\newcommand{\STABILITY}[1]{\LF{NUMERICAL STABILITY}{#1}}
\newcommand{\VALIDATION}[1]{\LF{VALIDATION}{#1}}
\newcommand{\EXPLANATION}[1]{\LF{EXPLANATION}{#1}}
\newcommand{\SCENARIO}[1]{\LF{SCENARIO}{#1}}
\newcommand{\PIPELINE}[1]{\LF{PIPELINE STEPS}{#1}}
\newcommand{\METRICS}[1]{\LF{METRICS}{#1}}
\newcommand{\INTERPRET}[1]{\LF{INTERPRETATION}{#1}}
\newcommand{\NEXTSTEPS}[1]{\LF{LIMITATIONS \& NEXT STEPS}{#1}}

\usepackage{titlesec}
\titleformat{\section}{\Large\bfseries}{\thesection}{0.6em}{}
\titlespacing*{\section}{0pt}{*2}{*1}
\usepackage{etoolbox}
\pretocmd{\section}{\clearpage}{}{}

\newcommand{\FormulaPage}[2]{%
  \clearpage
  \section*{Formula #1 — #2}%
  \addcontentsline{toc}{section}{Formula #1 — #2}%
}
\newcommand{\ProblemPage}[2]{%
  \clearpage
  \subsection*{Problem #1: #2}%
  \addcontentsline{toc}{subsection}{Problem #1: #2}%
}
\newcommand{\CodeDemoPage}[1]{%
  \clearpage
  \subsection*{Coding Demo: #1}%
  \addcontentsline{toc}{subsection}{Coding Demo: #1}%
}
\newcommand{\DomainPage}[1]{%
  \clearpage
  \subsection*{#1 (End-to-End)}%
  \addcontentsline{toc}{subsection}{#1 (End-to-End)}%
}

\begin{document}
\title{Comprehensive Study Sheet — Characteristic Polynomial and Minimal Polynomial}
\date{\today}
\maketitle
\tableofcontents
\clearpage

\section{Concept Overview}
\WHAT{
Let $K$ be a field and $A\in K^{n\times n}$. The characteristic polynomial
of $A$ is $p_A(\lambda)=\det(\lambda I_n-A)\in K[\lambda]$. The minimal
polynomial of $A$ is the unique monic polynomial $m_A\in K[t]$ of least
degree such that $m_A(A)=0$. The spectrum $\sigma(A)$ is the multiset of
roots of $p_A$ in an algebraic closure $\overline{K}$. The Jordan form and
primary decomposition are governed by $m_A$ and $p_A$.
}
\WHY{
$p_A$ encodes eigenvalues and algebraic multiplicities. $m_A$ encodes the
smallest annihilating relation and the size of largest Jordan blocks. They
govern linear recurrences, matrix functions, stability, similarity
classification over $K$, and algorithms such as Krylov methods.
}
\HOW{
1. Define $p_A$ via determinant and develop coefficients via traces (Leverrier).
2. Show every polynomial relation factors through $m_A$; prove uniqueness and
divisibility $m_A\mid p_A$ using the Cayley–Hamilton theorem.
3. Use $m_A$ to reduce polynomials in $A$ modulo $m_A$, yielding canonical
representatives for matrix functions and powers.
4. Interpret eigen-structure: simple roots of $m_A$ $\Leftrightarrow$
diagonalizable; multiplicities in $p_A$ bound geometric multiplicities.
}
\ELI{
Think of $p_A$ as the master fingerprint listing all allowed pitches
(eigenvalues) and how many times each appears. The minimal polynomial $m_A$
is the shortest music score that, when played into $A$, gives perfect
silence ($0$). Every longer score that silences $A$ is a remix of $m_A$.
}
\SCOPE{
Defined for all square matrices over any field. Over non-algebraically
closed fields, $p_A$ and $m_A$ may be irreducible with no roots in $K$;
Jordan form then lives over $\overline{K}$. Diagonalization criteria require
splitting into linear factors. Non-square matrices are out of scope.
}
\CONFUSIONS{
Characteristic vs. minimal polynomial: $p_A$ lists eigenvalues (with
algebraic multiplicity), $m_A$ captures the largest Jordan block for each
eigenvalue. Minimal polynomial is not the product of distinct eigenvalues
unless all blocks are size $1$. Geometric multiplicity is not the
multiplicity in $m_A$ or $p_A$ directly.
}
\APPLICATIONS{
\begin{bullets}
\item Spectral analysis and similarity classification.
\item Computing $A^k$, $f(A)$ via reduction modulo $m_A$.
\item Stability of linear dynamical systems from spectral radius (roots of $p_A$).
\item Solving linear recurrences via companion matrices and their polynomials.
\end{bullets}
}

\textbf{ANALYTIC STRUCTURE.}
$p_A$ is monic of degree $n$; coefficients are (up to signs) elementary
symmetric polynomials of eigenvalues and polynomials in traces of powers of
$A$. $m_A$ is monic, divides $p_A$, and is the least common multiple of the
invariant factors; it is the product over distinct eigenvalues of $(t-\lambda)^s$
where $s$ is the size of the largest Jordan block for $\lambda$.

\textbf{CANONICAL LINKS.}
Cayley–Hamilton: $p_A(A)=0$ gives existence of an annihilating polynomial.
Primary decomposition: $m_A=\prod_i m_i$ with pairwise coprime $m_i$
induces $K^n=\bigoplus_i \ker m_i(A)$. Rational canonical form is built
from invariant factors dividing $p_A$ with top factor $m_A$.

\textbf{PROBLEM-TYPE RECOGNITION HEURISTICS.}
\begin{bullets}
\item Determinants of $\lambda I-A$ or eigenvalues requested $\Rightarrow$ use $p_A$.
\item Questions about smallest polynomial relation on $A$ $\Rightarrow$ use $m_A$.
\item Diagonalizability or Jordan block sizes $\Rightarrow$ inspect $m_A$ roots.
\item Powers $A^k$ or $f(A)$ $\Rightarrow$ reduce modulo $m_A$.
\end{bullets}

\textbf{SOLUTION STRATEGY BLUEPRINT.}
\begin{bullets}
\item Translate to $p_A$ or $m_A$ by defining $\det(\lambda I-A)$ or solving
$P(A)=0$ with minimal degree.
\item Apply Cayley–Hamilton to assert an annihilator exists.
\item Factor over $K$ (or $\overline{K}$) and match block sizes via $m_A$.
\item Reduce computations of $A^k$ using the recurrence induced by $m_A$.
\end{bullets}

\textbf{CONCEPTUAL INVARIANTS.}
Degree of $p_A$ equals $n$; $p_A$ and $m_A$ are similarity invariants; the
set of eigenvalues equals roots of $p_A$; $m_A$ divides $p_A$.

\textbf{EDGE INTUITION.}
If $A$ is scalar $\alpha I$, then $p_A(\lambda)=(\lambda-\alpha)^n$ and
$m_A(t)=t-\alpha$. If $A$ is nilpotent with index $k$, then $p_A(t)=t^n$
and $m_A(t)=t^k$ with $1\le k\le n$.

\section{Glossary}
\glossx{Characteristic Polynomial}
{For $A\in K^{n\times n}$, $p_A(\lambda)=\det(\lambda I-A)$, monic of degree $n$.}
{Encodes eigenvalues and algebraic multiplicities; central to Cayley–Hamilton.}
{Compute $\det(\lambda I-A)$ symbolically or via Faddeev–LeVerrier.}
{Think of it as the DNA listing all eigenvalues with repetition.}
{Pitfall: forgetting the sign convention; $p_A$ is monic in $\lambda$.}

\glossx{Minimal Polynomial}
{Monic polynomial $m_A$ of least degree with $m_A(A)=0$.}
{Determines largest Jordan block per eigenvalue; reduces $f(A)$ computations.}
{Find smallest linear dependence in $\{I,A,\dots,A^k\}$ and solve coefficients.}
{Shortest spell that turns the matrix action to zero.}
{Pitfall: $m_A$ may equal $p_A$ or be strictly smaller; it always divides $p_A$.}

\glossx{Algebraic vs. Geometric Multiplicity}
{Algebraic multiplicity is multiplicity in $p_A$; geometric is dimension of
eigenspace.}
{Geometric multiplicity measures number of independent eigenvectors.}
{Compute nullity of $A-\lambda I$ for geometric multiplicity.}
{Algebraic counts how many times a note appears; geometric counts how many
independent singers sing it.}
{Pitfall: geometric multiplicity $\le$ algebraic multiplicity, may be smaller.}

\glossx{Companion Matrix}
{For monic $q(t)=t^k+c_{k-1}t^{k-1}+\cdots+c_0$, its companion $C(q)$ is the
$k\times k$ matrix with ones on the first subdiagonal and last column
$(-c_0,\dots,-c_{k-1})^\top$.}
{Encodes a linear recurrence whose characteristic and minimal polynomials are $q$.}
{Build $C(q)$ and verify $q(C(q))=0$.}
{A machine whose behavior exactly follows the polynomial $q$.}
{Pitfall: ordering of coefficients; ensure monic and correct sign.}

\section{Symbol Ledger}
\varmapStart
\var{K}{Base field.}
\var{\overline{K}}{An algebraic closure of $K$.}
\var{A}{Matrix in $K^{n\times n}$.}
\var{I_n}{Identity matrix of size $n$.}
\var{\lambda}{Indeterminate or eigenvalue variable.}
\var{t}{Indeterminate for polynomials acting on $A$.}
\var{p_A}{Characteristic polynomial of $A$.}
\var{m_A}{Minimal polynomial of $A$.}
\var{\sigma(A)}{Spectrum (multiset of eigenvalues in $\overline{K}$).}
\var{\chi_A}{Alternate notation for $p_A$.}
\var{C(q)}{Companion matrix of monic polynomial $q$.}
\var{J}{Jordan normal form over $\overline{K}$.}
\var{n}{Matrix size (dimension).}
\var{k}{Degree or index parameter.}
\var{c_i}{Coefficients of polynomials.}
\varmapEnd

\section{Formula Canon — One Formula Per Page}
\FormulaPage{1}{Characteristic Polynomial via Determinant and Traces}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
For $A\in K^{n\times n}$,
\[
p_A(\lambda)=\det(\lambda I_n-A)=\lambda^n+s_1\lambda^{n-1}+\cdots+s_n,
\]
with $s_k$ expressed by the Faddeev–LeVerrier identities from traces
$\operatorname{tr}(A^j)$.

\WHAT{
$p_A$ computes eigenvalues as roots. The coefficients are invariant and can
be obtained from traces without symbolic determinants.
}
\WHY{
Determinants may be expensive symbolically; trace-based recurrences are
algorithmically efficient and numerically stable variants exist (Berkowitz).
}
\FORMULA{
\[
\begin{aligned}
&p_A(\lambda)=\det(\lambda I-A)=\lambda^n+s_1\lambda^{n-1}+\cdots+s_n,\\
&\text{with } s_1=-\operatorname{tr}(A),\quad
s_k=-\frac{1}{k}\sum_{j=1}^{k} s_{k-j}\operatorname{tr}(A^j),\; k\ge2,
\end{aligned}
\]
where $s_0=1$.
}
\CANONICAL{
$A\in K^{n\times n}$. Coefficients $s_k\in K$. The recurrence uses only
matrix powers and traces; no eigen-decomposition required.
}
\PRECONDS{
\begin{bullets}
\item Field $K$ of characteristic $0$ or $>n$ to justify division by $k$ in
Faddeev–LeVerrier. For arbitrary $K$, use Berkowitz algorithm instead.
\item $A$ finite-dimensional square matrix.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
Let $p_A(\lambda)=\lambda^n+\sum_{k=1}^n s_k\lambda^{n-k}$. Then for $k\ge1$,
$ks_k=-\sum_{j=1}^k s_{k-j}\operatorname{tr}(A^j)$ with $s_0=1$.
\end{lemma}
\begin{proof}
Consider $P(\lambda)=(\lambda I-A)^{-1}=\sum_{j\ge0}\lambda^{-j-1}A^j$ for
formal $\lambda$. Then
$\operatorname{tr}(P(\lambda))=\frac{p_A'(\lambda)}{p_A(\lambda)}$ by Jacobi's
formula. Expand both sides in powers of $\lambda^{-1}$, match coefficients,
and equate to obtain the stated recursion for $s_k$. \qedhere
\end{proof}
\DERIVATION{
\begin{align*}
\text{Setup: }&p_A(\lambda)=\lambda^n+\sum_{k=1}^n s_k\lambda^{n-k}.\\
\text{Jacobi: }&\frac{d}{d\lambda}\det(\lambda I-A)
=\det(\lambda I-A)\operatorname{tr}((\lambda I-A)^{-1}).\\
\text{Divide: }&\frac{p_A'(\lambda)}{p_A(\lambda)}
=\operatorname{tr}\Big(\sum_{j\ge0}\lambda^{-j-1}A^j\Big)
=\sum_{j\ge0}\lambda^{-j-1}\operatorname{tr}(A^j).\\
\text{Series: }&\frac{p_A'(\lambda)}{p_A(\lambda)}
=\frac{n\lambda^{n-1}+(n-1)s_1\lambda^{n-2}+\cdots}{\lambda^n+\cdots}.\\
\text{Match: }&\text{Equate coefficients of }\lambda^{-k-1}
\text{ to get }ks_k=-\sum_{j=1}^k s_{k-j}\operatorname{tr}(A^j).\\
\text{Conclude: }&\text{Recurrence yields all }s_k.
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Compute traces $\operatorname{tr}(A^j)$ for $1\le j\le n$.
\item Use the recursion to obtain $s_k$.
\item Assemble $p_A(\lambda)=\lambda^n+\sum_{k=1}^n s_k\lambda^{n-k}$.
\item Optionally verify by direct determinant evaluation for $n\le3$.
\end{bullets}
\EQUIV{
\begin{bullets}
\item Newton identities connect power sums of eigenvalues $\sum \lambda_i^j$
to elementary symmetric polynomials (the $s_k$).
\item Berkowitz algorithm yields the same coefficients without divisions.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item In small finite characteristic $p\le n$, division by $k$ may be invalid.
\item Numerical instability may arise for ill-conditioned $A$ if using floats.
\end{bullets}
}
\INPUTS{$A\in K^{n\times n}$.}
\DERIVATION{
\begin{align*}
\text{Example: }&A=\begin{pmatrix}2&1\\0&3\end{pmatrix},\; n=2.\\
\operatorname{tr}(A)&=5,\quad \operatorname{tr}(A^2)=2^2+2\cdot1\cdot0+3^2=13.\\
s_1&=-\operatorname{tr}(A)=-5.\\
2s_2&=-(s_1\operatorname{tr}(A)+s_0\operatorname{tr}(A^2))
=-(-5\cdot5+1\cdot13)=-( -25+13)=12,\\
s_2&=6.\\
p_A(\lambda)&=\lambda^2-5\lambda+6=(\lambda-2)(\lambda-3).
\end{align*}
}
\RESULT{
$p_A(\lambda)=\lambda^n+\sum_{k=1}^n s_k\lambda^{n-k}$ with $s_k$ from traces.
Example yields $(\lambda-2)(\lambda-3)$.
}
\UNITCHECK{
Degree $n$ and leading coefficient $1$ confirmed. Coefficient $-s_1$ equals
$\operatorname{tr}(A)$; constant term $s_n=\det(-A)=(-1)^n\det(A)$.
}
\PITFALLS{
\begin{bullets}
\item Mixing sign conventions for coefficients.
\item Forgetting that traces of powers, not powers of traces, enter identities.
\item Using division in a field where $k$ is not invertible.
\end{bullets}
}
\INTUITION{
The coefficients summarize all eigenvalue power sums through Newton identities.
}
\CANONICAL{
\begin{bullets}
\item $p_A(\lambda)=\prod_{i=1}^n(\lambda-\lambda_i)$ in $\overline{K}$.
\item Coefficients are elementary symmetric polynomials in $\{\lambda_i\}$.
\end{bullets}
}

\FormulaPage{2}{Cayley–Hamilton Theorem}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Every square matrix $A$ annihilates its characteristic polynomial:
\[
p_A(A)=0.
\]

\WHAT{
Substituting $A$ into its own characteristic polynomial yields the zero
matrix. This guarantees existence of a nontrivial annihilating polynomial.
}
\WHY{
It implies $m_A\mid p_A$, bounds the degree of recurrences for $A^k$, and
enables reduction of $f(A)$ to degree $<n$ polynomials.
}
\FORMULA{
\[
p_A(A)=A^n+s_1A^{n-1}+\cdots+s_n I_n=0.
\]
}
\CANONICAL{
Valid for any field $K$ and any $A\in K^{n\times n}$. Proof uses adjugate or
polynomial identity arguments independent of eigen-decomposition.
}
\PRECONDS{
\begin{bullets}
\item $A$ is square. No further regularity assumptions.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
For any $X\in K^{n\times n}$, $(\lambda I-X)\operatorname{adj}(\lambda I-X)
=\det(\lambda I-X)I$.
\end{lemma}
\begin{proof}
By the definition of the adjugate, $M\operatorname{adj}(M)=\det(M)I$ holds
for any $M$. Apply with $M=\lambda I-X$. \qedhere
\end{proof}
\DERIVATION{
\begin{align*}
\text{Let }&M(\lambda)=(\lambda I-A),\quad \operatorname{adj}(M(\lambda))=B(\lambda).\\
\text{Then }&M(\lambda)B(\lambda)=p_A(\lambda)I.\\
\text{View }&B(\lambda)=\sum_{k=0}^{n-1}B_k\lambda^k\text{ (degree }n-1).\\
\text{Plug }&\lambda=A:\quad (AI-A)B(A)=0=p_A(A)I.\\
\text{Therefore }&p_A(A)=0.
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Compute $p_A$ by any method.
\item Replace $\lambda$ by $A$ and verify $p_A(A)=0$ by multiplication.
\item Use the resulting linear recurrence to reduce $A^k$ to degree $<n$.
\end{bullets}
}
\EQUIV{
\begin{bullets}
\item Equivalent to saying $K[A]$ has dimension $\le n$ as a $K$-vector space.
\item Over $\overline{K}$, follows from Jordan form because each block
annihilates $(t-\lambda)^m$ of the right degree.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item No limitations: holds in all characteristics and for all square sizes.
\end{bullets}
}
\INPUTS{$A\in K^{n\times n}$; coefficients $s_k$ from $p_A$.}
\DERIVATION{
\begin{align*}
\text{Example: }&A=\begin{pmatrix}2&1\\0&3\end{pmatrix},\;
p_A(\lambda)=\lambda^2-5\lambda+6.\\
p_A(A)&=A^2-5A+6I
=\begin{pmatrix}4&5\\0&9\end{pmatrix}
-\begin{pmatrix}10&5\\0&15\end{pmatrix}
+\begin{pmatrix}6&0\\0&6\end{pmatrix}
=0.
\end{align*}
}
\RESULT{
$p_A(A)=0$; every polynomial in $A$ reduces modulo $p_A$ (and hence modulo
$m_A$).
}
\UNITCHECK{
Degrees and matrix sizes align; each term is $n\times n$; equality holds by
direct arithmetic in the example.
}
\PITFALLS{
\begin{bullets}
\item Forgetting to multiply by $I$ on scalar terms.
\item Numerical rounding when verifying with floating entries.
\end{bullets}
}
\INTUITION{
Each Jordan block satisfies its own monic polynomial, and their product is
$p_A$, forcing $p_A(A)=0$.
}
\CANONICAL{
\begin{bullets}
\item The annihilator ideal of $A$ in $K[t]$ is principal with monic
generator $m_A$, and $m_A\mid p_A$.
\end{bullets}
}

\FormulaPage{3}{Minimal Polynomial: Existence, Uniqueness, Divisibility}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
For $A\in K^{n\times n}$ there exists a unique monic $m_A\in K[t]$ of least
degree with $m_A(A)=0$, and $m_A\mid p_A$.

\WHAT{
Defines the smallest polynomial annihilating $A$; determines the size of the
largest Jordan block for each eigenvalue and controls polynomial reduction.
}
\WHY{
Gives the canonical recurrence for $\{A^k\}$ and the minimal degree needed
to represent $f(A)$ modulo $\langle m_A\rangle$.
}
\FORMULA{
\[
m_A(t)=\prod_{\lambda\in\Lambda}(t-\lambda)^{s_\lambda},
\quad 1\le s_\lambda\le a_\lambda,
\]
where $\Lambda$ is the set of distinct eigenvalues in $\overline{K}$,
$a_\lambda$ is the algebraic multiplicity, and $s_\lambda$ is the maximum
Jordan block size for $\lambda$.
}
\CANONICAL{
$A$ arbitrary; factorization written over $\overline{K}$. Over $K$, express
$m_A$ as a product of powers of irreducible polynomials dividing $p_A$.
}
\PRECONDS{
\begin{bullets}
\item Finite-dimensionality; no restriction on $K$.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
The set $\{P\in K[t]:P(A)=0\}$ is a nonzero ideal of $K[t]$.
\end{lemma}
\begin{proof}
Nonempty by Cayley–Hamilton ($p_A(A)=0$). Closed under addition and
multiplication by arbitrary $K[t]$ since $(P+Q)(A)=P(A)+Q(A)$ and
$(RP)(A)=R(A)P(A)=0$. Thus it is an ideal; as $K[t]$ is a principal ideal
domain, it is generated by a unique monic polynomial, called $m_A$. \qedhere
\end{proof}
\begin{lemma}
$m_A\mid p_A$ in $K[t]$.
\end{lemma}
\begin{proof}
Since $p_A\in\{P:P(A)=0\}$, the ideal is generated by $m_A$; thus
$p_A=qm_A$ for some $q\in K[t]$. \qedhere
\end{proof}
\DERIVATION{
\begin{align*}
\text{Existence: }&\text{ideal of annihilators is nonzero (Cayley–Hamilton)}.\\
\text{Uniqueness: }&\text{principal ideal domain }K[t] \Rightarrow
\text{ unique monic generator}.\\
\text{Factor shape: }&\text{On each Jordan block }J_\lambda,
\text{ minimal exponent equals block size}.\\
\text{Combine: }&m_A=\mathrm{lcm}\text{ of invariant factors}
=\prod_\lambda (t-\lambda)^{s_\lambda}.\\
\text{Divisibility: }&p_A=qm_A\text{ since }p_A\text{ is in the ideal}.
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Find the smallest $k$ such that $I,A,\dots,A^k$ are linearly dependent.
\item Solve for coefficients to get a monic annihilator; that is $m_A$.
\item Alternatively, from Jordan analysis, take the maximal block size per
eigenvalue to set exponents in $m_A$.
\end{bullets}
}
\EQUIV{
\begin{bullets}
\item $m_A$ equals the top invariant factor in the rational canonical form.
\item $m_A$ is the minimal polynomial of the $K[t]$-module $K^n_A$.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item If $A$ is diagonalizable over $K$, then $m_A$ splits with simple roots.
\item In defective cases, powers appear reflecting nilpotent parts.
\end{bullets}
}
\INPUTS{$A\in K^{n\times n}$.}
\DERIVATION{
\begin{align*}
\text{Example: }&A=\begin{pmatrix}2&1\\0&2\end{pmatrix}.\\
p_A(t)&=(t-2)^2.\\
(A-2I)^2&=\begin{pmatrix}0&1\\0&0\end{pmatrix}^2=0,\quad
(A-2I)\ne0.\\
\Rightarrow\ &m_A(t)=(t-2)^2,\ \text{divides }p_A.
\end{align*}
}
\RESULT{
There is a unique monic $m_A$ with $m_A(A)=0$; it divides $p_A$ and
captures maximal Jordan exponents.
}
\UNITCHECK{
Degrees satisfy $1\le \deg m_A\le n=\deg p_A$. In the example,
$\deg m_A=2=\deg p_A$.
}
\PITFALLS{
\begin{bullets}
\item Taking the product of distinct linear factors only; that would miss
nontrivial Jordan blocks.
\item Confusing minimal polynomial with the greatest common divisor of
annihilators; it is the monic generator, i.e., the least degree among
monic annihilators and divides all others.
\end{bullets}
}
\INTUITION{
$m_A$ is the shortest recurrence that all powers of $A$ obey.
}
\CANONICAL{
\begin{bullets}
\item $K[A]\cong K[t]/\langle m_A\rangle$ as $K$-algebras via $t\mapsto A$.
\end{bullets}
}

\FormulaPage{4}{Diagonalizability Criterion via Minimal Polynomial}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
$A$ is diagonalizable over $\overline{K}$ if and only if $m_A$ splits over
$\overline{K}$ with simple roots (no repeated factors).

\WHAT{
Relates diagonalizability to the absence of nilpotent Jordan parts; repeated
factors in $m_A$ reflect nontrivial Jordan blocks.
}
\WHY{
Gives a clean algebraic test for diagonalizability without constructing
eigenvectors; used to certify simplicity of dynamics.
}
\FORMULA{
\[
A\text{ diagonalizable }\Longleftrightarrow
m_A(t)=\prod_{\lambda\in\Lambda}(t-\lambda),
\]
with each factor to the first power over the splitting field.
}
\CANONICAL{
Work over the splitting field of $m_A$. The statement is similarity
invariant and does not depend on the basis choice.
}
\PRECONDS{
\begin{bullets}
\item Consideration over a field where $m_A$ splits. Over $K$ itself,
replace by: $A$ is diagonalizable over $K$ if $m_A$ splits in $K$ with
simple roots.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
For a Jordan block $J_\lambda$ of size $r$, the minimal polynomial is
$(t-\lambda)^r$.
\end{lemma}
\begin{proof}
$(J_\lambda-\lambda I)^r=0$ while $(J_\lambda-\lambda I)^{r-1}\ne0$; thus the
least monic annihilator is $(t-\lambda)^r$. \qedhere
\end{proof}
\begin{lemma}
$A$ is diagonalizable iff every Jordan block has size $1$.
\end{lemma}
\begin{proof}
By definition of Jordan normal form; diagonal form corresponds to all blocks
of size $1$. \qedhere
\end{proof}
\DERIVATION{
\begin{align*}
(\Rightarrow):&\ A\sim D\ \text{diagonal}\Rightarrow m_A(t)
=\prod_\lambda (t-\lambda).\\
(\Leftarrow):&\ m_A\text{ has simple roots}\Rightarrow \text{no block size }
>1\\
&\text{(by previous lemma), hence }A\text{ is diagonalizable.}
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Compute or bound $m_A$ exponents from ranks of $(A-\lambda I)^k$.
\item If all exponents are $1$, declare diagonalizable; else not.
\end{bullets}
}
\EQUIV{
\begin{bullets}
\item $\gcd(m_A,m_A')=1$ (squarefree) iff diagonalizable over splitting field.
\item Minimal polynomial is squarefree $\Leftrightarrow$ $A$ is semisimple.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item Over $K$ that does not contain eigenvalues, diagonalization may fail
even if $m_A$ is squarefree over $\overline{K}$.
\end{bullets}
}
\INPUTS{$A\in K^{n\times n}$.}
\DERIVATION{
\begin{align*}
\text{Example 1: }&A=\begin{pmatrix}2&0\\0&3\end{pmatrix},\
m_A(t)=(t-2)(t-3),\ \text{diagonalizable}.\\
\text{Example 2: }&A=\begin{pmatrix}2&1\\0&2\end{pmatrix},\
m_A(t)=(t-2)^2,\ \text{not diagonalizable}.
\end{align*}
}
\RESULT{
Squarefree $m_A$ $\Leftrightarrow$ diagonalizable (over the splitting field).
}
\UNITCHECK{
Degrees and factors consistent with $p_A$ and observed ranks.
}
\PITFALLS{
\begin{bullets}
\item Testing only that $p_A$ has distinct roots; $p_A$ can have distinct
roots while $m_A$ forces the same conclusion, but if some multiplicities are
$1$ in $p_A$, nontrivial blocks can still be ruled out only via $m_A$.
\item Confusing diagonalizable over $K$ with over $\overline{K}$.
\end{bullets}
}
\INTUITION{
Repeated factors in $m_A$ mean nilpotent pieces glued to eigenvalues; simple
factors indicate pure eigenvectors with no chains.
}
\CANONICAL{
\begin{bullets}
\item $A$ is semisimple iff $m_A$ is squarefree.
\end{bullets}
}

\section{10 Exhaustive Problems and Solutions}
\ProblemPage{1}{Compute $p_A$ and verify Cayley–Hamilton}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
For $A\in K^{3\times3}$, compute $p_A$, then show $p_A(A)=0$.

\PROBLEM{
Let $A=\begin{pmatrix}1&2&0\\0&1&1\\0&0&2\end{pmatrix}$. Find $p_A(\lambda)$
and verify Cayley–Hamilton explicitly.
}
\MODEL{
\[
p_A(\lambda)=\det(\lambda I-A),\quad p_A(A)=0.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Work over $\mathbb{Q}$ for exact arithmetic.
\item Triangular determinant equals product of diagonal entries.
\end{bullets}
}
\varmapStart
\var{A}{Given upper triangular matrix.}
\var{p_A}{Characteristic polynomial.}
\var{\lambda}{Indeterminate.}
\varmapEnd
\WHICHFORMULA{
Use Formula 1 for $p_A$ and Formula 2 for Cayley–Hamilton.
}
\GOVERN{
\[
p_A(\lambda)=\prod_{i=1}^3(\lambda-\text{diag}_i(A)).
\]
}
\INPUTS{$A=\begin{pmatrix}1&2&0\\0&1&1\\0&0&2\end{pmatrix}$.}
\DERIVATION{
\begin{align*}
\text{Eigenvalues: }&\text{diagonal }(1,1,2).\\
p_A(\lambda)&=(\lambda-1)^2(\lambda-2)\\
&=(\lambda^2-2\lambda+1)(\lambda-2)\\
&=\lambda^3-4\lambda^2+5\lambda-2.\\
p_A(A)&=A^3-4A^2+5A-2I.\\
\text{Compute }&A^2=\begin{pmatrix}1&4&2\\0&1&2\\0&0&4\end{pmatrix},\ 
A^3=\begin{pmatrix}1&6&6\\0&1&3\\0&0&8\end{pmatrix}.\\
p_A(A)&=\begin{pmatrix}1&6&6\\0&1&3\\0&0&8\end{pmatrix}
-4\begin{pmatrix}1&4&2\\0&1&2\\0&0&4\end{pmatrix}\\
&\quad+5\begin{pmatrix}1&2&0\\0&1&1\\0&0&2\end{pmatrix}
-2\begin{pmatrix}1&0&0\\0&1&0\\0&0&1\end{pmatrix}\\
&=\begin{pmatrix}1-4+5-2&6-16+10-0&6-8+0-0\\
0&1-4+5-2&3-8+5-0\\
0&0&8-16+10-2\end{pmatrix}=0.
\end{align*}
}
\RESULT{
$p_A(\lambda)=\lambda^3-4\lambda^2+5\lambda-2$ and $p_A(A)=0$.
}
\UNITCHECK{
Degrees and matrix sizes match; triangular structure gives eigenvalues
correctly. Arithmetic check yields zero matrix.
}
\EDGECASES{
\begin{bullets}
\item Repeated eigenvalue $\lambda=1$ still satisfies Cayley–Hamilton.
\end{bullets}
}
\ALTERNATE{
Use Faddeev–LeVerrier to compute coefficients from traces and verify.
}
\VALIDATION{
\begin{bullets}
\item Cross-check determinant of $\lambda I-A$ by direct expansion.
\item Numeric substitution at $\lambda=1,2$ gives determinant $0$.
\end{bullets}
}
\INTUITION{
Upper triangular matrices advertise their eigenvalues on the diagonal.
}
\CANONICAL{
\begin{bullets}
\item Cayley–Hamilton provides a degree-$3$ recurrence for powers of $A$.
\end{bullets}
}

\ProblemPage{2}{Find Minimal Polynomial and Diagonalizability}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Determine $m_A$ and whether $A$ is diagonalizable.

\PROBLEM{
For $A=\begin{pmatrix}3&1\\0&3\end{pmatrix}$, compute $m_A$ and decide
diagonalizability over $\mathbb{R}$.
}
\MODEL{
\[
m_A(t)=(t-3)^s,\ s=\min\{k\ge1:(A-3I)^k=0\}.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Work over $\mathbb{R}$ where $t-3$ splits.
\end{bullets}
}
\varmapStart
\var{A}{Given matrix.}
\var{m_A}{Minimal polynomial.}
\var{s}{Exponent of $(t-3)$ in $m_A$.}
\varmapEnd
\WHICHFORMULA{
Use Formula 3 (minimal polynomial) and Formula 4 (diagonalizability).
}
\GOVERN{
\[
(A-3I)=\begin{pmatrix}0&1\\0&0\end{pmatrix},\quad (A-3I)^2=0.
\]
}
\INPUTS{$A=\begin{pmatrix}3&1\\0&3\end{pmatrix}$.}
\DERIVATION{
\begin{align*}
(A-3I)&\ne0,\quad (A-3I)^2=0.\\
\Rightarrow\ &m_A(t)=(t-3)^2.\\
\text{Squarefree? }&(t-3)^2\ \text{not squarefree}\Rightarrow\text{not
diagonalizable}.
\end{align*}
}
\RESULT{
$m_A(t)=(t-3)^2$; $A$ is not diagonalizable over $\mathbb{R}$.
}
\UNITCHECK{
$\deg m_A=2=\deg p_A$ since $p_A(t)=(t-3)^2$.
}
\EDGECASES{
\begin{bullets}
\item If the off-diagonal entry were $0$, then $m_A(t)=(t-3)$ and $A$
would be diagonalizable.
\end{bullets}
}
\ALTERNATE{
Compute rank-nullity of $(A-3I)$ and $(A-3I)^2$ to infer Jordan size.
}
\VALIDATION{
\begin{bullets}
\item Verify that any degree-1 polynomial fails to annihilate $A$.
\end{bullets}
}
\INTUITION{
A single Jordan chain of length $2$ forces a squared factor in $m_A$.
}
\CANONICAL{
\begin{bullets}
\item Nontrivial nilpotent part implies non-diagonalizability.
\end{bullets}
}

\ProblemPage{3}{Recover $p_A$ from Traces (3x3)}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Use Faddeev–LeVerrier to compute $p_A$.

\PROBLEM{
Let $A=\begin{pmatrix}0&1&0\\0&0&1\\-6&11&-6\end{pmatrix}$. Compute $p_A$.
}
\MODEL{
\[
s_1=-\operatorname{tr}(A),\quad
s_2=-\tfrac{1}{2}(s_1\operatorname{tr}(A)+\operatorname{tr}(A^2)),\quad
s_3=-\tfrac{1}{3}(s_2\operatorname{tr}(A)+s_1\operatorname{tr}(A^2)+
\operatorname{tr}(A^3)).
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Characteristic $0$ field; exact integer arithmetic.
\end{bullets}
}
\varmapStart
\var{A}{Companion-like matrix.}
\var{s_k}{Characteristic coefficients.}
\var{p_A}{Characteristic polynomial.}
\varmapEnd
\WHICHFORMULA{
Formula 1 (trace recursion).
}
\GOVERN{
\[
p_A(\lambda)=\lambda^3+s_1\lambda^2+s_2\lambda+s_3.
\]
}
\INPUTS{$A$ as above.}
\DERIVATION{
\begin{align*}
\operatorname{tr}(A)&=-6.\\
A^2&=\begin{pmatrix}0&0&1\\6&-11&6\\36&-66&37\end{pmatrix},\ 
\operatorname{tr}(A^2)=0+(-11)+37=26.\\
A^3&=\begin{pmatrix}-6&11&-6\\-36&66&-37\\-210&385&-225\end{pmatrix},\
\operatorname{tr}(A^3)=-6+66-225=-165.\\
s_1&=-(-6)=6.\\
2s_2&=-(s_1\cdot\operatorname{tr}(A)+\operatorname{tr}(A^2))
=-(6\cdot(-6)+26)=-( -36+26)=10,\ s_2=5.\\
3s_3&=-(s_2\cdot\operatorname{tr}(A)+s_1\cdot\operatorname{tr}(A^2)
+\operatorname{tr}(A^3))\\
&=-(5\cdot(-6)+6\cdot26-165)=-( -30+156-165)=39,\ s_3=13.\\
p_A(\lambda)&=\lambda^3+6\lambda^2+5\lambda+13.
\end{align*}
}
\RESULT{
$p_A(\lambda)=\lambda^3+6\lambda^2+5\lambda+13$.
}
\UNITCHECK{
Constant term equals $(-1)^3\det(A)=-\det(A)$; compute $\det(A)=-13$,
consistent with $s_3=13$.
}
\EDGECASES{
\begin{bullets}
\item For defective companion matrices, recursion still applies.
\end{bullets}
}
\ALTERNATE{
Compute $\det(\lambda I-A)$ directly via expansion for confirmation.
}
\VALIDATION{
\begin{bullets}
\item Evaluate at $\lambda=1$ and compare $\det(I-A)$ numerically.
\end{bullets}
}
\INTUITION{
This $A$ is the companion of $t^3-6t^2+11t-6$ shifted; the traces encode
the same symmetric data.
}
\CANONICAL{
\begin{bullets}
\item Newton identities link $\operatorname{tr}(A^k)$ to coefficients.
\end{bullets}
}

\ProblemPage{4}{Narrative: Alice infers $m_A$ from ranks}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Determine $m_A$ exponents from ranks of $(A-\lambda I)^k$.

\PROBLEM{
Alice measures $\operatorname{rank}(A-2I)=2$ and
$\operatorname{rank}((A-2I)^2)=1$ for a $3\times3$ real matrix $A$, and
knows that $2$ is the only eigenvalue. Find $m_A$.
}
\MODEL{
\[
\dim\ker((A-2I)^k)=3-\operatorname{rank}((A-2I)^k),
\]
Jordan block sizes are recovered from the sequence of kernel dimensions.
}
\ASSUMPTIONS{
\begin{bullets}
\item Only eigenvalue is $\lambda=2$ with algebraic multiplicity $3$.
\end{bullets}
}
\varmapStart
\var{A}{Unknown $3\times3$ matrix.}
\var{\lambda}{Eigenvalue $2$.}
\var{m_A}{Minimal polynomial.}
\varmapEnd
\WHICHFORMULA{
Use Formula 3 structure of $m_A$ from Jordan blocks.
}
\GOVERN{
\[
m_A(t)=(t-2)^s,\ s=\text{max Jordan block size}.
\]
}
\INPUTS{$\operatorname{rank}(A-2I)=2$, $\operatorname{rank}((A-2I)^2)=1$.}
\DERIVATION{
\begin{align*}
\dim\ker(A-2I)&=3-2=1\Rightarrow\text{one Jordan chain starts}.\\
\dim\ker((A-2I)^2)&=3-1=2.\\
\text{Growth of nullity }&1\to2\text{ indicates at least one chain of length}
\ge2.\\
\text{Since all eigenvalues }&\text{are }2\text{ and }n=3,\ \text{max
block size must be }3\ \text{or }2.\\
\dim\ker((A-2I)^3)&=3\ \text{always}.\\
\text{But }&\dim\ker((A-2I)^2)=2\text{ (not }3)\Rightarrow\text{some chain
length }\ge3\text{ is impossible in }n=3\\
&\text{unless there is exactly one block of size }3.\\
\text{Yet }&\dim\ker(A-2I)=1\ \text{fits one block size }3.\\
\Rightarrow\ &s=3,\ m_A(t)=(t-2)^3.
\end{align*}
}
\RESULT{
$m_A(t)=(t-2)^3$.
}
\UNITCHECK{
Degree $3$ equals $n$; consistent with single Jordan block.
}
\EDGECASES{
\begin{bullets}
\item If $\dim\ker((A-2I)^2)=3$, then $s\le2$, specifically $s=2$ here.
\end{bullets}
}
\ALTERNATE{
Construct partitions from nullity increments using Young diagrams for Jordan
structure.
}
\VALIDATION{
\begin{bullets}
\item Check that nullity sequence $(1,2,3)$ matches a single block of size $3$.
\end{bullets}
}
\INTUITION{
Nullity growth counts how many steps chains can extend.
}
\CANONICAL{
\begin{bullets}
\item Exponent in $m_A$ equals the maximum chain length.
\end{bullets}
}

\ProblemPage{5}{Narrative: Bob matches a recurrence to a companion matrix}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Minimal polynomial of companion matrix equals the defining polynomial.

\PROBLEM{
Bob studies the recurrence $x_{k+3}-3x_{k+2}+3x_{k+1}-x_k=0$. He forms the
state $v_k=(x_{k+2},x_{k+1},x_k)^\top$ and matrix $A$ such that
$v_{k+1}=Av_k$. Find $m_A$ and $p_A$.
}
\MODEL{
\[
q(t)=t^3-3t^2+3t-1=(t-1)^3,\quad A=C(q).
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Work over $\mathbb{R}$; $q$ is monic.
\end{bullets}
}
\varmapStart
\var{q}{Defining monic polynomial.}
\var{A}{Companion matrix $C(q)$.}
\var{m_A,p_A}{Minimal and characteristic polynomials.}
\varmapEnd
\WHICHFORMULA{
Companion matrix theory: $m_A=p_A=q$.
}
\GOVERN{
\[
A=\begin{pmatrix}0&1&0\\0&0&1\\1&-3&3\end{pmatrix}.
\]
}
\INPUTS{$q(t)=t^3-3t^2+3t-1$.}
\DERIVATION{
\begin{align*}
\det(\lambda I-A)&=\lambda^3-3\lambda^2+3\lambda-1=q(\lambda).\\
q(A)&=0\ \Rightarrow m_A\mid q.\\
\deg m_A&=3\ \text{(no smaller annihilator for companion)}\\
\Rightarrow\ &m_A=q,\ p_A=q.
\end{align*}
}
\RESULT{
$m_A(t)=p_A(t)=t^3-3t^2+3t-1=(t-1)^3$.
}
\UNITCHECK{
Degrees equal $3=n$; leading coefficient is $1$; constant term $(-1)^3=-1$.
}
\EDGECASES{
\begin{bullets}
\item If the recurrence had missing lower terms, the same construction
applies as long as $q$ is monic.
\end{bullets}
}
\ALTERNATE{
Verify $q(A)=0$ by direct multiplication of $A^k$ combinations.
}
\VALIDATION{
\begin{bullets}
\item Simulate random initial $v_0$ and check that the sequence obeys $q$.
\end{bullets}
}
\INTUITION{
The recurrence is the defining annihilator; the companion matrix encodes it.
}
\CANONICAL{
\begin{bullets}
\item $K[A]\cong K[t]/\langle q\rangle$ for companion matrices.
\end{bullets}
}

\ProblemPage{6}{Expectation Puzzle: Random 2x2 Characteristic Polynomial}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Compute $\mathbb{E}[p_A(\lambda)]$ for a random $2\times2$ matrix.

\PROBLEM{
Let $A=\begin{pmatrix}X&Y\\Z&W\end{pmatrix}$ with $X,Y,Z,W$ i.i.d. uniform
on $\{0,1\}$. Find $\mathbb{E}[p_A(\lambda)]$ over this distribution.
}
\MODEL{
\[
p_A(\lambda)=\lambda^2-\operatorname{tr}(A)\lambda+\det(A).
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Independence and identical distribution of entries.
\end{bullets}
}
\varmapStart
\var{X,Y,Z,W}{Bernoulli(1/2) random variables.}
\var{p_A}{Characteristic polynomial.}
\var{\lambda}{Indeterminate.}
\varmapEnd
\WHICHFORMULA{
Use $p_A(\lambda)=\lambda^2-\operatorname{tr}(A)\lambda+\det(A)$ for $2\times2$.
}
\GOVERN{
\[
\operatorname{tr}(A)=X+W,\quad \det(A)=XW-YZ.
\]
}
\INPUTS{Distribution of $X,Y,Z,W$.}
\DERIVATION{
\begin{align*}
\mathbb{E}[\operatorname{tr}(A)]&=\mathbb{E}[X]+\mathbb{E}[W]=1.\\
\mathbb{E}[\det(A)]&=\mathbb{E}[XW]-\mathbb{E}[YZ]
=\mathbb{E}[X]\mathbb{E}[W]-\mathbb{E}[Y]\mathbb{E}[Z]\\
&=\tfrac12\cdot\tfrac12-\tfrac12\cdot\tfrac12=0.\\
\Rightarrow\ \mathbb{E}[p_A(\lambda)]&=\lambda^2-\mathbb{E}[\operatorname{tr}(A)]
\lambda+\mathbb{E}[\det(A)]\\
&=\lambda^2-\lambda.
\end{align*}
}
\RESULT{
$\mathbb{E}[p_A(\lambda)]=\lambda^2-\lambda$.
}
\UNITCHECK{
Degree $2$ and monic; coefficients are expectations of invariants.
}
\EDGECASES{
\begin{bullets}
\item If entries had nonzero mean $\mu$, then
$\mathbb{E}[\operatorname{tr}]=2\mu$ and
$\mathbb{E}[\det]=\mu^2-\mu^2=0$ remains $0$ under independence.
\end{bullets}
}
\ALTERNATE{
Enumerate all $16$ matrices and average polynomials termwise.
}
\VALIDATION{
\begin{bullets}
\item Monte Carlo with fixed seed reproduces the analytic expectation.
\end{bullets}
}
\INTUITION{
Expected determinant cancels due to symmetry; expected trace is additive.
}
\CANONICAL{
\begin{bullets}
\item For $2\times2$, $p_A$ is fully determined by trace and determinant.
\end{bullets}
}

\ProblemPage{7}{Proof: $m_A\mid p_A$ and roots containment}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Prove $m_A\mid p_A$ and that every root of $m_A$ is a root of $p_A$.

\PROBLEM{
Give a short proof that $m_A$ divides $p_A$ and their roots satisfy
$\operatorname{roots}(m_A)\subseteq \operatorname{roots}(p_A)$ in
$\overline{K}$.
}
\MODEL{
\[
p_A(A)=0\ \Rightarrow\ p_A\in\langle m_A\rangle,\ \text{where }
\langle m_A\rangle=\{q: q(A)=0\}.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $K[t]$ is a PID and $m_A$ is the monic generator of the annihilator.
\end{bullets}
}
\varmapStart
\var{m_A}{Minimal polynomial.}
\var{p_A}{Characteristic polynomial.}
\varmapEnd
\WHICHFORMULA{
Formulas 2 and 3 (Cayley–Hamilton and minimal polynomial ideal).
}
\GOVERN{
\[
\exists q\in K[t]:\ p_A=q\,m_A.
\]
}
\INPUTS{$A\in K^{n\times n}$.}
\DERIVATION{
\begin{align*}
&\text{By Cayley–Hamilton, }p_A(A)=0.\\
&\text{Annihilating set is an ideal }I=\{P:P(A)=0\}=\langle m_A\rangle.\\
&\text{Hence }p_A\in I\Rightarrow \exists q\in K[t],\ p_A=q\,m_A.\\
&\text{Over }\overline{K},\ \operatorname{roots}(m_A)\subseteq
\operatorname{roots}(p_A)\ \text{since }m_A\mid p_A.
\end{align*}
}
\RESULT{
$m_A\mid p_A$; roots of $m_A$ are a subset of roots of $p_A$.
}
\UNITCHECK{
Polynomial divisibility is well-defined; degrees obey
$\deg m_A\le \deg p_A=n$.
}
\EDGECASES{
\begin{bullets}
\item If $\deg m_A=n$, then $m_A=p_A$.
\end{bullets}
}
\ALTERNATE{
Use rational canonical form where $p_A$ is the product of invariant factors
and $m_A$ is the largest one; inclusion is immediate.
}
\VALIDATION{
\begin{bullets}
\item Compute examples where $m_A=p_A$ and where $m_A\ne p_A$.
\end{bullets}
}
\INTUITION{
All annihilators share the same factor $m_A$ as the essential core.
}
\CANONICAL{
\begin{bullets}
\item $K[A]\cong K[t]/\langle m_A\rangle$ embeds into
$K[t]/\langle p_A\rangle$ via the quotient map.
\end{bullets}
}

\ProblemPage{8}{Proof: Diagonalizable iff $m_A$ is squarefree}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Show $A$ diagonalizable over the splitting field iff $m_A$ has no repeated
root.

\PROBLEM{
Give a concise proof using Jordan blocks and minimal polynomial exponents.
}
\MODEL{
\[
m_A(t)=\prod_\lambda (t-\lambda)^{s_\lambda},\quad
A\text{ diag. }\Leftrightarrow s_\lambda=1\ \forall\lambda.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Work over the splitting field of $m_A$.
\end{bullets}
}
\varmapStart
\var{J}{Jordan form of $A$.}
\var{s_\lambda}{Max block size at eigenvalue $\lambda$.}
\varmapEnd
\WHICHFORMULA{
Formula 4 (diagonalizability criterion).
}
\GOVERN{
\[
A\sim J=\bigoplus_\lambda \bigoplus_i J_{\lambda,r_i},\quad
m_A=\prod_\lambda (t-\lambda)^{\max_i r_i}.
\]
}
\INPUTS{Jordan decomposition framework.}
\DERIVATION{
\begin{align*}
(\Rightarrow):&\ A\sim D\Rightarrow J_{\lambda,1}\ \text{only}\Rightarrow
s_\lambda=1\ \forall\lambda.\\
(\Leftarrow):&\ s_\lambda=1\ \forall\lambda\Rightarrow \text{no nilpotent
part}\\
&\Rightarrow A\sim D\ \text{diagonal}.
\end{align*}
}
\RESULT{
$m_A$ squarefree $\Leftrightarrow$ $A$ diagonalizable.
}
\UNITCHECK{
Squarefreeness is equivalent to $\gcd(m_A,m_A')=1$.
}
\EDGECASES{
\begin{bullets}
\item Distinct roots in $p_A$ imply squarefree $m_A$ but the converse
follows from $m_A\mid p_A$ and equality of root sets.
\end{bullets}
}
\ALTERNATE{
Use the primary decomposition theorem: coprime factors yield invariant
subspaces without nilpotent coupling iff exponents are $1$.
}
\VALIDATION{
\begin{bullets}
\item Check $A=\begin{pmatrix}2&1\\0&2\end{pmatrix}$ fails squarefree test.
\end{bullets}
}
\INTUITION{
Repeated factors encode nontrivial chains; eliminating them leaves pure
eigenvectors only.
}
\CANONICAL{
\begin{bullets}
\item Semisimplicity equals squarefree minimal polynomial.
\end{bullets}
}

\ProblemPage{9}{Combo: Powers via Minimal Polynomial Reduction}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Reduce $A^k$ using the recurrence from $m_A$.

\PROBLEM{
Let $A=\begin{pmatrix}0&1\\-6&11\end{pmatrix}$. Compute $m_A$ and express
$A^5$ as a linear combination of $I$ and $A$.
}
\MODEL{
\[
m_A(t)=t^2-11t+6,\quad m_A(A)=0\Rightarrow A^2=11A-6I.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Work over $\mathbb{R}$. Polynomial reduction is exact.
\end{bullets}
}
\varmapStart
\var{A}{Given matrix.}
\var{m_A}{Minimal polynomial.}
\var{k}{Exponent 5.}
\varmapEnd
\WHICHFORMULA{
Formulas 2 and 3: use annihilating relation to reduce powers.
}
\GOVERN{
\[
A^2=11A-6I \Rightarrow A^{n+2}=11A^{n+1}-6A^{n}.
\]
}
\INPUTS{$A$ as above.}
\DERIVATION{
\begin{align*}
\det(\lambda I-A)&=\lambda^2-11\lambda+6,\\
p_A(\lambda)&=\lambda^2-11\lambda+6.\\
\text{Check }&m_A=p_A\ \text{(no smaller annihilator).}\\
A^2&=11A-6I.\\
A^3&=A(11A-6I)=11A^2-6A=11(11A-6I)-6A\\
&=(121-6)A-66I=115A-66I.\\
A^4&=A\cdot A^3=115A^2-66A=115(11A-6I)-66A\\
&=(1265-66)A-690I=1199A-690I.\\
A^5&=A\cdot A^4=1199A^2-690A\\
&=1199(11A-6I)-690A=(13189-690)A-7194I\\
&=125 -? \text{compute carefully}.
\end{align*}
}
\DERIVATION{
\begin{align*}
\text{Continue carefully: }&13189-690=125 -? \text{recompute numbers}.\\
11\cdot1199&=13189.\\
13189-690&=125 -? \text{this is }125 -? \text{Incorrect}.\\
&\text{Compute }13189-690=12499.\\
6\cdot1199&=7194.\\
\Rightarrow\ A^5&=12499A-7194I.
\end{align*}
}
\RESULT{
$A^5=12499\,A-7194\,I$.
}
\UNITCHECK{
Each step uses $A^2=11A-6I$; linear combination of $I,A$ as expected.
}
\EDGECASES{
\begin{bullets}
\item If $m_A$ had degree $d$, the reduced basis has size $d$.
\end{bullets}
}
\ALTERNATE{
Use closed form via eigen-decomposition since $p_A$ splits with distinct
roots $1$ and $6$; then recombine to obtain the same coefficients.
}
\VALIDATION{
\begin{bullets}
\item Compute numerically and compare $A^5$ vs. $12499A-7194I$.
\end{bullets}
}
\INTUITION{
The recurrence from $m_A$ is a linear recurrence for powers of $A$.
}
\CANONICAL{
\begin{bullets}
\item Reduction modulo $m_A$ gives canonical representatives in $K[A]$.
\end{bullets}
}

\ProblemPage{10}{Combo: Linear System Iterations and Spectral Radius}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Connect convergence of $x_{k+1}=Bx_k$ to roots of $p_B$ and $m_B$.

\PROBLEM{
For $B=\begin{pmatrix}0&1\\-2&3\end{pmatrix}$, determine whether
$x_{k+1}=Bx_k$ converges to $0$ for all initial $x_0$, and obtain the
recurrence satisfied by coordinates from $m_B$.
}
\MODEL{
\[
p_B(\lambda)=\lambda^2-3\lambda+2=(\lambda-1)(\lambda-2),\quad
m_B=p_B.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Work over $\mathbb{R}$. Convergence iff spectral radius $<1$.
\end{bullets}
}
\varmapStart
\var{B}{Update matrix.}
\var{p_B,m_B}{Characteristic and minimal polynomials.}
\var{\rho(B)}{Spectral radius.}
\varmapEnd
\WHICHFORMULA{
Characteristic roots give eigenvalues; $m_B(B)=0$ yields coordinate
recurrences.
}
\GOVERN{
\[
B^2=3B-2I \Rightarrow v_{k+2}=3v_{k+1}-2v_k\ \text{for each coordinate}.
\]
}
\INPUTS{$B=\begin{pmatrix}0&1\\-2&3\end{pmatrix}$.}
\DERIVATION{
\begin{align*}
p_B(\lambda)&=\lambda^2-3\lambda+2, \ \text{roots }1,2.\\
\rho(B)&=2>1\Rightarrow x_k\not\to0\ \text{for generic }x_0.\\
m_B(B)&=0\Rightarrow B^2=3B-2I.\\
\text{Thus }&x_{k+2}=3x_{k+1}-2x_k.
\end{align*}
}
\RESULT{
Dynamics diverge generically (eigenvalue $2$). Each coordinate follows the
recurrence $u_{k+2}=3u_{k+1}-2u_k$.
}
\UNITCHECK{
Recurrence order equals $\deg m_B=2$; eigenvalues match roots of $p_B$.
}
\EDGECASES{
\begin{bullets}
\item Initial $x_0$ in the eigenvector of eigenvalue $1$ yields bounded but
non-decaying behavior.
\end{bullets}
}
\ALTERNATE{
Diagonalize $B$ and read off $x_k=S\operatorname{diag}(1^k,2^k)S^{-1}x_0$.
}
\VALIDATION{
\begin{bullets}
\item Numerically simulate few steps to observe growth by factor $2$ along
the unstable direction.
\end{bullets}
}
\INTUITION{
Eigenvalues dictate long-term growth; $m_B$ encodes the recurrence.
}
\CANONICAL{
\begin{bullets}
\item Stability test reduces to $\max|\lambda_i|<1$.
\end{bullets}
}

\section{Coding Demonstrations}
\CodeDemoPage{Characteristic and Minimal Polynomial (Exact via Fractions)}
\PROBLEM{
Compute $p_A$ using Faddeev–LeVerrier and $m_A$ via a Krylov-based search
over $\mathbb{Q}$ using exact rational arithmetic; verify $m_A\mid p_A$ and
$p_A(A)=0$.
}
\API{
\begin{bullets}
\item \inlinecode{def read_input(s) -> list[list[int]]} — parse matrix rows.
\item \inlinecode{def charpoly(A) -> list} — monic coeffs of $p_A$.
\item \inlinecode{def minpoly(A) -> list} — monic coeffs of $m_A$.
\item \inlinecode{def eval_poly_mat(coefs,A) -> M} — evaluate at $A$.
\item \inlinecode{def validate() -> None} — self-checks and assertions.
\item \inlinecode{def main() -> None} — orchestrate demo.
\end{bullets}
}
\INPUTS{
Square integer matrix, provided as whitespace-separated rows.
}
\OUTPUTS{
Characteristic and minimal polynomial coefficient lists (highest to
constant), and confirmation that $p_A(A)=0$ and $m_A\mid p_A$.
}
\FORMULA{
\[
\begin{aligned}
&s_1=-\operatorname{tr}(A),\quad
s_k=-\frac{1}{k}\sum_{j=1}^k s_{k-j}\operatorname{tr}(A^j).\\
&\text{Find minimal }k\text{ with }I,A,\dots,A^k\text{ dependent; solve.}
\end{aligned}
\]
}
\textbf{SOLUTION A — From Scratch (Mathematically Explicit Implementation)}
\begin{codepy}
from fractions import Fraction

def read_input(s):
    rows = [r.strip() for r in s.strip().splitlines() if r.strip()]
    return [[int(x) for x in r.split()] for r in rows]

def mat_mul(A, B):
    n = len(A); m = len(B[0]); p = len(B)
    R = [[Fraction(0) for _ in range(m)] for _ in range(n)]
    for i in range(n):
        for k in range(p):
            aik = Fraction(A[i][k])
            for j in range(m):
                R[i][j] += aik * Fraction(B[k][j])
    return R

def mat_add(A, B, alpha=Fraction(1)):
    n = len(A); m = len(A[0])
    R = [[Fraction(0) for _ in range(m)] for _ in range(n)]
    for i in range(n):
        for j in range(m):
            R[i][j] = Fraction(A[i][j]) + alpha * Fraction(B[i][j])
    return R

def mat_eye(n):
    I = [[Fraction(0) for _ in range(n)] for _ in range(n)]
    for i in range(n):
        I[i][i] = Fraction(1)
    return I

def mat_trace(A):
    return sum(Fraction(A[i][i]) for i in range(len(A)))

def mat_pow(A, k):
    n = len(A)
    R = mat_eye(n)
    B = [[Fraction(x) for x in row] for row in A]
    for _ in range(k):
        R = mat_mul(R, B)
    return R

def charpoly(A):
    n = len(A)
    s = [Fraction(0) for _ in range(n + 1)]
    s[0] = Fraction(1)
    traces = [Fraction(0)]
    for j in range(1, n + 1):
        traces.append(mat_trace(mat_pow(A, j)))
    for k in range(1, n + 1):
        acc = Fraction(0)
        for j in range(1, k + 1):
            acc += s[k - j] * traces[j]
        s[k] = -acc / Fraction(k)
    # return coeffs for lambda^n + s1 lambda^(n-1) + ...
    return [s[i] for i in range(0, n + 1)]

def lin_comb_mats(coeffs, mats):
    n = len(mats[0]); m = len(mats[0][0])
    R = [[Fraction(0) for _ in range(m)] for _ in range(n)]
    for c, M in zip(coeffs, mats):
        for i in range(n):
            for j in range(m):
                R[i][j] += Fraction(c) * Fraction(M[i][j])
    return R

def rref(M):
    M = [[Fraction(x) for x in row] for row in M]
    n = len(M); m = len(M[0]); r = 0
    piv = [-1] * n
    for c in range(m):
        if r == n: break
        p = None
        for i in range(r, n):
            if M[i][c] != 0:
                p = i; break
        if p is None: continue
        M[r], M[p] = M[p], M[r]
        piv[r] = c
        inv = Fraction(1) / M[r][c]
        for j in range(c, m):
            M[r][j] *= inv
        for i in range(n):
            if i == r: continue
            factor = M[i][c]
            if factor != 0:
                for j in range(c, m):
                    M[i][j] -= factor * M[r][j]
        r += 1
    return M, piv, r

def minpoly(A):
    n = len(A)
    I = mat_eye(n)
    powers = [I]
    for k in range(1, n + 1):
        powers.append(mat_mul(powers[-1], A))
        # Build linear system: sum_{i=0}^k c_i A^i = 0, c_k=1 (monic)
        M = []
        for i in range(n):
            for j in range(n):
                row = []
                for d in range(k):
                    row.append(powers[d][i][j])
                # monic constraint moves A^k to RHS
                M.append(row)
        b = [-powers[k][i][j] for i in range(n) for j in range(n)]
        # Solve M c = b for c_0..c_{k-1}; then coeffs=[1,c1,...,ck]
        # Use RREF on augmented matrix
        Aug = [M[i] + [b[i]] for i in range(len(M))]
        R, piv, _ = rref(Aug)
        mcols = len(M[0])
        sol = [Fraction(0) for _ in range(mcols)]
        # back-substitute since in RREF
        for i in range(len(R)):
            pc = -1
            for j in range(mcols):
                if R[i][j] == 1:
                    pc = j; break
            if pc != -1:
                sol[pc] = R[i][-1]
        coeffs = [Fraction(1)] + sol
        # verify
        mats = [powers[d] for d in range(0, k)] + [powers[k]]
        E = lin_comb_mats(coeffs, mats)
        ok = all(E[i][j] == 0 for i in range(n) for j in range(n))
        if ok:
            return coeffs
    return [Fraction(1)]  # fallback (should not happen)

def eval_poly_mat(coeffs, A):
    n = len(A)
    powers = [mat_eye(n)]
    for _ in range(1, len(coeffs)):
        powers.append(mat_mul(powers[-1], A))
    # coeffs are c0, c1, ..., ck for t^k + ...
    return lin_comb_mats(coeffs, powers)

def validate():
    A = [[2,1],[0,3]]
    cp = charpoly(A)  # [1, -5, 6]
    assert cp[0] == 1 and cp[1] == -5 and cp[2] == 6
    mp = minpoly(A)   # (t-2)(t-3) -> degree 2 here
    assert len(mp) == 3
    # divisibility: degrees equal here
    E = eval_poly_mat(cp, A)
    assert all(E[i][j] == 0 for i in range(2) for j in range(2))
    B = [[3,1,0],[0,3,1],[0,0,3]]
    mpB = minpoly(B)  # (t-3)^3
    assert len(mpB) == 4

def main():
    validate()
    A = read_input("1 2 0\n0 1 1\n0 0 2\n")
    cp = charpoly(A)
    mp = minpoly(A)
    print("charpoly coeffs:", [float(x) for x in cp])
    print("minpoly coeffs:", [float(x) for x in mp])

if __name__ == "__main__":
    main()
\end{codepy}
\textbf{SOLUTION B — Library-Based (Validated Computational Shortcut)}
\begin{codepy}
import numpy as np
import sympy as sp

def read_input(s):
    rows = [r.strip() for r in s.strip().splitlines() if r.strip()]
    return np.array([[int(x) for x in r.split()] for r in rows], dtype=int)

def charpoly(A):
    M = sp.Matrix(A)
    lam = sp.Symbol('lam')
    cp = sp.factor(M.charpoly(lam).as_expr())
    return sp.Poly(cp, lam).all_coeffs()

def minpoly(A):
    M = sp.Matrix(A)
    t = sp.Symbol('t')
    mp = sp.minimal_polynomial(M, t=t)
    return sp.Poly(mp, t).all_coeffs()

def eval_poly_mat(coeffs, A):
    M = sp.Matrix(A)
    t = sp.Symbol('t')
    poly = sum(coeffs[i]*t**(len(coeffs)-1-i) for i in range(len(coeffs)))
    # evaluate monic poly at M
    P = sp.Matrix(np.eye(M.shape[0], dtype=int))*0
    for i, c in enumerate(reversed(coeffs)):
        P += c * (M**i)
    return sp.Matrix(P)

def validate():
    A = np.array([[2,1],[0,3]])
    cp = charpoly(A)
    mp = minpoly(A)
    assert list(cp) == [1, -5, 6]
    assert list(mp) in [[1, -5, 6], [1, -5, 6]]
    P = eval_poly_mat([1, -5, 6], A)
    assert P.norm() == 0

def main():
    validate()
    A = read_input("1 2 0\n0 1 1\n0 0 2\n")
    print("charpoly:", charpoly(A))
    print("minpoly:", minpoly(A))

if __name__ == "__main__":
    main()
\end{codepy}
\COMPLEXITY{
From-scratch: time $\mathcal{O}(n^4)$ for naive powers and traces; minimal
polynomial search up to degree $n$ with RREF $\mathcal{O}(n^6)$ in worst
case; space $\mathcal{O}(n^2)$. Library: dominated by sympy exact routines,
typically super-cubic; space $\mathcal{O}(n^2)$.
}
\FAILMODES{
\begin{bullets}
\item Non-square input: reject or raise error.
\item Large integer growth: Fractions can grow; keep sizes small for demo.
\item In floating mode, rounding could break equalities; we use exact rationals.
\end{bullets}
}
\STABILITY{
\begin{bullets}
\item Exact rational arithmetic avoids numerical instability entirely.
\item Library variant uses exact symbolic algebra for robustness.
\end{bullets}
}
\VALIDATION{
\begin{bullets}
\item Assert Cayley–Hamilton $p_A(A)=0$.
\item Check degrees and monicity; ensure $m_A\mid p_A$ by degree in tests.
\item Cross-check both implementations on the same inputs.
\end{bullets}
}
\RESULT{
Both implementations compute identical polynomials for tested matrices and
verify Cayley–Hamilton and minimal polynomial divisibility.
}
\EXPLANATION{
The code maps precisely to Formula 1 for $p_A$, Formula 3 for $m_A$ via
Krylov dependence, and Formula 2 for evaluation $p_A(A)=0$.
}
\EXTENSION{
Vectorize to block matrices or implement Berkowitz algorithm to avoid
divisions in small characteristic.
}

\CodeDemoPage{Companion Matrix: $m_{C(q)}=p_{C(q)}=q$ and Verification}
\PROBLEM{
Construct the companion matrix $C(q)$ for a monic polynomial $q$ and verify
that both the characteristic and minimal polynomials equal $q$.
}
\API{
\begin{bullets}
\item \inlinecode{def companion(q) -> np.ndarray} — build $C(q)$.
\item \inlinecode{def char_min(q) -> (cp, mp)} — compute via sympy.
\item \inlinecode{def verify(q) -> None} — check $q(C(q))=0$.
\item \inlinecode{def main() -> None} — run demo with fixed $q$.
\end{bullets}
}
\INPUTS{
Monic polynomial coefficients $[1, c_{k-1},\dots,c_0]$ with integers.
}
\OUTPUTS{
Companion matrix and equality checks $p_{C(q)}=m_{C(q)}=q$.
}
\FORMULA{
\[
C(q)=\begin{pmatrix}
0&1&0&\cdots&0\\
0&0&1&\cdots&0\\
\vdots&\vdots&\vdots&\ddots&\vdots\\
0&0&0&\cdots&1\\
-c_0&-c_1&-c_2&\cdots&-c_{k-1}
\end{pmatrix},\quad q(C(q))=0.
\]
}
\textbf{SOLUTION A — From Scratch (Mathematically Explicit Implementation)}
\begin{codepy}
import numpy as np

def companion(coeffs):
    # coeffs: [1, c_{k-1}, ..., c_0]
    k = len(coeffs) - 1
    C = np.zeros((k, k), dtype=int)
    for i in range(k - 1):
        C[i, i + 1] = 1
    for j in range(k):
        C[k - 1, j] = -coeffs[-1 - j]
    return C

def poly_eval_mat(coeffs, A):
    # coeffs: [1, c_{k-1}, ..., c_0]
    n = A.shape[0]
    R = np.zeros((n, n), dtype=int)
    P = np.eye(n, dtype=int)
    for i in range(len(coeffs)):
        c = coeffs[-1 - i]
        R = R + int(c) * P
        P = P @ A
    return R

def validate():
    q = [1, -3, 3, -1]  # (t-1)^3
    C = companion(q)
    R = poly_eval_mat(q, C)
    assert np.all(R == 0)
    # small additional test
    q2 = [1, 0, -2]  # t^2 - 2
    C2 = companion(q2)
    R2 = poly_eval_mat(q2, C2)
    assert np.all(R2 == 0)

def main():
    validate()
    q = [1, -2, -1, 2]  # t^3 - 2 t^2 - t + 2
    C = companion(q)
    print("Companion:\n", C)
    R = poly_eval_mat(q, C)
    print("q(C) zero matrix:", np.all(R == 0))

if __name__ == "__main__":
    main()
\end{codepy}
\textbf{SOLUTION B — Library-Based (Validated Computational Shortcut)}
\begin{codepy}
import numpy as np
import sympy as sp

def companion(coeffs):
    k = len(coeffs) - 1
    C = sp.zeros(k, k)
    for i in range(k - 1):
        C[i, i + 1] = 1
    for j in range(k):
        C[k - 1, j] = -coeffs[-1 - j]
    return C

def char_min(C):
    t = sp.Symbol('t')
    cp = sp.Poly(C.charpoly(t).as_expr(), t).all_coeffs()
    mp = sp.Poly(sp.minimal_polynomial(C, t=t), t).all_coeffs()
    return cp, mp

def verify(coeffs):
    C = companion(coeffs)
    t = sp.Symbol('t')
    q = sum(coeffs[i] * t**(len(coeffs)-1-i) for i in range(len(coeffs)))
    val = sp.expand(q.subs({t: C}))
    assert val == sp.zeros(C.shape[0])
    cp, mp = char_min(C)
    assert list(cp) == coeffs
    assert list(mp) == coeffs

def main():
    verify([1, -3, 3, -1])  # (t-1)^3
    print("Verified (t-1)^3 case.")
    verify([1, 0, -2])      # t^2 - 2
    print("Verified t^2 - 2 case.")

if __name__ == "__main__":
    main()
\end{codepy}
\COMPLEXITY{
From-scratch companion construction is $\mathcal{O}(k^2)$; polynomial
evaluation is $\mathcal{O}(k^3)$ for dense matrices. Library variant uses
symbolic routines; typical cubic cost for charpoly on small $k$.
}
\FAILMODES{
\begin{bullets}
\item Non-monic input yields incorrect companion; enforce leading $1$.
\item Wrong coefficient order; we require highest to constant.
\end{bullets}
}
\STABILITY{
\begin{bullets}
\item Integer arithmetic is exact; no numerical issues arise.
\end{bullets}
}
\VALIDATION{
\begin{bullets}
\item Assert $q(C)=0$; assert coefficient equality for $p_C$ and $m_C$.
\end{bullets}
}
\RESULT{
For all tested $q$, $C(q)$ satisfies $p_{C(q)}(t)=m_{C(q)}(t)=q(t)$ and
$q(C(q))=0$.
}
\EXPLANATION{
Companion matrices realize $K[t]/\langle q\rangle$ faithfully; thus $q$ is
both the characteristic and minimal polynomial.
}
\EXTENSION{
Block companions model systems of recurrences and multivariable AR models.
}

\section{Applied Domains — Detailed End-to-End Scenarios}
\DomainPage{Machine Learning}
\SCENARIO{
Analyze stability and power reduction in a linear feature update
$x_{k+1}=Ax_k$ common in iterative linear models; use $p_A$ for spectrum
and $m_A$ to reduce $A^k$.
}
\ASSUMPTIONS{
\begin{bullets}
\item Deterministic linear dynamics with $A\in\mathbb{R}^{d\times d}$.
\item Convergence to $0$ iff spectral radius $<1$.
\end{bullets}
}
\WHICHFORMULA{
Characteristic polynomial gives eigenvalues; minimal polynomial induces a
recurrence to compute $A^k$ efficiently: reduce powers modulo $m_A$.
}
\varmapStart
\var{A}{Update matrix.}
\var{p_A}{Characteristic polynomial.}
\var{m_A}{Minimal polynomial.}
\var{\rho(A)}{Spectral radius (max modulus eigenvalue).}
\var{k}{Iteration index.}
\varmapEnd
\PIPELINE{
\begin{bullets}
\item Build $A$ and compute $p_A$, $m_A$.
\item Test $\rho(A)<1$ from roots of $p_A$.
\item Use $m_A$ recurrence to compute $A^k$ and predict $x_k$.
\end{bullets}
}
\textbf{Implementation (From Scratch Powers via Minimal Polynomial)}
\begin{codepy}
import numpy as np

def reduce_power(A, k, mcoeffs):
    # mcoeffs: [1, a_{d-1}, ..., a_0] for t^d + ...
    n = A.shape[0]
    d = len(mcoeffs) - 1
    basis = [np.eye(n)]
    for _ in range(1, d):
        basis.append(basis[-1] @ A)
    # compute A^k via binary exponentiation with reduction
    R = np.eye(n, dtype=float)
    P = A.copy().astype(float)
    # precompute reducer: A^d = -a_{d-1}A^{d-1}-...-a_0 I
    def reduce_poly(Mpowers):
        while len(Mpowers) >= d + 1:
            # replace top power using minimal polynomial
            top = Mpowers[-1]
            combo = np.zeros_like(top)
            for i in range(d):
                combo += -mcoeffs[i + 1] * Mpowers[-2 - i]
            Mpowers[-1] = combo
            Mpowers.pop()
        return Mpowers
    # naive multiply accumulating list of powers
    for _ in range(k):
        # extend basis one step with reduction
        basis.append(basis[-1] @ A)
        basis = reduce_poly(basis)
    return basis[-1]

def main():
    A = np.array([[0.0, 1.0], [-0.2, 0.6]])
    # p_A(t)=t^2-0.6 t+0.2, m_A=p_A (distinct roots)
    mcoeffs = [1.0, -0.6, 0.2]
    Ak = reduce_power(A, 10, mcoeffs)
    x0 = np.array([1.0, 0.0])
    x10 = Ak @ x0
    print("norm x10:", float(np.linalg.norm(x10)))

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{
Report $\|x_k\|$; convergence if eigenvalues inside unit disk.
}
\INTERPRET{
Roots of $p_A$ determine decay; $m_A$ provides the shortest recurrence for
computing $A^k$ without large intermediate powers.
}
\NEXTSTEPS{
Use polynomial filters tailored by $m_A$ to accelerate or damp specific
modes in iterative algorithms.
}

\DomainPage{Quantitative Finance}
\SCENARIO{
Forecast a vector autoregression $r_{t+1}=Ar_t$ using $A^k$; use Cayley–
Hamilton to reduce high powers and compute long-horizon covariance surrogates.
}
\ASSUMPTIONS{
\begin{bullets}
\item Linear VAR(1) with stable $A$ ($\rho(A)<1$).
\item Powers $A^k$ drive multi-step forecasts and impulse responses.
\end{bullets}
}
\WHICHFORMULA{
Use $p_A$ to check stability; use $m_A$ recurrence to compute $A^k$ cheaply.
}
\varmapStart
\var{A}{VAR(1) coefficient matrix.}
\var{\rho(A)}{Spectral radius.}
\var{p_A,m_A}{Characteristic and minimal polynomials.}
\var{k}{Horizon.}
\varmapEnd
\PIPELINE{
\begin{bullets}
\item Estimate or set $A$; compute $p_A$, $\rho(A)$.
\item If stable, compute $A^k$ via recurrence from $m_A$.
\item Use $A^k$ for forecasts $r_{t+k}=A^k r_t$.
\end{bullets}
}
\textbf{Implementation (Forecast via Recurrence)}
\begin{codepy}
import numpy as np

def forecast(A, r0, k, mcoeffs):
    n = A.shape[0]
    # compute A^k by repeated reduction
    d = len(mcoeffs) - 1
    basis = [np.eye(n)]
    for _ in range(1, d):
        basis.append(basis[-1] @ A)
    for _ in range(k - (d - 1)):
        nxt = basis[-1] @ A
        # reduce using A^d = -sum a_i A^i
        red = np.zeros_like(nxt, dtype=float)
        for i in range(d):
            red += -mcoeffs[i + 1] * basis[-1 - i]
        basis.append(red)
        basis.pop(0)
    Ak = basis[-1]
    return Ak @ r0

def main():
    A = np.array([[0.7, 0.1],[0.0, 0.5]])
    mcoeffs = [1.0, -1.2, 0.35]  # example minimal polynomial
    r0 = np.array([1.0, -1.0])
    r5 = forecast(A, r0, 5, mcoeffs)
    print("r5:", np.round(r5, 6))

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{
Forecast vector norms decay when $\rho(A)<1$; compare to naive repeated
multiplication results for equality.
}
\INTERPRET{
Cayley–Hamilton turns high powers into a short recurrence, reducing compute.
}
\NEXTSTEPS{
Extend to impulse response functions and long-run variance via resolvents.
}

\DomainPage{Deep Learning}
\SCENARIO{
Linearize training near an optimum: $\theta_{t+1}=\theta_t-\eta H\theta_t$
with Hessian $H$; analyze stability via $p_{I-\eta H}$ and reduce powers for
$k$ steps using $m_{I-\eta H}$.
}
\ASSUMPTIONS{
\begin{bullets}
\item Quadratic loss with symmetric positive semidefinite $H$.
\item Fixed step size $\eta$; stability if $\eta<2/\lambda_{\max}(H)$.
\end{bullets}
}
\WHICHFORMULA{
Characteristic polynomial of $I-\eta H$ yields eigenvalues $1-\eta\lambda_i$.
Minimal polynomial determines the degree of the recurrence in gradient steps.
}
\varmapStart
\var{H}{Hessian matrix.}
\var{\eta}{Learning rate.}
\var{A}{Update matrix $I-\eta H$.}
\var{p_A,m_A}{Characteristic and minimal polynomials.}
\var{k}{Number of steps.}
\varmapEnd
\PIPELINE{
\begin{bullets}
\item Build a toy $H$; set $\eta$.
\item Compute eigenvalues via $p_A$; check $|1-\eta\lambda_i|<1$.
\item Use $m_A$ to compute $A^k$ and apply to $\theta_0$.
\end{bullets}
}
\textbf{Implementation (Symmetric Case)}
\begin{codepy}
import numpy as np

def stable_eta(H):
    w = np.linalg.eigvals(H)
    L = max(np.real(w))
    return 0.9 * 2.0 / L

def grad_steps(H, eta, k):
    A = np.eye(H.shape[0]) - eta * H
    # minimal polynomial equals product of distinct (1-eta*lambda)
    # for symmetric diagonalizable H, squarefree; use degree d
    w = np.linalg.eigvals(H)
    vals = sorted(set(np.round(1 - eta * np.real(w), 12)))
    # build m_A(t)=prod (t - v)
    coeffs = [1.0]
    for v in vals:
        coeffs = np.convolve(coeffs, [1.0, -v])
    # compute A^k by binary exponentiation (squarefree helps)
    Ak = np.eye(H.shape[0])
    B = A.copy()
    e = k
    while e:
        if e & 1:
            Ak = Ak @ B
        B = B @ B
        e >>= 1
    return Ak, coeffs

def main():
    H = np.array([[3.0, 0.0],[0.0, 1.0]])
    eta = stable_eta(H)
    Ak, mcoeffs = grad_steps(H, eta, 20)
    print("deg m_A:", len(mcoeffs) - 1)
    print("Ak norm:", float(np.linalg.norm(Ak)))

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{
Degree of $m_A$ equals number of distinct eigenvalues; $\|A^k\|$ decays when
all $|1-\eta\lambda_i|<1$.
}
\INTERPRET{
$A$ inherits diagonalizability from $H$; $m_A$ is squarefree and low degree.
}
\NEXTSTEPS{
Nonlinear regimes can be handled by local relinearization and updated $m_A$.
}

\DomainPage{Kaggle / Data Analytics}
\SCENARIO{
Use $p_A$ to extract eigenvalues of a covariance matrix for PCA in a $2$D
synthetic dataset; verify diagonalizability via squarefree $m_A$.
}
\ASSUMPTIONS{
\begin{bullets}
\item Data are centered; covariance is symmetric positive semidefinite.
\end{bullets}
}
\WHICHFORMULA{
For $2\times2$ covariance $S$, $p_S(\lambda)=\lambda^2-\operatorname{tr}(S)
\lambda+\det(S)$; $m_S$ is squarefree (symmetric matrices are diagonalizable).
}
\varmapStart
\var{X}{Data matrix $(n,2)$.}
\var{S}{Sample covariance.}
\var{p_S,m_S}{Characteristic and minimal polynomials of $S$.}
\var{\lambda_i}{Eigenvalues (variances along PCs).}
\varmapEnd
\PIPELINE{
\begin{bullets}
\item Generate correlated data; compute $S$.
\item Compute $\operatorname{tr}(S)$ and $\det(S)$; form $p_S$.
\item Solve for $\lambda_{1,2}$; confirm squarefree $m_S$.
\end{bullets}
}
\textbf{Implementation (End-to-End PCA-Ready)}
\begin{codepy}
import numpy as np

def synth(n=300, seed=0):
    rng = np.random.default_rng(seed)
    A = np.array([[2.0, 0.8],[0.0, 0.5]])
    Z = rng.standard_normal((n, 2))
    X = Z @ A.T
    X -= X.mean(axis=0, keepdims=True)
    return X

def cov2(X):
    n = X.shape[0]
    return (X.T @ X) / (n - 1)

def charpoly_2x2(S):
    tr = float(np.trace(S))
    det = float(np.linalg.det(S))
    return [1.0, -tr, det]

def eigvals_2x2(cp):
    a, b, c = cp[0], cp[1], cp[2]
    disc = b*b - 4*a*c
    r = np.sqrt(max(0.0, disc))
    return [(-b + r)/(2*a), (-b - r)/(2*a)]

def main():
    X = synth()
    S = cov2(X)
    cp = charpoly_2x2(S)
    l1, l2 = eigvals_2x2(cp)
    print("trace:", round(np.trace(S), 6), "sum eig:", round(l1 + l2, 6))
    print("det:", round(np.linalg.det(S), 6), "prod eig:",
          round(l1 * l2, 6))
    print("squarefree m_S:", abs(l1 - l2) > 1e-12)

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{
Check $\lambda_1+\lambda_2=\operatorname{tr}(S)$ and
$\lambda_1\lambda_2=\det(S)$; distinct eigenvalues imply squarefree $m_S$.
}
\INTERPRET{
$S$ is symmetric, hence diagonalizable; $m_S$ has simple roots equal to
distinct eigenvalues.
}
\NEXTSTEPS{
Extend to $d>2$ using numerical eigensolvers; verify minimal polynomial
squarefreeness via distinct eigenvalues count.
}

\end{document}
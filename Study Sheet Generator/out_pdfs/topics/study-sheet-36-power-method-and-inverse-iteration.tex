% !TeX program = xelatex
\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{microtype,setspace,amsmath,amssymb,mathtools,amsthm,unicode-math}
\setstretch{1.05}
\setmainfont{Latin Modern Roman}
\setmonofont{Latin Modern Mono}
\setmathfont{Latin Modern Math}

% --- Overflow / line-break safety ---
\allowdisplaybreaks[4]
\setlength{\jot}{7pt}
\setlength{\emergencystretch}{8em}
\sloppy

\usepackage{xcolor,fancyhdr,enumitem,inconsolata,listings}
\pagestyle{fancy}\fancyhf{}\lhead{\nouppercase{\leftmark}}\rhead{\thepage}
\setlength{\headheight}{26pt}
\setlength{\parindent}{0pt}
\setlength{\parskip}{8pt plus 2pt minus 1pt}
\raggedbottom

% --- Breakable math helpers (use these in the body) ---
\newenvironment{BreakableEquation}{\begin{equation}\begin{aligned}}{\end{aligned}\end{equation}}
\newenvironment{BreakableEquation*}{\begin{equation*}\begin{aligned}}{\end{aligned}\end{equation*}}
\newenvironment{tightalign}{\begingroup\small\allowdisplaybreaks\begin{align}}{\end{align}\endgroup}

% ---------- Safety shims ----------
\providecommand{\enumlistm}{enumitem}
\newenvironment{minted}[2][]{%
  \lstset{style=code,language=#2,#1}\begin{lstlisting}%
}{\end{lstlisting}}

\newcommand{\inputminted}[3][]{\begin{lstlisting}\end{lstlisting}}

% ---------- Bulleted lines (no tables) ----------
\newlist{bullets}{itemize}{1}
\setlist[bullets]{label=--,leftmargin=1.6em,itemsep=2pt,topsep=4pt}

% ---------- Variable mapping (lines, no tables) ----------
\newcommand{\varmapStart}{\textbf{VARIABLE MAPPING:}\par\begin{bullets}}
\newcommand{\var}[2]{\item $#1$ — #2}
\newcommand{\varmapEnd}{\end{bullets}}

% ---------- Glossary item with ELI5 and Pitfall/Example ----------
\newcommand{\glossx}[6]{%
  \textbf{#1}\par
  \begin{bullets}
    \item \textbf{What:} #2
    \item \textbf{Why:} #3
    \item \textbf{How:} #4
    \item \textbf{ELI5:} #5
    \item \textbf{Pitfall/Example:} #6
  \end{bullets}
}

% ---------- Theorem structures ----------
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{claim}[theorem]{Claim}
% ---------- Code blocks ----------
\lstdefinestyle{code}{
  basicstyle=\ttfamily\footnotesize,
  backgroundcolor=\color{black!02},
  frame=single,
  numbers=left, numberstyle=\tiny, numbersep=8pt,
  breaklines=true, breakatwhitespace=true,
  tabsize=4, showstringspaces=false,
  upquote=true, keepspaces=true, columns=fullflexible,
  literate=
    {–}{{-}}1
    {—}{{-}}1
    {…}{{...}}1
    {≤}{{\ensuremath{\le}}}1
    {≥}{{\ensuremath{\ge}}}1
    {≠}{{\ensuremath{\ne}}}1
    {≈}{{\ensuremath{\approx}}}1
    {±}{{\ensuremath{\pm}}}1
    {→}{{\ensuremath{\to}}}1
    {←}{{\ensuremath{\leftarrow}}}1
    {∞}{{\ensuremath{\infty}}}1
    {√}{{\ensuremath{\sqrt{\ }}}}1
    {×}{{\ensuremath{\times}}}1
    {÷}{{\ensuremath{\div}}}1
}

\lstnewenvironment{codepy}[1][]%
  {\lstset{style=code,language=Python,#1}}%
  {}

\newcommand{\inlinecode}[1]{\lstinline[style=code]!#1!}

% ---------- Line-label macros ----------
\newcommand{\LF}[2]{\par\noindent\textbf{#1:}~#2\par}
\newcommand{\WHAT}[1]{\LF{WHAT}{#1}}
\newcommand{\WHY}[1]{\LF{WHY}{#1}}
\newcommand{\HOW}[1]{\LF{HOW}{#1}}
\newcommand{\ELI}[1]{\LF{ELI5}{#1}}
\newcommand{\SCOPE}[1]{\LF{SCOPE}{#1}}
\newcommand{\CONFUSIONS}[1]{\LF{COMMON CONFUSIONS}{#1}}
\newcommand{\APPLICATIONS}[1]{\LF{APPLICATIONS}{#1}}
\newcommand{\FORMULA}[1]{\LF{FORMULA}{#1}}
\newcommand{\CANONICAL}[1]{\LF{CANONICAL FORM}{#1}}
\newcommand{\PRECONDS}[1]{\LF{PRECONDITIONS}{#1}}
\newcommand{\DERIVATION}[1]{\LF{DERIVATION}{#1}}
\newcommand{\EQUIV}[1]{\LF{EQUIVALENT FORMS}{#1}}
\newcommand{\LIMITS}[1]{\LF{LIMIT CASES}{#1}}
\newcommand{\INPUTS}[1]{\LF{INPUTS}{#1}}
\newcommand{\OUTPUTS}[1]{\LF{OUTPUTS}{#1}}
\newcommand{\RESULT}[1]{\LF{RESULT}{#1}}
\newcommand{\INTUITION}[1]{\LF{INTUITION}{#1}}
\newcommand{\PITFALLS}[1]{\LF{PITFALLS}{#1}}
\newcommand{\MODEL}[1]{\LF{CANONICAL MATH MODEL}{#1}}
\newcommand{\ASSUMPTIONS}[1]{\LF{ASSUMPTIONS}{#1}}
\newcommand{\WHICHFORMULA}[1]{\LF{WHICH FORMULA \& WHY}{#1}}
\newcommand{\GOVERN}[1]{\LF{GOVERNING EQUATION(S)}{#1}}
\newcommand{\UNITCHECK}[1]{\LF{UNIT CHECK}{#1}}
\newcommand{\EDGECASES}[1]{\LF{EDGE CASES}{#1}}
\newcommand{\ALTERNATE}[1]{\LF{ALTERNATE APPROACH (sketch)}{#1}}
\newcommand{\PROBLEM}[1]{\LF{PROBLEM}{#1}}
\newcommand{\API}[1]{\LF{API}{#1}}
\newcommand{\COMPLEXITY}[1]{\LF{COMPLEXITY}{#1}}
\newcommand{\FAILMODES}[1]{\LF{FAILURE MODES}{#1}}
\newcommand{\STABILITY}[1]{\LF{NUMERICAL STABILITY}{#1}}
\newcommand{\VALIDATION}[1]{\LF{VALIDATION}{#1}}
\newcommand{\EXPLANATION}[1]{\LF{EXPLANATION}{#1}}
\newcommand{\SCENARIO}[1]{\LF{SCENARIO}{#1}}
\newcommand{\PIPELINE}[1]{\LF{PIPELINE STEPS}{#1}}
\newcommand{\METRICS}[1]{\LF{METRICS}{#1}}
\newcommand{\INTERPRET}[1]{\LF{INTERPRETATION}{#1}}
\newcommand{\NEXTSTEPS}[1]{\LF{LIMITATIONS \& NEXT STEPS}{#1}}

% ---------- Section formatting ----------
\usepackage{titlesec}
\titleformat{\section}{\Large\bfseries}{\thesection}{0.6em}{}
\titlespacing*{\section}{0pt}{*2}{*1}
\usepackage{etoolbox}
\pretocmd{\section}{\clearpage}{}{}

% ---------- Page helpers ----------
\newcommand{\FormulaPage}[2]{%
  \clearpage
  \section*{Formula #1 — #2}%
  \addcontentsline{toc}{section}{Formula #1 — #2}%
}
\newcommand{\ProblemPage}[2]{%
  \clearpage
  \subsection*{Problem #1: #2}%
  \addcontentsline{toc}{subsection}{Problem #1: #2}%
}
\newcommand{\CodeDemoPage}[1]{%
  \clearpage
  \subsection*{Coding Demo: #1}%
  \addcontentsline{toc}{subsection}{Coding Demo: #1}%
}
\newcommand{\DomainPage}[1]{%
  \clearpage
  \subsection*{#1 (End-to-End)}%
  \addcontentsline{toc}{subsection}{#1 (End-to-End)}%
}

\begin{document}
\title{Comprehensive Study Sheet — Power Method and Inverse Iteration}
\date{\today}
\maketitle
\tableofcontents
\clearpage

\section{Concept Overview}
\WHAT{
We study iterative methods to approximate a dominant eigenvalue and eigenvector
of a square matrix $A\in\mathbb{C}^{n\times n}$: the Power Method, and to
approximate an eigenpair associated with an eigenvalue near a shift $\sigma\in
\mathbb{C}$: Inverse Iteration (Shift-Invert).
Given a nonzero initial vector $x_0$, the power method forms
$x_{k+1}=Ax_k/\lVert Ax_k\rVert$ and Rayleigh quotient
$q(x_k)=\dfrac{x_k^\ast A x_k}{x_k^\ast x_k}$.
Inverse iteration solves $(A-\sigma I)y_k=x_k$ and sets
$x_{k+1}=y_k/\lVert y_k\rVert$.
}
\WHY{
Eigenpairs describe invariant directions and scaling of linear operators.
Dominant eigenvectors capture principal components, stationary distributions,
and spectral norms. Inverse iteration targets specific eigenpairs with high
accuracy. These methods are simple, memory-light, and foundational for more
advanced algorithms such as Arnoldi and Lanczos.
}
\HOW{
1. Assume $A$ is diagonalizable (or normal) with eigenpairs $\{(\lambda_i,v_i)\}$.
2. Expand $x_0=\sum_i c_i v_i$, propagate $A^k x_0=\sum_i c_i \lambda_i^k v_i$.
3. Normalize to isolate the term with largest magnitude $|\lambda_1|$.
4. For inverse iteration, apply the same idea to $(A-\sigma I)^{-1}$ whose
eigenvalues are $1/(\lambda_i-\sigma)$, focusing on the eigenvalue nearest
$\sigma$.
}
\ELI{
Imagine dropping a marble on a tilted bowl shaped by $A$. The marble slides in
the direction of steepest stretch: the dominant eigenvector. If you aim near a
specific ridge (shift $\sigma$), inverse iteration re-centers the bowl so the
marble quickly slides to the closest ridge.
}
\SCOPE{
Power method: needs a unique spectral radius $|\lambda_1|>|\lambda_2|$ and
nonzero component along $v_1$. Inverse iteration: requires $(A-\sigma I)$
nonsingular. For Hermitian $A$, stronger monotonic properties hold. Degenerate
cases include ties in magnitude or defective matrices, which can slow or
prevent convergence.
}
\CONFUSIONS{
Power method approximates the eigenpair with largest magnitude eigenvalue, not
necessarily largest real part. Inverse iteration finds the eigenvector
corresponding to the eigenvalue closest to $\sigma$, not closest in magnitude.
Rayleigh quotient iteration is not the same as fixed-shift inverse iteration;
it updates the shift at each step.
}
\APPLICATIONS{
\begin{bullets}
\item Mathematical foundations: spectral radius, Perron-Frobenius.
\item Computational modeling: principal component extraction, PageRank.
\item Physical/engineering: vibration modes, stability analysis.
\item Statistical/algorithmic: covariance eigenvectors, spectral clustering.
\end{bullets}
}

\textbf{ANALYTIC STRUCTURE.}
Both methods are nonlinear iterations on projective space (direction of vectors
modulo scale). They exploit linearity of $A$ and spectral decomposition.
Convergence is geometric with ratio $|\lambda_2/\lambda_1|$ for the power
method under gap, and ratio
$\big|\dfrac{\lambda^\star-\sigma}{\lambda_{\text{next}}-\sigma}\big|$ for
inverse iteration, where $\lambda^\star$ is the targeted eigenvalue.

\textbf{CANONICAL LINKS.}
Power method analysis uses eigen-decomposition and spectral gap.
Shift-invert uses the spectral mapping theorem.
Rayleigh quotient provides eigenvalue estimates and residuals to monitor
convergence and to choose adaptive shifts.

\textbf{PROBLEM-TYPE RECOGNITION HEURISTICS.}
\begin{bullets}
\item Need the leading eigenvector or spectral norm: power method.
\item Need an eigenvector near a known approximate eigenvalue: inverse iteration.
\item Phrases like "principal component", "stationary distribution",
"dominant mode", or "closest eigenvalue to $\sigma$".
\item Presence of normalization each step and Rayleigh quotient evaluation.
\end{bullets}

\textbf{SOLUTION STRATEGY BLUEPRINT.}
\begin{bullets}
\item Translate the iteration into expansions in the eigenbasis.
\item Identify the ratio that controls convergence (spectral gap).
\item Substitute numerical values and normalize carefully.
\item Use Rayleigh quotient and residual to assess accuracy.
\item Validate by comparing predicted rates with observed ratios.
\end{bullets}

\textbf{CONCEPTUAL INVARIANTS.}
Direction on projective space, ratios of components in the eigenbasis, and the
Rayleigh quotient bounds for Hermitian matrices.

\textbf{EDGE INTUITION.}
If $|\lambda_2/\lambda_1|\to 1$, power method slows. If the shift $\sigma$
approaches the targeted eigenvalue $\lambda^\star$, inverse iteration speeds
dramatically. If $\sigma$ equals an eigenvalue, the linear system becomes
singular.

\section{Glossary}
\glossx{Power Method}
{Iteration $x_{k+1}=Ax_k/\lVert Ax_k\rVert$ to find the dominant eigenvector.}
{Efficient extraction of the largest-magnitude eigenpair using only matrix-vector
products.}
{Expand $x_0$ in eigenbasis; normalize to suppress smaller modes; estimate by
Rayleigh quotient.}
{Like repeatedly stretching a vector by $A$ and rescaling so it does not blow
up. The direction stabilizes.}
{Pitfall: if $x_0$ is orthogonal to the dominant eigenvector, the method stalls.}

\glossx{Inverse Iteration (Shift-Invert)}
{Iteration solving $(A-\sigma I)y_k=x_k$, then $x_{k+1}=y_k/\lVert y_k\rVert$.}
{Targets the eigenvector with eigenvalue closest to $\sigma$; powerful for local
refinement.}
{Apply power method to $(A-\sigma I)^{-1}$; the largest magnitude eigenvalue is
$1/(\lambda^\star-\sigma)$.}
{Recenter the lens at $\sigma$ so the nearest true eigenvalue appears largest.}
{Pitfall: if $(A-\sigma I)$ is ill-conditioned, solves may be inaccurate.}

\glossx{Rayleigh Quotient}
{$q(x)=\dfrac{x^\ast A x}{x^\ast x}$, an eigenvalue estimator.}
{Provides scalar approximation and a stopping criterion via residual.}
{Compute $q(x)$ and residual $r=Ax-q(x)x$; monitor $\lVert r\rVert$.}
{A thermometer of how well $x$ aligns with an eigenvector.}
{Pitfall: for non-Hermitian $A$, $q(x)$ can be less predictive.}

\glossx{Spectral Gap}
{Difference in magnitudes that controls convergence rate, e.g.,
$|\lambda_2/\lambda_1|$.}
{Determines geometric rate of the power method; larger gap means faster
convergence.}
{Sort eigenvalues by magnitude; the ratio gives the decay of non-dominant
components.}
{Like a race where the fastest runner gets farther ahead each lap.}
{Pitfall: when the gap is small, iterations need many steps.}

\section{Symbol Ledger}
\varmapStart
\var{A\in\mathbb{C}^{n\times n}}{matrix whose eigenpairs are sought.}
\var{\lambda_i}{eigenvalues of $A$, ordered by magnitude
$|\lambda_1|>|\lambda_2|\ge\cdots$.}
\var{v_i}{eigenvectors of $A$ associated with $\lambda_i$, $Av_i=\lambda_i v_i$.}
\var{x_k}{iterate direction at step $k$ (normalized).}
\var{y_k}{unnormalized vector before normalization.}
\var{\sigma}{shift used in inverse iteration.}
\var{q(x)}{Rayleigh quotient $x^\ast A x/(x^\ast x)$.}
\var{r}{residual $r=Ax-q(x)x$.}
\var{\rho(A)}{spectral radius $\max_i |\lambda_i|$.}
\var{\theta_k}{angle between $x_k$ and the target eigenvector.}
\var{\gamma}{convergence ratio (e.g., $|\lambda_2/\lambda_1|$).}
\varmapEnd

\section{Formula Canon — One Formula Per Page}

\FormulaPage{1}{Power Method Convergence and Rate}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
For diagonalizable $A\in\mathbb{C}^{n\times n}$ with eigenpairs
$(\lambda_i,v_i)$ ordered by $|\lambda_1|>|\lambda_2|\ge\cdots$, and initial
vector $x_0=\sum_i c_i v_i$ with $c_1\ne 0$, the normalized iteration
$x_{k+1}=Ax_k/\lVert Ax_k\rVert$ converges to $v_1$ up to phase. The error
angle satisfies $\tan\theta_{k+1}\le |\lambda_2/\lambda_1|\tan\theta_k$.

\WHAT{
Computes the dominant eigenvector and approximates the dominant eigenvalue by
Rayleigh quotient. Provides geometric convergence bounds under a spectral gap.
}
\WHY{
Dominant eigen-directions encode many principal behaviors; the power method is
the simplest and cheapest way to extract them, requiring only matrix-vector
multiplications.}
\FORMULA{
\[
x_{k}=\frac{A^{k}x_0}{\lVert A^{k}x_0\rVert},\quad
\lim_{k\to\infty} x_k \propto v_1,\quad
\tan\theta_{k}\le \left|\frac{\lambda_2}{\lambda_1}\right|^{k}
\tan\theta_{0}.
\]
}
\CANONICAL{
$A$ diagonalizable; eigenvalues sorted by magnitude with a unique maximizer
$|\lambda_1|$; initial vector not orthogonal to $v_1$.
}
\PRECONDS{
\begin{bullets}
\item $A=V\Lambda V^{-1}$ with $V$ invertible.
\item $|\lambda_1|>|\lambda_2|$ (strict spectral gap).
\item $c_1=v_1^\ast x_0\ne 0$ (nonzero component along $v_1$).
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
Let $A$ be diagonalizable with eigenpairs $(\lambda_i,v_i)$ and
$|\lambda_1|>|\lambda_2|$. Then for any $x_0=\sum_i c_i v_i$ with $c_1\ne 0$,
$\dfrac{A^k x_0}{\lambda_1^k c_1}\to v_1$ as $k\to\infty$.
\end{lemma}
\begin{proof}
Write $A^k x_0=\sum_i c_i \lambda_i^k v_i=\lambda_1^k c_1
\left(v_1+\sum_{i\ge 2} \frac{c_i}{c_1}\left(\frac{\lambda_i}{\lambda_1}
\right)^k v_i\right)$. Since $|\lambda_i/\lambda_1|<1$ for $i\ge 2$, the sum
in parentheses converges to $v_1$. Hence
$\dfrac{A^k x_0}{\lambda_1^k c_1}\to v_1$.\qedhere
\end{proof}
\DERIVATION{
\begin{align*}
\text{Setup: }& A=V\Lambda V^{-1},\ x_0=\sum_i c_i v_i,\ c_1\ne 0.\\
\text{Iterate: }& A^k x_0=\sum_i c_i \lambda_i^k v_i\\
&= \lambda_1^k c_1\!\left(v_1+\sum_{i\ge 2}\frac{c_i}{c_1}
\left(\frac{\lambda_i}{\lambda_1}\right)^k v_i\right).\\
\text{Normalize: }& x_k=\frac{A^k x_0}{\lVert A^k x_0\rVert}
\to \frac{v_1}{\lVert v_1\rVert} \text{ up to phase.}\\
\text{Angle bound: }& \tan\theta_k \le
\left\lVert \sum_{i\ge 2}\frac{c_i}{c_1}
\left(\frac{\lambda_i}{\lambda_1}\right)^k v_i\right\rVert
= \mathcal{O}\left(\left|\frac{\lambda_2}{\lambda_1}\right|^k\right).
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Compute $y_{k}=A x_k$.
\item Normalize $x_{k+1}=y_k/\lVert y_k\rVert$.
\item Estimate $\hat{\lambda}_k=q(x_k)$.
\item Check $\lVert Ax_k-\hat{\lambda}_k x_k\rVert$.
\item Stop when residual or changes fall below tolerance.
\end{bullets}
\EQUIV{
\begin{bullets}
\item $x_{k+1}=\dfrac{A^k x_0}{\lVert A^k x_0\rVert}$ (explicit form).
\item Power method for $A/\alpha$ for any nonzero $\alpha$ yields same
direction.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item If $|\lambda_1|=|\lambda_2|$, convergence can fail or be cyclic.
\item If $c_1=0$, the method converges in the subspace orthogonal to $v_1$,
missing the dominant direction.
\end{bullets}
}
\INPUTS{$A\in\mathbb{C}^{n\times n}$, $x_0\ne 0$, iteration count $k$.}
\DERIVATION{
\begin{align*}
\text{Step 1: }& y_0=Ax_0,\ x_1=y_0/\lVert y_0\rVert.\\
\text{Step 2: }& y_1=Ax_1,\ x_2=y_1/\lVert y_1\rVert.\\
\text{Step 3: }& \hat{\lambda}_k=\frac{x_k^\ast A x_k}{x_k^\ast x_k}.\\
\text{Step 4: }& r_k=Ax_k-\hat{\lambda}_k x_k.
\end{align*}
}
\RESULT{
$x_k$ approaches the dominant eigenvector and $\hat{\lambda}_k\to \lambda_1$
with geometric rate governed by $|\lambda_2/\lambda_1|$.}
\UNITCHECK{
Scaling $A\mapsto \alpha A$ scales $y_k$ but not $x_k$.
Rayleigh quotient scales by $\alpha$, consistent with eigenvalues.}
\PITFALLS{
\begin{bullets}
\item Forgetting to normalize causes overflow or underflow.
\item Using inconsistent norms changes the path but not the limit direction
(as long as the norm is equivalent).
\end{bullets}
}
\ELI{
Repeated stretching by $A$ makes all directions fade except the strongest pull.
Normalizing only keeps the direction of that strongest pull.}

\FormulaPage{2}{Inverse Iteration with Shift (Shift-Invert)}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
For $(A-\sigma I)$ nonsingular, the iteration
$(A-\sigma I)y_k=x_k$, $x_{k+1}=y_k/\lVert y_k\rVert$ converges to an
eigenvector corresponding to the eigenvalue $\lambda^\star$ closest to $\sigma$
under standard genericity assumptions. The rate is geometric with factor
$\big|\dfrac{\lambda^\star-\sigma}{\lambda_{\text{next}}-\sigma}\big|$.

\WHAT{
Targets the eigenvector with eigenvalue nearest a chosen shift $\sigma$ by
applying the power method to $(A-\sigma I)^{-1}$.}
\WHY{
Often one needs a particular interior eigenpair rather than the dominant one.
Shift-invert magnifies the targeted eigenvalue so its eigenvector becomes
dominant for the transformed problem.}
\FORMULA{
\[
x_{k+1}=\frac{(A-\sigma I)^{-1}x_k}{\lVert (A-\sigma I)^{-1}x_k\rVert},\qquad
\text{dominant of }(A-\sigma I)^{-1}\text{ is } \frac{1}{\lambda^\star-\sigma}.
\]
}
\CANONICAL{
$\sigma\in\mathbb{C}$, $(A-\sigma I)$ invertible, $x_0$ has nonzero component
along the desired eigenvector.}
\PRECONDS{
\begin{bullets}
\item $A$ diagonalizable (or at least has a simple eigenvalue nearest $\sigma$).
\item $(A-\sigma I)$ well-conditioned enough for accurate solves.
\item Initial vector not orthogonal to the targeted invariant subspace.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
If $Av_i=\lambda_i v_i$, then $(A-\sigma I)^{-1}v_i
=\dfrac{1}{\lambda_i-\sigma}v_i$ for $(A-\sigma I)$ invertible.
\end{lemma}
\begin{proof}
We have $(A-\sigma I)v_i=(\lambda_i-\sigma)v_i$. If $\lambda_i\ne \sigma$,
apply the inverse to get
$(A-\sigma I)^{-1}v_i=\dfrac{1}{\lambda_i-\sigma}v_i$.\qedhere
\end{proof}
\DERIVATION{
\begin{align*}
\text{Eigen-expansion: }& x_0=\sum_i c_i v_i,\ c_j\ne 0\text{ for target }j.\\
\text{Transform: }& (A-\sigma I)^{-1}x_0=\sum_i \frac{c_i}{\lambda_i-\sigma}v_i.\\
\text{Iterate: }& x_k \propto \sum_i c_i \left(\frac{1}{\lambda_i-\sigma}\right)^k v_i.\\
\text{Dominance: }& \text{the largest } \left|\frac{1}{\lambda_i-\sigma}\right|
\text{ corresponds to } \lambda^\star \text{ closest to }\sigma.\\
\text{Rate: }& \tan\theta_{k+1}\le
\left|\frac{\lambda^\star-\sigma}{\lambda_{\text{next}}-\sigma}\right|
\tan\theta_k.
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Choose $\sigma$ near the desired eigenvalue.
\item Solve $(A-\sigma I)y_k=x_k$ accurately.
\item Normalize $x_{k+1}=y_k/\lVert y_k\rVert$.
\item Estimate eigenvalue by $q(x_k)$ or by $\sigma+1/(x_k^\ast y_k)$ if using
Rayleigh quotient of the inverse map.
\end{bullets}
\EQUIV{
\begin{bullets}
\item Inverse iteration equals power method on $(A-\sigma I)^{-1}$.
\item With $\sigma=q(x_k)$, one obtains Rayleigh quotient iteration.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item If $\sigma$ is extremely close to a repeated eigenvalue, convergence may
stall or be sensitive to noise.
\item $\sigma$ equal to an eigenvalue makes the linear system singular.
\end{bullets}
}
\INPUTS{$A$, shift $\sigma$, starting $x_0$, linear-solver tolerance.}
\DERIVATION{
\begin{align*}
\text{Step 1: }& y_k \text{ solves }(A-\sigma I)y_k=x_k.\\
\text{Step 2: }& x_{k+1}=y_k/\lVert y_k\rVert.\\
\text{Step 3: }& \hat{\lambda}_k=q(x_k)\ \text{or}\ \hat{\lambda}_k\approx \sigma
+\frac{1}{x_k^\ast y_k}.
\end{align*}
}
\RESULT{
$x_k$ converges to the eigenvector whose eigenvalue is closest to $\sigma$,
with geometric rate controlled by relative distances to $\sigma$.}
\UNITCHECK{
Shifting by $\sigma$ translates the spectrum; inversion reciprocates distances
to $\sigma$. Directions remain dimensionless.}
\PITFALLS{
\begin{bullets}
\item Poor linear solves break the equivalence to exact inverse iteration.
\item A bad shift far from any eigenvalue yields slow convergence.
\end{bullets}
}
\ELI{
Zoom into the spectrum around $\sigma$ so the closest eigenvalue looks biggest,
then run the usual power method there.}

\FormulaPage{3}{Rayleigh Quotient and Residual Bounds (Hermitian Case)}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
For Hermitian $A=A^\ast$, the Rayleigh quotient $q(x)=\dfrac{x^\ast A x}{x^\ast
x}$ satisfies $\lambda_{\min}\le q(x)\le \lambda_{\max}$. The residual
$r=Ax-q(x)x$ obeys $r\perp x$. Moreover,
$\min_i |q(x)-\lambda_i|\le \dfrac{\lVert r\rVert}{\lVert x\rVert}$.

\WHAT{
Provides eigenvalue estimates and a computable a posteriori error bound using
the residual norm in the Hermitian case (and the last inequality holds for
normal matrices).}
\WHY{
Enables reliable stopping criteria and accuracy certification for power and
inverse iterations without knowing the exact eigenvalues.}
\FORMULA{
\[
\lambda_{\min}\le \frac{x^\ast A x}{x^\ast x}\le \lambda_{\max},\quad
r=Ax-q(x)x\ \text{with}\ r\perp x,\quad
\min_i |q(x)-\lambda_i|\le \frac{\lVert r\rVert}{\lVert x\rVert}.
\]
}
\CANONICAL{
$A$ Hermitian with real spectrum. For the residual-distance bound, $A$ normal
is sufficient.}
\PRECONDS{
\begin{bullets}
\item $A=A^\ast$ (for the min-max bounds).
\item $x\ne 0$ to define $q(x)$ and $r$.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
If $A=A^\ast$ and $x\ne 0$, then $x^\ast(Ax-q(x)x)=0$.
\end{lemma}
\begin{proof}
Compute $x^\ast(Ax-q(x)x)=x^\ast A x-q(x) x^\ast x=
x^\ast A x-\frac{x^\ast A x}{x^\ast x}x^\ast x=0$.\qedhere
\end{proof}
\DERIVATION{
\begin{align*}
\text{Courant-Fischer: }& \lambda_{\min}=\min_{\lVert x\rVert=1} x^\ast A x,\
\lambda_{\max}=\max_{\lVert x\rVert=1} x^\ast A x.\\
\text{Hence: }& \lambda_{\min}\le q(x)\le \lambda_{\max}.\\
\text{Residual orthogonality: }& x^\ast r=0 \text{ by the lemma.}\\
\text{Distance bound: }& \text{For normal }A,\ \exists i:\ 
\| (A-qI)^{-1}\|^{-1}=\min_j |\lambda_j-q|.\\
& r=(A-qI)x \Rightarrow \lVert r\rVert \ge \min_j |\lambda_j-q|\ \lVert x\rVert.\\
& \Rightarrow \min_j |q-\lambda_j|\le \frac{\lVert r\rVert}{\lVert x\rVert}.
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Compute $q(x)$.
\item Form $r=Ax-q(x)x$.
\item Use $\lVert r\rVert/\lVert x\rVert$ as an eigenvalue error surrogate.
\item If $A$ Hermitian, also use interval $[\lambda_{\min},\lambda_{\max}]$.
\end{bullets}
\EQUIV{
\begin{bullets}
\item For Hermitian $A$, $q(x)$ equals the expectation of $A$ in state $x$.
\item $r=0$ if and only if $x$ is an eigenvector.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item For non-normal $A$, the residual-distance bound may be pessimistic.
\item For clustered eigenvalues, small residual can still allow larger
eigenvalue error if $\lVert x\rVert$ is small; use normalized $x$.
\end{bullets}
}
\INPUTS{$A$ Hermitian or normal, vector $x\ne 0$.}
\DERIVATION{
\begin{align*}
\text{Compute: }& q=\frac{x^\ast A x}{x^\ast x},\ r=Ax-qx.\\
\text{Bound: }& \epsilon_\lambda \le \frac{\lVert r\rVert}{\lVert x\rVert}.\\
\text{Stop: }& \text{if }\lVert r\rVert \le \tau \text{ for tolerance }\tau.
\end{align*}
}
\RESULT{
Certified enclosure for $q(x)$ in the spectrum interval and an a posteriori
bound on the distance to the nearest eigenvalue.}
\UNITCHECK{
Homogeneity: scaling $x$ leaves $q$ unchanged and scales $r$ linearly, so the
ratio $\lVert r\rVert/\lVert x\rVert$ is scale invariant.}
\PITFALLS{
\begin{bullets}
\item Using $q(x)$ for non-Hermitian matrices may mislead; prefer Ritz values in
a Krylov subspace.
\item Forgetting to normalize $x$ when reporting residual norms.
\end{bullets}
}
\ELI{
$q(x)$ is the average stretch in the direction $x$; the residual measures the
sideways wobble. Small wobble means you are close to a true eigen-direction.}

\FormulaPage{4}{Rayleigh Quotient Iteration (Hermitian, Local Cubic Rate)}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
For Hermitian $A$ with simple eigenpair $(\lambda,u)$ and starting vector $x_0$
sufficiently close to $u$, the iteration with dynamic shift
$\sigma_k=q(x_k)$:
$(A-\sigma_k I)y_k=x_k$, $x_{k+1}=y_k/\lVert y_k\rVert$ converges cubically to
$u$ and $\lambda$.

\WHAT{
Uses the Rayleigh quotient as an adaptive shift in inverse iteration to obtain
rapid local convergence to an eigenpair of Hermitian matrices.}
\WHY{
Combines the stability of Hermitian problems with a nearly optimal local rate,
often requiring very few steps once sufficiently close.}
\FORMULA{
\[
\sigma_k=q(x_k),\quad (A-\sigma_k I)y_k=x_k,\quad x_{k+1}=y_k/\lVert y_k\rVert,
\]
with $\|x_{k+1}-u\|=\mathcal{O}(\|x_k-u\|^3)$ for $x_k$ near $u$.
}
\CANONICAL{
$A=A^\ast$, simple eigenvalue $\lambda$, starting guess close enough to $u$.}
\PRECONDS{
\begin{bullets}
\item $A$ Hermitian; eigenvalue $\lambda$ simple.
\item $x_0$ has nonzero component in direction of $u$ and is sufficiently close.
\item Linear solves with $(A-\sigma_k I)$ are accurate.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
If $A=A^\ast$, then $\nabla q(x)=\dfrac{2}{x^\ast x}\left(Ax-q(x)x\right)$
for real-differentiable $q$ restricted to $\mathbb{R}^n$ and extended
appropriately to $\mathbb{C}^n$.
\end{lemma}
\begin{proof}
For $q(x)=\dfrac{x^\ast A x}{x^\ast x}$ and real $x$, use quotient rule:
$\mathrm{d}q=\dfrac{2x^\top A\,\mathrm{d}x\cdot x^\top x-
2x^\top \mathrm{d}x\cdot x^\top A x}{(x^\top x)^2}
=\dfrac{2(Ax-q(x)x)^\top \mathrm{d}x}{x^\top x}$, hence the gradient formula.
The complex-case differential uses Wirtinger calculus analogously.\qedhere
\end{proof}
\DERIVATION{
\begin{align*}
\text{Local model: }& x_k=\cos\theta_k u+\sin\theta_k w_k,\ w_k\perp u.\\
\text{Shift: }& \sigma_k=q(x_k)=\lambda+\mathcal{O}(\sin^2\theta_k).\\
\text{Solve: }& (A-\sigma_k I)y_k=x_k \Rightarrow
y_k \approx \frac{1}{\lambda-\sigma_k}\cos\theta_k u
+ \mathcal{O}(1)\sin\theta_k w_k.\\
\text{Normalize: }& x_{k+1}=y_k/\lVert y_k\rVert \Rightarrow
\sin\theta_{k+1}=\mathcal{O}(\sin^3\theta_k).\\
\text{Conclusion: }& \|x_{k+1}-u\|=\mathcal{O}(\|x_k-u\|^3).
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Start with a reasonable approximation to the target eigenvector.
\item Compute $\sigma_k=q(x_k)$.
\item Solve $(A-\sigma_k I)y_k=x_k$ and normalize.
\item Check residual; iterate until below tolerance.
\end{bullets}
\EQUIV{
\begin{bullets}
\item Equivalent to Newton's method on the stationary equation
$Ax=\lambda x$ with normalization constraint for Hermitian $A$.
\item Fixed-shift inverse iteration has geometric rate; RQI accelerates to
cubic locally.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item If not close to an eigenvector, RQI can converge to a different eigenpair
or be erratic.
\item For non-Hermitian $A$, cubic rate generally fails.
\end{bullets}
}
\INPUTS{$A=A^\ast$, initial $x_0$, linear solver, tolerance.}
\DERIVATION{
\begin{align*}
\text{Step 1: }& \sigma_k=q(x_k).\\
\text{Step 2: }& (A-\sigma_k I)y_k=x_k.\\
\text{Step 3: }& x_{k+1}=y_k/\lVert y_k\rVert.\\
\text{Step 4: }& r_{k+1}=Ax_{k+1}-q(x_{k+1})x_{k+1}.
\end{align*}
}
\RESULT{
Local cubic convergence to $(\lambda,u)$ for Hermitian $A$ with a simple
eigenvalue and a sufficiently accurate starting guess.}
\UNITCHECK{
Shifts $\sigma_k$ are scalars; linear solves reflect consistent dimensions.
Normalization removes scale.}
\PITFALLS{
\begin{bullets}
\item Failing to solve accurately degrades the cubic rate.
\item If $x_k$ is orthogonal to $u$, stagnation occurs.
\end{bullets}
}
\ELI{
Each step redraws the target to where you currently think it is, then takes an
inverse step; being close means your redraw is so accurate that you triple your
digits each time.}

\section{10 Exhaustive Problems and Solutions}

\ProblemPage{1}{Power Method on a Symmetric $2\times 2$}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Compute four iterations of the power method for
$A=\begin{bmatrix}2&1\\1&2\end{bmatrix}$ starting at $x_0=(1,0)^\top$.
Estimate $\lambda_1$ via Rayleigh quotient and compare the error to the
predicted rate $|\lambda_2/\lambda_1|$.

\PROBLEM{
Carry out the iterations explicitly, normalizing by the Euclidean norm, and
report $x_k$, $\hat{\lambda}_k$, and residual norms.}
\MODEL{
\[
A=\begin{bmatrix}2&1\\1&2\end{bmatrix},\quad
x_{k+1}=\frac{Ax_k}{\lVert Ax_k\rVert},\quad
\hat{\lambda}_k=\frac{x_k^\top A x_k}{x_k^\top x_k}.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $A$ is symmetric with eigenvalues $\lambda_1=3$, $\lambda_2=1$.
\item $x_0$ has a nonzero projection on $v_1=\frac{1}{\sqrt{2}}(1,1)^\top$.
\end{bullets}
}
\varmapStart
\var{A}{matrix, real symmetric.}
\var{x_k}{normalized iterate at step $k$.}
\var{\hat{\lambda}_k}{Rayleigh quotient estimate.}
\var{r_k}{residual $Ax_k-\hat{\lambda}_k x_k$.}
\varmapEnd
\WHICHFORMULA{
Formula 1 (Power Method Convergence and Rate).}
\GOVERN{
\[
x_k=\frac{A^k x_0}{\lVert A^k x_0\rVert},\quad
\tan\theta_k\le \left|\frac{1}{3}\right|^k \tan\theta_0.
\]
}
\INPUTS{$x_0=(1,0)^\top$.}
\DERIVATION{
\begin{align*}
x_0&=\begin{bmatrix}1\\0\end{bmatrix},\quad y_0=Ax_0=\begin{bmatrix}2\\1\end{bmatrix},
\ \lVert y_0\rVert=\sqrt{5},\\
x_1&=\frac{1}{\sqrt{5}}\begin{bmatrix}2\\1\end{bmatrix},\
\hat{\lambda}_1=\frac{x_1^\top A x_1}{x_1^\top x_1}=\frac{(2,1)A(2,1)^\top}{5}\\
&=\frac{(2,1)\begin{bmatrix}5\\4\end{bmatrix}}{5}=\frac{14}{5}=2.8,\\
r_1&=Ax_1-\hat{\lambda}_1 x_1,\
\lVert r_1\rVert=\left\lVert \begin{bmatrix}5\\4\end{bmatrix}/\sqrt{5}
-2.8\begin{bmatrix}2\\1\end{bmatrix}/\sqrt{5}\right\rVert\\
&=\frac{1}{\sqrt{5}}\left\lVert \begin{bmatrix}-0.6\\1.2\end{bmatrix}\right\rVert
=\frac{\sqrt{1.8}}{\sqrt{5}}\approx 0.6.\\
y_1&=Ax_1=\frac{1}{\sqrt{5}}\begin{bmatrix}5\\4\end{bmatrix},\
\lVert y_1\rVert=\frac{\sqrt{41}}{\sqrt{5}},\\
x_2&=\frac{1}{\sqrt{41}}\begin{bmatrix}5\\4\end{bmatrix},\
\hat{\lambda}_2=\frac{(5,4)A(5,4)^\top}{41}=\frac{(5,4)(14,13)^\top}{41}\\
&=\frac{(70+52)}{41}=\frac{122}{41}\approx 2.9756.\\
y_2&=Ax_2=\frac{1}{\sqrt{41}}\begin{bmatrix}14\\13\end{bmatrix},\
\lVert y_2\rVert=\frac{\sqrt{365}}{\sqrt{41}},\\
x_3&=\frac{1}{\sqrt{365}}\begin{bmatrix}14\\13\end{bmatrix},\
\hat{\lambda}_3=\frac{(14,13)(41,40)^\top}{365}=\frac{(574+520)}{365}\\
&=\frac{1094}{365}\approx 2.996.\\
x_4&=\frac{1}{\sqrt{3313}}\begin{bmatrix}41\\40\end{bmatrix},\
\hat{\lambda}_4\approx \frac{(41,40)(122,121)^\top}{3313}\\
&=\frac{(5002+4840)}{3313}\approx 2.9991.
\end{align*}
}
\RESULT{
Convergence to $v_1=(1,1)^\top/\sqrt{2}$ with eigenvalue estimates
$2.8,2.9756,2.996,2.9991$ approaching $3$ at a rate near $1/3$.}
\UNITCHECK{
Estimates are dimensionless as $A$ is dimensionless. Residual decreases
geometrically, consistent with $|\lambda_2/\lambda_1|=1/3$.}
\EDGECASES{
\begin{bullets}
\item If $x_0=(1,-1)^\top/\sqrt{2}$, the method would converge to $v_2$ with
eigenvalue $1$.
\item If $x_0$ orthogonal to $v_1$, no convergence to the dominant direction.
\end{bullets}
}
\ALTERNATE{
Normalize by $\infty$-norm instead of $2$-norm; the limit direction remains the
same, with minor differences in intermediate steps.}
\VALIDATION{
\begin{bullets}
\item Compare $\hat{\lambda}_k$ to the exact $3$.
\item Check $\lVert r_k\rVert$ decreases approximately by factor $1/3$.
\end{bullets}
}
\INTUITION{
Each multiplication biases more towards the equal-entries direction, the
strongest stretch.}

\ProblemPage{2}{Observed Rate vs. Spectral Gap on a $3\times 3$}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Given $A=\mathrm{diag}(5,2,-1)$ and $x_0=(1,1,1)^\top/\sqrt{3}$, verify that
the error angle decays as $\mathcal{O}(|2/5|^k)$.

\PROBLEM{
Run three steps, compute the angle to $v_1=e_1$, and compare with the bound.}
\MODEL{
\[
A=\begin{bmatrix}5&0&0\\0&2&0\\0&0&-1\end{bmatrix},\quad
x_{k+1}=\frac{Ax_k}{\lVert Ax_k\rVert}.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $|\lambda_1|=5>2=|\lambda_2|>1=|\lambda_3|$.
\item $x_0$ has nonzero element in coordinate $1$.
\end{bullets}
}
\varmapStart
\var{A}{diagonal, real.}
\var{v_1}{dominant eigenvector $e_1$.}
\var{\theta_k}{angle between $x_k$ and $e_1$.}
\varmapEnd
\WHICHFORMULA{
Formula 1 (Power Method Convergence and Rate).}
\GOVERN{
\[
\tan\theta_k\le \left|\frac{2}{5}\right|^k \tan\theta_0.
\]
}
\INPUTS{$x_0=(1,1,1)^\top/\sqrt{3}$.}
\DERIVATION{
\begin{align*}
y_0&=Ax_0=\frac{1}{\sqrt{3}}(5,2,-1)^\top,\
\lVert y_0\rVert=\frac{\sqrt{30}}{\sqrt{3}},\\
x_1&=\frac{1}{\sqrt{30}}(5,2,-1)^\top.\\
\cos\theta_1&=\frac{x_1^\top e_1}{\lVert x_1\rVert}=\frac{5}{\sqrt{30}},
\ \sin\theta_1=\sqrt{1-\frac{25}{30}}=\sqrt{\frac{5}{30}}.\\
\tan\theta_1&=\sqrt{\frac{5}{25}}=\sqrt{\frac{1}{5}}.\\
y_1&=Ax_1=\frac{1}{\sqrt{30}}(25,4,1)^\top,\
\lVert y_1\rVert=\frac{\sqrt{25^2+4^2+1}}{\sqrt{30}}=\frac{\sqrt{642}}{\sqrt{30}},\\
x_2&=\frac{1}{\sqrt{642}}(25,4,1)^\top,\ \cos\theta_2=\frac{25}{\sqrt{642}}.\\
\tan\theta_2&=\sqrt{1-\frac{625}{642}}\Big/\frac{25}{\sqrt{642}}
=\frac{\sqrt{17}}{25}.\\
\text{Bound: }& \tan\theta_2\le \left(\frac{2}{5}\right)^2\tan\theta_0.\
\text{Compute }\tan\theta_0=\sqrt{2}.\\
\text{Right side: }& (4/25)\sqrt{2}\approx 0.2263,\
\text{Left side: }\sqrt{17}/25\approx 0.1649.
\end{align*}
}
\RESULT{
Observed angles satisfy the predicted geometric decay with factor $2/5$.}
\UNITCHECK{
Angles are dimensionless; norms scale consistently.}
\EDGECASES{
\begin{bullets}
\item If $\lambda_2$ equals $\lambda_1$ in magnitude, the decay bound fails.
\item If $x_0=e_1$, the method converges in zero steps.
\end{bullets}
}
\ALTERNATE{
Use direct formula $x_k \propto (5^k,2^k,(-1)^k)^\top$ to compute angles.}
\VALIDATION{
\begin{bullets}
\item Numerically verify $\tan\theta_{k+1}/\tan\theta_k\approx 2/5$.
\end{bullets}
}
\INTUITION{
Coordinates not aligned with $e_1$ shrink by powers of $2/5$ and $1/5$.}

\ProblemPage{3}{Normalization Choices: $1$-norm vs. $2$-norm}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Show that using $1$-norm normalization yields the same limiting direction as
$2$-norm normalization for $A=\begin{bmatrix}3&1\\0&2\end{bmatrix}$,
$x_0=(1,1)^\top$.

\PROBLEM{
Run two iterations with both norms and compare directions.}
\MODEL{
\[
x_{k+1}^{(p)}=\frac{Ax_k^{(p)}}{\lVert Ax_k^{(p)}\rVert_p},\ p\in\{1,2\}.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $|\lambda_1|=3>2=|\lambda_2|$; eigenvectors $v_1=(1,0)^\top$,
$v_2=(1,? )^\top$ (any nonzero second component).
\item $x_0$ has nonzero projection on $v_1$.
\end{bullets}
}
\varmapStart
\var{p}{norm index: $1$ or $2$.}
\var{x_k^{(p)}}{iterate under norm $p$.}
\varmapEnd
\WHICHFORMULA{
Formula 1 (Power Method Convergence and Rate).}
\GOVERN{
\[
x_k^{(p)}\to v_1 \text{ directionally for any equivalent norm }\lVert\cdot\rVert_p.
\]
}
\INPUTS{$x_0=(1,1)^\top$.}
\DERIVATION{
\begin{align*}
y_0&=Ax_0=(4,2)^\top.\\
\lVert y_0\rVert_2&=\sqrt{20},\ x_1^{(2)}=(4,2)^\top/\sqrt{20}=(2,1)^\top/\sqrt{5}.\\
\lVert y_0\rVert_1&=6,\ x_1^{(1)}=(4,2)^\top/6=(2,1)^\top/3.\\
y_1^{(2)}&=A x_1^{(2)}=(3\cdot 2+1\cdot 1,2\cdot 1)^\top/\sqrt{5}=(7,2)^\top/\sqrt{5}.\\
y_1^{(1)}&=A x_1^{(1)}=(7,2)^\top/3.\\
x_2^{(2)}&=(7,2)^\top/\lVert(7,2)\rVert_2=(7,2)^\top/\sqrt{53}.\\
x_2^{(1)}&=(7,2)^\top/\lVert(7,2)\rVert_1=(7,2)^\top/9.\\
\text{Both }& \text{are positive multiples of }(7,2)^\top, hence same direction.
\end{align*}
}
\RESULT{
Both normalizations yield identical directions each step for this upper
triangular $A$, and thus the same limit $v_1=(1,0)^\top$.}
\UNITCHECK{
Direction on projective space is independent of scalar normalization.}
\EDGECASES{
\begin{bullets}
\item If signs flip, $1$-norm and $2$-norm can pick different scaling but the
direction still coincides asymptotically.
\end{bullets}
}
\ALTERNATE{
Normalize by setting the largest-magnitude entry to $1$; the limit direction
remains $v_1$.}
\VALIDATION{
\begin{bullets}
\item Compare $x_2^{(1)}$ and $x_2^{(2)}$ after scaling to unit $2$-norm.
\end{bullets}
}
\INTUITION{
Any reasonable rescaling keeps the same ray; only the ray matters.}

\ProblemPage{4}{Narrative: Alice vs. Bob on Shift Choice}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Alice uses $\sigma=0$; Bob uses $\sigma=2.9$ to find the eigenvector near
$\lambda^\star=3$ for $A=\begin{bmatrix}3&1\\1&2\end{bmatrix}$. Who converges
faster and why?

\PROBLEM{
Compare inverse iteration rates for both shifts from $x_0=(1,0)^\top$.}
\MODEL{
\[
(A-\sigma I)y_k=x_k,\ x_{k+1}=\frac{y_k}{\lVert y_k\rVert}.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $A$ symmetric with eigenvalues approximately $\lambda^\star\approx 3.618$
and $\lambda_2\approx 1.382$? No, compute exact eigenvalues of
$\begin{bmatrix}3&1\\1&2\end{bmatrix}$: $\lambda_{1,2}=\frac{5\pm \sqrt{5}}{2}$.
\item Targeting the larger eigenvalue $\lambda^\star=\frac{5+\sqrt{5}}{2}
\approx 3.618$.
\end{bullets}
}
\varmapStart
\var{\sigma}{shift parameter.}
\var{\lambda^\star}{closest eigenvalue to $\sigma$.}
\var{\rho}{rate factor $\left|\frac{\lambda^\star-\sigma}
{\lambda_2-\sigma}\right|$.}
\varmapEnd
\WHICHFORMULA{
Formula 2 (Inverse Iteration with Shift).}
\GOVERN{
\[
\text{Rate }\rho(\sigma)=\left|\frac{\lambda^\star-\sigma}{\lambda_2-\sigma}
\right|.
\]
}
\INPUTS{$\lambda^\star\approx 3.618,\ \lambda_2\approx 1.382$.}
\DERIVATION{
\begin{align*}
\text{Alice: }& \sigma=0,\ \rho(0)=\left|\frac{3.618}{1.382}\right|\approx 2.619.\\
& \text{But the factor exceeding $1$ refers to the inverse map dominance;
the error decays by } \left|\frac{\lambda_2}{\lambda^\star}\right|
\approx 0.382.\\
\text{Bob: }& \sigma=2.9,\ \rho(2.9)=\left|\frac{3.618-2.9}{1.382-2.9}\right|
=\left|\frac{0.718}{-1.518}\right|\approx 0.473.\\
\text{Conclusion: }& \text{Bob's shift yields a smaller decay factor than Alice's
unshifted power method factor } 0.382?\\
& \text{No, compare correctly: inverse iteration error factor is }
\left|\frac{\lambda^\star-\sigma}{\lambda_2-\sigma}\right|,\\
& \text{power method factor is } \left|\frac{\lambda_2}{\lambda^\star}\right|
\approx 0.382.\\
& 0.473>0.382 \Rightarrow \text{here unshifted power method is faster than
Bob's chosen shift.}
\end{align*}
}
\RESULT{
For this choice, Alice (power method) has error factor $0.382$, Bob has $0.473$;
Alice converges faster.}
\UNITCHECK{
Both factors are dimensionless ratios of eigenvalue differences or magnitudes.}
\EDGECASES{
\begin{bullets}
\item If Bob chose $\sigma$ closer to $\lambda^\star$, his factor could be much
smaller, even arbitrarily small.
\end{bullets}
}
\ALTERNATE{
Use $\sigma=q(x_k)$ (Rayleigh quotient iteration) to accelerate locally.}
\VALIDATION{
\begin{bullets}
\item Compute a few steps numerically to observe the rates.
\end{bullets}
}
\INTUITION{
A good shift should be very close to the target eigenvalue; $2.9$ is not close
enough here.}

\ProblemPage{5}{Narrative: Dynamic Shift and Cubic Snap}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Bob runs Rayleigh quotient iteration on
$A=\begin{bmatrix}2&0\\0&5\end{bmatrix}$ from $x_0=(1,1)^\top/\sqrt{2}$. Show
that after one step the error is already very small and the next step nearly
converges.

\PROBLEM{
Compute two RQI steps exactly.}
\MODEL{
\[
\sigma_k=q(x_k),\ (A-\sigma_k I)y_k=x_k,\ x_{k+1}=\frac{y_k}{\lVert y_k\rVert}.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $A$ diagonal, Hermitian, simple eigenvalues $2$ and $5$.
\end{bullets}
}
\varmapStart
\var{x_k}{direction estimate.}
\var{\sigma_k}{Rayleigh shift.}
\var{u}{target eigenvector $e_2$.}
\varmapEnd
\WHICHFORMULA{
Formula 4 (Rayleigh Quotient Iteration).}
\GOVERN{
\[
q(x)=\frac{x^\top A x}{x^\top x},\quad r=Ax-q(x)x.
\]
}
\INPUTS{$x_0=(1,1)^\top/\sqrt{2}$.}
\DERIVATION{
\begin{align*}
q(x_0)&=\frac{(1,1)A(1,1)^\top}{2}=\frac{2+5}{2}=3.5.\\
A-\sigma_0 I&=\begin{bmatrix}-1.5&0\\0&1.5\end{bmatrix}.\\
y_0&=(A-\sigma_0 I)^{-1}x_0=\begin{bmatrix}-2/3&0\\0&2/3\end{bmatrix}
\frac{(1,1)^\top}{\sqrt{2}}=\frac{1}{\sqrt{2}}\begin{bmatrix}-2/3\\2/3\end{bmatrix}.\\
x_1&=\frac{y_0}{\lVert y_0\rVert}=\frac{1}{\sqrt{2}}\begin{bmatrix}-1\\1\end{bmatrix}.
\\
q(x_1)&=\frac{(1,1)A(1,1)^\top}{2}\ \text{but with signs}\ 
=\frac{2+5}{2}=3.5.\\
\text{However, }& x_1 \text{ is orthogonal to } e_1,\ \text{closer to }e_2.\\
\text{Next shift: }& \sigma_1=q(x_1)=3.5.\
\text{Repeat step yields }x_2=e_2.
\end{align*}
}
\RESULT{
After two steps, RQI converges exactly on this diagonal example.}
\UNITCHECK{
Inverses and norms are consistent; directions dimensionless.}
\EDGECASES{
\begin{bullets}
\item Starting exactly at $e_1$ or $e_2$ converges in zero steps.
\end{bullets}
}
\ALTERNATE{
Run fixed-shift inverse iteration with $\sigma=4$; it converges geometrically
but slower.}
\VALIDATION{
\begin{bullets}
\item Compute $r_k$; it becomes zero at $x_2=e_2$.
\end{bullets}
}
\INTUITION{
Adaptive shifting centers the spectrum near the current guess, causing a rapid
snap to the target.}

\ProblemPage{6}{Expectation Puzzle: Random Start Alignment}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
For $A$ with dominant eigenvector $v_1$ and $x_0$ uniformly random with entries
$\pm 1$ with equal probability, compute
$\mathbb{E}\left[\lvert v_1^\ast x_0\rvert^2/\lVert x_0\rVert^2\right]$ and
estimate iterations needed to reduce $\tan\theta_k$ below $\tau$.

\PROBLEM{
Show the expected squared cosine with $v_1$ is $1/n$ and use the power method
rate to estimate $k$.}
\MODEL{
\[
x_0\sim \{\pm 1\}^n,\quad \cos^2\theta_0=
\frac{|v_1^\ast x_0|^2}{\lVert x_0\rVert^2}.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $v_1$ unit-norm; $x_0$ i.i.d. Rademacher entries.
\item Spectral gap $\gamma=|\lambda_2/\lambda_1|<1$.
\end{bullets}
}
\varmapStart
\var{n}{dimension.}
\var{\gamma}{convergence factor $|\lambda_2/\lambda_1|$.}
\var{\tau}{target tangent threshold.}
\varmapEnd
\WHICHFORMULA{
Formula 1 (Power Method Convergence and Rate).}
\GOVERN{
\[
\mathbb{E}\cos^2\theta_0=\frac{1}{n},\quad
\tan\theta_k\le \gamma^k \tan\theta_0.
\]
}
\INPUTS{$n$, $\gamma$, $\tau$.}
\DERIVATION{
\begin{align*}
\mathbb{E}\left[|v_1^\ast x_0|^2\right]
&=\mathbb{E}\left[\left|\sum_{i=1}^n \bar{v}_{1i} x_{0i}\right|^2\right]
=\sum_{i=1}^n |v_{1i}|^2 \mathbb{E}[x_{0i}^2]
=\sum_{i=1}^n |v_{1i}|^2=1.\\
\mathbb{E}\left[\lVert x_0\rVert^2\right]&=n.\\
\Rightarrow\ \mathbb{E}\cos^2\theta_0&=\frac{1}{n}.\\
\text{Assume }& \tan\theta_0\approx \sqrt{\frac{1-\frac{1}{n}}{\frac{1}{n}}}
=\sqrt{n-1}.\\
\text{Require }& \gamma^k \tan\theta_0 \le \tau \Rightarrow
k \ge \frac{\log(\tau/\tan\theta_0)}{\log \gamma}.\\
& = \frac{\log\left(\tau/\sqrt{n-1}\right)}{\log \gamma}.
\end{align*}
}
\RESULT{
Expected initial alignment $1/n$. Iteration estimate
$k \gtrsim \dfrac{\log(\tau/\sqrt{n-1})}{\log \gamma}$.}
\UNITCHECK{
All quantities are dimensionless.}
\EDGECASES{
\begin{bullets}
\item For $n=1$, alignment is $1$ and $k=0$.
\item As $\gamma\to 1$, $k$ grows unbounded for fixed $\tau$.
\end{bullets}
}
\ALTERNATE{
Use a Gaussian random start; the result for $\mathbb{E}\cos^2\theta_0$ remains
$1/n$.}
\VALIDATION{
\begin{bullets}
\item Monte Carlo simulation with fixed seed matches the estimate.
\end{bullets}
}
\INTUITION{
A random start points roughly equally in all directions; only $1/n$ of it hits
the target.}

\ProblemPage{7}{Proof: Limit of Normalized Powers}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Prove that if $A$ is diagonalizable with $|\lambda_1|>|\lambda_2|$, then
$x_k=\dfrac{A^k x_0}{\lVert A^k x_0\rVert}$ converges to the direction of $v_1$
for any $x_0$ with $v_1^\ast x_0\ne 0$.

\PROBLEM{
Provide a complete proof using eigen-expansion.}
\MODEL{
\[
A=V\Lambda V^{-1},\quad x_0=\sum_i c_i v_i,\ c_1\ne 0.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $A$ diagonalizable; $|\lambda_1|>|\lambda_2|$.
\item $x_0$ has $c_1\ne 0$.
\end{bullets}
}
\varmapStart
\var{c_i}{coordinates of $x_0$ in eigenbasis.}
\varmapEnd
\WHICHFORMULA{
Formula 1 and its supporting lemma.}
\GOVERN{
\[
A^k x_0=\lambda_1^k c_1\left(v_1+\sum_{i\ge 2}\frac{c_i}{c_1}
\left(\frac{\lambda_i}{\lambda_1}\right)^k v_i\right).
\]
}
\INPUTS{$A$, $x_0$.}
\DERIVATION{
\begin{align*}
\frac{A^k x_0}{\lambda_1^k c_1}&=
v_1+\sum_{i\ge 2}\frac{c_i}{c_1}\left(\frac{\lambda_i}{\lambda_1}\right)^k v_i
\to v_1.\\
\Rightarrow\ \frac{A^k x_0}{\lVert A^k x_0\rVert}
&= \frac{\lambda_1^k c_1}{\lVert A^k x_0\rVert}\cdot
\frac{A^k x_0}{\lambda_1^k c_1}\to \alpha v_1/\lVert v_1\rVert,
\end{align*}
with $|\alpha|=1$ absorbing phase.
}
\RESULT{
Directional convergence to $v_1$ up to a unimodular scalar.}
\UNITCHECK{
Normalization removes scaling; the limit is a unit direction.}
\EDGECASES{
\begin{bullets}
\item If $c_1=0$, the limit is governed by the next-largest magnitude eigenvalue.
\end{bullets}
}
\ALTERNATE{
Normalize at each step; the same limit holds by continuity of normalization.}
\VALIDATION{
\begin{bullets}
\item Apply to $A=\mathrm{diag}(5,2)$ and check explicit convergence.
\end{bullets}
}
\INTUITION{
The largest-magnitude mode dominates all others exponentially.}

\ProblemPage{8}{Proof: Inverse Iteration Equals Power on Inverse-Shift}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Show that inverse iteration with shift $\sigma$ is the power method applied to
$(A-\sigma I)^{-1}$.

\PROBLEM{
Prove the equivalence including the rate factor expression.}
\MODEL{
\[
x_{k+1}=\frac{(A-\sigma I)^{-1}x_k}{\lVert (A-\sigma I)^{-1}x_k\rVert}.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $(A-\sigma I)$ is invertible.
\item $x_0$ has a nonzero component along the targeted eigenvector.
\end{bullets}
}
\varmapStart
\var{\mu_i}{eigenvalues of $(A-\sigma I)^{-1}$, $\mu_i=1/(\lambda_i-\sigma)$.}
\varmapEnd
\WHICHFORMULA{
Formula 2 and its lemma on spectral mapping.}
\GOVERN{
\[
(A-\sigma I)^{-1}v_i=\frac{1}{\lambda_i-\sigma}v_i.
\]
}
\INPUTS{$A$, $\sigma$.}
\DERIVATION{
\begin{align*}
x_0&=\sum_i c_i v_i\Rightarrow
x_k \propto \sum_i c_i \mu_i^k v_i,\\
\text{with }& \mu_i=\frac{1}{\lambda_i-\sigma}.\
\text{Dominance is by maximal }|\mu_i|,\\
& \text{i.e., minimal }|\lambda_i-\sigma|.\
\text{Rate factor } \left|\frac{\mu_{\text{next}}}{\mu^\star}\right|
=\left|\frac{\lambda^\star-\sigma}{\lambda_{\text{next}}-\sigma}\right|.
\end{align*}
}
\RESULT{
Inverse iteration steps exactly match power method on $(A-\sigma I)^{-1}$, with
convergence governed by reciprocal distances to $\sigma$.}
\UNITCHECK{
Reciprocal distances are dimensionless; normalization preserves direction.}
\EDGECASES{
\begin{bullets}
\item If $\sigma$ equals an eigenvalue, $(A-\sigma I)$ is singular and the
iteration is undefined.
\end{bullets}
}
\ALTERNATE{
Use preconditioned solves to approximate $(A-\sigma I)^{-1}$ while retaining
the same fixed points.}
\VALIDATION{
\begin{bullets}
\item Compare trajectories of both iterations numerically; they coincide.
\end{bullets}
}
\INTUITION{
Shift-invert flips closeness into largeness, so the nearest eigenpair becomes
dominant.}

\ProblemPage{9}{Combo: Stationary Distribution via Power Method}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Find the stationary distribution $\pi$ of a Markov matrix
$P=\begin{bmatrix}0.5&0.5&0\\0.25&0.5&0.25\\0&0.5&0.5\end{bmatrix}$ via the
power method.

\PROBLEM{
Run iterations on $x_{k+1}=P^\top x_k/\lVert P^\top x_k\rVert_1$ from
$x_0=(1,0,0)^\top$ and report $\pi$.}
\MODEL{
\[
x_{k+1}=\frac{P^\top x_k}{\lVert P^\top x_k\rVert_1}.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $P$ is stochastic, irreducible, aperiodic; eigenvalue $1$ is simple.
\end{bullets}
}
\varmapStart
\var{P}{transition matrix.}
\var{\pi}{stationary distribution: $P^\top \pi=\pi$, $\lVert \pi\rVert_1=1$.}
\varmapEnd
\WHICHFORMULA{
Formula 1 applied to $P^\top$ whose dominant eigenvalue is $1$.}
\GOVERN{
\[
x_k\to \pi,\quad \hat{\lambda}_k=x_k^\top P^\top x_k \to 1.
\]
}
\INPUTS{$x_0=(1,0,0)^\top$.}
\DERIVATION{
\begin{align*}
x_1&=\frac{P^\top x_0}{\lVert P^\top x_0\rVert_1}=
\frac{(0.5,0.5,0)^\top}{1}=(0.5,0.5,0)^\top.\\
x_2&=\frac{P^\top x_1}{\lVert\cdot\rVert_1}=\frac{(0.375,0.5,0.125)^\top}{1}.\\
x_3&=\frac{P^\top x_2}{\lVert\cdot\rVert_1}
=\frac{(0.3125,0.5,0.1875)^\top}{1}.\\
\text{Pattern: }& x_k\to (0.25,0.5,0.25)^\top.\\
\pi&=(0.25,0.5,0.25)^\top \text{ solves }P^\top\pi=\pi.
\end{align*}
}
\RESULT{
$\pi=(0.25,0.5,0.25)^\top$ is the stationary distribution.}
\UNITCHECK{
Entries sum to $1$ and are nonnegative; $P^\top\pi=\pi$.}
\EDGECASES{
\begin{bullets}
\item If $P$ is reducible, stationary distributions need not be unique.
\end{bullets}
}
\ALTERNATE{
Solve $(P^\top-I)\pi=0$ with constraint $1^\top\pi=1$.}
\VALIDATION{
\begin{bullets}
\item Verify $P^\top \pi=\pi$ explicitly.
\end{bullets}
}
\INTUITION{
Power method seeks the dominant eigenvector of $P^\top$, which is the steady
state.}

\ProblemPage{10}{Combo: Graph Spectral Radius via Power Method}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Approximate the largest eigenvalue and eigenvector of the path graph $P_4$
adjacency matrix
$A=\begin{bmatrix}0&1&0&0\\1&0&1&0\\0&1&0&1\\0&0&1&0\end{bmatrix}$.

\PROBLEM{
Run three iterations from $x_0=(1,0,0,0)^\top$ and compare
$\hat{\lambda}_k=q(x_k)$ with the exact $\lambda_1=2\cos\frac{\pi}{5}$.}
\MODEL{
\[
x_{k+1}=\frac{Ax_k}{\lVert Ax_k\rVert},\quad
\hat{\lambda}_k=\frac{x_k^\top A x_k}{x_k^\top x_k}.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Symmetric $A$ with simple largest eigenvalue.
\end{bullets}
}
\varmapStart
\var{A}{adjacency matrix of $P_4$.}
\var{\lambda_1}{largest eigenvalue $2\cos(\pi/5)\approx 1.6180$.}
\varmapEnd
\WHICHFORMULA{
Formula 1 and Rayleigh quotient from Formula 3.}
\GOVERN{
\[
x_k\to v_1,\quad \hat{\lambda}_k\to \lambda_1.
\]
}
\INPUTS{$x_0=(1,0,0,0)^\top$.}
\DERIVATION{
\begin{align*}
y_0&=Ax_0=(0,1,0,0)^\top,\ \lVert y_0\rVert=1,\
x_1=(0,1,0,0)^\top.\\
y_1&=Ax_1=(1,0,1,0)^\top,\ \lVert y_1\rVert=\sqrt{2},\
x_2=(1,0,1,0)^\top/\sqrt{2}.\\
\hat{\lambda}_2&=\frac{x_2^\top A x_2}{x_2^\top x_2}
=\frac{(1,0,1,0)A(1,0,1,0)^\top}{2}\\
&=\frac{(1,0,1,0)(0,2,0,1)^\top}{2}=\frac{2}{2}=1.\\
y_2&=Ax_2=\frac{1}{\sqrt{2}}(0,2,0,1)^\top,\
\lVert y_2\rVert=\sqrt{\frac{5}{2}},\
x_3=\frac{1}{\sqrt{5}}(0,2,0,1)^\top.\\
\hat{\lambda}_3&=\frac{(0,2,0,1)A(0,2,0,1)^\top}{5}
=\frac{(0,2,0,1)(2,0,3,0)^\top}{5}=\frac{4+0}{5}=0.8?
\end{align*}
}
\RESULT{
The simple sequence above shows oscillation due to bipartite structure; using
more steps and a non-bipartite normalization yields convergence to
$\lambda_1\approx 1.6180$.}
\UNITCHECK{
Rayleigh quotient remains within $[\lambda_{\min},\lambda_{\max}]$.}
\EDGECASES{
\begin{bullets}
\item Bipartite graphs can cause sign oscillations; use $|\cdot|$-based
normalization or track absolute values.
\end{bullets}
}
\ALTERNATE{
Start with $x_0=(1,1,1,1)^\top$ to avoid initial support on a single node and
observe smoother convergence.}
\VALIDATION{
\begin{bullets}
\item Compare with explicit $\lambda_1=2\cos(\pi/5)\approx 1.6180$.
\end{bullets}
}
\INTUITION{
Repeated walks amplify high-centrality patterns encoded by the leading
eigenvector.}

\section{Coding Demonstrations}

\CodeDemoPage{Power Method with Residual-Based Stopping}
\PROBLEM{
Implement the power method for a symmetric matrix, estimate the largest
eigenpair, and validate against NumPy eigen-solver.}
\API{
\begin{bullets}
\item \inlinecode{def read_input(s) -> tuple} — parse matrix and vector.
\item \inlinecode{def solve_case(A, x0, iters, tol) -> tuple} — power method.
\item \inlinecode{def validate() -> None} — asserts on a known case.
\item \inlinecode{def main() -> None} — orchestrate a demo run.
\end{bullets}
}
\INPUTS{
$A\in\mathbb{R}^{n\times n}$ symmetric, $x_0\in\mathbb{R}^n$, max iters,
tolerance $\tau$.}
\OUTPUTS{
Approximate $(\hat{\lambda},\hat{v})$, residual norm, and iteration count.}
\FORMULA{
\[
x_{k+1}=\frac{Ax_k}{\lVert Ax_k\rVert_2},\quad
\hat{\lambda}=q(x_k),\quad r=Ax_k-\hat{\lambda}x_k.
\]
}
\textbf{SOLUTION A — From Scratch (Mathematically Explicit Implementation)}
\begin{codepy}
import numpy as np

def read_input(s):
    vals = [float(x) for x in s.split()]
    n = int(vals[0])
    A = np.array(vals[1:1+n*n], dtype=float).reshape(n, n)
    x0 = np.array(vals[1+n*n:1+n*n+n], dtype=float)
    return A, x0

def power_method(A, x0, iters=1000, tol=1e-10):
    x = x0.astype(float)
    x /= np.linalg.norm(x)
    lam = float(x @ (A @ x))
    for k in range(iters):
        y = A @ x
        ny = np.linalg.norm(y)
        if ny == 0.0:
            return 0.0, x, 0.0, k
        x = y / ny
        lam = float(x @ (A @ x))
        r = A @ x - lam * x
        rn = float(np.linalg.norm(r))
        if rn < tol:
            return lam, x, rn, k+1
    r = A @ x - lam * x
    rn = float(np.linalg.norm(r))
    return lam, x, rn, iters

def solve_case(A, x0, iters=1000, tol=1e-10):
    return power_method(A, x0, iters, tol)

def validate():
    A = np.array([[2.0,1.0],[1.0,2.0]], dtype=float)
    x0 = np.array([1.0,0.0], dtype=float)
    lam, v, rn, k = solve_case(A, x0, 100, 1e-12)
    w, Z = np.linalg.eigh(A)
    assert abs(lam - max(w)) < 1e-9
    assert rn < 1e-9
    assert k > 0

def main():
    validate()
    A = np.array([[3.0,1.0,0.0],[1.0,2.0,1.0],[0.0,1.0,1.0]])
    x0 = np.array([1.0,1.0,1.0])
    lam, v, rn, k = solve_case(A, x0, 1000, 1e-12)
    print("lam", round(lam,8), "rn", rn, "iters", k)

if __name__ == "__main__":
    main()
\end{codepy}
\textbf{SOLUTION B — Library-Based (Validated Computational Shortcut)}
\begin{codepy}
import numpy as np

def read_input(s):
    vals = [float(x) for x in s.split()]
    n = int(vals[0])
    A = np.array(vals[1:1+n*n], dtype=float).reshape(n, n)
    x0 = np.array(vals[1+n*n:1+n*n+n], dtype=float)
    return A, x0

def solve_case(A, x0, iters=1000, tol=1e-10):
    w, Z = np.linalg.eigh(A)
    lam = float(w[-1])
    v = Z[:, -1]
    r = A @ v - lam * v
    rn = float(np.linalg.norm(r))
    return lam, v, rn, 1

def validate():
    A = np.array([[2.0,1.0],[1.0,2.0]], dtype=float)
    x0 = np.array([1.0,0.0], dtype=float)
    lam_lib, v_lib, rn_lib, _ = solve_case(A, x0)
    w, Z = np.linalg.eigh(A)
    assert abs(lam_lib - max(w)) < 1e-12
    assert rn_lib < 1e-12

def main():
    validate()
    A = np.array([[3.0,1.0,0.0],[1.0,2.0,1.0],[0.0,1.0,1.0]])
    x0 = np.array([1.0,1.0,1.0])
    lam, v, rn, k = solve_case(A, x0)
    print("lam", round(lam,8), "rn", rn, "iters", k)

if __name__ == "__main__":
    main()
\end{codepy}
\COMPLEXITY{
Time $\mathcal{O}(mn)$ per iteration for sparse $A$ with $m$ nonzeros
($\mathcal{O}(n^2)$ dense). Space $\mathcal{O}(n)$. Library eigensolve is
$\mathcal{O}(n^3)$ time, $\mathcal{O}(n^2)$ space.}
\FAILMODES{
\begin{bullets}
\item Zero vector during iteration (guard by checking $\lVert y\rVert=0$).
\item Non-convergence for tied magnitudes or orthogonal start; use random restarts.
\item Overflow if not normalizing each step.
\end{bullets}
}
\STABILITY{
\begin{bullets}
\item Accumulated rounding can affect angles when spectral gap is very small.
\item Normalize each step; optionally reorthogonalize to stabilize direction.
\end{bullets}
}
\VALIDATION{
\begin{bullets}
\item Compare with NumPy eigenpairs for small problems.
\item Check residual norm decreases below tolerance.
\end{bullets}
}
\RESULT{
Both implementations agree on the dominant eigenpair; residual verifies
accuracy.}
\EXPLANATION{
Iteration matches Formula 1; Rayleigh quotient and residual match Formula 3.}

\CodeDemoPage{Inverse Iteration and Rayleigh Quotient Iteration}
\PROBLEM{
Implement fixed-shift inverse iteration and Rayleigh quotient iteration for a
Hermitian matrix and verify convergence to a targeted eigenpair.}
\API{
\begin{bullets}
\item \inlinecode{def inverse_iter(A, x0, sigma, iters, tol)} — fixed-shift.
\item \inlinecode{def rqi(A, x0, iters, tol)} — dynamic shift by $q(x)$.
\item \inlinecode{def validate()} — compare to NumPy.
\item \inlinecode{def main()} — demo runs.
\end{bullets}
}
\INPUTS{
$A$ Hermitian, initial $x_0$, for inverse iteration: shift $\sigma$.}
\OUTPUTS{
Approximate eigenpair and residual norms.}
\FORMULA{
\[
(A-\sigma I)y_k=x_k,\ x_{k+1}=y_k/\lVert y_k\rVert,\quad
\sigma_k=q(x_k)\ \text{for RQI}.
\]
}
\textbf{SOLUTION A — From Scratch (Mathematically Explicit Implementation)}
\begin{codepy}
import numpy as np

def inverse_iter(A, x0, sigma, iters=100, tol=1e-12):
    n = A.shape[0]
    x = x0.astype(float)
    x /= np.linalg.norm(x)
    I = np.eye(n)
    for k in range(iters):
        y = np.linalg.solve(A - sigma*I, x)
        ny = np.linalg.norm(y)
        if ny == 0.0:
            break
        x = y / ny
        lam = float(x @ (A @ x))
        r = A @ x - lam * x
        if np.linalg.norm(r) < tol:
            return lam, x, np.linalg.norm(r), k+1
    lam = float(x @ (A @ x))
    r = A @ x - lam * x
    return lam, x, np.linalg.norm(r), iters

def rqi(A, x0, iters=50, tol=1e-14):
    n = A.shape[0]
    x = x0.astype(float)
    x /= np.linalg.norm(x)
    I = np.eye(n)
    for k in range(iters):
        sigma = float(x @ (A @ x))
        y = np.linalg.solve(A - sigma*I, x)
        x = y / np.linalg.norm(y)
        lam = float(x @ (A @ x))
        r = A @ x - lam * x
        rn = float(np.linalg.norm(r))
        if rn < tol:
            return lam, x, rn, k+1
    lam = float(x @ (A @ x))
    r = A @ x - lam * x
    return lam, x, float(np.linalg.norm(r)), iters

def validate():
    A = np.array([[3.0,1.0],[1.0,2.0]], dtype=float)
    w, Z = np.linalg.eigh(A)
    lam_true = float(w[-1])
    x0 = np.array([1.0,0.2], dtype=float)
    lam, v, rn, k = inverse_iter(A, x0, sigma=2.5, iters=100, tol=1e-12)
    assert abs(lam - lam_true) < 1e-8
    assert rn < 1e-8
    lam2, v2, rn2, k2 = rqi(A, x0, iters=20, tol=1e-14)
    assert abs(lam2 - lam_true) < 1e-12
    assert rn2 < 1e-12

def main():
    validate()
    A = np.array([[2.0,0.0,0.0],[0.0,3.0,1.0],[0.0,1.0,4.0]])
    x0 = np.array([1.0,1.0,1.0])
    lam, v, rn, k = inverse_iter(A, x0, sigma=3.2, iters=50, tol=1e-12)
    print("inv lam", round(lam,8), "rn", rn, "iters", k)
    lam2, v2, rn2, k2 = rqi(A, x0, iters=20, tol=1e-14)
    print("rqi lam", round(lam2,8), "rn", rn2, "iters", k2)

if __name__ == "__main__":
    main()
\end{codepy}
\textbf{SOLUTION B — Library-Based (Validated Computational Shortcut)}
\begin{codepy}
import numpy as np

def inverse_iter_lib(A, x0, sigma, iters=100, tol=1e-12):
    # Use factorization approach via eigh for demonstration result check
    w, Z = np.linalg.eigh(A)
    j = int(np.argmin(np.abs(w - sigma)))
    lam = float(w[j])
    v = Z[:, j]
    r = A @ v - lam * v
    return lam, v, float(np.linalg.norm(r)), 1

def rqi_lib(A, x0, iters=50, tol=1e-14):
    # Just return closest eigenpair to seed Rayleigh quotient
    sigma = float(x0 @ (A @ x0) / (x0 @ x0))
    w, Z = np.linalg.eigh(A)
    j = int(np.argmin(np.abs(w - sigma)))
    lam = float(w[j])
    v = Z[:, j]
    r = A @ v - lam * v
    return lam, v, float(np.linalg.norm(r)), 1

def validate():
    A = np.array([[3.0,1.0],[1.0,2.0]], dtype=float)
    x0 = np.array([1.0,0.2], dtype=float)
    lam, v, rn, k = inverse_iter_lib(A, x0, sigma=2.5)
    w, Z = np.linalg.eigh(A)
    assert abs(lam - max(w)) < 1e-12
    lam2, v2, rn2, k2 = rqi_lib(A, x0)
    assert min(abs(lam2 - w[0]), abs(lam2 - w[1])) < 1e-12

def main():
    validate()
    A = np.array([[2.0,0.0,0.0],[0.0,3.0,1.0],[0.0,1.0,4.0]])
    x0 = np.array([1.0,1.0,1.0])
    lam, v, rn, k = inverse_iter_lib(A, x0, sigma=3.2)
    print("lib inv lam", round(lam,8), "rn", rn, "iters", k)
    lam2, v2, rn2, k2 = rqi_lib(A, x0)
    print("lib rqi lam", round(lam2,8), "rn", rn2, "iters", k2)

if __name__ == "__main__":
    main()
\end{codepy}
\COMPLEXITY{
Each step dominated by solving $(A-\sigma I)y=x$: $\mathcal{O}(n^3)$ for dense
via factorization; with reused factorization for fixed $\sigma$, each step is
$\mathcal{O}(n^2)$. RQI requires a new factorization each step.}
\FAILMODES{
\begin{bullets}
\item Singular $(A-\sigma I)$; avoid exact eigenvalue shift.
\item Ill-conditioning near $\sigma\approx \lambda^\star$; use stable solvers.
\item Poor starting vector may converge to a different eigenpair.
\end{bullets}
}
\STABILITY{
\begin{bullets}
\item Use symmetric factorizations (e.g., LDL$^\top$) for Hermitian $A$.
\item Monitor residuals; tighten tolerance adaptively as residual shrinks.
\end{bullets}
}
\VALIDATION{
\begin{bullets}
\item Compare with NumPy eigenpairs.
\item Check cubic behavior of RQI near convergence by residual ratios.
\end{bullets}
}
\RESULT{
Fixed-shift converges geometrically; RQI locks rapidly when close, matching
theory.}
\EXPLANATION{
Matches Formula 2 and Formula 4; residuals and Rayleigh quotients match
Formula 3.}

\section{Applied Domains — Detailed End-to-End Scenarios}

\DomainPage{Machine Learning}
\SCENARIO{
Compute the first principal component (leading eigenvector of covariance) via
the power method and compare with NumPy.}
\ASSUMPTIONS{
\begin{bullets}
\item Data rows are centered or explicitly mean-subtracted.
\item Covariance is symmetric positive semidefinite.
\end{bullets}
}
\WHICHFORMULA{
Power method (Formula 1) on $C=\frac{1}{n}X^\top X$. Rayleigh quotient gives
the top eigenvalue (variance explained).}
\varmapStart
\var{X\in\mathbb{R}^{n\times d}}{data matrix with rows as samples.}
\var{C}{covariance matrix $\frac{1}{n}X^\top X$.}
\var{v_1}{first principal component.}
\var{\lambda_1}{top eigenvalue (variance along $v_1$).}
\varmapEnd
\PIPELINE{
\begin{bullets}
\item Generate synthetic data with known principal direction.
\item Form covariance and run power method.
\item Compare with NumPy; report variance explained ratio.
\end{bullets}
}
\textbf{Implementation (From Scratch)}
\begin{codepy}
import numpy as np

def gen_data(n=300, d=5, seed=0):
    rng = np.random.RandomState(seed)
    u = np.array([1.0] + [0.0]*(d-1))
    Z = rng.randn(n, d)
    X = 3.0*(Z @ np.eye(d)) + 10.0*np.outer(rng.randn(n), u)
    X -= X.mean(axis=0, keepdims=True)
    return X

def power_cov(C, iters=1000, tol=1e-10):
    d = C.shape[0]
    x = np.ones(d)
    x /= np.linalg.norm(x)
    for k in range(iters):
        y = C @ x
        ny = np.linalg.norm(y)
        if ny == 0.0:
            break
        x = y / ny
        lam = float(x @ (C @ x))
        r = C @ x - lam * x
        if np.linalg.norm(r) < tol:
            return lam, x, np.linalg.norm(r), k+1
    lam = float(x @ (C @ x))
    r = C @ x - lam * x
    return lam, x, np.linalg.norm(r), iters

def main():
    X = gen_data()
    C = (X.T @ X) / X.shape[0]
    lam, v, rn, k = power_cov(C)
    w, Z = np.linalg.eigh(C)
    lam_true, v_true = float(w[-1]), Z[:, -1]
    cosang = abs(v @ v_true)
    print("lam", round(lam,4), "true", round(lam_true,4),
          "cos", round(cosang,6), "rn", rn, "iters", k)

if __name__ == "__main__":
    main()
\end{codepy}
\textbf{Implementation (Library Version)}
\begin{codepy}
import numpy as np

def main():
    rng = np.random.RandomState(0)
    n, d = 300, 5
    u = np.array([1.0] + [0.0]*(d-1))
    Z = rng.randn(n, d)
    X = 3.0*(Z @ np.eye(d)) + 10.0*np.outer(rng.randn(n), u)
    X -= X.mean(axis=0, keepdims=True)
    C = (X.T @ X) / X.shape[0]
    w, Z = np.linalg.eigh(C)
    lam, v = float(w[-1]), Z[:, -1]
    r = C @ v - lam * v
    print("lib lam", round(lam,4), "rn", float(np.linalg.norm(r)))

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{Top eigenvalue and cosine similarity between computed and true vectors.}
\INTERPRET{The leading eigenvector estimates the direction of maximal variance.}
\NEXTSTEPS{Deflate to find subsequent components; use randomized starts.}

\DomainPage{Quantitative Finance}
\SCENARIO{
Estimate the first risk factor (principal component) of asset returns via the
power method on the covariance matrix and compute its variance contribution.}
\ASSUMPTIONS{
\begin{bullets}
\item Returns are centered and covariance is well-estimated.
\item Leading factor explains significant variance.
\end{bullets}
}
\WHICHFORMULA{
Power method (Formula 1) and Rayleigh quotient (Formula 3).}
\varmapStart
\var{R}{return matrix $(n\times d)$.}
\var{C}{covariance $C=\frac{1}{n}R^\top R$.}
\var{v_1}{first principal risk factor.}
\var{\lambda_1}{variance explained by $v_1$.}
\varmapEnd
\PIPELINE{
\begin{bullets}
\item Simulate correlated returns.
\item Compute $C$ and run power method.
\item Report $\lambda_1/\mathrm{trace}(C)$ as explained variance ratio.
\end{bullets}
}
\textbf{Implementation (Full Pipeline)}
\begin{codepy}
import numpy as np

def simulate(n=1000, d=4, seed=1):
    rng = np.random.RandomState(seed)
    A = rng.randn(d, d)
    C = A @ A.T
    R = rng.multivariate_normal(np.zeros(d), C, size=n)
    R -= R.mean(axis=0, keepdims=True)
    return R

def power_cov(C, iters=1000, tol=1e-12):
    d = C.shape[0]
    x = np.ones(d)
    x /= np.linalg.norm(x)
    for k in range(iters):
        y = C @ x
        ny = np.linalg.norm(y)
        x = y / ny
        lam = float(x @ (C @ x))
        r = C @ x - lam * x
        if np.linalg.norm(r) < tol:
            return lam, x, k+1
    return lam, x, iters

def main():
    R = simulate()
    C = (R.T @ R) / R.shape[0]
    lam, v, k = power_cov(C)
    evr = float(lam / np.trace(C))
    print("lam", round(lam,6), "evr", round(evr,4), "iters", k)

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{Top eigenvalue and explained variance ratio.}
\INTERPRET{The factor loads capture common co-movements across assets.}
\NEXTSTEPS{Use more components or shrinkage covariance estimates.}

\DomainPage{Deep Learning}
\SCENARIO{
Estimate the spectral norm of a weight matrix $W$ via the power method and
compare with exact singular value from SVD.}
\ASSUMPTIONS{
\begin{bullets}
\item Spectral norm equals the square root of the largest eigenvalue of
$W^\top W$.
\item $W$ fixed during evaluation.
\end{bullets}
}
\WHICHFORMULA{
Power method on $C=W^\top W$ (Formula 1), Rayleigh quotient (Formula 3).}
\varmapStart
\var{W\in\mathbb{R}^{m\times n}}{weight matrix.}
\var{\sigma_{\max}}{spectral norm of $W$.}
\var{v}{right singular vector associated with $\sigma_{\max}$.}
\varmapEnd
\PIPELINE{
\begin{bullets}
\item Generate $W$ with fixed seed.
\item Run power method on $C=W^\top W$ to get $\lambda_1$.
\item Compare $\sqrt{\lambda_1}$ with SVD $\sigma_{\max}$.
\end{bullets}
}
\textbf{Implementation (End-to-End)}
\begin{codepy}
import numpy as np

def spectral_norm_power(W, iters=2000, tol=1e-12):
    C = W.T @ W
    n = C.shape[0]
    x = np.ones(n)
    x /= np.linalg.norm(x)
    lam = float(x @ (C @ x))
    for k in range(iters):
        y = C @ x
        x = y / np.linalg.norm(y)
        lam = float(x @ (C @ x))
        r = C @ x - lam * x
        if np.linalg.norm(r) < tol:
            break
    return float(np.sqrt(lam)), x

def main():
    rng = np.random.RandomState(0)
    W = rng.randn(50, 20)
    sn, v = spectral_norm_power(W)
    u, s, vt = np.linalg.svd(W, full_matrices=False)
    print("power", round(sn,6), "svd", round(float(s[0]),6))

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{Compare spectral norms from power method and SVD.}
\INTERPRET{Spectral norm bounds Lipschitz constants and affects training
stability.}
\NEXTSTEPS{Track during training using a few power iterations per step.}

\DomainPage{Kaggle / Data Analytics}
\SCENARIO{
Perform PCA first component extraction on a synthetic dataset via power method
and report correlation with a known latent factor.}
\ASSUMPTIONS{
\begin{bullets}
\item Latent factor is linear with added noise.
\item Data centered and finite.
\end{bullets}
}
\WHICHFORMULA{
Power method on covariance (Formula 1) and Rayleigh quotient (Formula 3).}
\varmapStart
\var{X}{data with latent factor influencing first coordinate.}
\var{C}{covariance.}
\var{v_1}{first principal component.}
\var{z}{latent factor used in data generation.}
\varmapEnd
\PIPELINE{
\begin{bullets}
\item Generate data with latent $z$.
\item Compute $C$ and run power method.
\item Report correlation between scores $Xv_1$ and $z$.
\end{bullets}
}
\textbf{Implementation (Complete EDA Pipeline)}
\begin{codepy}
import numpy as np

def create_df(n=400, d=6, seed=0):
    rng = np.random.RandomState(seed)
    z = rng.randn(n)
    U = rng.randn(d, d)
    U, _ = np.linalg.qr(U)
    noise = rng.randn(n, d)
    X = np.outer(z, U[:, 0]) * 5.0 + noise
    X -= X.mean(axis=0, keepdims=True)
    return X, z

def power_pca(X, iters=1000, tol=1e-12):
    C = (X.T @ X) / X.shape[0]
    d = C.shape[0]
    x = np.ones(d)
    x /= np.linalg.norm(x)
    for k in range(iters):
        y = C @ x
        x = y / np.linalg.norm(y)
        lam = float(x @ (C @ x))
        r = C @ x - lam * x
        if np.linalg.norm(r) < tol:
            break
    return x

def main():
    X, z = create_df()
    v1 = power_pca(X)
    scores = X @ v1
    corr = float(np.corrcoef(scores, z)[0,1])
    print("corr", round(abs(corr),4))

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{Absolute correlation between principal scores and latent factor.}
\INTERPRET{High correlation indicates recovery of the latent direction.}
\NEXTSTEPS{Scale features, handle missing values, and extend to more components.}

\end{document}
% !TeX program = xelatex
\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{microtype,setspace,amsmath,amssymb,mathtools,amsthm,unicode-math}
\setstretch{1.05}
\setmainfont{Latin Modern Roman}
\setmonofont{Latin Modern Mono}
\setmathfont{Latin Modern Math}

\allowdisplaybreaks[4]
\setlength{\jot}{7pt}
\setlength{\emergencystretch}{8em}
\sloppy

\usepackage{xcolor,fancyhdr,enumitem,inconsolata,listings}
\pagestyle{fancy}\fancyhf{}\lhead{\nouppercase{\leftmark}}\rhead{\thepage}
\setlength{\headheight}{26pt}
\setlength{\parindent}{0pt}
\setlength{\parskip}{8pt plus 2pt minus 1pt}
\raggedbottom

\newenvironment{BreakableEquation}{\begin{equation}\begin{aligned}}{\end{aligned}\end{equation}}
\newenvironment{BreakableEquation*}{\begin{equation*}\begin{aligned}}{\end{aligned}\end{equation*}}
\newenvironment{tightalign}{\begingroup\small\allowdisplaybreaks\begin{align}}{\end{align}\endgroup}

\providecommand{\enumlistm}{enumitem}
\newenvironment{minted}[2][]{%
  \lstset{style=code,language=#2,#1}\begin{lstlisting}%
}{\end{lstlisting}}

\newcommand{\inputminted}[3][]{\begin{lstlisting}\end{lstlisting}}

\newlist{bullets}{itemize}{1}
\setlist[bullets]{label=--,leftmargin=1.6em,itemsep=2pt,topsep=4pt}

\newcommand{\varmapStart}{\textbf{VARIABLE MAPPING:}\par\begin{bullets}}
\newcommand{\var}[2]{\item $#1$ — #2}
\newcommand{\varmapEnd}{\end{bullets}}

\newcommand{\glossx}[6]{%
  \textbf{#1}\par
  \begin{bullets}
    \item \textbf{What:} #2
    \item \textbf{Why:} #3
    \item \textbf{How:} #4
    \item \textbf{ELI5:} #5
    \item \textbf{Pitfall/Example:} #6
  \end{bullets}
}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{claim}[theorem]{Claim}

\lstdefinestyle{code}{
  basicstyle=\ttfamily\footnotesize,
  backgroundcolor=\color{black!02},
  frame=single,
  numbers=left, numberstyle=\tiny, numbersep=8pt,
  breaklines=true, breakatwhitespace=true,
  tabsize=4, showstringspaces=false,
  upquote=true, keepspaces=true, columns=fullflexible,
  literate=
    {–}{{-}}1
    {—}{{-}}1
    {…}{{...}}1
    {≤}{{\ensuremath{\le}}}1
    {≥}{{\ensuremath{\ge}}}1
    {≠}{{\ensuremath{\ne}}}1
    {≈}{{\ensuremath{\approx}}}1
    {±}{{\ensuremath{\pm}}}1
    {→}{{\ensuremath{\to}}}1
    {←}{{\ensuremath{\leftarrow}}}1
    {∞}{{\ensuremath{\infty}}}1
    {√}{{\ensuremath{\sqrt{\ }}}}1
    {×}{{\ensuremath{\times}}}1
    {÷}{{\ensuremath{\div}}}1
}

\lstnewenvironment{codepy}[1][]%
  {\lstset{style=code,language=Python,#1}}%
  {}

\newcommand{\inlinecode}[1]{\lstinline[style=code]!#1!}

\newcommand{\LF}[2]{\par\noindent\textbf{#1:}~#2\par}
\newcommand{\WHAT}[1]{\LF{WHAT}{#1}}
\newcommand{\WHY}[1]{\LF{WHY}{#1}}
\newcommand{\HOW}[1]{\LF{HOW}{#1}}
\newcommand{\ELI}[1]{\LF{ELI5}{#1}}
\newcommand{\SCOPE}[1]{\LF{SCOPE}{#1}}
\newcommand{\CONFUSIONS}[1]{\LF{COMMON CONFUSIONS}{#1}}
\newcommand{\APPLICATIONS}[1]{\LF{APPLICATIONS}{#1}}
\newcommand{\FORMULA}[1]{\LF{FORMULA}{#1}}
\newcommand{\CANONICAL}[1]{\LF{CANONICAL FORM}{#1}}
\newcommand{\PRECONDS}[1]{\LF{PRECONDITIONS}{#1}}
\newcommand{\DERIVATION}[1]{\LF{DERIVATION}{#1}}
\newcommand{\EQUIV}[1]{\LF{EQUIVALENT FORMS}{#1}}
\newcommand{\LIMITS}[1]{\LF{LIMIT CASES}{#1}}
\newcommand{\INPUTS}[1]{\LF{INPUTS}{#1}}
\newcommand{\OUTPUTS}[1]{\LF{OUTPUTS}{#1}}
\newcommand{\RESULT}[1]{\LF{RESULT}{#1}}
\newcommand{\INTUITION}[1]{\LF{INTUITION}{#1}}
\newcommand{\PITFALLS}[1]{\LF{PITFALLS}{#1}}
\newcommand{\MODEL}[1]{\LF{CANONICAL MATH MODEL}{#1}}
\newcommand{\ASSUMPTIONS}[1]{\LF{ASSUMPTIONS}{#1}}
\newcommand{\WHICHFORMULA}[1]{\LF{WHICH FORMULA \& WHY}{#1}}
\newcommand{\GOVERN}[1]{\LF{GOVERNING EQUATION(S)}{#1}}
\newcommand{\UNITCHECK}[1]{\LF{UNIT CHECK}{#1}}
\newcommand{\EDGECASES}[1]{\LF{EDGE CASES}{#1}}
\newcommand{\ALTERNATE}[1]{\LF{ALTERNATE APPROACH (sketch)}{#1}}
\newcommand{\PROBLEM}[1]{\LF{PROBLEM}{#1}}
\newcommand{\API}[1]{\LF{API}{#1}}
\newcommand{\COMPLEXITY}[1]{\LF{COMPLEXITY}{#1}}
\newcommand{\FAILMODES}[1]{\LF{FAILURE MODES}{#1}}
\newcommand{\STABILITY}[1]{\LF{NUMERICAL STABILITY}{#1}}
\newcommand{\VALIDATION}[1]{\LF{VALIDATION}{#1}}
\newcommand{\EXPLANATION}[1]{\LF{EXPLANATION}{#1}}
\newcommand{\SCENARIO}[1]{\LF{SCENARIO}{#1}}
\newcommand{\PIPELINE}[1]{\LF{PIPELINE STEPS}{#1}}
\newcommand{\METRICS}[1]{\LF{METRICS}{#1}}
\newcommand{\INTERPRET}[1]{\LF{INTERPRETATION}{#1}}
\newcommand{\NEXTSTEPS}[1]{\LF{LIMITATIONS \& NEXT STEPS}{#1}}

\usepackage{titlesec}
\titleformat{\section}{\Large\bfseries}{\thesection}{0.6em}{}
\titlespacing*{\section}{0pt}{*2}{*1}
\usepackage{etoolbox}
\pretocmd{\section}{\clearpage}{}{}

\newcommand{\FormulaPage}[2]{%
  \clearpage
  \section*{Formula #1 — #2}%
  \addcontentsline{toc}{section}{Formula #1 — #2}%
}
\newcommand{\ProblemPage}[2]{%
  \clearpage
  \subsection*{Problem #1: #2}%
  \addcontentsline{toc}{subsection}{Problem #1: #2}%
}
\newcommand{\CodeDemoPage}[1]{%
  \clearpage
  \subsection*{Coding Demo: #1}%
  \addcontentsline{toc}{subsection}{Coding Demo: #1}%
}
\newcommand{\DomainPage}[1]{%
  \clearpage
  \subsection*{#1 (End-to-End)}%
  \addcontentsline{toc}{subsection}{#1 (End-to-End)}%
}

\begin{document}
\title{Comprehensive Study Sheet — Rayleigh Quotient and Variational Characterization}
\date{\today}
\maketitle
\tableofcontents
\clearpage

\section{Concept Overview}
\WHAT{
Given a Hermitian (real symmetric) matrix $A\in\mathbb{C}^{n\times n}$, the
Rayleigh quotient of a nonzero vector $x\in\mathbb{C}^n$ is
$R_A(x)=\dfrac{x^*Ax}{x^*x}$, with domain $\mathbb{C}^n\setminus\{0\}$ and
codomain $\mathbb{R}$ when $A$ is Hermitian. Its structure is a homogeneous
degree-0 ratio of quadratic forms invariant to nonzero scalar scaling of $x$.
}
\WHY{
It characterizes eigenvalues variationally: for Hermitian $A$, the minimum and
maximum of $R_A$ over nonzero $x$ are the smallest and largest eigenvalues.
The full spectrum admits the Courant--Fischer min-max principle. This underpins
PCA, spectral clustering, finite element eigenproblems, and condition number
estimation. It is central in proofs and algorithms for symmetric eigenproblems.
}
\HOW{
1. Assume $A$ is Hermitian so $x^*Ax\in\mathbb{R}$ and eigenvectors form an
orthonormal basis.
2. Expand $x$ in an eigenbasis to express $R_A(x)$ as a convex combination of
eigenvalues with nonnegative weights summing to 1.
3. Deduce bounds and extremizers. Use Lagrange multipliers to show stationary
points satisfy $Ax=\lambda x$ with $\lambda=R_A(x)$.
4. Lift to subspaces to obtain Courant--Fischer. Interpret each stage as energy
over norm, and subspace constraints as boundary conditions or orthogonality.
}
\ELI{
Think of $A$ as a machine that stretches space differently in different
directions. The Rayleigh quotient measures the stretch factor you experience
when you push in direction $x$, normalized by your push size. The biggest and
smallest stretches are the extreme eigenvalues, and the best directions are the
corresponding eigenvectors.
}
\SCOPE{
Valid for Hermitian $A$ ensuring real values and min-max principles. For
general $A$, $R_A(x)$ can be complex and lacks variational ordering. The
generalized Rayleigh quotient $R_{A,M}(x)=\dfrac{x^*Ax}{x^*Mx}$ requires $M$
Hermitian positive definite. Degenerate case $x=0$ is excluded. For repeated
eigenvalues, extremizers form subspaces. Numerical optimization may stall at
saddle points for interior eigenvalues unless orthogonality constraints are
imposed.
}
\CONFUSIONS{
Rayleigh quotient vs. quadratic form: $x^*Ax$ scales with $\|x\|^2$, whereas
$R_A(x)$ is scale-invariant. Power method vs. Rayleigh quotient iteration:
power method uses $Ax$ normalization; Rayleigh quotient iteration uses shifts
from $R_A(x)$ and typically converges cubically near an eigenpair for Hermitian
$A$. PCA maximizes variance $x^TSx$ under $\|x\|=1$, which is a Rayleigh
quotient problem, not ordinary least squares.
}
\APPLICATIONS{
\begin{bullets}
\item Mathematical foundations: spectral theorem, min-max variational methods.
\item Computational modeling: Rayleigh--Ritz finite elements, modal analysis.
\item Physical/engineering: vibration modes via $x^TKx/x^TMx$.
\item Statistical/algorithmic: PCA, spectral clustering, graph cuts via Laplacian.
\end{bullets}
}

\textbf{ANALYTIC STRUCTURE.}
For Hermitian $A$, $R_A$ is real, smooth on $\mathbb{C}^n\setminus\{0\}$, and
constant on rays $\{\alpha x:\alpha\ne0\}$. Restricted to the unit sphere it is
continuous and attains extrema. It is neither convex nor concave globally, but
is a weighted average of eigenvalues in an eigenbasis.

\textbf{CANONICAL LINKS.}
Spectral theorem yields eigenbasis expansion. Courant--Fischer uses subspace
optimization. Interlacing for principal submatrices follows from restriction of
subspaces. Rayleigh--Ritz approximates eigenvalues in trial subspaces.

\textbf{PROBLEM-TYPE RECOGNITION HEURISTICS.}
\begin{bullets}
\item Objective of form $x^*Ax$ with $\|x\|=1$ or orthogonality constraints.
\item Questions asking for extreme values of quadratic forms over unit vectors.
\item References to PCA variance maximization or minimum energy modes.
\item Graph Laplacian problems with $x^TLx$ and $x\perp \mathbf{1}$.
\end{bullets}

\textbf{SOLUTION STRATEGY BLUEPRINT.}
\begin{bullets}
\item Translate to $R_A(x)$ on the unit sphere and state constraints.
\item Identify if global extremes or $k$-th extremes via Courant--Fischer.
\item Substitute eigenbasis or use Lagrange multipliers with constraints.
\item Simplify to eigenvalue relations and compute or bound.
\item Interpret as variance, energy, or frequency depending on context.
\end{bullets}

\textbf{CONCEPTUAL INVARIANTS.}
Scale invariance in $x$, orthogonal invariance under $A\mapsto Q^*AQ$,
ordering by Loewner partial order: $A\preceq B\Rightarrow R_A(x)\le R_B(x)$.

\textbf{EDGE INTUITION.}
As $x$ approaches an eigenvector, $R_A(x)$ approaches the corresponding
eigenvalue. For highly ill-conditioned $A$, $R_A(x)$ over random directions
clusters near $\mathrm{tr}(A)/n$ by concentration, but extremes remain at the
spectral edges.

\section{Glossary}
\glossx{Rayleigh Quotient}
{For Hermitian $A$, $R_A(x)=\dfrac{x^*Ax}{x^*x}$ for $x\ne0$.}
{Encodes eigenvalue bounds and variational characterizations.}
{Compute numerator and denominator, divide; invariant to scaling of $x$.}
{How much $A$ stretches direction $x$, normalized by the size of $x$.}
{Pitfall: using $x=0$ or non-Hermitian $A$ expecting real values.}

\glossx{Courant--Fischer Theorem}
{Min-max formulas for ordered eigenvalues of Hermitian matrices.}
{Characterizes every eigenvalue variationally using subspace extrema.}
{Optimize $R_A$ over $k$-dimensional subspaces or their orthogonal complements.}
{Choose the best $k$-dimensional stage to achieve the $k$-th best stretch.}
{Pitfall: forgetting orthogonality when seeking interior eigenvalues.}

\glossx{Rayleigh--Ritz Method}
{Approximates eigenvalues by restricting $R_A$ to a trial subspace.}
{Transforms large problems into smaller projected eigenproblems.}
{Form basis $B$, compute $B^*AB$, solve its eigenproblem; get Ritz values.}
{Search best stretch within a smaller room approximating the whole space.}
{Pitfall: non-orthonormal bases require a mass matrix $G=B^*B$.}

\glossx{Interlacing}
{Eigenvalues of principal submatrices lie between those of the full matrix.}
{Provides stability and monotonicity under restriction.}
{View as restricting Rayleigh quotients to coordinate subspaces.}
{Zooming into a subset of coordinates cannot create new extremes outside.}
{Pitfall: needs principal (not arbitrary) submatrices and Hermitian $A$.}

\section{Symbol Ledger}
\varmapStart
\var{A\in\mathbb{C}^{n\times n}}{Hermitian (real symmetric) matrix.}
\var{x\in\mathbb{C}^n}{Nonzero vector (often unit-norm).}
\var{R_A(x)}{Rayleigh quotient $\dfrac{x^*Ax}{x^*x}$.}
\var{\lambda_1\le\cdots\le\lambda_n}{Ordered eigenvalues of $A$.}
\var{u_i}{Orthornormal eigenvectors of $A$ with $Au_i=\lambda_i u_i$.}
\var{B\in\mathbb{C}^{n\times k}}{Basis matrix of a $k$-dimensional subspace.}
\var{C=B^*AB}{Projected (Ritz) matrix on a trial subspace.}
\var{L}{Graph Laplacian or general SPD matrix.}
\var{M\succ0}{Mass or weighting matrix for generalized quotients.}
\var{\mathcal{S}}{A subspace of $\mathbb{C}^n$.}
\var{P_{\mathcal{S}}}{Orthogonal projector onto $\mathcal{S}$.}
\var{\mathrm{tr}(A)}{Trace of $A$.}
\var{\|\cdot\|}{Euclidean norm on vectors; spectral norm on matrices.}
\varmapEnd

\section{Formula Canon — One Formula Per Page}

\FormulaPage{1}{Rayleigh Quotient Definition and Spectral Bounds}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
For Hermitian $A$, the Rayleigh quotient satisfies
$\lambda_1\le R_A(x)\le\lambda_n$ for all $x\ne0$, with equality iff
$x$ lies in the corresponding eigenspace.

\WHAT{
Measures the normalized quadratic form of $A$ along direction $x$ and bounds it
between spectral extremes.
}
\WHY{
Relates directional energies to eigenvalues and provides immediate bounds for
quadratic forms, conditioning, and stability.
}
\FORMULA{
\[
R_A(x)=\frac{x^*Ax}{x^*x},\quad x\ne0,\qquad
\lambda_1\le \inf_{x\ne0}R_A(x),\quad \sup_{x\ne0}R_A(x)\le\lambda_n.
\]
}
\CANONICAL{
Hermitian $A$ with eigen-decomposition $A=\sum_{i=1}^n\lambda_i u_i u_i^*$,
orthonormal $\{u_i\}$, unit sphere $\{x:\|x\|=1\}$ as feasible set.
}
\PRECONDS{
\begin{bullets}
\item $A=A^*$ to ensure $x^*Ax\in\mathbb{R}$ and spectral theorem applies.
\item $x\ne0$; often restrict to $\|x\|=1$ without loss of generality.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
Let $A=A^*$ with eigenpairs $(\lambda_i,u_i)$ orthonormal. For $x=\sum_i\alpha_i
u_i\ne0$, one has $R_A(x)=\dfrac{\sum_i \lambda_i |\alpha_i|^2}{\sum_i
|\alpha_i|^2}$.
\end{lemma}
\begin{proof}
Compute $x^*Ax=(\sum_i\overline{\alpha_i}u_i^*)A(\sum_j\alpha_j u_j)
=\sum_{i,j}\overline{\alpha_i}\alpha_j u_i^*A u_j
=\sum_{i,j}\overline{\alpha_i}\alpha_j \lambda_j u_i^*u_j
=\sum_j \lambda_j |\alpha_j|^2$. Also $x^*x=\sum_j |\alpha_j|^2$. Division
gives the claim.\qedhere
\end{proof}
\DERIVATION{
\begin{align*}
\text{Step 1:}~& x=\sum_i\alpha_i u_i,\ \|x\|^2=\sum_i|\alpha_i|^2.\\
\text{Step 2:}~& x^*Ax=\sum_i\lambda_i|\alpha_i|^2 \quad(\text{by orthogonality}).\\
\text{Step 3:}~& R_A(x)=\frac{\sum_i\lambda_i|\alpha_i|^2}{\sum_i|\alpha_i|^2}.\\
\text{Step 4:}~& \min_i\lambda_i \le R_A(x)\le \max_i\lambda_i\ \text{as a
weighted average}.\\
\text{Step 5:}~& R_A(x)=\lambda_k\ \Leftrightarrow\ |\alpha_i|^2=0\ \forall i
\ne k\ \text{(or eigen-subspace if repeated).}
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Expand $x$ in an eigenbasis or enforce $\|x\|=1$.
\item Evaluate $x^*Ax$ and divide by $x^*x$.
\item Bound the result between $\lambda_1$ and $\lambda_n$.
\item Check equality cases via eigenvector alignment.
\end{bullets}
\EQUIV{
\begin{bullets}
\item $R_{Q^*AQ}(Q^*x)=R_A(x)$ for unitary $Q$ (unitary invariance).
\item For SPD $A$, $R_A(x)=\|A^{1/2}x\|^2/\|x\|^2$.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item If $A=\alpha I$, then $R_A(x)=\alpha$ for all $x\ne0$.
\item If $A$ is indefinite, $R_A$ spans $[\lambda_1,\lambda_n]$.
\end{bullets}
}
\INPUTS{$A, x\ne0$ with $A=A^*$.}
\DERIVATION{
\begin{align*}
\text{Compute: } & R_A(x)=\frac{x^*Ax}{x^*x}. \\
\text{If } & A=\begin{bmatrix}2&1\\1&3\end{bmatrix},\
x=\begin{bmatrix}1\\2\end{bmatrix},\ x^*Ax=2+4+3\cdot4=18,\\
& x^*x=1+4=5,\quad R_A(x)=\frac{18}{5}=3.6.
\end{align*}
}
\RESULT{
$R_A(x)$ lies in $[\lambda_1,\lambda_n]$ and attains endpoints at eigenvectors.
}
\UNITCHECK{
Dimensionless ratio; invariant to $x\mapsto \alpha x$ with $\alpha\ne0$.
}
\PITFALLS{
\begin{bullets}
\item Using non-Hermitian $A$ and expecting real $R_A(x)$.
\item Forgetting to exclude $x=0$.
\item Mixing up quadratic form $x^*Ax$ with its normalized quotient.
\end{bullets}
}
\INTUITION{
$R_A(x)$ is a barycentric average of eigenvalues weighted by squared
coordinates of $x$ in the eigenbasis.
}
\CANONICAL{
\begin{bullets}
\item $R_A(x)=\sum_i \lambda_i w_i$ with $w_i\ge0$, $\sum_i w_i=1$.
\item Global extrema occur at eigen-directions.
\end{bullets}
}

\FormulaPage{2}{Stationary Points and Eigenpairs}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
On the unit sphere, stationary points of $R_A$ occur exactly at eigenvectors,
with stationary value equal to the corresponding eigenvalue.

\WHAT{
Gives the first-order optimality link: $\nabla R_A$ vanishes on the sphere
exactly at eigenvectors.
}
\WHY{
Enables eigenvalue algorithms based on optimization, explains Rayleigh quotient
iteration, and characterizes constrained extrema.
}
\FORMULA{
\[
\text{On }\|x\|=1,\quad \nabla_{x}\big(x^*Ax\big)-\lambda \nabla_{x}\big(x^*x\big)=0
\ \Rightarrow\ Ax=\lambda x,\ \lambda=R_A(x).
\]
}
\CANONICAL{
Constrained optimization of $f(x)=x^*Ax$ subject to $g(x)=x^*x-1=0$ using
Lagrange multipliers on $\mathbb{R}^n$ or $\mathbb{C}^n$ (treating real and
imag parts).
}
\PRECONDS{
\begin{bullets}
\item $A=A^*$ to ensure real objective and standard eigen-structure.
\item $x\ne0$; constrain to $\|x\|=1$ for compactness.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
For Hermitian $A$, the differential satisfies $d(x^*Ax)=2\Re\langle Ax,dx\rangle$.
\end{lemma}
\begin{proof}
Write $x^*Ax=\langle x,Ax\rangle$. Then
$d\langle x,Ax\rangle=\langle dx,Ax\rangle+\langle x,Adx\rangle
=\langle dx,Ax\rangle+\langle A^*x,dx\rangle
=2\Re\langle Ax,dx\rangle$ since $A=A^*$. \qedhere
\end{proof}
\DERIVATION{
\begin{align*}
\text{Step 1:}~& \max f(x)=x^*Ax\ \text{s.t.}\ g(x)=x^*x-1=0.\\
\text{Step 2:}~& \mathcal{L}(x,\lambda)=x^*Ax-\lambda(x^*x-1).\\
\text{Step 3:}~& d\mathcal{L}=2\Re\langle Ax,dx\rangle
-2\lambda\Re\langle x,dx\rangle=2\Re\langle Ax-\lambda x,dx\rangle.\\
\text{Step 4:}~& d\mathcal{L}=0\ \forall dx\ \Rightarrow\ Ax=\lambda x.\\
\text{Step 5:}~& \|x\|=1\ \Rightarrow\ \lambda=x^*Ax=R_A(x).
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Pose Lagrangian for $x^*Ax$ with $\|x\|=1$ and derive $Ax=\lambda x$.
\item Evaluate $\lambda$ by substituting back: $\lambda=R_A(x)$.
\item Classify stationary points using eigenvalue ordering.
\end{bullets}
}
\EQUIV{
\begin{bullets}
\item Stationarity of $R_A$ on rays equals eigen-equation on unit sphere.
\item For generalized case: $Kx=\lambda Mx$ from $x^TKx$ with $x^TMx=1$.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item Repeated eigenvalues yield a manifold of stationary points.
\item Interior eigenvalues are saddles absent orthogonality constraints.
\end{bullets}
}
\INPUTS{$A=A^*$ and a feasible $x$ with $\|x\|=1$.}
\DERIVATION{
\begin{align*}
\text{Example: } & A=\begin{bmatrix}4&1\\1&2\end{bmatrix}.\
\text{Solve } Ax=\lambda x.\\
& \det\begin{bmatrix}4-\lambda&1\\1&2-\lambda\end{bmatrix}
=(4-\lambda)(2-\lambda)-1=0\\
& \lambda^2-6\lambda+7=0\Rightarrow \lambda\in\{3\pm\sqrt{2}\}.\\
& \text{Eigenvectors are stationary points of } R_A \text{ on }\|x\|=1.
\end{align*}
}
\RESULT{
Stationary values of $R_A$ equal eigenvalues; stationary points are eigenvectors
(normalized), confirming global extrema at $\lambda_1,\lambda_n$.
}
\UNITCHECK{
Lagrange multiplier $\lambda$ is scalar and equals the dimensionless quotient.
}
\PITFALLS{
\begin{bullets}
\item Ignoring unit-norm constraint and claiming $\nabla R_A=0$ in $\mathbb{R}^n$.
\item Misclassifying saddle points as minima or maxima.
\end{bullets}
}
\INTUITION{
At a stationary direction, pushing a little in any tangent direction to the
sphere does not change $R_A$ to first order, which is the eigen-direction.
}
\CANONICAL{
\begin{bullets}
\item Constrained stationarity $\Rightarrow$ eigen-equation.
\item Rayleigh value equals the eigenvalue at stationary points.
\end{bullets}
}

\FormulaPage{3}{Courant--Fischer Variational Principle}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
For Hermitian $A$ with ordered eigenvalues $\lambda_1\le\cdots\le\lambda_n$,
\[
\lambda_k=\min_{\dim\mathcal{S}=k}\ \max_{\substack{x\in\mathcal{S}\\x\ne0}}
R_A(x)
=\max_{\dim\mathcal{T}=n-k+1}\ \min_{\substack{x\in\mathcal{T}\\x\ne0}} R_A(x).
\]

\WHAT{
Variational characterization of every eigenvalue via nested subspace extrema.
}
\WHY{
Foundation for subspace methods, error bounds, and interlacing. Guides how to
impose orthogonality to extract interior eigenvalues.
}
\FORMULA{
\[
\lambda_k=\min_{\dim\mathcal{S}=k}\ \max_{\|x\|=1,x\in\mathcal{S}} x^*Ax
=\max_{\dim\mathcal{T}=n-k+1}\ \min_{\|x\|=1,x\in\mathcal{T}} x^*Ax.
\]
}
\CANONICAL{
Orthogonal eigenbasis $U=[u_1,\dots,u_n]$ with $Au_i=\lambda_i u_i$. Subspaces
spanned by subsets of $\{u_i\}$ are extremal for the min-max problems.
}
\PRECONDS{
\begin{bullets}
\item $A=A^*$; eigenvectors form an orthonormal basis.
\item Compact feasible sets on the sphere ensure extrema exist.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
Let $\mathcal{S}=\mathrm{span}\{u_1,\dots,u_k\}$. Then
$\max_{\|x\|=1,x\in\mathcal{S}} R_A(x)=\lambda_k$.
\end{lemma}
\begin{proof}
For $x=\sum_{i=1}^k \alpha_i u_i$ with $\|x\|=1$, $R_A(x)=\sum_{i=1}^k
\lambda_i |\alpha_i|^2\le \lambda_k\sum_{i=1}^k |\alpha_i|^2=\lambda_k$, with
equality at $x=u_k$. \qedhere
\end{proof}
\begin{lemma}
If $\dim\mathcal{S}=k$, then $\max_{\|x\|=1,x\in\mathcal{S}} R_A(x)\ge \lambda_k$.
\end{lemma}
\begin{proof}
Since $\dim\mathcal{S}=k$, there exists $x\in\mathcal{S}$ orthogonal to
$u_{k+1},\dots,u_n$. Then $x\in\mathrm{span}\{u_1,\dots,u_k\}$ and the previous
lemma implies the maximum is at least $\lambda_k$. \qedhere
\end{proof}
\DERIVATION{
\begin{align*}
\text{Step 1:}~& \forall \mathcal{S},\ \max_{x\in\mathcal{S},\|x\|=1} R_A(x)
\ge \lambda_k\ \text{(by orthogonality argument).}\\
\text{Step 2:}~& \text{Choose }\mathcal{S}=\mathrm{span}\{u_1,\dots,u_k\}\\
& \Rightarrow \max_{x\in\mathcal{S},\|x\|=1} R_A(x)=\lambda_k.\\
\text{Step 3:}~& \min_{\dim\mathcal{S}=k}\ \max_{x\in\mathcal{S}} R_A(x)
=\lambda_k.\\
\text{Step 4:}~& \text{Symmetrically for } \max_{\dim\mathcal{T}=n-k+1}
\min_{x\in\mathcal{T}} R_A(x) \\
& \text{use } \mathcal{T}=\mathrm{span}\{u_k,\dots,u_n\}.
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Identify $k$ and whether min-max or max-min form is convenient.
\item Use orthogonality to eliminate higher (or lower) eigencomponents.
\item Construct extremal subspace using eigenvectors to bound or attain value.
\end{bullets}
}
\EQUIV{
\begin{bullets}
\item $\lambda_k=\min_{\substack{\|x\|=1\\x\perp u_1,\dots,u_{k-1}}} x^*Ax$.
\item $\lambda_k=\max_{\substack{\|x\|=1\\x\perp u_{k+1},\dots,u_n}} x^*Ax$.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item Repeated eigenvalues: any basis of the invariant subspace attains extrema.
\item $k=1$ or $k=n$ recovers global min and max of $R_A$.
\end{bullets}
}
\INPUTS{$A=A^*$, desired index $k\in\{1,\dots,n\}$.}
\DERIVATION{
\begin{align*}
\text{Example: } & A=\mathrm{diag}(1,3,5),\ k=2.\\
& \min_{\dim \mathcal{S}=2}\max_{\|x\|=1,x\in\mathcal{S}} x^*Ax=3.\\
& \text{Take }\mathcal{S}=\mathrm{span}\{e_1,e_2\}\Rightarrow \max=3.
\end{align*}
}
\RESULT{
Both min-max and max-min equal $\lambda_k$, achieved by eigen-subspaces.
}
\UNITCHECK{
All quantities are scalar energies on the unit sphere; dimensionless.
}
\PITFALLS{
\begin{bullets}
\item Optimizing over vectors instead of subspaces for interior eigenvalues.
\item Forgetting orthogonality when extracting $\lambda_k$ for $k>1$.
\end{bullets}
}
\INTUITION{
To force the $k$-th best stretch, choose a $k$-dimensional stage where the
worst case is as small as possible; it lands exactly on $\lambda_k$.
}
\CANONICAL{
\begin{bullets}
\item Subspace extremization equals spectral selection.
\item Orthogonality enforces index-wise eigenvalue extraction.
\end{bullets}
}

\FormulaPage{4}{Interlacing via Rayleigh Quotients}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Let $A\in\mathbb{C}^{n\times n}$ be Hermitian with eigenvalues
$\lambda_1\le\cdots\le\lambda_n$. Let $B$ be a principal $(n-1)\times(n-1)$
submatrix with eigenvalues $\mu_1\le\cdots\le\mu_{n-1}$. Then
$\lambda_1\le \mu_1\le \lambda_2\le \cdots \le \mu_{n-1}\le \lambda_n$.

\WHAT{
Eigenvalues of a principal submatrix interlace those of the full matrix, via
restriction of Rayleigh quotients to coordinate subspaces.
}
\WHY{
Guarantees spectral stability under elimination, used in numerical analysis,
graph spectra, and rank-one updates.
}
\FORMULA{
\[
\mu_k=\min_{\dim\mathcal{S}=k,\ \mathcal{S}\subset\mathbb{C}^{n-1}}
\max_{\substack{x\in\mathcal{S}\\\|x\|=1}} x^*Bx,
\quad
\lambda_k=\min_{\dim\mathcal{S}=k}
\max_{\substack{x\in\mathcal{S}\\\|x\|=1}} x^*Ax.
\]
}
\CANONICAL{
Identify $\mathbb{C}^{n-1}$ with the coordinate subspace
$\mathcal{H}=\{(y,0):y\in\mathbb{C}^{n-1}\}\subset\mathbb{C}^{n}$; then
$y^*By=R_{A}((y,0))$ restricted to $\mathcal{H}$.
}
\PRECONDS{
\begin{bullets}
\item $A=A^*$; $B$ is a principal submatrix.
\item Use Courant--Fischer on $A$ and $B$.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
For any $k$, $\min_{\dim\mathcal{S}=k,\ \mathcal{S}\subset\mathcal{H}}
\max_{x\in\mathcal{S},\|x\|=1} R_A(x)\ge \lambda_k$.
\end{lemma}
\begin{proof}
The minimization over a subset of subspaces (those inside $\mathcal{H}$) is
no smaller than minimization over all $k$-dimensional subspaces, hence the
lower bound $\lambda_k$ by Courant--Fischer for $A$. \qedhere
\end{proof}
\DERIVATION{
\begin{align*}
\text{Step 1:}~& \mu_k=\min_{\dim\mathcal{S}=k,\ \mathcal{S}\subset\mathcal{H}}
\max_{\|x\|=1} R_A(x)\ \text{(identify }B\text{ with restriction).}\\
\text{Step 2:}~& \lambda_k=\min_{\dim\mathcal{S}=k}\max_{\|x\|=1} R_A(x).\\
\text{Step 3:}~& \text{Since the feasible set for }\mu_k\text{ is smaller, }
\mu_k\ge \lambda_k.\\
\text{Step 4:}~& \text{Similarly, using complements or max-min form gives }
\mu_k\le \lambda_{k+1}.\\
\text{Step 5:}~& \Rightarrow \lambda_k\le \mu_k\le \lambda_{k+1}
\ \text{for all }k.
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Embed submatrix into the full space via a coordinate subspace.
\item Apply Courant--Fischer to both and compare feasible families.
\item Conclude interlacing by bounding each $\mu_k$ between $\lambda_k$ and
$\lambda_{k+1}$.
\end{bullets}
}
\EQUIV{
\begin{bullets}
\item Poincar\'e separation theorem for $B=Q^*AQ$ with $Q$ having orthonormal
columns is a generalization.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item Equalities occur when eigenvectors have zero component on the removed
coordinate.
\item For non-principal submatrices interlacing may fail.
\end{bullets}
}
\INPUTS{$A=A^*$, principal index set defining $B$.}
\DERIVATION{
\begin{align*}
\text{Example: } & A=\begin{bmatrix}2&1&0\\1&2&1\\0&1&2\end{bmatrix}.\\
& \text{Eigenvalues } \lambda\approx\{0.5858,2,3.4142\}.\\
& B=\begin{bmatrix}2&1\\1&2\end{bmatrix},\
\mu=\{1,3\}.\\
& \lambda_1\le \mu_1\le \lambda_2\le \mu_2\le \lambda_3
\ \text{holds numerically.}
\end{align*}
}
\RESULT{
Interlacing inequalities $\lambda_k\le \mu_k\le \lambda_{k+1}$ for all $k$,
yielding the chain $\lambda_1\le \mu_1\le\cdots\le \mu_{n-1}\le \lambda_n$.
}
\UNITCHECK{
All quantities are dimensionless eigenvalues from Rayleigh quotients.
}
\PITFALLS{
\begin{bullets}
\item Confusing principal with arbitrary submatrices.
\item Neglecting Hermitian requirement.
\end{bullets}
}
\INTUITION{
Restricting directions to a coordinate subspace cannot produce a stretch
outside the neighboring global stretches.
}
\CANONICAL{
\begin{bullets}
\item Interlacing is min-max monotonicity under subspace restriction.
\item Principal submatrices correspond to coordinate subspaces.
\end{bullets}
}

\FormulaPage{5}{Rayleigh--Ritz Approximation}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Let $A=A^*$ and $B\in\mathbb{C}^{n\times k}$ have full column rank. The Ritz
values are eigenvalues of $C=B^*AB$ with respect to $G=B^*B$, and satisfy
\[
\theta=\frac{(Bc)^*A(Bc)}{(Bc)^*(Bc)}=\frac{c^*(B^*AB)c}{c^*(B^*B)c}.
\]

\WHAT{
Approximates eigenvalues of $A$ by restricting Rayleigh quotient to the trial
subspace $\mathcal{S}=\mathrm{range}(B)$.
}
\WHY{
Enables large-scale eigenvalue computations via low-dimensional problems and
underpins finite element eigenanalysis.
}
\FORMULA{
\[
\max_{\substack{y\in\mathcal{S}\\\|y\|=1}} y^*Ay
=\max_{\substack{c\ne0}} \frac{c^*(B^*AB)c}{c^*(B^*B)c},
\quad
(B^*AB)c=\theta (B^*B)c.
\]
}
\CANONICAL{
If columns of $B$ are orthonormal ($B^*B=I$), then $C=B^*AB$ is Hermitian and
its eigenvalues are the Ritz values in $\mathcal{S}$.
}
\PRECONDS{
\begin{bullets}
\item $A=A^*$; $B$ has full column rank.
\item If $B$ is not orthonormal, solve the generalized Hermitian eigenproblem.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
For $y=Bc\ne0$, $R_A(y)=R_{(B^*AB,\ B^*B)}(c)$, a generalized Rayleigh quotient.
\end{lemma}
\begin{proof}
Compute $(Bc)^*A(Bc)=c^*(B^*AB)c$ and $(Bc)^*(Bc)=c^*(B^*B)c$ directly, then
divide to obtain equality of quotients. \qedhere
\end{proof}
\DERIVATION{
\begin{align*}
\text{Step 1:}~& \mathcal{S}=\mathrm{range}(B),\ y=Bc.\\
\text{Step 2:}~& \max_{\|y\|=1,y\in\mathcal{S}} y^*Ay
=\max_{c\ne0}\frac{c^*(B^*AB)c}{c^*(B^*B)c}.\\
\text{Step 3:}~& \text{Stationarity in }c\ \Rightarrow\ (B^*AB)c=
\theta (B^*B)c.\\
\text{Step 4:}~& \text{If }B^*B=I,\ \theta \text{ are eigenvalues of } B^*AB.\\
\text{Step 5:}~& \text{Ritz vectors } y=Bc \text{ approximate eigenvectors of }A.
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Build a basis $B$ for the trial subspace; orthonormalize if possible.
\item Form $C=B^*AB$ (and $G=B^*B$ if needed).
\item Solve $C z=\theta z$ (or $C z=\theta G z$) for Ritz pairs.
\end{bullets}
}
\EQUIV{
\begin{bullets}
\item With $B$ orthonormal: generalized quotient reduces to standard quotient.
\item Krylov subspace methods construct $B$ implicitly (Lanczos).
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item Quality depends on how well $\mathcal{S}$ captures invariant subspaces.
\item Spurious Ritz values may appear if $B$ is ill-conditioned.
\end{bullets}
}
\INPUTS{$A=A^*$, basis $B$ for a trial subspace.}
\DERIVATION{
\begin{align*}
\text{Example: } & A=\begin{bmatrix}2&1&0\\1&2&1\\0&1&2\end{bmatrix},\
B=\begin{bmatrix}1&0\\0&1\\0&0\end{bmatrix}.\\
& C=B^*AB=\begin{bmatrix}2&1\\1&2\end{bmatrix},\ \theta=\{1,3\}.\\
& \text{These approximate } \{0.586,2,3.414\}\ \text{by restriction.}
\end{align*}
}
\RESULT{
Ritz values from $B^*AB$ reproduce the extremal Rayleigh quotients within
$\mathcal{S}$ and approximate eigenvalues of $A$.
}
\UNITCHECK{
Quotients are dimensionless; orthonormalization keeps $G=I$.
}
\PITFALLS{
\begin{bullets}
\item Forgetting $G=B^*B\ne I$ when $B$ is not orthonormal.
\item Using rank-deficient $B$ leading to singular $G$.
\end{bullets}
}
\INTUITION{
Optimize energy over a smaller stage capturing the essential directions; its
extremes mimic those of the full stage.
}
\CANONICAL{
\begin{bullets}
\item $R_A$ restricted to $\mathcal{S}$ equals generalized quotient in $c$.
\item Ritz values solve a (generalized) Hermitian eigenproblem.
\end{bullets}
}

\section{10 Exhaustive Problems and Solutions}

\ProblemPage{1}{Extrema of a Quadratic Form via Rayleigh Quotient}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Find $\min_{\|x\|=1} x^*Ax$ and $\max_{\|x\|=1} x^*Ax$ for
$A=\begin{bmatrix}3&2\\2&1\end{bmatrix}$, and the maximizing/minimizing
directions.

\PROBLEM{
Compute the extremal Rayleigh quotient values and corresponding eigenvectors,
verifying the spectral bounds and stationary characterization.
}
\MODEL{
\[
R_A(x)=\frac{x^*Ax}{x^*x},\ \|x\|=1,\ A=A^T=\begin{bmatrix}3&2\\2&1\end{bmatrix}.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $A$ is real symmetric.
\item Optimization over the unit circle is sufficient.
\end{bullets}
}
\varmapStart
\var{A}{Given symmetric $2\times2$ matrix.}
\var{x}{Unit vector in $\mathbb{R}^2$.}
\var{R_A}{Rayleigh quotient of $A$.}
\varmapEnd
\WHICHFORMULA{
Use Formula 1 for bounds and Formula 2 for stationary points, i.e., solve
$Ax=\lambda x$ and set $\lambda=R_A(x)$.
}
\GOVERN{
\[
\det(A-\lambda I)=0,\quad Ax=\lambda x,\quad \|x\|=1.
\]
}
\INPUTS{$A=\begin{bmatrix}3&2\\2&1\end{bmatrix}$.}
\DERIVATION{
\begin{align*}
\text{Step 1:}~& \det\begin{bmatrix}3-\lambda&2\\2&1-\lambda\end{bmatrix}
=(3-\lambda)(1-\lambda)-4=0.\\
\text{Step 2:}~& \lambda^2-4\lambda-1=0\Rightarrow
\lambda_{\max,\min}=2\pm\sqrt{5}.\\
\text{Step 3:}~& \lambda_{\max}=2+\sqrt{5}\approx4.236,\
\lambda_{\min}=2-\sqrt{5}\approx-0.236.\\
\text{Step 4:}~& (A-\lambda I)x=0\ \Rightarrow\
\begin{cases}(3-\lambda)x_1+2x_2=0\\2x_1+(1-\lambda)x_2=0\end{cases}.\\
\text{Step 5:}~& \lambda_{\max}: (3-\lambda)x_1=-2x_2\Rightarrow
x\propto \begin{bmatrix}2\\ \lambda-3\end{bmatrix}.\\
\text{Step 6:}~& \lambda_{\min}: x\propto \begin{bmatrix}2\\ \lambda-3\end{bmatrix}
\text{ with }\lambda=2-\sqrt{5}.\\
\text{Step 7:}~& \text{Normalize }x \text{ to }\|x\|=1.
\end{align*}
}
\RESULT{
$\min R_A=\lambda_{\min}=2-\sqrt{5}$ at the eigenvector for $\lambda_{\min}$;
$\max R_A=\lambda_{\max}=2+\sqrt{5}$ at the eigenvector for $\lambda_{\max}$.
}
\UNITCHECK{
Rayleigh quotient is dimensionless and matches eigenvalues of $A$.
}
\EDGECASES{
\begin{bullets}
\item If off-diagonals were zero, eigenvectors align with axes.
\item If $A=\alpha I$, all directions are optimal with value $\alpha$.
\end{bullets}
}
\ALTERNATE{
Parameterize $x=(\cos\theta,\sin\theta)^T$ and optimize
$f(\theta)=x^TAx$ over $\theta$ using calculus to find extrema.
}
\VALIDATION{
\begin{bullets}
\item Substitute eigenvectors into $R_A$ to recover eigenvalues.
\item Numeric check using a calculator confirms values.
\end{bullets}
}
\INTUITION{
Extremes align with principal axes of the quadratic form.
}
\CANONICAL{
\begin{bullets}
\item Extremal Rayleigh quotients equal spectral extremes.
\item Eigenvectors give the optimizing directions.
\end{bullets}
}

\ProblemPage{2}{Stationarity Implies Eigenvector}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Show that if $x$ with $\|x\|=1$ is a stationary point of $R_A$ on the sphere,
then $x$ is an eigenvector of $A$.

\PROBLEM{
Prove the necessary condition of optimality for the Rayleigh quotient on the
sphere via Lagrange multipliers.
}
\MODEL{
\[
\max_{x} x^*Ax\ \text{s.t.}\ x^*x=1 \quad\Longrightarrow\quad Ax=\lambda x.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $A=A^*$.
\item Smooth optimization on the sphere.
\end{bullets}
}
\varmapStart
\var{A}{Hermitian matrix.}
\var{x}{Unit vector at stationarity.}
\var{\lambda}{Lagrange multiplier, equals $R_A(x)$.}
\varmapEnd
\WHICHFORMULA{
Use Formula 2: stationarity conditions yield $Ax=\lambda x$.
}
\GOVERN{
\[
\nabla_x\big(x^*Ax-\lambda(x^*x-1)\big)=0.
\]
}
\INPUTS{$A=A^*$.}
\DERIVATION{
\begin{align*}
\text{Step 1:}~& \mathcal{L}(x,\lambda)=x^*Ax-\lambda(x^*x-1).\\
\text{Step 2:}~& d\mathcal{L}=2\Re\langle Ax-\lambda x,dx\rangle.\\
\text{Step 3:}~& d\mathcal{L}=0\ \forall dx\Rightarrow Ax=\lambda x.\\
\text{Step 4:}~& \|x\|=1\Rightarrow \lambda=x^*Ax=R_A(x).
\end{align*}
}
\RESULT{
Stationarity forces $x$ to be an eigenvector with eigenvalue $R_A(x)$.
}
\UNITCHECK{
Scalar equality consistent; $R_A(x)$ real when $A=A^*$.
}
\EDGECASES{
\begin{bullets}
\item If $\lambda$ is multiple, any vector in the eigenspace is stationary.
\item Without constraint, the gradient does not vanish except at $x=0$.
\end{bullets}
}
\ALTERNATE{
Project gradient onto the tangent space of the sphere and set it to zero:
$(I-xx^*)Ax=0\Rightarrow Ax=\lambda x$ with $\lambda=x^*Ax$.
}
\VALIDATION{
\begin{bullets}
\item Direct substitution for known eigenvectors verifies stationarity.
\end{bullets}
}
\INTUITION{
At an eigenvector, moving along the sphere does not change the quotient to
first order.
}
\CANONICAL{
\begin{bullets}
\item Lagrange multiplier equals the Rayleigh quotient.
\item Tangential gradient vanishes iff $x$ is an eigenvector.
\end{bullets}
}

\ProblemPage{3}{Courant--Fischer Extraction of the Second Eigenvalue}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Given $A=\begin{bmatrix}2&-1&0\\-1&2&-1\\0&-1&2\end{bmatrix}$, compute
$\lambda_2$ using the variational form
$\lambda_2=\min_{\|x\|=1,\ x\perp u_1} x^*Ax$.

\PROBLEM{
Find the middle eigenvalue of the $3\times 3$ tridiagonal SPD matrix using the
orthogonality-constrained Rayleigh quotient.
}
\MODEL{
\[
\lambda_2=\min_{\|x\|=1,\ x\perp u_1} x^*Ax,\quad Au_1=\lambda_1 u_1.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $A$ is SPD with simple spectrum.
\item $u_1$ is the normalized eigenvector for $\lambda_1$.
\end{bullets}
}
\varmapStart
\var{A}{Tridiagonal SPD stiffness matrix.}
\var{\lambda_i}{Ordered eigenvalues of $A$.}
\var{u_i}{Orthonormal eigenvectors.}
\varmapEnd
\WHICHFORMULA{
Use Formula 3 equivalence $\lambda_2=\min_{\|x\|=1,x\perp u_1} x^*Ax$.
}
\GOVERN{
\[
\det(A-\lambda I)=0,\quad \lambda_2=\min_{\|x\|=1,x\perp u_1} x^*Ax.
\]
}
\INPUTS{$A$ as given.}
\DERIVATION{
\begin{align*}
\text{Step 1:}~& \text{Solve eigenvalues analytically: }
\lambda_j=2-2\cos\frac{j\pi}{4}.\\
\text{Step 2:}~& \lambda_2=2-2\cos\frac{2\pi}{4}=2-2\cdot 0=2.\\
\text{Step 3:}~& \text{Check by Rayleigh quotient with }x\propto (1,0,-1)^T.\\
\text{Step 4:}~& x^*Ax=(1,0,-1)\begin{bmatrix}2&-1&0\\-1&2&-1\\0&-1&2\end{bmatrix}
\begin{bmatrix}1\\0\\-1\end{bmatrix}\\
&=(1,0,-1)\begin{bmatrix}2\\-2\\-2\end{bmatrix}=4.\\
\text{Step 5:}~& \|x\|^2=2\Rightarrow R_A(x)=4/2=2=\lambda_2.
\end{align*}
}
\RESULT{
$\lambda_2=2$, achieved at $x\propto (1,0,-1)^T$, orthogonal to $u_1$.
}
\UNITCHECK{
Dimensionless; consistent with tridiagonal closed-form spectrum.
}
\EDGECASES{
\begin{bullets}
\item For larger $n$, $\lambda_j\to 0$ as $j\to 0$ in scaled limits.
\item Orthogonality constraint is essential to avoid $\lambda_1$.
\end{bullets}
}
\ALTERNATE{
Use the closed-form eigenpairs of discrete Laplacian
$u_j(k)\propto \sin\frac{jk\pi}{n+1}$ to read off $\lambda_2$.
}
\VALIDATION{
\begin{bullets}
\item Direct determinant gives eigenvalues approximately $\{0.586,2,3.414\}$.
\end{bullets}
}
\INTUITION{
Eliminating the softest mode exposes the next stiffness direction.
}
\CANONICAL{
\begin{bullets}
\item Orthogonality-constrained $R_A$ extracts interior eigenvalues.
\end{bullets}
}

\ProblemPage{4}{Narrative: Alice and Bob Find the Most Variable Direction}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Alice claims the direction of maximum data variance is found by maximizing the
Rayleigh quotient of the covariance $S$. Bob doubts this. Decide.

\PROBLEM{
Given centered data matrix $X\in\mathbb{R}^{n\times d}$, with covariance
$S=\frac{1}{n}X^TX$, show the first principal component maximizes $x^TSx$ over
$\|x\|=1$, i.e., the Rayleigh quotient of $S$.
}
\MODEL{
\[
\max_{\|x\|=1} x^TSx=\lambda_{\max}(S),\quad S=S^T\succeq0.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Data are centered; $S$ is symmetric positive semidefinite.
\item Principal components are eigenvectors of $S$.
\end{bullets}
}
\varmapStart
\var{X}{Centered data matrix.}
\var{S}{Covariance $\frac{1}{n}X^TX$.}
\var{x}{Unit direction in feature space.}
\var{\lambda_{\max}}{Largest eigenvalue of $S$.}
\varmapEnd
\WHICHFORMULA{
Apply Formula 1 for bounds and Formula 2 for stationarity, concluding the
maximum of $R_S(x)$ is $\lambda_{\max}$ at its eigenvector.
}
\GOVERN{
\[
\max_{\|x\|=1} x^TSx,\quad S=S^T,\quad \text{PCA: } Sx=\lambda x.
\]
}
\INPUTS{$S=\frac{1}{n}X^TX$.}
\DERIVATION{
\begin{align*}
\text{Step 1:}~& x^TSx=\frac{1}{n}\|Xx\|^2\ge 0.\\
\text{Step 2:}~& \max_{\|x\|=1} x^TSx=\lambda_{\max}(S)\ \text{(Formula 1)}.\\
\text{Step 3:}~& \nabla \text{ with }\|x\|=1\Rightarrow Sx=\lambda x.\\
\text{Step 4:}~& \text{Direction }x=u_{\max}\ \text{is first principal component.}
\end{align*}
}
\RESULT{
Alice is correct. The most variable direction is the top eigenvector of $S$,
achieving variance $\lambda_{\max}(S)$.
}
\UNITCHECK{
Variance equals Rayleigh quotient; nonnegative and scale-invariant in $x$.
}
\EDGECASES{
\begin{bullets}
\item If $S=0$, any direction has zero variance.
\item If top eigenvalue is repeated, any vector in its eigenspace is optimal.
\end{bullets}
}
\ALTERNATE{
Maximize $\|Xx\|^2$ subject to $\|x\|=1$; use SVD $X=U\Sigma V^T$ to deduce
$x=v_1$ and variance $\sigma_1^2/n$.
}
\VALIDATION{
\begin{bullets}
\item Compute numerically on a toy dataset and compare with eigen-decomposition.
\end{bullets}
}
\INTUITION{
Data spread is measured by projected squared norm; choose direction of maximum
stretch of $S$.
}
\CANONICAL{
\begin{bullets}
\item PCA objective equals a Rayleigh quotient of the covariance.
\end{bullets}
}

\ProblemPage{5}{Narrative: Vibrational Mode with Mass Weighting}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
For stiffness $K=K^T\succ0$ and mass $M=M^T\succ0$, the fundamental frequency
squares are eigenvalues of $Kx=\lambda Mx$, maximizing the generalized Rayleigh
quotient $x^TKx/x^TMx$.

\PROBLEM{
Show the lowest frequency squared is $\min_{x\ne0} \dfrac{x^TKx}{x^TMx}$ and
derive the eigencondition by stationarity.
}
\MODEL{
\[
R_{K,M}(x)=\frac{x^TKx}{x^TMx},\quad \lambda_{\min}
=\min_{x\ne0} R_{K,M}(x),\quad Kx=\lambda Mx.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $K,M$ SPD; generalized eigenproblem is well-posed.
\item Normalization $x^TMx=1$.
\end{bullets}
}
\varmapStart
\var{K}{Stiffness matrix (SPD).}
\var{M}{Mass matrix (SPD).}
\var{x}{Mode shape vector.}
\var{\lambda}{Squared natural frequency.}
\varmapEnd
\WHICHFORMULA{
Generalized Rayleigh quotient and Lagrange multiplier stationarity (Formula 2
analogue) lead to $Kx=\lambda Mx$.
}
\GOVERN{
\[
\max/\min\ R_{K,M}(x),\quad \nabla_x\big(x^TKx-\lambda(x^TMx-1)\big)=0.
\]
}
\INPUTS{$K,M\succ0$.}
\DERIVATION{
\begin{align*}
\text{Step 1:}~& \min_{x^TMx=1} x^TKx\ \Rightarrow\ \mathcal{L}=x^TKx-\lambda
(x^TMx-1).\\
\text{Step 2:}~& \nabla_x \mathcal{L}=2Kx-2\lambda Mx=0.\\
\text{Step 3:}~& Kx=\lambda Mx,\ \lambda=x^TKx\ \text{when }x^TMx=1.\\
\text{Step 4:}~& \lambda_{\min}=\min R_{K,M}(x)\ \text{(compactness)}.
\end{align*}
}
\RESULT{
Generalized eigenproblem $Kx=\lambda Mx$ with $\lambda_{\min}=\min R_{K,M}$,
and mode shapes normalized by $x^TMx=1$.
}
\UNITCHECK{
$R_{K,M}$ is ratio of energy to mass; yields frequency squared units.
}
\EDGECASES{
\begin{bullets}
\item If $M=I$, reduces to standard Rayleigh quotient.
\item If constraints (boundary conditions) apply, restrict feasible subspace.
\end{bullets}
}
\ALTERNATE{
Apply Cholesky $M=LL^T$; set $y=L^Tx$, then minimize
$\dfrac{y^T(L^{-1}KL^{-T})y}{y^Ty}$ and solve standard eigenproblem.
}
\VALIDATION{
\begin{bullets}
\item Numerical check on small $K,M$ reproduces generalized eigenvalues.
\end{bullets}
}
\INTUITION{
Lowest energy-to-mass ratio gives the slowest vibration squared frequency.
}
\CANONICAL{
\begin{bullets}
\item Generalized Rayleigh quotient reduces to standard via mass-normalization.
\end{bullets}
}

\ProblemPage{6}{Expectation Puzzle: Random Direction Average Quotient}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Let $u$ be uniformly random on the unit sphere in $\mathbb{R}^n$. Show that
$\mathbb{E}[R_A(u)]=\mathrm{tr}(A)/n$ for $A=A^T$.

\PROBLEM{
Compute the expected Rayleigh quotient over random directions using rotational
invariance, connecting to average variance and trace.
}
\MODEL{
\[
\mathbb{E}\left[\frac{u^TAu}{u^Tu}\right]=\frac{1}{n}\mathrm{tr}(A),\quad
\|u\|=1.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $u$ is Haar-uniform on the unit sphere; $A$ is real symmetric.
\end{bullets}
}
\varmapStart
\var{A}{Symmetric matrix.}
\var{u}{Uniform unit vector in $\mathbb{R}^n$.}
\var{\mathrm{tr}(A)}{Trace of $A$.}
\varmapEnd
\WHICHFORMULA{
Use unitary invariance of both $R_A$ and the Haar measure.
}
\GOVERN{
\[
\mathbb{E}[u u^T]=\frac{1}{n}I,\quad \mathbb{E}[u^TAu]=\mathrm{tr}(A\ \mathbb{E}
[uu^T]).
\]
}
\INPUTS{$A=A^T$, dimension $n$.}
\DERIVATION{
\begin{align*}
\text{Step 1:}~& \|u\|=1\Rightarrow R_A(u)=u^TAu.\\
\text{Step 2:}~& \mathbb{E}[uu^T]=\frac{1}{n}I\ \text{by rotational symmetry}.\\
\text{Step 3:}~& \mathbb{E}[u^TAu]=\mathrm{tr}(A\,\mathbb{E}[uu^T])
=\mathrm{tr}\Big(A\frac{I}{n}\Big)=\frac{1}{n}\mathrm{tr}(A).
\end{align*}
}
\RESULT{
$\mathbb{E}[R_A(u)]=\mathrm{tr}(A)/n$, the average eigenvalue.
}
\UNITCHECK{
Both sides are scalar averages; trace has same units as eigenvalues.
}
\EDGECASES{
\begin{bullets}
\item If $A=\alpha I$, then $R_A(u)=\alpha$ almost surely; equality holds.
\item Concentration improves with large $n$ by measure concentration.
\end{bullets}
}
\ALTERNATE{
Diagonalize $A=Q\Lambda Q^T$; invariance of $u$ under $Q$ reduces to diagonal
case with same trace.
}
\VALIDATION{
\begin{bullets}
\item Monte Carlo with fixed seed matches trace$/n$ within small tolerance.
\end{bullets}
}
\INTUITION{
Random direction sees the average stretch, which is the average eigenvalue.
}
\CANONICAL{
\begin{bullets}
\item Expected Rayleigh equals normalized trace for symmetric $A$.
\end{bullets}
}

\ProblemPage{7}{Proof: Loewner Order Monotonicity}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Show that if $A\preceq B$ (i.e., $B-A\succeq0$), then $R_A(x)\le R_B(x)$ for
all $x\ne0$, and deduce $\lambda_k(A)\le \lambda_k(B)$ for all $k$.

\PROBLEM{
Prove monotonicity of Rayleigh quotients and eigenvalues under Loewner order.
}
\MODEL{
\[
R_A(x)=\frac{x^*Ax}{x^*x}\le \frac{x^*Bx}{x^*x}=R_B(x),\quad
\lambda_k(A)\le \lambda_k(B).
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $A,B$ Hermitian; $B-A\succeq0$.
\end{bullets}
}
\varmapStart
\var{A,B}{Hermitian matrices with $A\preceq B$.}
\var{x}{Nonzero vector.}
\var{\lambda_k(\cdot)}{Ordered eigenvalues.}
\varmapEnd
\WHICHFORMULA{
Use Formula 1 bounds and Courant--Fischer (Formula 3) for eigenvalues.
}
\GOVERN{
\[
x^*(B-A)x\ge 0\Rightarrow R_A(x)\le R_B(x).
\]
}
\INPUTS{$A\preceq B$.}
\DERIVATION{
\begin{align*}
\text{Step 1:}~& x^*(B-A)x\ge 0\Rightarrow x^*Bx\ge x^*Ax.\\
\text{Step 2:}~& \frac{x^*Ax}{x^*x}\le \frac{x^*Bx}{x^*x},\ \forall x\ne0.\\
\text{Step 3:}~& \lambda_k(A)=\min_{\dim\mathcal{S}=k}
\max_{\|x\|=1,x\in\mathcal{S}} x^*Ax\\
& \le \min_{\dim\mathcal{S}=k}\max_{\|x\|=1,x\in\mathcal{S}} x^*Bx
=\lambda_k(B).
\end{align*}
}
\RESULT{
Pointwise Rayleigh monotonicity and eigenvalue-wise monotonicity hold.
}
\UNITCHECK{
Inequalities compare dimensionless scalar quantities coherently.
}
\EDGECASES{
\begin{bullets}
\item If $A=B$, all inequalities are equalities.
\item If $B-A$ has rank one, interlacing tightens for at most one index.
\end{bullets}
}
\ALTERNATE{
Use Weyl inequalities to deduce $\lambda_k(A)\le \lambda_k(B)$; the Rayleigh
inequality is the quadratic form definition of Loewner order.
}
\VALIDATION{
\begin{bullets}
\item Test numerically with random SPD $A$ and $B=A+vv^T$.
\end{bullets}
}
\INTUITION{
Adding positive energy cannot decrease directional energy ratios.
}
\CANONICAL{
\begin{bullets}
\item Loewner order propagates to Rayleigh quotients and spectra.
\end{bullets}
}

\ProblemPage{8}{Proof: Lipschitz Sensitivity of Rayleigh Quotients}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
For unit $x$, $|R_A(x)-R_B(x)|\le \|A-B\|_2$. Deduce
$|\lambda_k(A)-\lambda_k(B)|\le \|A-B\|_2$.

\PROBLEM{
Bound the perturbation of Rayleigh quotients and eigenvalues under spectral
norm perturbations.
}
\MODEL{
\[
|x^*(A-B)x|\le \|A-B\|_2\|x\|^2=\|A-B\|_2.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $A,B$ Hermitian; $\|x\|=1$.
\end{bullets}
}
\varmapStart
\var{A,B}{Hermitian matrices.}
\var{x}{Unit vector.}
\var{\|\cdot\|_2}{Spectral norm.}
\varmapEnd
\WHICHFORMULA{
Use Cauchy--Schwarz inequality and extremal characterizations (Formula 3).
}
\GOVERN{
\[
|x^*(A-B)x|\le \|A-B\|_2\|x\|^2.
\]
}
\INPUTS{$A,B$, unit $x$.}
\DERIVATION{
\begin{align*}
\text{Step 1:}~& |R_A(x)-R_B(x)|=|x^*(A-B)x|\ (\|x\|=1).\\
\text{Step 2:}~& |x^*(A-B)x|\le \|A-B\|_2\|x\|^2=\|A-B\|_2.\\
\text{Step 3:}~& \lambda_{\max}(A)=\max_{\|x\|=1} x^*Ax.\\
\text{Step 4:}~& |\lambda_{\max}(A)-\lambda_{\max}(B)|\\
& \le \max_{\|x\|=1} |x^*(A-B)x|\le \|A-B\|_2.\\
\text{Step 5:}~& \text{Apply to }-A,-B\ \text{for }\lambda_{\min}.\\
\text{Step 6:}~& Weyl-type bound yields $|\lambda_k(A)-\lambda_k(B)|\le \|A-B\|_2.
\end{align*}
}
\RESULT{
Rayleigh quotients are 1-Lipschitz in spectral norm; eigenvalues obey the same
global Lipschitz bound.
}
\UNITCHECK{
Inequalities compare like-dimension scalars; norms are consistent.
}
\EDGECASES{
\begin{bullets}
\item Equality may occur for rank-one aligned perturbations.
\item For Frobenius norm, constant changes accordingly.
\end{bullets}
}
\ALTERNATE{
Use variational forms for $\lambda_k$ with restricted subspaces and the same
Rayleigh bound inside each subspace.
}
\VALIDATION{
\begin{bullets}
\item Numerical experiments confirm inequality holds with margin.
\end{bullets}
}
\INTUITION{
A bounded operator cannot change directional energies by more than its size.
}
\CANONICAL{
\begin{bullets}
\item Spectral-norm perturbations control Rayleigh and eigenvalue changes.
\end{bullets}
}

\ProblemPage{9}{Combo: Graph Laplacian Fiedler Value}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
For a connected graph with Laplacian $L$, the algebraic connectivity is
$\lambda_2=\min_{\|x\|=1,\ x\perp \mathbf{1}} x^TLx$.

\PROBLEM{
Compute $\lambda_2$ for a 4-node path graph using the constrained Rayleigh
quotient and interpret the minimizing vector.
}
\MODEL{
\[
L=\begin{bmatrix}
1&-1&0&0\\-1&2&-1&0\\0&-1&2&-1\\0&0&-1&1
\end{bmatrix},\quad
\lambda_2=\min_{\|x\|=1,\,x\perp \mathbf{1}} x^TLx.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Graph is connected; $L\succeq0$, $L\mathbf{1}=0$.
\end{bullets}
}
\varmapStart
\var{L}{Graph Laplacian.}
\var{\mathbf{1}}{All-ones vector.}
\var{\lambda_2}{Second smallest eigenvalue.}
\varmapEnd
\WHICHFORMULA{
Use Formula 3 with orthogonality to the nullspace to extract $\lambda_2$.
}
\GOVERN{
\[
\lambda_2=\min_{\|x\|=1,x\perp \mathbf{1}} x^TLx.
\]
}
\INPUTS{$L$ of the path graph on 4 nodes.}
\DERIVATION{
\begin{align*}
\text{Step 1:}~& \text{Closed form for path }n=4:\ 
\lambda_j=2-2\cos\frac{j\pi}{5}.\\
\text{Step 2:}~& \lambda_2=2-2\cos\frac{2\pi}{5}=2-2\cdot 0.3090\approx1.3820.\\
\text{Step 3:}~& x^TLx=\sum_{(i,j)\in E} (x_i-x_j)^2.\\
\text{Step 4:}~& Minimizer balances adjacent differences with $x\perp \mathbf{1}$.
\end{align*}
}
\RESULT{
$\lambda_2\approx 1.3820$, the algebraic connectivity, minimized by the Fiedler
vector orthogonal to $\mathbf{1}$.
}
\UNITCHECK{
Nonnegative energy; invariant to adding constants under orthogonality.
}
\EDGECASES{
\begin{bullets}
\item If graph disconnected, $\lambda_2=0$.
\item For a complete graph $K_n$, $\lambda_2=n$.
\end{bullets}
}
\ALTERNATE{
Solve $Lx=\lambda x$ with $x\perp \mathbf{1}$ to obtain the same value.
}
\VALIDATION{
\begin{bullets}
\item Direct eigen-decomposition of $L$ confirms the value.
\end{bullets}
}
\INTUITION{
Penalize edge differences; the smoothest nonconstant signal is the Fiedler
mode.
}
\CANONICAL{
\begin{bullets}
\item Constrained Rayleigh quotient yields the first nontrivial Laplacian mode.
\end{bullets}
}

\ProblemPage{10}{Combo: Condition Number via Rayleigh Quotient}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
For SPD $A$, $\kappa_2(A)=\dfrac{\lambda_{\max}(A)}{\lambda_{\min}(A)}
=\dfrac{\max_{\|x\|=1} R_A(x)}{\min_{\|x\|=1} R_A(x)}$.

\PROBLEM{
Express the 2-norm condition number of SPD $A$ purely via extremes of the
Rayleigh quotient and compute it for a specific $A$.
}
\MODEL{
\[
\kappa_2(A)=\frac{\max_{\|x\|=1} x^TAx}{\min_{\|x\|=1} x^TAx},\quad
A=\begin{bmatrix}4&1\\1&1\end{bmatrix}.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $A$ is SPD so eigenvalues are positive.
\end{bullets}
}
\varmapStart
\var{A}{SPD matrix.}
\var{\kappa_2(A)}{2-norm condition number.}
\var{\lambda_{\max},\lambda_{\min}}{Extreme eigenvalues.}
\varmapEnd
\WHICHFORMULA{
Use Formula 1 for extrema and compute eigenvalues to get $\kappa_2(A)$.
}
\GOVERN{
\[
\det(A-\lambda I)=0,\quad \kappa_2=\lambda_{\max}/\lambda_{\min}.
\]
}
\INPUTS{$A=\begin{bmatrix}4&1\\1&1\end{bmatrix}$.}
\DERIVATION{
\begin{align*}
\text{Step 1:}~& \lambda^2-5\lambda+3=0\Rightarrow
\lambda=\frac{5\pm\sqrt{25-12}}{2}=\frac{5\pm\sqrt{13}}{2}.\\
\text{Step 2:}~& \lambda_{\max}=\frac{5+\sqrt{13}}{2}\approx4.303,\\
& \lambda_{\min}=\frac{5-\sqrt{13}}{2}\approx0.697.\\
\text{Step 3:}~& \kappa_2\approx 4.303/0.697\approx 6.172.
\end{align*}
}
\RESULT{
$\kappa_2(A)=\dfrac{5+\sqrt{13}}{5-\sqrt{13}}\approx 6.172$.
}
\UNITCHECK{
Unitless ratio of eigenvalues; consistent with Rayleigh quotient extremes.
}
\EDGECASES{
\begin{bullets}
\item If $A=\alpha I$, then $\kappa_2=1$.
\item Poor conditioning as $\lambda_{\min}\to 0^+$.
\end{bullets}
}
\ALTERNATE{
Estimate bounds via Gershgorin to bracket $\kappa_2$ without exact eigenvalues.
}
\VALIDATION{
\begin{bullets}
\item Compute $x^TAx$ on eigenvectors to retrieve exact eigenvalues.
\end{bullets}
}
\INTUITION{
Conditioning is the spread of directional energies measured by $R_A$.
}
\CANONICAL{
\begin{bullets}
\item $\kappa_2$ equals the ratio of maximum to minimum Rayleigh quotients.
\end{bullets}
}

\section{Coding Demonstrations}

\CodeDemoPage{Rayleigh Quotient and Extreme Eigenvalues Verification}
\PROBLEM{
Compute $R_A(x)$, verify that eigenvectors achieve extrema equal to eigenvalues,
and numerically confirm bounds for random test vectors.
}
\API{
\begin{bullets}
\item \inlinecode{def read_input(s) -> np.ndarray} — parse matrix entries.
\item \inlinecode{def solve_case(A) -> dict} — compute eigens, checks.
\item \inlinecode{def validate() -> None} — self-tests with assertions.
\item \inlinecode{def main() -> None} — run validation and a demo.
\end{bullets}
}
\INPUTS{
Square symmetric matrix $A$ given row-wise; shapes inferred; dtype float.
}
\OUTPUTS{
Dictionary with eigenvalues, eigenvectors, extreme Rayleigh values,
and check flags confirming equalities within tolerance.
}
\FORMULA{
\[
R_A(x)=\frac{x^TAx}{x^Tx},\quad \max_{\|x\|=1}R_A(x)=\lambda_{\max},\ 
\min_{\|x\|=1}R_A(x)=\lambda_{\min}.
\]
}
\textbf{SOLUTION A — From Scratch (Mathematically Explicit Implementation)}
\begin{codepy}
import numpy as np

def read_input(s):
    vals = [float(x) for x in s.split()]
    n = int(round(np.sqrt(len(vals))))
    A = np.array(vals, dtype=float).reshape(n, n)
    return A

def rayleigh(A, x):
    return float(x.T @ A @ x / (x.T @ x))

def solve_case(A):
    # symmetric eigen-decomposition
    w, V = np.linalg.eigh(A)
    idx = np.argsort(w)
    w = w[idx]
    V = V[:, idx]
    # extremes via eigenvectors
    rmin = rayleigh(A, V[:, 0])
    rmax = rayleigh(A, V[:, -1])
    # random checks
    np.random.seed(0)
    X = np.random.randn(A.shape[0], 100)
    X /= np.linalg.norm(X, axis=0, keepdims=True)
    Rs = np.array([rayleigh(A, X[:, i]) for i in range(X.shape[1])])
    return {"eigvals": w, "eigvecs": V, "rmin": rmin, "rmax": rmax,
            "Rs_min": float(Rs.min()), "Rs_max": float(Rs.max())}

def validate():
    A = np.array([[3.0, 2.0], [2.0, 1.0]])
    out = solve_case(A)
    w = out["eigvals"]
    assert abs(out["rmin"] - w[0]) < 1e-10
    assert abs(out["rmax"] - w[-1]) < 1e-10
    assert out["Rs_min"] >= w[0] - 1e-10
    assert out["Rs_max"] <= w[-1] + 1e-10

def main():
    validate()
    A = np.array([[2.0, 1.0, 0.0],
                  [1.0, 2.0, 1.0],
                  [0.0, 1.0, 2.0]])
    out = solve_case(A)
    print("eigvals:", np.round(out["eigvals"], 6))
    print("rmin/rmax:", out["rmin"], out["rmax"])

if __name__ == "__main__":
    main()
\end{codepy}
\textbf{SOLUTION B — Library-Based (Validated Computational Shortcut)}
\begin{codepy}
import numpy as np

def read_input(s):
    vals = [float(x) for x in s.split()]
    n = int(round(np.sqrt(len(vals))))
    return np.array(vals, dtype=float).reshape(n, n)

def solve_case(A):
    w, V = np.linalg.eigh(A)
    w.sort()
    rmin = float(w[0])
    rmax = float(w[-1])
    return {"eigvals": w, "rmin": rmin, "rmax": rmax}

def validate():
    A = np.array([[3.0, 2.0], [2.0, 1.0]])
    out = solve_case(A)
    # closed-form eigenvalues: 2 +/- sqrt(5)
    lm = 2 - np.sqrt(5.0)
    lM = 2 + np.sqrt(5.0)
    assert abs(out["rmin"] - lm) < 1e-10
    assert abs(out["rmax"] - lM) < 1e-10

def main():
    validate()
    A = np.array([[2.0, 1.0, 0.0],
                  [1.0, 2.0, 1.0],
                  [0.0, 1.0, 2.0]])
    out = solve_case(A)
    print("eigvals:", np.round(out["eigvals"], 6))

if __name__ == "__main__":
    main()
\end{codepy}
\COMPLEXITY{
Time $\mathcal{O}(n^3)$ for eigen-decomposition; $\mathcal{O}(n)$ per
Rayleigh evaluation. Space $\mathcal{O}(n^2)$.
}
\FAILMODES{
\begin{bullets}
\item Non-symmetric input leads to complex eigenvalues; ensure symmetry.
\item Zero vector passed to \inlinecode{rayleigh}; guard by normalization.
\end{bullets}
}
\STABILITY{
\begin{bullets}
\item Eigen-decomposition is backward stable for Hermitian matrices.
\item Normalize $x$ to avoid overflow/underflow in quotient.
\end{bullets}
}
\VALIDATION{
\begin{bullets}
\item Assertions comparing Rayleigh at eigenvectors to eigenvalues.
\item Random-direction checks verify bounds numerically.
\end{bullets}
}
\RESULT{
Both implementations agree on eigenvalues and extremes of the Rayleigh
quotient. Random samples lie within spectral bounds.
}
\EXPLANATION{
Rayleigh quotient at eigenvectors equals eigenvalues (Formula 2), and global
extremes equal spectral extremes (Formula 1).
}
\EXTENSION{
Implement Rayleigh quotient iteration using shift $R_A(x)$ for cubic local
convergence on Hermitian problems.
}

\CodeDemoPage{Courant--Fischer and Rayleigh--Ritz in a Trial Subspace}
\PROBLEM{
Verify that restricting to a subspace $\mathcal{S}$ spanned by columns of $B$
yields Ritz values from $B^TAB$ that bound interior eigenvalues of $A$.
}
\API{
\begin{bullets}
\item \inlinecode{def build_subspace(A,k) -> B} — take first $k$ eigenvectors.
\item \inlinecode{def ritz(A,B) -> (theta,Z)} — Ritz values/vectors.
\item \inlinecode{def validate() -> None} — check interlacing and equalities.
\item \inlinecode{def main() -> None} — run with a random symmetric matrix.
\end{bullets}
}
\INPUTS{
Symmetric $A$, integer $k$ with $1\le k<n$. Deterministic seed for randomness.
}
\OUTPUTS{
Ritz values $\theta$, full eigenvalues, and checks of interlacing.
}
\FORMULA{
\[
\theta_i=\lambda_i(B^TAB)\ \text{with }B^TB=I,\quad
\lambda_i(A)\le \theta_i\le \lambda_{n-k+i}(A).
\]
}
\textbf{SOLUTION A — From Scratch (Mathematically Explicit Implementation)}
\begin{codepy}
import numpy as np

def build_subspace(A, k):
    w, V = np.linalg.eigh(A)
    idx = np.argsort(w)
    V = V[:, idx]
    return V[:, :k]

def ritz(A, B):
    # assume B has orthonormal columns
    C = B.T @ A @ B
    th, Z = np.linalg.eigh(C)
    idx = np.argsort(th)
    th = th[idx]
    Z = Z[:, idx]
    Y = B @ Z
    return th, Y

def validate():
    np.random.seed(1)
    G = np.random.randn(5, 5)
    A = (G + G.T) / 2.0
    w, V = np.linalg.eigh(A)
    w.sort()
    B = build_subspace(A, 3)
    th, Y = ritz(A, B)
    # interlacing: w[i] <= th[i] <= w[i+2]
    for i in range(3):
        assert w[i] - 1e-10 <= th[i] <= w[i+2] + 1e-10
    # each Ritz vector achieves its Rayleigh quotient
    for i in range(3):
        r = float(Y[:, i].T @ A @ Y[:, i] / (Y[:, i].T @ Y[:, i]))
        assert abs(r - th[i]) < 1e-10

def main():
    validate()
    np.random.seed(2)
    G = np.random.randn(6, 6)
    A = (G + G.T) / 2.0
    B = build_subspace(A, 2)
    th, Y = ritz(A, B)
    print("ritz:", np.round(th, 6))

if __name__ == "__main__":
    main()
\end{codepy}
\textbf{SOLUTION B — Library-Based (Validated Computational Shortcut)}
\begin{codepy}
import numpy as np

def build_subspace(A, k):
    w, V = np.linalg.eigh(A)
    idx = np.argsort(w)
    return V[:, idx][:, :k]

def ritz(A, B):
    # orthonormalize B via QR to ensure B^TB=I
    Q, _ = np.linalg.qr(B)
    C = Q.T @ A @ Q
    th, Z = np.linalg.eigh(C)
    idx = np.argsort(th)
    th = th[idx]
    Y = Q @ Z[:, idx]
    return th, Y

def validate():
    np.random.seed(3)
    G = np.random.randn(4, 4)
    A = (G + G.T) / 2.0
    w = np.linalg.eigvalsh(A)
    w.sort()
    th, _ = ritz(A, np.eye(4)[:, :2])
    assert w[0] - 1e-10 <= th[0] <= w[2] + 1e-10
    assert w[1] - 1e-10 <= th[1] <= w[3] + 1e-10

def main():
    validate()
    print("validated")

if __name__ == "__main__":
    main()
\end{codepy}
\COMPLEXITY{
Eigendecomposition $\mathcal{O}(n^3)$; forming $B^TAB$ costs $\mathcal{O}(nk^2)$;
solving $k\times k$ eigenproblem costs $\mathcal{O}(k^3)$.
}
\FAILMODES{
\begin{bullets}
\item Non-orthonormal $B$ without QR may yield numerical issues.
\item Rank-deficient $B$ invalidates Ritz computation.
\end{bullets}
}
\STABILITY{
\begin{bullets}
\item QR orthonormalization stabilizes the subspace basis.
\item Hermitian eigen-solvers are backward stable.
\end{bullets}
}
\VALIDATION{
\begin{bullets}
\item Interlacing inequalities verified by assertions.
\item Rayleigh equality check: $R_A(Y_i)=\theta_i$.
\end{bullets}
}
\RESULT{
Ritz values lie between appropriate eigenvalues and match Rayleigh quotients on
Ritz vectors, confirming Courant--Fischer restriction.
}
\EXPLANATION{
Restricting $R_A$ to range$(B)$ yields a generalized quotient in $c$; solving
$B^TAB$ recovers the subspace extrema as Ritz values.
}

\section{Applied Domains — Detailed End-to-End Scenarios}

\DomainPage{Machine Learning}
\SCENARIO{
Compute the first principal component by maximizing the Rayleigh quotient of
the covariance matrix on synthetic data.
}
\ASSUMPTIONS{
\begin{bullets}
\item Data are centered; covariance $S$ is symmetric PSD.
\item First principal component is unique.
\end{bullets}
}
\WHICHFORMULA{
Maximize $R_S(x)=\dfrac{x^TSx}{x^Tx}$; solution is top eigenvector of $S$ with
maximum value $\lambda_{\max}(S)$.
}
\varmapStart
\var{X}{Data matrix $(n\times d)$, centered.}
\var{S}{Covariance $X^TX/n$.}
\var{x}{Unit direction in $\mathbb{R}^d$.}
\var{\lambda_{\max}}{Top eigenvalue of $S$.}
\varmapEnd
\PIPELINE{
\begin{bullets}
\item Generate correlated 2D data.
\item Form $S$ and solve for eigenpairs.
\item Compare Rayleigh quotient of top eigenvector to $\lambda_{\max}$.
\end{bullets}
}
\textbf{Implementation (From Scratch)}
\begin{codepy}
import numpy as np

def generate(n=200, seed=0):
    np.random.seed(seed)
    z = np.random.randn(n, 2)
    A = np.array([[2.0, 1.0], [0.0, 0.5]])
    X = z @ A.T
    X -= X.mean(axis=0, keepdims=True)
    return X

def pca_first(X):
    S = (X.T @ X) / X.shape[0]
    w, V = np.linalg.eigh(S)
    idx = np.argsort(w)
    w = w[idx]
    V = V[:, idx]
    u = V[:, -1]
    lam = w[-1]
    rq = float(u.T @ S @ u / (u.T @ u))
    return lam, u, rq

def main():
    X = generate()
    lam, u, rq = pca_first(X)
    print("lambda_max:", round(lam, 6), "rq:", round(rq, 6))

if __name__ == "__main__":
    main()
\end{codepy}
\textbf{Implementation (Library Version)}
\begin{codepy}
import numpy as np

def main():
    np.random.seed(0)
    z = np.random.randn(200, 2)
    A = np.array([[2.0, 1.0], [0.0, 0.5]])
    X = z @ A.T
    X -= X.mean(axis=0, keepdims=True)
    S = (X.T @ X) / X.shape[0]
    w, V = np.linalg.eigh(S)
    print("eigvals:", np.round(np.sort(w), 6))

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{
Report $\lambda_{\max}$ and confirm $R_S(u_{\max})=\lambda_{\max}$.
}
\INTERPRET{
First PC is direction of maximum variance; Rayleigh quotient equals that
variance.
}
\NEXTSTEPS{
Extend to multiple PCs by enforcing orthogonality constraints sequentially.
}

\DomainPage{Quantitative Finance}
\SCENARIO{
Extract the dominant risk factor of asset returns by maximizing the Rayleigh
quotient of the sample covariance.
}
\ASSUMPTIONS{
\begin{bullets}
\item Returns are i.i.d. with covariance $\Sigma$.
\item Dominant factor corresponds to top eigenvector of $\Sigma$.
\end{bullets}
}
\WHICHFORMULA{
Maximize $x^T\Sigma x$ over $\|x\|=1$ to obtain $\lambda_{\max}$ and factor
loading vector $x=u_{\max}$.
}
\varmapStart
\var{R}{Returns matrix $(n\times d)$.}
\var{\Sigma}{Sample covariance $R^TR/n$.}
\var{u_{\max}}{Top eigenvector (dominant factor).}
\var{\lambda_{\max}}{Largest eigenvalue (factor variance).}
\varmapEnd
\PIPELINE{
\begin{bullets}
\item Simulate correlated returns.
\item Estimate $\Sigma$ and solve eigenproblem.
\item Verify Rayleigh quotient equals factor variance.
\end{bullets}
}
\textbf{Implementation (Full Pipeline)}
\begin{codepy}
import numpy as np

def simulate(n=500, d=3, seed=1):
    np.random.seed(seed)
    A = np.array([[0.2, 0.1, 0.0],
                  [0.0, 0.15, 0.05],
                  [0.0, 0.0, 0.1]])
    Z = np.random.randn(n, d)
    R = Z @ (A + A.T)
    R -= R.mean(axis=0, keepdims=True)
    return R

def factor(R):
    S = (R.T @ R) / R.shape[0]
    w, V = np.linalg.eigh(S)
    idx = np.argsort(w)
    w = w[idx]
    V = V[:, idx]
    u = V[:, -1]
    lam = w[-1]
    rq = float(u.T @ S @ u)
    return lam, u, rq

def main():
    R = simulate()
    lam, u, rq = factor(R)
    print("lambda_max:", round(lam, 6), "rq:", round(rq, 6))

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{
Largest eigenvalue and Rayleigh quotient match; interpret as variance captured
by the dominant factor.
}
\INTERPRET{
Top eigenvector is the portfolio direction with maximum variance per unit norm.
}
\NEXTSTEPS{
Impose $\mathbf{1}^Tw=1$ budget constraint leading to a generalized problem.
}

\DomainPage{Deep Learning}
\SCENARIO{
Estimate the spectral norm of a weight matrix $W$ via Rayleigh quotient of
$W^TW$ using the power method.
}
\ASSUMPTIONS{
\begin{bullets}
\item Spectral norm $\|W\|_2=\sqrt{\lambda_{\max}(W^TW)}$.
\item Power iterations converge to the dominant eigenpair.
\end{bullets}
}
\WHICHFORMULA{
Compute $R_{W^TW}(x)=\dfrac{x^T(W^TW)x}{x^Tx}$ and take square root of the
maximum to get $\|W\|_2$.
}
\varmapStart
\var{W}{Weight matrix $(m\times n)$.}
\var{A=W^TW}{Symmetric PSD matrix.}
\var{x}{Unit vector in $\mathbb{R}^n$.}
\var{\|W\|_2}{Largest singular value.}
\varmapEnd
\PIPELINE{
\begin{bullets}
\item Initialize a random vector.
\item Iterate $x\leftarrow A x/\|A x\|$.
\item Track Rayleigh quotient convergence and return $\sqrt{R_A(x)}$.
\end{bullets}
}
\textbf{Implementation (End-to-End)}
\begin{codepy}
import numpy as np

def spectral_norm(W, iters=50, seed=0):
    np.random.seed(seed)
    n = W.shape[1]
    x = np.random.randn(n)
    x /= np.linalg.norm(x)
    A = W.T @ W
    for _ in range(iters):
        y = A @ x
        x = y / np.linalg.norm(y)
    rq = float(x.T @ A @ x)
    return np.sqrt(rq), x

def main():
    np.random.seed(0)
    W = np.random.randn(50, 20)
    s, x = spectral_norm(W, iters=60, seed=1)
    print("spec_norm_est:", round(s, 6))

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{
Report $\sqrt{R_{W^TW}(x)}$ after iterations; compare to NumPy \inlinecode{svd}
if desired for validation.
}
\INTERPRET{
Rayleigh quotient of $W^TW$ at the dominant right singular vector equals the
largest singular value squared.
}
\NEXTSTEPS{
Use power iteration during training for spectral normalization constraints.
}

\DomainPage{Kaggle / Data Analytics}
\SCENARIO{
Perform PCA on a synthetic dataset and compute explained variance ratio from
Rayleigh quotients of the covariance.
}
\ASSUMPTIONS{
\begin{bullets}
\item Data centered; covariance eigenvalues sum to total variance.
\end{bullets}
}
\WHICHFORMULA{
Explained variance ratio of component $i$ is
$\lambda_i/\sum_j \lambda_j=\lambda_i/\mathrm{tr}(S)$ with
$\lambda_i=\max_{\|x\|=1,\,x\perp u_1,\dots,u_{i-1}} x^TSx$.
}
\PIPELINE{
\begin{bullets}
\item Generate synthetic correlated features.
\item Compute $S$, its eigenvalues/eigenvectors.
\item Report ratios $\lambda_i/\mathrm{tr}(S)$.
\end{bullets}
}
\textbf{Implementation (Complete EDA Pipeline)}
\begin{codepy}
import numpy as np

def create_df(seed=0, n=300):
    np.random.seed(seed)
    A = np.array([[1.5, 0.8, 0.2],
                  [0.0, 1.0, 0.5],
                  [0.0, 0.0, 0.7]])
    Z = np.random.randn(n, 3)
    X = Z @ A.T
    X -= X.mean(axis=0, keepdims=True)
    return X

def pca(X):
    S = (X.T @ X) / X.shape[0]
    w, V = np.linalg.eigh(S)
    idx = np.argsort(w)[::-1]
    w = w[idx]
    V = V[:, idx]
    ratio = w / w.sum()
    # verify Rayleigh for first component
    u1 = V[:, 0]
    rq1 = float(u1.T @ S @ u1 / (u1.T @ u1))
    return w, ratio, rq1

def main():
    X = create_df()
    w, ratio, rq1 = pca(X)
    print("eigvals:", np.round(w, 6))
    print("ratios:", np.round(ratio, 6))
    print("rq1:", round(rq1, 6))

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{
Eigenvalues and explained variance ratios; verify $R_S(u_1)=\lambda_1$.
}
\INTERPRET{
Rayleigh quotients quantify component variances; ratios measure contribution to
total variance.
}
\NEXTSTEPS{
Enforce orthogonality to extract subsequent components; apply to preprocessing
pipelines for modeling.
}

\end{document}
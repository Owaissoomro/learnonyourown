% !TeX program = xelatex
\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{microtype,setspace,amsmath,amssymb,mathtools,amsthm,unicode-math}
\setstretch{1.05}
\setmainfont{Latin Modern Roman}
\setmonofont{Latin Modern Mono}
\setmathfont{Latin Modern Math}

% --- Overflow / line-break safety ---
\allowdisplaybreaks[4]
\setlength{\jot}{7pt}
\setlength{\emergencystretch}{8em}
\sloppy

\usepackage{xcolor,fancyhdr,enumitem,inconsolata,listings}
\pagestyle{fancy}\fancyhf{}\lhead{\nouppercase{\leftmark}}\rhead{\thepage}
\setlength{\headheight}{26pt}
\setlength{\parindent}{0pt}
\setlength{\parskip}{8pt plus 2pt minus 1pt}
\raggedbottom

% --- Breakable math helpers (use these in the body) ---
\newenvironment{BreakableEquation}{\begin{equation}\begin{aligned}}{\end{aligned}\end{equation}}
\newenvironment{BreakableEquation*}{\begin{equation*}\begin{aligned}}{\end{aligned}\end{equation*}}
\newenvironment{tightalign}{\begingroup\small\allowdisplaybreaks\begin{align}}{\end{align}\endgroup}

% ---------- Safety shims ----------
% Robust minted → listings shim (no shell-escape; supports [opts]{language})
\providecommand{\enumlistm}{enumitem}
\newenvironment{minted}[2][]{%
  \lstset{style=code,language=#2,#1}\begin{lstlisting}%
}{\end{lstlisting}}

% Fallback for \inputminted (ignore file; keep build unbroken)
\newcommand{\inputminted}[3][]{\begin{lstlisting}\end{lstlisting}}

% ---------- Bulleted lines (no tables) ----------
\newlist{bullets}{itemize}{1}
\setlist[bullets]{label=--,leftmargin=1.6em,itemsep=2pt,topsep=4pt}

% ---------- Variable mapping (lines, no tables) ----------
\newcommand{\varmapStart}{\textbf{VARIABLE MAPPING:}\par\begin{bullets}}
\newcommand{\var}[2]{\item $#1$ — #2}
\newcommand{\varmapEnd}{\end{bullets}}

% ---------- Glossary item with ELI5 and Pitfall/Example ----------
\newcommand{\glossx}[6]{%
  \textbf{#1}\par
  \begin{bullets}
    \item \textbf{What:} #2
    \item \textbf{Why:} #3
    \item \textbf{How:} #4
    \item \textbf{ELI5:} #5
    \item \textbf{Pitfall/Example:} #6
  \end{bullets}
}

% ---------- Theorem structures ----------
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{claim}[theorem]{Claim}
% ---------- Code blocks ----------
\lstdefinestyle{code}{
  basicstyle=\ttfamily\footnotesize,
  backgroundcolor=\color{black!02},
  frame=single,
  numbers=left, numberstyle=\tiny, numbersep=8pt,
  breaklines=true, breakatwhitespace=true,
  tabsize=4, showstringspaces=false,
  upquote=true, keepspaces=true, columns=fullflexible,
  % Unicode safety: map common symbols so listings never chokes
  literate=
    {–}{{-}}1
    {—}{{-}}1
    {…}{{...}}1
    {≤}{{\ensuremath{\le}}}1
    {≥}{{\ensuremath{\ge}}}1
    {≠}{{\ensuremath{\ne}}}1
    {≈}{{\ensuremath{\approx}}}1
    {±}{{\ensuremath{\pm}}}1
    {→}{{\ensuremath{\to}}}1
    {←}{{\ensuremath{\leftarrow}}}1
    {∞}{{\ensuremath{\infty}}}1
    {√}{{\ensuremath{\sqrt{\ }}}}1
    {×}{{\ensuremath{\times}}}1
    {÷}{{\ensuremath{\div}}}1
}

% Main code environment for all Python blocks
\lstnewenvironment{codepy}[1][]%
  {\lstset{style=code,language=Python,#1}}%
  {}

% Inline code; change delimiters if your snippet contains '!'
\newcommand{\inlinecode}[1]{\lstinline[style=code]!#1!}

% ---------- Line-label macros ----------
\newcommand{\LF}[2]{\par\noindent\textbf{#1:}~#2\par}
\newcommand{\WHAT}[1]{\LF{WHAT}{#1}}
\newcommand{\WHY}[1]{\LF{WHY}{#1}}
\newcommand{\HOW}[1]{\LF{HOW}{#1}}
\newcommand{\ELI}[1]{\LF{ELI5}{#1}}
\newcommand{\SCOPE}[1]{\LF{SCOPE}{#1}}
\newcommand{\CONFUSIONS}[1]{\LF{COMMON CONFUSIONS}{#1}}
\newcommand{\APPLICATIONS}[1]{\LF{APPLICATIONS}{#1}}
\newcommand{\FORMULA}[1]{\LF{FORMULA}{#1}}
\newcommand{\CANONICAL}[1]{\LF{CANONICAL FORM}{#1}}
\newcommand{\PRECONDS}[1]{\LF{PRECONDITIONS}{#1}}
\newcommand{\DERIVATION}[1]{\LF{DERIVATION}{#1}}
\newcommand{\EQUIV}[1]{\LF{EQUIVALENT FORMS}{#1}}
\newcommand{\LIMITS}[1]{\LF{LIMIT CASES}{#1}}
\newcommand{\INPUTS}[1]{\LF{INPUTS}{#1}}
\newcommand{\OUTPUTS}[1]{\LF{OUTPUTS}{#1}}
\newcommand{\RESULT}[1]{\LF{RESULT}{#1}}
\newcommand{\INTUITION}[1]{\LF{INTUITION}{#1}}
\newcommand{\PITFALLS}[1]{\LF{PITFALLS}{#1}}
\newcommand{\MODEL}[1]{\LF{CANONICAL MATH MODEL}{#1}}
\newcommand{\ASSUMPTIONS}[1]{\LF{ASSUMPTIONS}{#1}}
\newcommand{\WHICHFORMULA}[1]{\LF{WHICH FORMULA \& WHY}{#1}}
\newcommand{\GOVERN}[1]{\LF{GOVERNING EQUATION(S)}{#1}}
\newcommand{\UNITCHECK}[1]{\LF{UNIT CHECK}{#1}}
\newcommand{\EDGECASES}[1]{\LF{EDGE CASES}{#1}}
\newcommand{\ALTERNATE}[1]{\LF{ALTERNATE APPROACH (sketch)}{#1}}
\newcommand{\PROBLEM}[1]{\LF{PROBLEM}{#1}}
\newcommand{\API}[1]{\LF{API}{#1}}
\newcommand{\COMPLEXITY}[1]{\LF{COMPLEXITY}{#1}}
\newcommand{\FAILMODES}[1]{\LF{FAILURE MODES}{#1}}
\newcommand{\STABILITY}[1]{\LF{NUMERICAL STABILITY}{#1}}
\newcommand{\VALIDATION}[1]{\LF{VALIDATION}{#1}}
\newcommand{\EXPLANATION}[1]{\LF{EXPLANATION}{#1}}
\newcommand{\SCENARIO}[1]{\LF{SCENARIO}{#1}}
\newcommand{\PIPELINE}[1]{\LF{PIPELINE STEPS}{#1}}
\newcommand{\METRICS}[1]{\LF{METRICS}{#1}}
\newcommand{\INTERPRET}[1]{\LF{INTERPRETATION}{#1}}
\newcommand{\NEXTSTEPS}[1]{\LF{LIMITATIONS \& NEXT STEPS}{#1}}

% ---------- Section formatting ----------
\usepackage{titlesec}
\titleformat{\section}{\Large\bfseries}{\thesection}{0.6em}{}
\titlespacing*{\section}{0pt}{*2}{*1}
\usepackage{etoolbox}
\pretocmd{\section}{\clearpage}{}{}

% ---------- Page helpers ----------
\newcommand{\FormulaPage}[2]{%
  \clearpage
  \section*{Formula #1 — #2}%
  \addcontentsline{toc}{section}{Formula #1 — #2}%
}
\newcommand{\ProblemPage}[2]{%
  \clearpage
  \subsection*{Problem #1: #2}%
  \addcontentsline{toc}{subsection}{Problem #1: #2}%
}
\newcommand{\CodeDemoPage}[1]{%
  \clearpage
  \subsection*{Coding Demo: #1}%
  \addcontentsline{toc}{subsection}{Coding Demo: #1}%
}
\newcommand{\DomainPage}[1]{%
  \clearpage
  \subsection*{#1 (End-to-End)}%
  \addcontentsline{toc}{subsection}{#1 (End-to-End)}%
}

\begin{document}
\title{Comprehensive Study Sheet — Matrix Exponential and Logarithm}
\date{\today}
\maketitle
\tableofcontents
\clearpage

\section{Concept Overview}

\WHAT{
Matrix exponential $\exp(A)$ of $A\in\mathbb{C}^{n\times n}$ is the entire
matrix function defined by the power series
$\exp(A)=\sum_{k=0}^{\infty}\frac{A^k}{k!}$.
Matrix logarithm $\log(A)$ is any matrix $X$ with $\exp(X)=A$ for invertible
$A$; the principal logarithm $\Log(A)$ is the unique solution whose spectrum
lies in $\{z\in\mathbb{C}:\ -\pi<\operatorname{Im} z<\pi\}$ when $A$ has no
eigenvalues on $(-\infty,0]$. Both are functions from subsets of
$\mathbb{C}^{n\times n}$ to $\mathbb{C}^{n\times n}$ preserving similarity
and commuting with polynomials of $A$.
}

\WHY{
They linearize multiplicative dynamics and solve linear time-invariant
systems. $\exp(At)$ gives the flow of $x'(t)=Ax(t)$. $\log(A)$ inverts
$\exp$ up to branches, enabling continuous-time interpretations of discrete
maps, and appears in formulas like $\det(\exp(A))=\exp(\operatorname{tr}A)$
and $\log\det(A)=\operatorname{tr}(\Log(A))$ for suitable $A$.
}

\HOW{
1. Define $\exp$ via absolutely convergent series using any submultiplicative
matrix norm. 2. Prove it solves $X'(t)=AX(t)$ with $X(0)=I$ by termwise
differentiation. 3. Extend by similarity invariance to Jordan form for
explicit computations. 4. Define $\log$ as inverse of $\exp$ through local
series $\log(I+X)$ for $\|X\|<1$ and extend by analytic functional calculus,
selecting the principal branch by a spectral cut. Interpret spectra mapping:
$\sigma(\exp(A))=\exp(\sigma(A))$ and $\sigma(\Log(A))=\Log(\sigma(A))$ when
defined.
}

\ELI{
Think of $\exp(A)$ as pressing a button that continuously applies the linear
rule $A$ for one time unit; holding it for time $t$ gives $\exp(tA)$. The
matrix logarithm asks: what constant rule would produce $A$ in one unit of
time if run continuously?
}

\SCOPE{
$\exp(A)$ exists for all $A\in\mathbb{C}^{n\times n}$. A logarithm exists for
every invertible $A$ over $\mathbb{C}$, though it may be nonprincipal and
nonunique when eigenvalues loop around the origin. The principal logarithm is
well defined when $A$ has no eigenvalues on the nonpositive real axis. For
real matrices, $\exp(A)$ is real, while $\Log(A)$ may be real only for $A$
with spectra avoiding negative reals and complex pairs handled conjugately.
}

\CONFUSIONS{
Do not confuse scalar and matrix exponentials: $\exp(A+B)=\exp(A)\exp(B)$
requires $AB=BA$. The matrix logarithm is multivalued: different branches
differ by $2\pi i$ times integers on Jordan blocks. $\log\det(A)$ equals
$\operatorname{tr}(\Log(A))$ only for the chosen principal branch and
admissible $A$.
}

\APPLICATIONS{
\begin{bullets}
\item Stability and solution of linear ODEs and continuous-time Markov chains.
\item Lie group\slash Lie algebra correspondence via $\exp$ and $\log$.
\item Covariance transforms: $\log$-det terms in Gaussian likelihoods.
\item Diffusion models and discrete-to-continuous model calibration.
\end{bullets}
}

\textbf{ANALYTIC STRUCTURE.}
$\exp$ is entire and similarity invariant: $\exp(S^{-1}AS)=S^{-1}\exp(A)S$.
It is nonpolynomial but analytic in entries. For normal $A$, $\exp$ respects
the spectral theorem. $\Log$ is analytic on matrices whose spectra avoid the
branch cut; it is single valued on the principal domain.

\textbf{CANONICAL LINKS.}
Spectral mapping: $\sigma(\exp(A))=\exp(\sigma(A))$. Determinant-trace link:
$\det(\exp(A))=\exp(\operatorname{tr}A)$. For admissible $A$,
$\log\det(A)=\operatorname{tr}(\Log(A))$. Jordan calculus gives explicit
block formulas.

\textbf{PROBLEM-TYPE RECOGNITION HEURISTICS.}
\begin{bullets}
\item Linear ODE with constant matrix $\Rightarrow$ $\exp(At)$.
\item Similarity or basis change $\Rightarrow$ compute in Jordan\slash Schur form.
\item Determinant or volume scaling $\Rightarrow$ use trace via $\exp$ or $\Log$.
\item Discrete map to continuous generator $\Rightarrow$ matrix $\log$.
\end{bullets}

\textbf{SOLUTION STRATEGY BLUEPRINT.}
\begin{bullets}
\item Translate to $X'(t)=AX(t)$ or $A=\exp(X)$ as needed.
\item Choose a representation: power series, Jordan, or Schur.
\item Substitute and compute blockwise; check commutation assumptions.
\item Use spectral mapping and invariants to interpret results.
\item Validate by limits $t\to0$, determinant\slash trace identities.
\end{bullets}

\textbf{CONCEPTUAL INVARIANTS.}
Similarity invariance, spectrum mapping under analytic functions, trace and
determinant relations for $\exp$ and $\Log$ under admissible conditions.

\textbf{EDGE INTUITION.}
As $t\to0$, $\exp(tA)=I+tA+\mathcal{O}(t^2)$. For nilpotent $N$,
$\exp(N)$ is a finite polynomial. For diagonalizable $A$, $\exp(A)$ acts by
exponentiating eigenvalues. For $\Log$, near $I$ the series $\log(I+X)$
captures behavior; near the branch cut, discontinuities appear.

\clearpage
\section{Glossary}

\glossx{Matrix Exponential}
{Entire matrix function $\exp(A)=\sum_{k\ge0}A^k/k!$.}
{Solves linear systems $x'=Ax$; links Lie algebras to Lie groups.}
{Compute via series, scaling and squaring with Pad\'e, or Jordan\slash Schur.}
{Like compounding interest continuously but for linear transformations.}
{Pitfall: assuming $\exp(A+B)=\exp(A)\exp(B)$ without $AB=BA$.}

\glossx{Matrix Logarithm}
{Any $X$ with $\exp(X)=A$ for invertible $A$; principal $\Log$ uses cut.}
{Turns multiplicative maps into additive generators; appears in $\log\det$.}
{Use series $\log(I+X)$ for $\|X\|<1$, or Schur\slash Jordan with branch cut.}
{Like asking which constant push yields a given final transform.}
{Pitfall: multivalued; ensure eigenvalues avoid the cut for principal branch.}

\glossx{Jordan Block}
{Upper triangular block $J=\lambda I+N$, $N$ nilpotent with ones on superdiag.}
{Enables closed forms for $f(J)$ via truncated series in $N$.}
{Compute $f(J)=\sum_{k=0}^{m-1} \frac{f^{(k)}(\lambda)}{k!}N^k$.}
{Think of $N$ as a shift that stops after $m-1$ steps.}
{Pitfall: ignoring nilpotent terms yields missing superdiagonal entries.}

\glossx{Principal Branch}
{Arg values in $(-\pi,\pi)$ define single-valued analytic $\Log$.}
{Provides uniqueness and continuity away from the negative real axis.}
{Select cut, map eigenvalues to branch, and compute via Schur.}
{Choose one continuous angle to avoid jumping by $2\pi$.}
{Pitfall: eigenvalues crossing the cut cause discontinuities.}

\clearpage
\section{Symbol Ledger}
\varmapStart
\var{A\in\mathbb{C}^{n\times n}}{Matrix argument.}
\var{I}{Identity matrix.}
\var{\exp(A)}{Matrix exponential of $A$.}
\var{\log(A)}{A matrix logarithm of invertible $A$.}
\var{\Log(A)}{Principal matrix logarithm.}
\var{t\in\mathbb{R}}{Time or scalar parameter.}
\var{\sigma(A)}{Spectrum (eigenvalues) of $A$.}
\var{S}{Invertible similarity transform.}
\var{J}{Jordan form of a matrix.}
\var{N}{Nilpotent Jordan off-diagonal part.}
\var{Q}{Real Schur factor, orthogonal.}
\var{T}{Real or complex Schur triangular factor.}
\var{x(t)}{State trajectory solving $x'(t)=Ax(t)$.}
\var{X(t)}{Fundamental matrix solution.}
\var{\|\cdot\|}{Submultiplicative matrix norm.}
\var{\operatorname{tr}(\cdot)}{Trace of a matrix.}
\var{\det(\cdot)}{Determinant of a matrix.}
\var{f(A)}{Analytic functional calculus for $f$.}
\var{H}{Direction matrix for Fr\'echet derivative.}
\var{L_{\exp}(A)[H]}{Fr\'echet derivative of $\exp$ at $A$ applied to $H$.}
\var{B}{Another matrix, often commuting with $A$.}
\var{w}{Vector or weights, context dependent.}
\var{Q_{\text{gen}}}{Generator matrix of a CTMC.}
\var{\Sigma}{Covariance matrix (SPD).}
\var{C}{SPD matrix used in log-det computations.}
\varmapEnd

\section{Formula Canon — One Formula Per Page}

\FormulaPage{1}{Matrix Exponential: Definition, ODE, and Basic Properties}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
For any $A\in\mathbb{C}^{n\times n}$, $\exp(A)$ is defined by the absolutely
convergent series $\sum_{k=0}^{\infty}A^k/k!$. The function $X(t)=\exp(tA)$
is the unique solution to $X'(t)=AX(t)$ with $X(0)=I$.

\WHAT{
Compute the time-$t$ linear flow generated by a constant matrix $A$, and
extend scalar exponential identities to matrices under commutation.
}

\WHY{
It provides closed-form solutions of linear constant-coefficient systems and
encodes stability, growth rates, and modal decomposition.
}

\FORMULA{
\[
\exp(A)=\sum_{k=0}^{\infty}\frac{A^k}{k!},\quad
X(t)=\exp(tA),\quad X'(t)=AX(t),\ X(0)=I.
\]
}

\CANONICAL{
$A\in\mathbb{C}^{n\times n}$ arbitrary; $t\in\mathbb{R}$. $\exp$ is entire.
Similarity invariance: $\exp(S^{-1}AS)=S^{-1}\exp(A)S$ for invertible $S$.
If $AB=BA$, then $\exp(A+B)=\exp(A)\exp(B)=\exp(B)\exp(A)$.
}

\PRECONDS{
\begin{bullets}
\item A submultiplicative norm exists; absolute convergence holds for all $A$.
\item For $\exp(A+B)=\exp(A)\exp(B)$, require $AB=BA$.
\end{bullets}
}

\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
For any submultiplicative matrix norm $\|\cdot\|$, the series
$\sum_{k\ge0}A^k/k!$ converges absolutely and defines an entire function.
\end{lemma}
\begin{proof}
By submultiplicativity, $\|A^k\|\le\|A\|^k$. Then
$\sum_{k\ge0}\|A^k\|/k!\le\sum_{k\ge0}\|A\|^k/k!=e^{\|A\|}<\infty$.
Hence absolute convergence and uniform convergence on bounded sets, so
$\exp$ is entire in $A$. \qedhere
\end{proof}

\begin{lemma}
If $AB=BA$, then $\exp(A+B)=\exp(A)\exp(B)$.
\end{lemma}
\begin{proof}
Using the Cauchy product and commutation,
\[
\exp(A)\exp(B)=\sum_{k=0}^{\infty}\frac{A^k}{k!}
\sum_{\ell=0}^{\infty}\frac{B^\ell}{\ell!}
=\sum_{m=0}^{\infty}\sum_{k+\ell=m}\frac{A^kB^\ell}{k!\,\ell!}.
\]
Since $AB=BA$, $\sum_{k+\ell=m}\frac{A^kB^\ell}{k!\,\ell!}=\frac{(A+B)^m}{m!}$
by the binomial theorem in the commuting algebra generated by $A$ and $B$.
Therefore the series equals $\sum_{m\ge0}(A+B)^m/m!=\exp(A+B)$. \qedhere
\end{proof}

\DERIVATION{
\begin{align*}
\text{Step 1 (Series Differentiation):}\quad
& X(t)=\sum_{k=0}^{\infty}\frac{(tA)^k}{k!}
=\sum_{k=0}^{\infty}\frac{t^kA^k}{k!}.\\
\text{Step 2 (Differentiate termwise):}\quad
& X'(t)=\sum_{k=1}^{\infty}\frac{k\,t^{k-1}A^k}{k!}
=\sum_{k=1}^{\infty}\frac{t^{k-1}A^k}{(k-1)!}.\\
\text{Step 3 (Factor $A$):}\quad
& X'(t)=A\sum_{k=1}^{\infty}\frac{t^{k-1}A^{k-1}}{(k-1)!}
=AX(t).\\
\text{Step 4 (Initial condition):}\quad
& X(0)=\sum_{k=0}^{\infty}\frac{0^kA^k}{k!}=I.\\
\text{Step 5 (Uniqueness):}\quad
& \text{Linear ODE with continuous coefficients has unique solution.}
\end{align*}
}

\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Reduce the problem to $X'(t)=AX(t)$ with $X(0)=I$.
\item Check commutation to simplify compositions of exponentials.
\item Use similarity to compute in Jordan\slash Schur form.
\item Validate with small-$t$ expansion and determinant\slash trace checks.
\end{bullets}

\EQUIV{
\begin{bullets}
\item $\exp(A)=\lim_{n\to\infty}\left(I+\frac{A}{n}\right)^n$.
\item $\exp(S^{-1}AS)=S^{-1}\exp(A)S$.
\item If $A$ diagonalizable, $\exp(A)=V\exp(\Lambda)V^{-1}$.
\end{bullets}
}

\LIMITS{
\begin{bullets}
\item $t\to0$: $\exp(tA)=I+tA+\mathcal{O}(t^2)$.
\item $A$ nilpotent: series truncates to degree $(\text{index}-1)$.
\item Large $t$: growth follows $\max\operatorname{Re}\sigma(A)$.
\end{bullets}
}

\INPUTS{$A\in\mathbb{C}^{n\times n},\ t\in\mathbb{R}$.}

\DERIVATION{
\begin{align*}
\text{Example: } A=\begin{bmatrix}0&1\\0&0\end{bmatrix},\ t=1.\quad
& A^2=0,\ \exp(A)=I+A=\begin{bmatrix}1&1\\0&1\end{bmatrix}.\\
\text{Check ODE:}\quad & X'(t)=AX(t),\
X(0)=I\ \Rightarrow\ X(1)=\exp(A).
\end{align*}
}

\RESULT{
$\exp$ is well defined, solves the linear flow, and respects similarity and
commutation where applicable.
}

\UNITCHECK{
Dimensionless algebra; verify consistency by $\det(\exp(A))>0$ for real
traces and $\operatorname{tr}(\exp(A))$ equals sum of exponentials of eigenvalues.
}

\PITFALLS{
\begin{bullets}
\item Assuming product rule without commutation.
\item Ignoring similarity invariance when choosing a basis for computation.
\end{bullets}
}

\INTUITION{
Exponentiating $A$ amounts to summing infinitely many infinitesimal actions
of $A$; the ODE identity formalizes this composition.
}

\CANONICAL{
\begin{bullets}
\item Universal solution operator for $x'(t)=Ax$: $x(t)=\exp(tA)x(0)$.
\item Semigroup: $\exp((s+t)A)=\exp(sA)\exp(tA)$.
\end{bullets}
}

\FormulaPage{2}{Jordan Block Formula for the Matrix Exponential}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
If $A=SJS^{-1}$ is a Jordan form with blocks $J_\ell=\lambda_\ell I+N_\ell$,
then
\[
\exp(A)=S\left(\bigoplus_{\ell}\exp(J_\ell)\right)S^{-1},\quad
\exp(J_\ell)=e^{\lambda_\ell}\sum_{k=0}^{m_\ell-1}\frac{N_\ell^k}{k!}.
\]

\WHAT{
Compute $\exp(A)$ explicitly blockwise in Jordan form; each block contributes
an exponential of the eigenvalue times a truncated nilpotent series.
}

\WHY{
Provides constructive evaluation and reveals upper triangular structure of
$\exp(A)$ even when $A$ is not diagonalizable.
}

\FORMULA{
\[
\exp(\lambda I+N)=e^{\lambda}\exp(N)
=e^{\lambda}\sum_{k=0}^{m-1}\frac{N^k}{k!},\quad N^m=0.
\]
}

\CANONICAL{
$A\in\mathbb{C}^{n\times n}$; $S$ invertible; each Jordan block has size
$m_\ell$. $N_\ell$ is nilpotent with index $m_\ell$.
}

\PRECONDS{
\begin{bullets}
\item Existence of Jordan form over $\mathbb{C}$.
\item Nilpotency $N_\ell^{m_\ell}=0$.
\end{bullets}
}

\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
For any scalar $\lambda$ and nilpotent $N$ with $N^m=0$,
$\exp(\lambda I+N)=e^{\lambda}\sum_{k=0}^{m-1}N^k/k!$.
\end{lemma}
\begin{proof}
By Lemma 1 of Formula 1, the series for $\exp$ converges absolutely. Using
$IN=NI$ and $\lambda I$ commuting with $N$,
\[
\exp(\lambda I+N)=\exp(\lambda I)\exp(N)=e^{\lambda}\sum_{k=0}^{\infty}
\frac{N^k}{k!}.
\]
Since $N^m=0$, the series truncates at $k=m-1$. \qedhere
\end{proof}

\DERIVATION{
\begin{align*}
\text{Step 1:}\quad & A=SJS^{-1},\ J=\bigoplus_\ell J_\ell.\\
\text{Step 2:}\quad & \exp(A)=S\exp(J)S^{-1}\ \text{ by similarity invariance}.\\
\text{Step 3:}\quad & \exp(J)=\bigoplus_\ell \exp(J_\ell)\ \text{(block diag).}\\
\text{Step 4:}\quad & \exp(J_\ell)=e^{\lambda_\ell}\sum_{k=0}^{m_\ell-1}
\frac{N_\ell^k}{k!}.\\
\text{Step 5:}\quad & \text{Assemble blocks and undo similarity with } S.
\end{align*}
}

\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Compute or reason about Jordan\slash Schur form.
\item Evaluate each block via truncated nilpotent series.
\item Conjugate back to the original basis.
\item Validate using small blocks and special cases (diagonalizable).
\end{bullets}

\EQUIV{
\begin{bullets}
\item For diagonalizable $A=V\Lambda V^{-1}$,
$\exp(A)=V\exp(\Lambda)V^{-1}$.
\item If $A$ is nilpotent $N$, $\exp(N)=\sum_{k=0}^{m-1}N^k/k!$.
\end{bullets}
}

\LIMITS{
\begin{bullets}
\item Block size $1$ reduces to scalar exponential.
\item Larger Jordan blocks produce superdiagonal polynomials in $t$.
\end{bullets}
}

\INPUTS{$A\in\mathbb{C}^{n\times n}$ with Jordan data $(\lambda_\ell,N_\ell,S)$.}

\DERIVATION{
\begin{align*}
\text{Example: } J=\begin{bmatrix}\lambda&1\\0&\lambda\end{bmatrix}
=\lambda I+N,\ N^2=0.\quad
& \exp(J)=e^{\lambda}\left(I+N\right)
=e^{\lambda}\begin{bmatrix}1&1\\0&1\end{bmatrix}.
\end{align*}
}

\RESULT{
Explicit block formula yields $\exp(A)$ for any $A$ over $\mathbb{C}$.
}

\UNITCHECK{
Upper triangular structure preserved; diagonal entries are $e^{\lambda_\ell}$,
matching spectral mapping.
}

\PITFALLS{
\begin{bullets}
\item Forgetting truncation due to nilpotency.
\item Misordering Jordan chains changes superdiagonal placement.
\end{bullets}
}

\INTUITION{
Exponentiating adds exponential growth from eigenvalues and polynomial drift
along generalized eigenvectors from nilpotent parts.
}

\CANONICAL{
\begin{bullets}
\item $\exp$ reduces to scalar exponential on each Jordan block.
\item Similarity invariance reconstructs the full matrix.
\end{bullets}
}

\FormulaPage{3}{Matrix Logarithm: Existence, Principal Branch, and Series}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
If $A\in\mathbb{C}^{n\times n}$ is invertible with no eigenvalues on
$(-\infty,0]$, then there exists a unique principal logarithm $\Log(A)$
satisfying $\exp(\Log(A))=A$ and $\sigma(\Log(A))\subset\{z:\ -\pi<\operatorname{Im}z<\pi\}$.
If $\|X\|<1$, then $\log(I+X)=\sum_{k=1}^{\infty}(-1)^{k+1}X^k/k$ and
$\exp(\log(I+X))=I+X$.

\WHAT{
Define and compute matrix logarithms, ensure uniqueness by branch selection,
and provide convergent series near the identity.
}

\WHY{
Logarithms linearize multiplicative relations and connect determinants to
traces. Principal $\Log$ yields stable, unique values in many applications.
}

\FORMULA{
\[
\log(I+X)=\sum_{k=1}^{\infty}\frac{(-1)^{k+1}}{k}X^k,\quad \|X\|<1.
\]
\[
\Log(A)=S\left(\bigoplus_\ell \Log(J_\ell)\right)S^{-1},\ 
\text{branch chosen with } -\pi<\operatorname{Im}\le\pi.
\]
}

\CANONICAL{
$A$ invertible; for principal $\Log$, $\sigma(A)\cap(-\infty,0]=\varnothing$.
For series, $\|X\|<1$ in any submultiplicative norm.
}

\PRECONDS{
\begin{bullets}
\item Invertibility of $A$.
\item Spectrum avoids the branch cut for principal $\Log$.
\item Series validity requires $\|X\|<1$.
\end{bullets}
}

\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
If $\|X\|<1$, then $\log(I+X)=\sum_{k\ge1}(-1)^{k+1}X^k/k$ converges and
$\exp(\log(I+X))=I+X$.
\end{lemma}
\begin{proof}
Use the scalar radius of convergence $1$ and the norm bound:
$\sum_{k\ge1}\|X\|^k/k<\infty$ implies absolute convergence. The formal
identity $\exp(\log(1+z))=1+z$ holds for $|z|<1$; extend to matrices by
functional calculus via uniform convergence on $\|X\|<r<1$. Then the
composed series coincide with $I+X$. \qedhere
\end{proof}

\begin{lemma}
If $A$ has no eigenvalues on $(-\infty,0]$, the principal $\Log(A)$ exists
and is unique with $\sigma(\Log(A))$ in the principal strip.
\end{lemma}
\begin{proof}
By the Schur decomposition, $A=QTQ^*$ with $T$ upper triangular and diagonal
entries equal to eigenvalues $\lambda_i$ off the cut. Define $f(z)=\Log(z)$
with branch cut on $(-\infty,0]$, analytic on a domain containing
$\{\lambda_i\}$. Apply analytic functional calculus for triangular matrices,
defining $f(T)$ by recurrences using divided differences; $f(T)$ is upper
triangular with diagonal entries $\Log(\lambda_i)$. Set $\Log(A)=Qf(T)Q^*$,
which is independent of the chosen Schur form. Uniqueness follows from the
analyticity and the requirement that eigenvalues of $\Log(A)$ lie in the
principal strip. \qedhere
\end{proof}

\DERIVATION{
\begin{align*}
\text{Step 1 (Near identity):}\quad & A=I+X,\ \|X\|<1.\\
& \log(A)=\sum_{k\ge1}(-1)^{k+1}X^k/k.\\
\text{Step 2 (General $A$):}\quad & A=QTQ^*,\ \sigma(T)=\sigma(A).\\
& \Log(A)=Q\,\Log(T)\,Q^*.\\
\text{Step 3 (Blockwise):}\quad & \Log(J)=\log(\lambda I+N)
=\log(\lambda)+\sum_{k=1}^{m-1}\frac{(-1)^{k+1}}{k}
\left(\frac{N}{\lambda}\right)^k.\\
\text{Step 4 (Branch):}\quad & \log(\lambda)=\log|\lambda|
+i\operatorname{Arg}(\lambda),\ \operatorname{Arg}\in(-\pi,\pi).
\end{align*}
}

\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Check spectral cut condition for principal $\Log$.
\item Use Schur\slash Jordan to compute $\Log$ blockwise.
\item Near identity, use the $\log(I+X)$ series.
\item Validate by exponentiating back and checking spectrum.
\end{bullets}

\EQUIV{
\begin{bullets}
\item If $A$ is diagonalizable, $\Log(A)=V\Log(\Lambda)V^{-1}$ (branch wise).
\item For SPD $C$, $\Log(C)$ is real symmetric with real spectrum.
\end{bullets}
}

\LIMITS{
\begin{bullets}
\item As $\|X\|\to0$, $\log(I+X)=X-\tfrac12X^2+\tfrac13X^3-\cdots$.
\item Approaching the cut, the principal $\Log$ becomes ill-conditioned.
\end{bullets}
}

\INPUTS{$A$ invertible, or $X$ with $\|X\|<1$ for the series.}

\DERIVATION{
\begin{align*}
\text{Example: } C=\begin{bmatrix}4&0\\0&9\end{bmatrix}\ \text{(SPD)}.\quad
& \Log(C)=\begin{bmatrix}\log 4&0\\0&\log 9\end{bmatrix}.\\
& \exp(\Log(C))=C,\ \operatorname{tr}(\Log(C))=\log\det(C)=\log 36.
\end{align*}
}

\RESULT{
Principal $\Log$ exists uniquely under the cut condition and matches series
near identity; exponentiating recovers $A$.
}

\UNITCHECK{
For SPD $C$, eigenvalues and their logs are real; trace of $\Log(C)$ equals
$\log\det(C)$, ensuring scalar consistency.
}

\PITFALLS{
\begin{bullets}
\item Ignoring branch selection yields wrong phases on eigenvalues.
\item Applying $\log(I+X)$ series when $\|X\|\not<1$ may diverge.
\end{bullets}
}

\INTUITION{
$\Log$ unwraps rotations and scalings encoded by $A$ into additive rates,
choosing the shortest angular path via the principal branch.
}

\CANONICAL{
\begin{bullets}
\item $\exp(\Log(A))=A$ under the principal cut hypothesis.
\item $\Log(\exp(X))=X$ if $\sigma(X)$ lies in the principal strip.
\end{bullets}
}

\FormulaPage{4}{Fr\'echet Derivative of the Matrix Exponential}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
The Fr\'echet derivative of $\exp$ at $A$ in the direction $H$ is
\[
L_{\exp}(A)[H]=\int_0^1 e^{(1-s)A}\,H\,e^{sA}\,ds.
\]

\WHAT{
Linear response of $\exp(A)$ to a perturbation $H$; yields first-order
sensitivity and supports gradient computations.
}

\WHY{
Needed in optimization, error analysis, and to bound differences
$\|\exp(A+H)-\exp(A)-L_{\exp}(A)[H]\|$.
}

\FORMULA{
\[
\frac{d}{dt}\Big|_{t=0}\exp(A+tH)=\int_0^1 e^{(1-s)A}He^{sA}\,ds.
\]
}

\CANONICAL{
$A,H\in\mathbb{C}^{n\times n}$. Integral converges absolutely; the mapping
$H\mapsto L_{\exp}(A)[H]$ is linear and bounded.
}

\PRECONDS{
\begin{bullets}
\item Entirety of $\exp$ ensures differentiability everywhere.
\item Bochner integral over a compact interval is well defined.
\end{bullets}
}

\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
For $Y(t)=\exp(A+tH)$, the derivative satisfies
$Y'(t)=\int_0^1 e^{(1-s)(A+tH)}He^{s(A+tH)}\,ds$.
\end{lemma}
\begin{proof}
Define $Z(s,t)=e^{(1-s)(A+tH)}He^{s(A+tH)}$. Note that
$\partial_s\left(e^{(1-s)(A+tH)}e^{s(A+tH)}\right)=0$, so
$e^{(1-s)(A+tH)}e^{s(A+tH)}=e^{A+tH}$ for all $s$. Differentiate
$Y(t)=e^{A+tH}$ using the identity
\[
Y'(t)=\int_0^1 e^{(1-s)(A+tH)}H e^{s(A+tH)}\,ds,
\]
which is verified by differentiating the function
$s\mapsto e^{(1-s)(A+tH)}Y'(t)e^{s(A+tH)}$ and integrating; details follow
standard proofs of the Wilcox formula. \qedhere
\end{proof}

\DERIVATION{
\begin{align*}
\text{Step 1:}\quad & Y(t)=\exp(A+tH).\\
\text{Step 2:}\quad & Y'(t)=\int_0^1 e^{(1-s)(A+tH)}He^{s(A+tH)}\,ds.\\
\text{Step 3:}\quad & \text{Evaluate at } t=0 \Rightarrow
L_{\exp}(A)[H]=\int_0^1 e^{(1-s)A}He^{sA}\,ds.\\
\text{Step 4:}\quad & \text{Linearity in } H \text{ is immediate; boundedness
by norm bounds}.
\end{align*}
}

\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Set $Y(t)=\exp(A+tH)$ and compute $Y'(0)$.
\item Use the integral formula for efficient evaluation.
\item Validate numerically by finite differences with small $t$.
\end{bullets}

\EQUIV{
\begin{bullets}
\item Series form: $L_{\exp}(A)[H]=\sum_{k\ge0}\frac{1}{(k+1)!}
\sum_{j=0}^k A^{k-j} H A^{j}$.
\item For $AH=HA$, $L_{\exp}(A)[H]=\int_0^1 e^{A} H\, ds = e^{A}H$.
\end{bullets}
}

\LIMITS{
\begin{bullets}
\item If $AH=HA$, reduces to $e^{A}H$.
\item Norm bound: $\|L_{\exp}(A)[H]\|\le
\left(\int_0^1\|e^{(1-s)A}\|\|e^{sA}\|ds\right)\|H\|$.
\end{bullets}
}

\INPUTS{$A,H\in\mathbb{C}^{n\times n}$.}

\DERIVATION{
\begin{align*}
\text{Example: } A=\begin{bmatrix}0&1\\0&0\end{bmatrix},\
H=\begin{bmatrix}1&0\\0&0\end{bmatrix}.\quad
& e^{A}=I+A.\\
& L_{\exp}(A)[H]=\int_0^1 (I+(1-s)A)H(I+sA)\,ds.\\
& =\int_0^1 \left(H+sH A+(1-s)A H+s(1-s)A H A\right)ds.\\
& AH=\begin{bmatrix}0&0\\0&0\end{bmatrix},\ HA=\begin{bmatrix}0&1\\0&0\end{bmatrix}.\\
& A H A=0,\ \int_0^1 s\,ds=\tfrac12.\\
& L_{\exp}(A)[H]=H+\tfrac12 H A
=\begin{bmatrix}1&\tfrac12\\0&0\end{bmatrix}.
\end{align*}
}

\RESULT{
Integral and series forms agree; special cases reduce as expected.
}

\UNITCHECK{
Dimensionless consistency; linear in $H$ and reduces correctly under
commutation.
}

\PITFALLS{
\begin{bullets}
\item Dropping noncommuting terms when expanding series.
\item Using symmetric finite differences with too large step size.
\end{bullets}
}

\INTUITION{
Perturbation $H$ is propagated left and right by partial flows of $A$,
averaged over splitting points, reflecting noncommutativity.
}

\CANONICAL{
\begin{bullets}
\item First-order variation of $\exp$ is a conjugation-averaged map.
\item Equivalent symmetric series over nested commutators.
\end{bullets}
}

\FormulaPage{5}{Trace and Determinant Identities via Exponential and Logarithm}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
For any $A\in\mathbb{C}^{n\times n}$,
$\det(\exp(A))=\exp(\operatorname{tr}A)$. If $A$ is invertible with
principal logarithm $\Log(A)$, then $\log\det(A)=\operatorname{tr}(\Log(A))$.

\WHAT{
Relate volume scaling (determinant) to additive trace through $\exp$ and
principal $\Log$.
}

\WHY{
These identities simplify likelihoods, regularizers, and spectral analyses,
and they validate computations of $\exp$ and $\Log$.
}

\FORMULA{
\[
\det(\exp(A))=\exp(\operatorname{tr}A),\quad
\log\det(A)=\operatorname{tr}(\Log(A)).
\]
}

\CANONICAL{
Hold over $\mathbb{C}$; the second requires the principal branch condition
for $\Log(A)$.
}

\PRECONDS{
\begin{bullets}
\item For the first identity, none beyond square matrices.
\item For the second, $A$ invertible and spectrum avoids $(-\infty,0]$.
\end{bullets}
}

\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
If $A$ is triangular with diagonal $\{\lambda_i\}$, then
$\det(\exp(A))=\prod_ie^{\lambda_i}=e^{\sum_i\lambda_i}=e^{\operatorname{tr}A}$.
\end{lemma}
\begin{proof}
$\exp(A)$ is upper triangular with diagonal entries $e^{\lambda_i}$ by
functional calculus on triangular matrices. Determinant is the product of
diagonal entries; trace is the sum of $\lambda_i$. \qedhere
\end{proof}

\begin{lemma}
If $\Log(A)$ exists, then $\det(A)=\det(\exp(\Log(A)))=\exp(\operatorname{tr}
(\Log(A)))$ and thus $\log\det(A)=\operatorname{tr}(\Log(A))$.
\end{lemma}
\begin{proof}
Apply the previous identity with $A$ replaced by $\Log(A)$ and use
$\exp(\Log(A))=A$. Take the scalar natural logarithm of both sides, which is
well defined since $\det(A)\neq0$ and the principal branch matches the trace
of $\Log(A)$. \qedhere
\end{proof}

\DERIVATION{
\begin{align*}
\text{Step 1:}\quad & A=QTQ^*\ \text{(Schur)}.\\
\text{Step 2:}\quad & \det(\exp(A))=\det(Q\exp(T)Q^*)=\det(\exp(T)).\\
\text{Step 3:}\quad & \exp(T)\ \text{upper triangular with diag } e^{\lambda_i}.\\
\text{Step 4:}\quad & \det(\exp(T))=\prod_i e^{\lambda_i}=e^{\sum_i \lambda_i}
=e^{\operatorname{tr}A}.\\
\text{Step 5:}\quad & \log\det(A)=\operatorname{tr}(\Log(A))\ \text{by Lemma 2}.
\end{align*}
}

\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Reduce to triangular form where diagonal equals eigenvalues.
\item Use multiplicativity of determinant and additivity of trace.
\item For $\log\det$, compute principal $\Log$ and trace it.
\end{bullets}

\EQUIV{
\begin{bullets}
\item For diagonalizable $A=V\Lambda V^{-1}$, 
$\det(\exp(A))=\prod_i e^{\lambda_i}$.
\item For SPD $C$, $\log\det(C)=\sum_i \log \lambda_i(C)$.
\end{bullets}
}

\LIMITS{
\begin{bullets}
\item If eigenvalues cross the cut, principal $\Log$ may fail; identity
needs consistent branch selection.
\item For singular $A$, $\log\det(A)$ is undefined, but the first identity
with $\exp$ still holds for any $A$.
\end{bullets}
}

\INPUTS{$A\in\mathbb{C}^{n\times n}$; for $\log\det$, $A$ invertible and
admissible.}

\DERIVATION{
\begin{align*}
\text{Example: } A=\begin{bmatrix}1&1\\0&2\end{bmatrix}.\quad
& \operatorname{tr}A=3.\\
& \det(\exp(A))=e^{3}.\\
& \text{Numerical check: }\exp(A)\ \text{has det } e^{3}.
\end{align*}
}

\RESULT{
Determinant equals exponential of trace of exponent; log-det equals trace of
principal log when defined.
}

\UNITCHECK{
Scalar identities preserved; both sides dimensionless and consistent.
}

\PITFALLS{
\begin{bullets}
\item Computing $\log\det$ by $\log\det(A)$ and $\operatorname{tr}(\Log(A))$
with different branches leads to mismatch.
\item Rounding errors when eigenvalues cluster near the cut.
\end{bullets}
}

\INTUITION{
Volume scaling by $\exp(A)$ equals the product of eigenvalue scalings, whose
logarithm sums to the trace.
}

\CANONICAL{
\begin{bullets}
\item $\det\circ\exp=\exp\circ\operatorname{tr}$.
\item $\log\det=\operatorname{tr}\circ\Log$ on the principal domain.
\end{bullets}
}

\section{10 Exhaustive Problems and Solutions}

\ProblemPage{1}{Solving a Linear ODE via Matrix Exponential (Classical)}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Solve $x'(t)=Ax(t)$ with $x(0)=x_0$ by $x(t)=\exp(tA)x_0$.

\PROBLEM{
Given $A=\begin{bmatrix}0&1\\-2&-3\end{bmatrix}$ and $x_0=\begin{bmatrix}1\\0\end{bmatrix}$,
compute $x(1)$ using $x(1)=\exp(A)x_0$ by two methods: Jordan\slash eigen
decomposition and direct series with truncation justified by nilpotent
structure after diagonalization.
}

\MODEL{
\[
x'(t)=Ax(t),\quad x(0)=x_0,\quad x(t)=\exp(tA)x_0.
\]
}

\ASSUMPTIONS{
\begin{bullets}
\item $A$ has distinct eigenvalues; system is stable.
\item Use exact arithmetic for eigen decomposition; series converges rapidly.
\end{bullets}
}

\varmapStart
\var{A}{System matrix.}
\var{x_0}{Initial state.}
\var{x(t)}{State trajectory.}
\var{V,\Lambda}{Eigenbasis and eigenvalues of $A$.}
\varmapEnd

\WHICHFORMULA{
Formula 1 (definition and ODE solution) and Formula 2 (block computation).
}

\GOVERN{
\[
x(t)=\exp(tA)x_0,\quad \exp(A)=V\exp(\Lambda)V^{-1}.
\]
}

\INPUTS{$A=\begin{bmatrix}0&1\\-2&-3\end{bmatrix},\ x_0=\begin{bmatrix}1\\0\end{bmatrix},\
t=1$.}

\DERIVATION{
\begin{align*}
\text{Step 1 (Eigenvalues):}\quad & \det(\lambda I-A)=\lambda^2+3\lambda+2
=(\lambda+1)(\lambda+2).\\
& \lambda_1=-1,\ \lambda_2=-2.\\
\text{Step 2 (Eigenvectors):}\quad &
(\lambda=-1):\ (A+I)v=0\Rightarrow v_1=\begin{bmatrix}1\\-1\end{bmatrix}.\\
& (\lambda=-2):\ (A+2I)v=0\Rightarrow v_2=\begin{bmatrix}1\\-2\end{bmatrix}.\\
\text{Step 3 (Assemble):}\quad &
V=\begin{bmatrix}1&1\\-1&-2\end{bmatrix},\
V^{-1}=\begin{bmatrix}2&1\\-1&-1\end{bmatrix}.\\
& \exp(A)=V\begin{bmatrix}e^{-1}&0\\0&e^{-2}\end{bmatrix}V^{-1}.\\
\text{Step 4 (Compute):}\quad &
\exp(A)=\begin{bmatrix}1&1\\-1&-2\end{bmatrix}
\begin{bmatrix}e^{-1}&0\\0&e^{-2}\end{bmatrix}
\begin{bmatrix}2&1\\-1&-1\end{bmatrix}.\\
& =\begin{bmatrix}
2e^{-1}-e^{-2} & e^{-1}-e^{-2}\\
-2e^{-1}+2e^{-2} & -e^{-1}+2e^{-2}
\end{bmatrix}.\\
\text{Step 5 (Apply to } x_0):\quad &
x(1)=\exp(A)x_0=
\begin{bmatrix}2e^{-1}-e^{-2}\\ -2e^{-1}+2e^{-2}\end{bmatrix}.
\end{align*}
}

\RESULT{
$x(1)=\bigl(2e^{-1}-e^{-2},\ -2e^{-1}+2e^{-2}\bigr)^\top$.
}

\UNITCHECK{
Solution decays since eigenvalues have negative real parts; magnitude is
consistent with $e^{-1}$ and $e^{-2}$ scales.
}

\EDGECASES{
\begin{bullets}
\item If eigenvalues repeated with defective $A$, use Jordan block formula.
\item For $t=0$, $x(0)=x_0$ as $\exp(0)=I$.
\end{bullets}
}

\ALTERNATE{
Compute $\exp(A)$ via the Lagrange interpolation formula for functions of
matrices using the distinct eigenvalues, yielding the same matrix.
}

\VALIDATION{
\begin{bullets}
\item Verify $x'(t)=Ax(t)$ numerically by finite difference at $t=1$.
\item Check $\det(\exp(A))=e^{\operatorname{tr}A}=e^{-3}$.
\end{bullets}
}

\INTUITION{
Two decaying modes combine according to the initial condition projection.
}

\CANONICAL{
\begin{bullets}
\item Mode decomposition: exponentiate eigenvalues and recombine.
\item Stability follows from $\max\operatorname{Re}\sigma(A)<0$.
\end{bullets}
}

\ProblemPage{2}{Nilpotent Case and Truncated Series (Classical)}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
For nilpotent $N^m=0$, $\exp(N)=\sum_{k=0}^{m-1}N^k/k!$.

\PROBLEM{
Let $N=\begin{bmatrix}0&1&0\\0&0&1\\0&0&0\end{bmatrix}$. Compute $\exp(N)$ and
verify $\det(\exp(N))=1$ and $\operatorname{tr}(\exp(N))=3$.
}

\MODEL{
\[
N^3=0,\quad \exp(N)=I+N+\tfrac12N^2.
\]
}

\ASSUMPTIONS{
\begin{bullets}
\item Nilpotency index $m=3$.
\item Standard arithmetic suffices.
\end{bullets}
}

\varmapStart
\var{N}{Strictly upper triangular nilpotent.}
\varmapEnd

\WHICHFORMULA{
Formula 2 (Jordan block formula) as a special case with $\lambda=0$.
}

\GOVERN{
\[
\exp(N)=\sum_{k=0}^{2}\frac{N^k}{k!}.
\]
}

\INPUTS{$N$ as given.}

\DERIVATION{
\begin{align*}
& N=\begin{bmatrix}0&1&0\\0&0&1\\0&0&0\end{bmatrix},\
N^2=\begin{bmatrix}0&0&1\\0&0&0\\0&0&0\end{bmatrix},\
N^3=0.\\
& \exp(N)=I+N+\tfrac12 N^2=
\begin{bmatrix}1&1&\tfrac12\\0&1&1\\0&0&1\end{bmatrix}.\\
& \det(\exp(N))=1\ \text{(upper triangular with ones on diagonal)}.\\
& \operatorname{tr}(\exp(N))=3.
\end{align*}
}

\RESULT{
$\exp(N)=\begin{bmatrix}1&1&\tfrac12\\0&1&1\\0&0&1\end{bmatrix}$ with unit
determinant and trace $3$.
}

\UNITCHECK{
Triangular determinant equals product of diagonals $1\cdot1\cdot1=1$.
}

\EDGECASES{
\begin{bullets}
\item If $N=0$, $\exp(N)=I$.
\item Higher nilpotent index extends the truncation length.
\end{bullets}
}

\ALTERNATE{
Use the binomial expansion of $(I+N/n)^n$ and take $n\to\infty$ to recover
the same finite sum due to nilpotency.
}

\VALIDATION{
\begin{bullets}
\item Confirm $\exp(N)$ is unipotent: all eigenvalues equal $1$.
\item Check $\exp(N)\exp(-N)=I$ with truncated series.
\end{bullets}
}

\INTUITION{
Finite chain of generalized eigenvectors yields finite Taylor contributions.
}

\CANONICAL{
\begin{bullets}
\item Nilpotent exponentials are unipotent upper triangular polynomials.
\end{bullets}
}

\ProblemPage{3}{Principal Logarithm of SPD Matrix (Classical)}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
For SPD $C$, $\Log(C)$ is real symmetric with eigenvalues $\log\lambda_i$.

\PROBLEM{
Given $C=\begin{bmatrix}5&2\\2&2\end{bmatrix}$, compute $\Log(C)$ via
eigendecomposition and verify $\exp(\Log(C))=C$ and 
$\log\det(C)=\operatorname{tr}(\Log(C))$.
}

\MODEL{
\[
C=Q\Lambda Q^\top,\ \Log(C)=Q\log(\Lambda)Q^\top.
\]
}

\ASSUMPTIONS{
\begin{bullets}
\item $C$ is SPD: symmetric with positive eigenvalues.
\item Use principal scalar logarithm.
\end{bullets}
}

\varmapStart
\var{C}{SPD matrix.}
\var{Q,\Lambda}{Orthogonal eigenvectors and positive eigenvalues.}
\varmapEnd

\WHICHFORMULA{
Formula 3 (principal logarithm) and Formula 5 (trace log-det identity).
}

\GOVERN{
\[
\Log(C)=Q\begin{bmatrix}\log\lambda_1&0\\0&\log\lambda_2\end{bmatrix}Q^\top.
\]
}

\INPUTS{$C=\begin{bmatrix}5&2\\2&2\end{bmatrix}$.}

\DERIVATION{
\begin{align*}
\text{Eigenvalues:}\quad & \det(C-\lambda I)=\lambda^2-7\lambda+6
=(\lambda-6)(\lambda-1).\\
& \lambda_1=6,\ \lambda_2=1.\\
\text{Eigenvectors:}\quad & (C-6I)v=0\Rightarrow v_1\propto\begin{bmatrix}2\\1\end{bmatrix}.\\
& (C-I)v=0\Rightarrow v_2\propto\begin{bmatrix}1\\-2\end{bmatrix}.\\
& Q=\frac{1}{\sqrt{5}}\begin{bmatrix}2&1\\1&-2\end{bmatrix},\
\Lambda=\operatorname{diag}(6,1).\\
\text{Compute:}\quad &
\log(\Lambda)=\operatorname{diag}(\log 6,0).\\
& \Log(C)=Q\log(\Lambda)Q^\top
=\frac{1}{5}\begin{bmatrix}4&2\\2&1\end{bmatrix}\log 6.
\end{align*}
}

\RESULT{
$\Log(C)=\frac{\log 6}{5}\begin{bmatrix}4&2\\2&1\end{bmatrix}$, with
$\exp(\Log(C))=C$ and $\operatorname{tr}(\Log(C))=\log 6=\log\det(C)$.
}

\UNITCHECK{
Trace equals sum of eigenvalue logs; determinant identity holds.
}

\EDGECASES{
\begin{bullets}
\item If an eigenvalue approaches $0^+$, $\Log(C)$ becomes large negative.
\item If eigenvalues cross the negative axis, principal $\Log$ fails.
\end{bullets}
}

\ALTERNATE{
Use Schur decomposition, then the Parlett recurrence to compute $\Log(C)$.
}

\VALIDATION{
\begin{bullets}
\item Numerically exponentiate $\Log(C)$ to recover $C$.
\item Compare with series $\log(I+X)$ for $X=C-I$ if $\|X\|<1$ (not here).
\end{bullets}
}

\INTUITION{
Taking logs of eigenvalues in the eigenbasis preserves symmetry and reality.
}

\CANONICAL{
\begin{bullets}
\item For SPD, $\Log$ coincides with spectral scalar log.
\end{bullets}
}

\ProblemPage{4}{Commutator Effects on $\exp(A)\exp(B)$ (Classical)}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
If $AB\neq BA$, then $\exp(A+B)\neq \exp(A)\exp(B)$ in general.

\PROBLEM{
Let $A=\begin{bmatrix}0&1\\0&0\end{bmatrix}$ and 
$B=\begin{bmatrix}0&0\\1&0\end{bmatrix}$. Compute $\exp(A+B)$ and
$\exp(A)\exp(B)$ and compare. Quantify the first nonzero discrepancy term.
}

\MODEL{
\[
\exp(A+B)=I+(A+B)+\tfrac12(A+B)^2+\cdots
\]
}

\ASSUMPTIONS{
\begin{bullets}
\item Finite truncation is exact since $(A+B)^3=0$ here.
\end{bullets}
}

\varmapStart
\var{A,B}{Noncommuting nilpotents.}
\var{[A,B]}{Commutator $AB-BA$.}
\varmapEnd

\WHICHFORMULA{
Formula 1 (series and commutation condition).
}

\GOVERN{
\[
\exp(A+B)=I+(A+B)+\tfrac12(A+B)^2.
\]
}

\INPUTS{$A,B$ as given.}

\DERIVATION{
\begin{align*}
& A^2=B^2=0,\ AB=\begin{bmatrix}1&0\\0&0\end{bmatrix},\
BA=\begin{bmatrix}0&0\\0&1\end{bmatrix}.\\
& (A+B)^2=A^2+AB+BA+B^2=AB+BA=
\begin{bmatrix}1&0\\0&1\end{bmatrix}=I.\\
& \exp(A+B)=I+(A+B)+\tfrac12 I.\\
& \exp(A)=I+A,\ \exp(B)=I+B.\\
& \exp(A)\exp(B)=(I+A)(I+B)=I+A+B+AB.\\
\text{Compare:}\quad &
\exp(A+B)-\exp(A)\exp(B)
=\tfrac12 I-AB.
\end{align*}
}

\RESULT{
Difference $\tfrac12 I-AB=\begin{bmatrix}-\tfrac12&0\\0&\tfrac12\end{bmatrix}$,
nonzero due to noncommutation.
}

\UNITCHECK{
All matrices are $2\times2$; truncations exact by nilpotency.
}

\EDGECASES{
\begin{bullets}
\item If $AB=BA$, difference vanishes.
\item If $A$ and $B$ small, discrepancy is $\tfrac12[A,B]+\cdots$ (BCH).
\end{bullets}
}

\ALTERNATE{
Use the first terms of the Baker-Campbell-Hausdorff expansion:
$\log(\exp(A)\exp(B))=A+B+\tfrac12[A,B]+\cdots$ to infer mismatch.
}

\VALIDATION{
\begin{bullets}
\item Compute norms: $\|\exp(A+B)-\exp(A)\exp(B)\|>0$.
\item Verify $(A+B)^3=0$ ensures exact truncation.
\end{bullets}
}

\INTUITION{
Noncommuting actions do not simply add in the exponent; order matters.
}

\CANONICAL{
\begin{bullets}
\item Commutation is the exact condition for additivity in the exponent.
\end{bullets}
}

\ProblemPage{5}{Story: Alice and Bob's Transformations (Narrative)}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Two operators applied for unit time correspond to $\exp(A)$ and $\exp(B)$.

\PROBLEM{
Alice applies a shear $A=\begin{bmatrix}0&1\\0&0\end{bmatrix}$ for unit time.
Bob applies a scaling $B=\begin{bmatrix}\log 2&0\\0&\log 3\end{bmatrix}$ for
unit time. They argue whether doing both in sequence equals one combined
transformation $\exp(A+B)$. Decide and compute the exact combined map for
Alice-then-Bob and compare to $\exp(A+B)$.
}

\MODEL{
\[
T_{\text{Alice}}=\exp(A)=I+A,\quad T_{\text{Bob}}=\exp(B)=\operatorname{diag}(2,3).
\]
}

\ASSUMPTIONS{
\begin{bullets}
\item Actions last unit time; $A$ and $B$ act on $\mathbb{R}^2$.
\end{bullets}
}

\varmapStart
\var{A}{Shear generator.}
\var{B}{Diagonal scaling generator.}
\varmapEnd

\WHICHFORMULA{
Formula 1 (commutation test) and Formula 2 (block computation for $A$).
}

\GOVERN{
\[
\exp(A+B)=\sum_{k\ge0}\frac{(A+B)^k}{k!}.
\]
}

\INPUTS{$A,B$ as given.}

\DERIVATION{
\begin{align*}
& \exp(A)=I+A=\begin{bmatrix}1&1\\0&1\end{bmatrix},\
\exp(B)=\begin{bmatrix}2&0\\0&3\end{bmatrix}.\\
& T_{\text{Bob}\circ \text{Alice}}=\exp(B)\exp(A)
=\begin{bmatrix}2&2\\0&3\end{bmatrix}.\\
& T_{\text{Alice}\circ \text{Bob}}=\exp(A)\exp(B)
=\begin{bmatrix}2&3\\0&3\end{bmatrix}.\\
& AB=\begin{bmatrix}0&\log 3\\0&0\end{bmatrix},\
BA=\begin{bmatrix}0&2\log 2\\0&0\end{bmatrix}\Rightarrow AB\neq BA.\\
& \exp(A+B)\neq \exp(A)\exp(B)\ \text{in general}.\\
\text{Compute } \exp(A+B):\quad &
A+B=\begin{bmatrix}\log 2&1\\0&\log 3\end{bmatrix}.\\
& \exp(A+B)=\begin{bmatrix}2&\tfrac{2-3}{\log 2-\log 3}\\0&3\end{bmatrix}
=\begin{bmatrix}2&\tfrac{-1}{\log(2/3)}\\0&3\end{bmatrix}.
\end{align*}
}

\RESULT{
Order matters; neither sequence equals $\exp(A+B)$. The exact $\exp(A+B)$ has
off-diagonal entry $\frac{-1}{\log(2/3)}$.
}

\UNITCHECK{
All transforms are $2\times2$; determinants:
$\det(\exp(A+B))=e^{\log 2+\log 3}=6$, matching products.
}

\EDGECASES{
\begin{bullets}
\item If $B=\alpha I$, then $AB=BA$ and $\exp(A+B)=\exp(A)\exp(B)$.
\item If scaling factors equal, limit of the formula recovers $2$ on the
superdiagonal via L'H\^opital: off-diagonal $=\frac{2-2}{\log 2-\log 2}$
limit equals $2$ times unit time.
\end{bullets}
}

\ALTERNATE{
Diagonalize $B$ and use the variation of parameters formula for the
superdiagonal integral to compute the off-diagonal entry.
}

\VALIDATION{
\begin{bullets}
\item Numerically compare norms of differences between the three matrices.
\item Verify determinant identity for each transform equals $6$.
\end{bullets}
}

\INTUITION{
Shear and scaling do not commute unless scaling is uniform; the exponential
captures the continuous blending that depends on order.
}

\CANONICAL{
\begin{bullets}
\item Noncommutation breaks simple exponent additivity.
\end{bullets}
}

\ProblemPage{6}{Story: Recovering a Generator from a Map (Narrative)}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Given an invertible map $M$, a generator $X$ with $M=\exp(X)$ can be chosen
as $X=\Log(M)$ on the principal domain.

\PROBLEM{
A discrete-time system applies $M=\begin{bmatrix}e&0\\0&1\end{bmatrix}$
each step. Alice claims the continuous generator is
$X=\begin{bmatrix}1&0\\0&0\end{bmatrix}$. Bob claims $X$ is not unique.
Determine the principal $\Log(M)$ and discuss nonuniqueness.
}

\MODEL{
\[
X=\Log(M)=\begin{bmatrix}1&0\\0&0\end{bmatrix},\ \exp(X)=M.
\]
}

\ASSUMPTIONS{
\begin{bullets}
\item Use principal branch; eigenvalues positive real.
\end{bullets}
}

\varmapStart
\var{M}{Map per step.}
\var{X}{Generator with $\exp(X)=M$.}
\varmapEnd

\WHICHFORMULA{
Formula 3 (principal logarithm) and spectral mapping.
}

\GOVERN{
\[
\Log(M)=V\Log(\Lambda)V^{-1}.
\]
}

\INPUTS{$M=\operatorname{diag}(e,1)$.}

\DERIVATION{
\begin{align*}
& \sigma(M)=\{e,1\},\ \Log(e)=1,\ \Log(1)=0.\\
& \Log(M)=\operatorname{diag}(1,0).\\
& \text{Nonuniqueness: } \log\ \text{branches allow }
\operatorname{diag}(1+2\pi i k, 2\pi i \ell).\\
& \exp(\operatorname{diag}(1+2\pi i k, 2\pi i \ell))=M\ \text{for integers }
k,\ell.
\end{align*}
}

\RESULT{
Principal generator $X=\begin{bmatrix}1&0\\0&0\end{bmatrix}$; other
generators differ by $2\pi i$ multiples on invariant subspaces.
}

\UNITCHECK{
Exponentiating any branch returns $M$; principal choice is real.
}

\EDGECASES{
\begin{bullets}
\item If an eigenvalue is negative, principal $\Log$ becomes complex.
\item If Jordan blocks present, add nilpotent terms in $\Log$.
\end{bullets}
}

\ALTERNATE{
Use Schur decomposition, then take principal scalar logs on diagonal and
solve for superdiagonal via Parlett recurrences.
}

\VALIDATION{
\begin{bullets}
\item Verify $\exp(\Log(M))=M$.
\item Check determinants: $\det(M)=e$, and $\operatorname{tr}(\Log(M))=1$.
\end{bullets}
}

\INTUITION{
Discrete scaling by $e$ per step corresponds to unit-rate continuous growth.
}

\CANONICAL{
\begin{bullets}
\item Principal $\Log$ picks the branch with smallest imaginary parts.
\end{bullets}
}

\ProblemPage{7}{Expectation Puzzle: CTMC Transition via $\exp(Q t)$}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Continuous-time Markov chain transition matrix is $P(t)=\exp(Q t)$.

\PROBLEM{
A two-state CTMC has generator
$Q=\begin{bmatrix}-\alpha&\alpha\\ \beta&-\beta\end{bmatrix}$ with
$\alpha=2,\ \beta=1$. Starting in state $1$, compute the expected time spent
in state $1$ during $[0,1]$ using $P(t)=\exp(Qt)$.
}

\MODEL{
\[
P(t)=\exp(Qt),\quad \mathbb{E}[T_1]=\int_0^1 P_{11}(t)\,dt.
\]
}

\ASSUMPTIONS{
\begin{bullets}
\item Time-homogeneous CTMC; generator valid ($Q\mathbf{1}=0$).
\item Integrals exist and are finite.
\end{bullets}
}

\varmapStart
\var{Q}{Generator matrix.}
\var{P(t)}{Transition matrix $\exp(Qt)$.}
\var{\alpha,\beta}{Jump rates.}
\var{T_1}{Time in state $1$ over $[0,1]$.}
\varmapEnd

\WHICHFORMULA{
Formula 1 (exponential as flow) applied to CTMC generators.
}

\GOVERN{
\[
P(t)=\exp(Qt)=\begin{bmatrix}
\frac{\beta}{\alpha+\beta}+\frac{\alpha}{\alpha+\beta}e^{-(\alpha+\beta)t}
& \frac{\alpha}{\alpha+\beta}(1-e^{-(\alpha+\beta)t})\\
\frac{\beta}{\alpha+\beta}(1-e^{-(\alpha+\beta)t})
& \frac{\alpha}{\alpha+\beta}+\frac{\beta}{\alpha+\beta}e^{-(\alpha+\beta)t}
\end{bmatrix}.
\]
}

\INPUTS{$\alpha=2,\ \beta=1,\ t\in[0,1]$.}

\DERIVATION{
\begin{align*}
& \alpha+\beta=3.\\
& P_{11}(t)=\tfrac{1}{3}+\tfrac{2}{3}e^{-3t}.\\
& \mathbb{E}[T_1]=\int_{0}^{1}P_{11}(t)\,dt
=\int_0^1 \left(\tfrac{1}{3}+\tfrac{2}{3}e^{-3t}\right)dt\\
& =\tfrac{1}{3}\cdot 1+\tfrac{2}{3}\cdot \left[-\tfrac{1}{3}e^{-3t}\right]_0^1
=\tfrac{1}{3}+\tfrac{2}{3}\left(\tfrac{1-e^{-3}}{3}\right)\\
& =\tfrac{1}{3}+\tfrac{2}{9}(1-e^{-3})=\tfrac{3+2-2e^{-3}}{9}
=\tfrac{5-2e^{-3}}{9}.
\end{align*}
}

\RESULT{
$\mathbb{E}[T_1]=(5-2e^{-3})/9\approx 0.4805$.
}

\UNITCHECK{
Probability entry $P_{11}(t)\in[0,1]$; integral yields time within $[0,1]$.
}

\EDGECASES{
\begin{bullets}
\item If $\alpha=\beta$, $P_{11}(t)=\tfrac{1}{2}+\tfrac{1}{2}e^{-2\alpha t}$.
\item As $t\to\infty$, $P_{11}(t)\to\beta/(\alpha+\beta)$.
\end{bullets}
}

\ALTERNATE{
Diagonalize $Q$ using eigenvalues $0$ and $-(\alpha+\beta)$ to derive $P(t)$.
}

\VALIDATION{
\begin{bullets}
\item Check $P(t)\mathbf{1}=\mathbf{1}$ and $P(0)=I$.
\item Verify $\frac{d}{dt}P(t)=QP(t)$.
\end{bullets}
}

\INTUITION{
The chain relaxes exponentially to stationarity; time in state $1$ integrates
the staying probability.
}

\CANONICAL{
\begin{bullets}
\item $\exp(Q t)$ governs CTMC transitions.
\end{bullets}
}

\ProblemPage{8}{Proof-Style: Similarity Invariance of $\exp$}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Show $\exp(S^{-1}AS)=S^{-1}\exp(A)S$.

\PROBLEM{
Prove similarity invariance of the matrix exponential for any invertible $S$.
}

\MODEL{
\[
\exp(S^{-1}AS)=\sum_{k\ge0}\frac{(S^{-1}AS)^k}{k!}.
\]
}

\ASSUMPTIONS{
\begin{bullets}
\item $S$ invertible; submultiplicative norms exist for convergence.
\end{bullets}
}

\varmapStart
\var{A}{Matrix.}
\var{S}{Invertible matrix.}
\varmapEnd

\WHICHFORMULA{
Formula 1 (series definition) and algebraic properties of similarity.
}

\GOVERN{
\[
(S^{-1}AS)^k=S^{-1}A^k S\ \text{by induction}.
\]
}

\INPUTS{$A,S$ arbitrary with $S$ invertible.}

\DERIVATION{
\begin{align*}
& (S^{-1}AS)^1=S^{-1}AS.\\
& \text{Assume }(S^{-1}AS)^k=S^{-1}A^k S.\\
& (S^{-1}AS)^{k+1}=(S^{-1}AS)(S^{-1}AS)^k
=S^{-1}A S S^{-1} A^k S=S^{-1}A^{k+1} S.\\
& \Rightarrow (S^{-1}AS)^k=S^{-1}A^k S\ \forall k\ge0.\\
& \exp(S^{-1}AS)=\sum_{k\ge0}\frac{S^{-1}A^k S}{k!}
=S^{-1}\left(\sum_{k\ge0}\frac{A^k}{k!}\right)S
=S^{-1}\exp(A)S.
\end{align*}
}

\RESULT{
$\exp$ is similarity invariant.
}

\UNITCHECK{
Dimensions consistent; identity preserved at $k=0$.
}

\EDGECASES{
\begin{bullets}
\item If $S$ is unitary, identity reduces to $Q^*\exp(A)Q$ invariance.
\item Works over $\mathbb{R}$ and $\mathbb{C}$.
\end{bullets}
}

\ALTERNATE{
Use the ODE characterization: if $X'(t)=AX(t)$, then
$Y(t)=S^{-1}X(t)S$ solves $Y'(t)=(S^{-1}AS)Y(t)$.
}

\VALIDATION{
\begin{bullets}
\item Test numerically with random $A,S$ verifying equality to tolerance.
\end{bullets}
}

\INTUITION{
Similarity is a change of basis; $\exp$ commutes with basis change.
}

\CANONICAL{
\begin{bullets}
\item Functional calculus respects similarity transformations.
\end{bullets}
}

\ProblemPage{9}{Proof-Style: $\det(\exp(A))=\exp(\operatorname{tr}A)$}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Prove $\det(\exp(A))=\exp(\operatorname{tr}A)$.

\PROBLEM{
Provide a complete proof using Schur triangularization.
}

\MODEL{
\[
A=QTQ^*,\ \exp(A)=Q\exp(T)Q^*,\ \det(\exp(A))=\det(\exp(T)).
\]
}

\ASSUMPTIONS{
\begin{bullets}
\item Schur decomposition exists over $\mathbb{C}$.
\item Determinant and trace invariant under similarity.
\end{bullets}
}

\varmapStart
\var{A}{Matrix.}
\var{Q}{Unitary factor.}
\var{T}{Upper triangular factor.}
\varmapEnd

\WHICHFORMULA{
Formula 5 and triangular properties of $\exp$.
}

\GOVERN{
\[
\exp(T)\ \text{upper triangular with diagonal } e^{t_{ii}}.
\]
}

\INPUTS{$A$ arbitrary complex square matrix.}

\DERIVATION{
\begin{align*}
& A=QTQ^*,\ \exp(A)=Q\exp(T)Q^*.\\
& \det(\exp(A))=\det(Q)\det(\exp(T))\det(Q^*)=\det(\exp(T))\\
& \text{since }|\det(Q)|=1.\\
& \exp(T)\ \text{upper triangular with diagonal } e^{t_{ii}}.\\
& \det(\exp(T))=\prod_i e^{t_{ii}}=e^{\sum_i t_{ii}}=e^{\operatorname{tr}T}
=e^{\operatorname{tr}A}.
\end{align*}
}

\RESULT{
$\det(\exp(A))=\exp(\operatorname{tr}A)$.
}

\UNITCHECK{
Scalar exponentials and sums correspond appropriately.
}

\EDGECASES{
\begin{bullets}
\item Real case follows since complex Schur still applies.
\end{bullets}
}

\ALTERNATE{
Diagonalize $A$ if possible and apply the same diagonal argument.
}

\VALIDATION{
\begin{bullets}
\item Numerical random tests confirm $\log|\det(\exp(A))|=\operatorname{tr}A$.
\end{bullets}
}

\INTUITION{
Product of exponentials of eigenvalues equals exponential of their sum.
}

\CANONICAL{
\begin{bullets}
\item Determinant converts additive trace through $\exp$.
\end{bullets}
}

\ProblemPage{10}{Combo: Whitening via $C^{-1/2}=\exp(-\tfrac12\Log(C))$}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
For SPD $C$, the inverse square root is $C^{-1/2}=\exp(-\tfrac12\Log(C))$.

\PROBLEM{
Given $C=\begin{bmatrix}3&1\\1&2\end{bmatrix}$, compute $W=C^{-1/2}$ using
$W=\exp(-\tfrac12\Log(C))$ and verify $W C W=I$.
}

\MODEL{
\[
C=Q\Lambda Q^\top,\ \Log(C)=Q\log(\Lambda)Q^\top,\ 
W=Q\Lambda^{-1/2}Q^\top.
\]
}

\ASSUMPTIONS{
\begin{bullets}
\item $C$ is SPD.
\item Use principal branches of log and square root.
\end{bullets}
}

\varmapStart
\var{C}{SPD covariance.}
\var{W}{Inverse square root.}
\varmapEnd

\WHICHFORMULA{
Formula 3 (principal log) and exponential identity; Formula 5 (trace\slash det).
}

\GOVERN{
\[
W=\exp\!\left(-\tfrac12\Log(C)\right).
\]
}

\INPUTS{$C=\begin{bmatrix}3&1\\1&2\end{bmatrix}$.}

\DERIVATION{
\begin{align*}
& \det(C-\lambda I)=\lambda^2-5\lambda+5.\\
& \lambda_{1,2}=\tfrac{5\pm\sqrt{5}}{2}.\\
& Q\ \text{orthonormal eigenvectors } \Rightarrow
W=Q\operatorname{diag}(\lambda_1^{-1/2},\lambda_2^{-1/2})Q^\top.\\
& \text{Compute } W C W=Q\Lambda^{-1/2}Q^\top Q\Lambda Q^\top
Q\Lambda^{-1/2}Q^\top=I.
\end{align*}
}

\RESULT{
$W=C^{-1/2}$ constructed via $\Log$ satisfies $W C W=I$.
}

\UNITCHECK{
Eigenvalues positive; inverse square root eigenvalues are reciprocals of
square roots; identity holds.
}

\EDGECASES{
\begin{bullets}
\item If $C$ ill conditioned, numerical errors magnify in inversion.
\item If not SPD, principal $\Log$ may be complex; whitening invalid.
\end{bullets}
}

\ALTERNATE{
Directly compute $W$ via Cholesky $C=LL^\top$ and set $W=L^{-\top}L^{-1}$,
which equals the spectral method.
}

\VALIDATION{
\begin{bullets}
\item Check symmetry and $W C W$ numerically equals $I$.
\item Verify $\operatorname{tr}(\Log(C))=\log\det(C)$.
\end{bullets}
}

\INTUITION{
Taking half of the negative log scales down each eigen direction to unit
variance.
}

\CANONICAL{
\begin{bullets}
\item Functional calculus composes log and exp to define matrix roots.
\end{bullets}
}

\section{Coding Demonstrations}

\CodeDemoPage{Scaling and Squaring Exponential (From Scratch vs. Library)}
\PROBLEM{
Compute $\exp(A)$ using scaling and squaring with a truncated Taylor series,
and compare with a library implementation. Verify similarity invariance and
the ODE property numerically.
}

\API{
\begin{bullets}
\item \inlinecode{def read_input(s) -> np.ndarray}
\item \inlinecode{def exp_taylor(A, m) -> np.ndarray}
\item \inlinecode{def exp_scale_square(A, s, m) -> np.ndarray}
\item \inlinecode{def validate() -> None}
\item \inlinecode{def main() -> None}
\end{bullets}
}

\INPUTS{
Square matrix $A$ as whitespace-separated rows; integers $s$ (scales) and $m$
(series degree) in code.
}

\OUTPUTS{
Matrix approximation to $\exp(A)$. Assertions verify accuracy and identities.
}

\FORMULA{
\[
\exp(A)\approx \left(\sum_{k=0}^{m}\frac{(A/2^s)^k}{k!}\right)^{2^s}.
\]
}

\textbf{SOLUTION A — From Scratch (Mathematically Explicit Implementation)}
\begin{codepy}
import numpy as np

def read_input(s):
    rows = s.strip().split(";")
    return np.array([[float(x) for x in r.split()] for r in rows])

def exp_taylor(A, m):
    n = A.shape[0]
    T = np.eye(n)
    X = np.eye(n)
    for k in range(1, m + 1):
        X = X @ A / k
        T = T + X
    return T

def exp_scale_square(A, s=3, m=20):
    B = A / (2 ** s)
    T = exp_taylor(B, m)
    for _ in range(s):
        T = T @ T
    return T

def validate():
    np.random.seed(0)
    A = np.array([[0.0, 1.0], [-2.0, -3.0]])
    E = exp_scale_square(A, s=4, m=30)
    # Similarity invariance
    S = np.array([[1.0, 2.0], [0.5, 1.0]])
    Sinv = np.linalg.inv(S)
    E1 = exp_scale_square(Sinv @ A @ S, s=4, m=30)
    E2 = Sinv @ E @ S
    assert np.linalg.norm(E1 - E2) < 1e-8
    # ODE check: E ~= I + A for small t
    t = 1e-4
    Et = exp_scale_square(t * A, s=1, m=12)
    assert np.linalg.norm(Et - (np.eye(2) + t * A)) < 1e-8

def main():
    validate()
    A = read_input("0 1; -2 -3")
    E = exp_scale_square(A, s=4, m=30)
    print("exp(A) approx:\n", np.round(E, 8))

if __name__ == "__main__":
    main()
\end{codepy}

\textbf{SOLUTION B — Library-Based (Validated Computational Shortcut)}
\begin{codepy}
import numpy as np
from scipy.linalg import expm

def read_input(s):
    rows = s.strip().split(";")
    return np.array([[float(x) for x in r.split()] for r in rows])

def solve_case(A):
    return expm(A)

def validate():
    np.random.seed(0)
    A = np.array([[0.0, 1.0], [-2.0, -3.0]])
    E = solve_case(A)
    # Determinant check vs. exp(tr A)
    lhs = np.linalg.det(E)
    rhs = np.exp(np.trace(A))
    assert abs(lhs - rhs) < 1e-10

def main():
    validate()
    A = read_input("0 1; -2 -3")
    E = solve_case(A)
    print("exp(A):\n", np.round(E, 8))

if __name__ == "__main__":
    main()
\end{codepy}

\COMPLEXITY{
Time $\mathcal{O}(n^3 s + n^3 m)$ for from-scratch method; space
$\mathcal{O}(n^2)$. Library uses Pad\'e with scaling and squaring:
time $\mathcal{O}(n^3)$, space $\mathcal{O}(n^2)$.
}

\FAILMODES{
\begin{bullets}
\item Too small $m$ or $s$ causes large truncation error.
\item Ill-conditioned $S$ in similarity test amplifies rounding.
\end{bullets}
}

\STABILITY{
\begin{bullets}
\item Use scaling to reduce series argument norm.
\item Squaring may amplify rounding; choose $s$ and $m$ conservatively.
\end{bullets}
}

\VALIDATION{
\begin{bullets}
\item Compare against library expm.
\item Check determinant identity and small-$t$ linearization.
\end{bullets}
}

\RESULT{
Both implementations agree to tight tolerances on tested inputs.
}

\EXPLANATION{
Scaling reduces the matrix norm so Taylor truncation is accurate; squaring
recovers the original scale. Library uses Pad\'e approximants with balancing.
}

\CodeDemoPage{Principal Matrix Logarithm on SPD Matrices}
\PROBLEM{
Compute $\Log(C)$ for SPD $C$ via spectral decomposition and compare to
library \inlinecode{logm}. Verify $\exp(\Log(C))=C$ and the log-det identity.
}

\API{
\begin{bullets}
\item \inlinecode{def spd(seed) -> np.ndarray}
\item \inlinecode{def log_spd(C) -> np.ndarray}
\item \inlinecode{def validate() -> None}
\item \inlinecode{def main() -> None}
\end{bullets}
}

\INPUTS{
Random SPD matrices generated reproducibly.
}

\OUTPUTS{
Principal matrix logarithm and validation of identities.
}

\FORMULA{
\[
\Log(C)=Q\log(\Lambda)Q^\top,\quad \log\det(C)=\operatorname{tr}(\Log(C)).
\]
}

\textbf{SOLUTION A — From Scratch (Mathematically Explicit Implementation)}
\begin{codepy}
import numpy as np

def spd(seed=0, n=4):
    np.random.seed(seed)
    A = np.random.randn(n, n)
    C = A @ A.T + n * np.eye(n)
    return C

def log_spd(C):
    w, Q = np.linalg.eigh(C)
    L = np.diag(np.log(w))
    return Q @ L @ Q.T

def validate():
    C = spd(0, 4)
    L = log_spd(C)
    C2 = (np.linalg.multi_dot([np.eye(4), L]))
    # Check exp(Log(C)) by eigen reconstruction
    w, Q = np.linalg.eigh(C)
    Lexp = Q @ np.diag(np.exp(np.log(w))) @ Q.T
    assert np.linalg.norm(Lexp - C) < 1e-10
    # Log-det identity
    lhs = np.log(np.linalg.det(C))
    rhs = np.trace(L)
    assert abs(lhs - rhs) < 1e-10

def main():
    validate()
    C = spd(1, 3)
    L = log_spd(C)
    print("Log(C):\n", np.round(L, 8))

if __name__ == "__main__":
    main()
\end{codepy}

\textbf{SOLUTION B — Library-Based (Validated Computational Shortcut)}
\begin{codepy}
import numpy as np
from scipy.linalg import logm, expm

def spd(seed=0, n=4):
    np.random.seed(seed)
    A = np.random.randn(n, n)
    return A @ A.T + n * np.eye(n)

def solve_case(C):
    return logm(C)

def validate():
    C = spd(2, 3)
    L = solve_case(C)
    # exp(log(C)) = C
    E = expm(L)
    assert np.linalg.norm(E - C) < 1e-8
    # logdet via trace
    lhs = np.log(np.linalg.det(C))
    rhs = np.trace(L).real
    assert abs(lhs - rhs) < 1e-8

def main():
    validate()
    C = spd(3, 3)
    L = solve_case(C)
    print("Library Log(C):\n", np.round(L.real, 8))

if __name__ == "__main__":
    main()
\end{codepy}

\COMPLEXITY{
Eigen decomposition $\mathcal{O}(n^3)$. Library logm also $\mathcal{O}(n^3)$.
Space $\mathcal{O}(n^2)$.
}

\FAILMODES{
\begin{bullets}
\item Non-SPD inputs may yield complex logs in spectral method.
\item Near singular SPD: log amplifies tiny eigenvalues.
\end{bullets}
}

\STABILITY{
\begin{bullets}
\item Use symmetric eigendecomposition for SPD to improve conditioning.
\item Add jitter $+\epsilon I$ if needed (not used in validation).
\end{bullets}
}

\VALIDATION{
\begin{bullets}
\item Check $\exp(\Log(C))=C$.
\item Compare $\log\det(C)$ with trace of $\Log(C)$.
\end{bullets}
}

\RESULT{
Accurate principal logs for SPD matrices with strong identity checks.
}

\EXPLANATION{
For SPD, orthogonal diagonalization reduces $\Log$ to scalar logs, ensuring
real symmetric output and exact exponentiation recovery.
}

\CodeDemoPage{Fr\'echet Derivative of $\exp$ via Integral vs. Finite Diff}
\PROBLEM{
Compute $L_{\exp}(A)[H]$ using the integral formula and compare to a finite
difference approximation $(\exp(A+\epsilon H)-\exp(A))/\epsilon$.
}

\API{
\begin{bullets}
\item \inlinecode{def linspace(n) -> np.ndarray}
\item \inlinecode{def lexp(A, H, m) -> np.ndarray}
\item \inlinecode{def fd(A, H, eps) -> np.ndarray}
\item \inlinecode{def validate() -> None}
\item \inlinecode{def main() -> None}
\end{bullets}
}

\INPUTS{
Matrices $A,H$ and parameters: quadrature points $m$, step $\epsilon$.
}

\OUTPUTS{
Two approximations of the Fr\'echet derivative and their difference norm.
}

\FORMULA{
\[
L_{\exp}(A)[H]=\int_0^1 e^{(1-s)A}He^{sA}ds\approx
\frac{1}{m}\sum_{j=1}^{m} e^{(1-s_j)A}He^{s_jA}.
\]
}

\textbf{SOLUTION A — From Scratch (Mathematically Explicit Implementation)}
\begin{codepy}
import numpy as np

def exp_taylor(A, m):
    n = A.shape[0]
    T = np.eye(n)
    X = np.eye(n)
    for k in range(1, m + 1):
        X = X @ A / k
        T = T + X
    return T

def lexp(A, H, quad=50, taylor=24):
    s = np.linspace(0.0, 1.0, quad, endpoint=False) + 0.5 / quad
    acc = np.zeros_like(A)
    for sj in s:
        L = exp_taylor((1 - sj) * A, taylor)
        R = exp_taylor(sj * A, taylor)
        acc = acc + L @ H @ R
    return acc / len(s)

def fd(A, H, eps=1e-6, taylor=30):
    E1 = exp_taylor(A + eps * H, taylor)
    E0 = exp_taylor(A, taylor)
    return (E1 - E0) / eps

def validate():
    A = np.array([[0.0, 1.0], [0.0, 0.0]])
    H = np.array([[1.0, 0.0], [0.0, -1.0]])
    L = lexp(A, H, quad=200, taylor=30)
    G = fd(A, H, eps=1e-7, taylor=40)
    err = np.linalg.norm(L - G)
    assert err < 1e-6

def main():
    validate()
    A = np.array([[0.0, 1.0], [-2.0, -3.0]])
    H = np.array([[1.0, 2.0], [0.0, -1.0]])
    L = lexp(A, H, quad=200, taylor=36)
    G = fd(A, H, eps=1e-7, taylor=48)
    print("||diff||:", float(np.linalg.norm(L - G)))

if __name__ == "__main__":
    main()
\end{codepy}

\textbf{SOLUTION B — Library-Based (Validated Computational Shortcut)}
\begin{codepy}
import numpy as np
from scipy.linalg import expm

def lexp_lib(A, H, quad=80):
    s = np.linspace(0.0, 1.0, quad, endpoint=False) + 0.5 / quad
    acc = np.zeros_like(A, dtype=float)
    for sj in s:
        L = expm((1 - sj) * A)
        R = expm(sj * A)
        acc = acc + L @ H @ R
    return acc / len(s)

def fd_lib(A, H, eps=1e-7):
    return (expm(A + eps * H) - expm(A)) / eps

def validate():
    A = np.array([[0.0, 1.0], [0.0, 0.0]])
    H = np.array([[1.0, 0.0], [0.0, -1.0]])
    L = lexp_lib(A, H, quad=200)
    G = fd_lib(A, H, eps=1e-7)
    err = np.linalg.norm(L - G)
    assert err < 1e-8

def main():
    validate()
    A = np.array([[0.0, 1.0], [-2.0, -3.0]])
    H = np.array([[1.0, 2.0], [0.0, -1.0]])
    L = lexp_lib(A, H, quad=200)
    G = fd_lib(A, H, eps=1e-7)
    print("||diff||:", float(np.linalg.norm(L - G)))

if __name__ == "__main__":
    main()
\end{codepy}

\COMPLEXITY{
From-scratch: per quadrature node, Taylor $\mathcal{O}(n^3 t)$ for $t$ terms.
Total $\mathcal{O}(n^3\cdot\text{quad}\cdot t)$. Library: $\mathcal{O}(n^3
\cdot\text{quad})$.
}

\FAILMODES{
\begin{bullets}
\item Quadrature too coarse or Taylor too short increases error.
\item Finite difference step too large causes bias; too small causes roundoff.
\end{bullets}
}

\STABILITY{
\begin{bullets}
\item Midpoint quadrature improves accuracy.
\item Use balanced step $\epsilon$ near $\sqrt{\epsilon_{\text{mach}}}$.
\end{bullets}
}

\VALIDATION{
\begin{bullets}
\item Compare integral formula with finite difference.
\item Increase quadrature and Taylor orders until convergence.
\end{bullets}
}

\RESULT{
Agreement within tight tolerance confirms the Fr\'echet derivative formula.
}

\EXPLANATION{
Integral formula averages conjugations by partial flows; finite difference
approximates the same derivative by definition.
}

\section{Applied Domains — Detailed End-to-End Scenarios}

\DomainPage{Machine Learning}
\SCENARIO{
Gaussian negative log-likelihood uses $\log\det(\Sigma)$ and quadratic form.
Verify $\log\det(\Sigma)=\operatorname{tr}(\Log(\Sigma))$ and compute the
whitening transform $W=\exp(-\tfrac12\Log(\Sigma))$ for feature standardizing.
}
\ASSUMPTIONS{
\begin{bullets}
\item Covariance $\Sigma$ is SPD.
\item Principal $\Log$ is used.
\end{bullets}
}
\WHICHFORMULA{
Formula 3 (principal log) and Formula 5 (log-det identity) with
$W=\exp(-\tfrac12\Log(\Sigma))$.
}
\varmapStart
\var{\Sigma}{SPD covariance matrix.}
\var{W}{Whitening operator $C^{-1/2}$.}
\var{X}{Data matrix with samples by rows.}
\varmapEnd

\PIPELINE{
\begin{bullets}
\item Generate synthetic correlated data with known covariance.
\item Estimate $\Sigma$, compute $\Log(\Sigma)$ and $W$.
\item Verify $\log\det(\Sigma)=\operatorname{tr}(\Log(\Sigma))$ and that
$\operatorname{Cov}(XW)\approx I$.
\end{bullets}
}

\textbf{Implementation (From Scratch)}
\begin{codepy}
import numpy as np

def synth(n=400, d=3, seed=0):
    np.random.seed(seed)
    A = np.array([[2.0, 0.6, 0.0],
                  [0.6, 1.5, 0.3],
                  [0.0, 0.3, 1.0]])
    X = np.random.randn(n, d) @ np.linalg.cholesky(A).T
    return X

def log_spd(C):
    w, Q = np.linalg.eigh(C)
    return Q @ np.diag(np.log(w)) @ Q.T

def whiten(C):
    w, Q = np.linalg.eigh(C)
    return Q @ np.diag(1.0 / np.sqrt(w)) @ Q.T

def main():
    X = synth()
    C = np.cov(X, rowvar=False, bias=True)
    L = log_spd(C)
    lhs = np.log(np.linalg.det(C))
    rhs = np.trace(L)
    print("logdet vs trace(Log):", float(lhs), float(rhs))
    W = whiten(C)
    Y = (X - X.mean(axis=0)) @ W
    Cw = np.cov(Y, rowvar=False, bias=True)
    print("Whitened diag:", np.round(np.diag(Cw), 4))

if __name__ == "__main__":
    main()
\end{codepy}

\textbf{Implementation (Library Version)}
\begin{codepy}
import numpy as np
from scipy.linalg import logm, expm

def synth(n=400, d=3, seed=0):
    np.random.seed(seed)
    A = np.array([[2.0, 0.6, 0.0],
                  [0.6, 1.5, 0.3],
                  [0.0, 0.3, 1.0]])
    X = np.random.randn(n, d) @ np.linalg.cholesky(A).T
    return X

def main():
    X = synth()
    C = np.cov(X, rowvar=False, bias=True)
    L = logm(C)
    print("logdet vs tr(Log):", float(np.log(np.linalg.det(C))),
          float(np.trace(L).real))
    W = expm(-0.5 * L)
    Y = (X - X.mean(axis=0)) @ W
    Cw = np.cov(Y, rowvar=False, bias=True)
    print("Whitened diag:", np.round(np.diag(Cw), 4))

if __name__ == "__main__":
    main()
\end{codepy}

\METRICS{
Compare $\log\det(\Sigma)$ with $\operatorname{tr}(\Log(\Sigma))$.
Whitening quality: diagonal of covariance near ones.
}
\INTERPRET{
Principal log converts determinant to trace; whitening makes features unit
variance in transformed coordinates.
}
\NEXTSTEPS{
Gradient of $\log\det$ via $\operatorname{tr}(\Sigma^{-1}\delta\Sigma)$ for
optimization tasks.
}

\DomainPage{Quantitative Finance}
\SCENARIO{
Mean and covariance propagation of a linear Ornstein-Uhlenbeck process
$dX_t=AX_t\,dt+\Gamma\,dW_t$ over horizon $t$ uses $\exp(At)$ and
$\int_0^t e^{As}\Gamma\Gamma^\top e^{A^\top s}ds$.
}
\ASSUMPTIONS{
\begin{bullets}
\item $A$ is stable (eigenvalues with negative real parts).
\item $\Gamma$ is constant.
\end{bullets}
}
\WHICHFORMULA{
Formula 1 (flow $\exp(At)$) and Formula 4 (variation of constants implicates
integrals of conjugations).
}
\varmapStart
\var{A}{Drift matrix.}
\var{\Gamma}{Diffusion loading.}
\var{t}{Horizon.}
\var{\mu_t,\Sigma_t}{Mean and covariance at time $t$.}
\varmapEnd

\PIPELINE{
\begin{bullets}
\item Set $(A,\Gamma)$ and initial $(\mu_0,\Sigma_0)$.
\item Compute $\mu_t=e^{At}\mu_0$.
\item Compute $\Sigma_t=e^{At}\Sigma_0 e^{A^\top t}+
\int_0^t e^{As}\Gamma\Gamma^\top e^{A^\top s}ds$.
\end{bullets}
}

\textbf{Implementation (Full Pipeline)}
\begin{codepy}
import numpy as np
from scipy.linalg import expm

def ou_propagate(A, G, t, mu0, S0, m=400):
    E = expm(A * t)
    mu = E @ mu0
    # Covariance integral via Riemann sum
    s = np.linspace(0.0, t, m, endpoint=False) + 0.5 * t / m
    acc = np.zeros_like(S0)
    for sj in s:
        L = expm(A * sj)
        acc = acc + L @ (G @ G.T) @ L.T
    Q = acc * (t / m)
    S = E @ S0 @ E.T + Q
    return mu, S

def main():
    A = np.array([[-1.0, 0.5], [-0.5, -2.0]])
    G = np.array([[0.4, 0.0], [0.1, 0.3]])
    mu0 = np.array([0.5, -0.3])
    S0 = np.eye(2)
    t = 1.0
    mu, S = ou_propagate(A, G, t, mu0, S0)
    print("mu_t:", np.round(mu, 6))
    print("trace Sigma_t:", float(np.trace(S)))

if __name__ == "__main__":
    main()
\end{codepy}

\METRICS{
Mean vector and covariance trace at horizon $t$; verify positive definiteness
of $\Sigma_t$.
}
\INTERPRET{
Exponential flow transports mean and covariances; the integral adds process
noise energy.
}
\NEXTSTEPS{
Solve the continuous Lyapunov equation for steady-state covariance.
}

\DomainPage{Deep Learning}
\SCENARIO{
Neural ODE with linear layer $y'(t)=Wy(t)$ has $y(t)=\exp(tW)y(0)$. Given
pairs $(y(0),y(1))$, estimate $W$ via $W=\Log(Y_1 Y_0^{-1})$ for invertible
linear maps, then validate rollout.
}
\ASSUMPTIONS{
\begin{bullets}
\item Linear map between batches is invertible.
\item Principal $\Log$ applicable to $Y_1 Y_0^{-1}$.
\end{bullets}
}
\WHICHFORMULA{
Formulas 1 and 3: $\exp$ for flow and $\Log$ to recover the generator.
}
\PIPELINE{
\begin{bullets}
\item Generate random stable $W$; compute $Y_1=\exp(W)Y_0$.
\item Recover $\widehat{W}=\Log(Y_1 Y_0^{-1})$.
\item Roll forward with $\exp(t\widehat{W})$ and compare to ground truth.
\end{bullets}
}

\textbf{Implementation (End-to-End)}
\begin{codepy}
import numpy as np
from scipy.linalg import expm, logm

def generate(d=3, seed=0):
    np.random.seed(seed)
    U = np.random.randn(d, d)
    W = U - 2.0 * np.eye(d)
    Y0 = np.eye(d)
    Y1 = expm(W) @ Y0
    return W, Y0, Y1

def recover(Y0, Y1):
    M = Y1 @ np.linalg.inv(Y0)
    return logm(M)

def main():
    W, Y0, Y1 = generate()
    What = recover(Y0, Y1)
    E = expm(What)
    err = np.linalg.norm(E - Y1)
    print("||exp(Log(M))-M||:", float(err))
    # Rollout at t=0.5
    y = np.array([1.0, 0.0, -1.0])
    y1 = expm(0.5 * W) @ y
    y2 = expm(0.5 * What) @ y
    print("rollout diff:", float(np.linalg.norm(y1 - y2)))

if __name__ == "__main__":
    main()
\end{codepy}

\METRICS{
Residual norm $\|\exp(\Log(M))-M\|$ and rollout difference norms.
}
\INTERPRET{
Exact recovery holds for noiseless invertible maps on the principal domain.
}
\NEXTSTEPS{
Handle noninvertible or noisy data via least squares on tangent space.
}

\DomainPage{Kaggle / Data Analytics}
\SCENARIO{
Construct a whitening transform using $W=\exp(-\tfrac12\Log(C))$ for a
synthetic dataset and verify the correlation matrix becomes the identity.
}
\ASSUMPTIONS{
\begin{bullets}
\item Feature covariance is SPD.
\item Principal $\Log$ exists and is computed numerically.
\end{bullets}
}
\WHICHFORMULA{
Formulas 3 and 5: $\Log$ for SPD matrices, and correctness via
$\log\det=\operatorname{tr}\Log$.
}
\PIPELINE{
\begin{bullets}
\item Generate correlated features and compute covariance $C$.
\item Compute $W=\exp(-\tfrac12\Log(C))$.
\item Transform data and check covariance is near identity.
\end{bullets}
}

\textbf{Implementation (Complete EDA Pipeline)}
\begin{codepy}
import numpy as np
from scipy.linalg import logm, expm

def create_df(seed=0, n=300):
    np.random.seed(seed)
    A = np.array([[1.0, 0.8, 0.5],
                  [0.8, 1.0, 0.2],
                  [0.5, 0.2, 1.0]])
    X = np.random.randn(n, 3) @ np.linalg.cholesky(A).T
    return X

def main():
    X = create_df()
    Xc = X - X.mean(axis=0)
    C = np.cov(Xc, rowvar=False, bias=True)
    L = logm(C)
    W = expm(-0.5 * L)
    Y = Xc @ W
    Cw = np.cov(Y, rowvar=False, bias=True)
    print("Whitened cov diag:", np.round(np.diag(Cw), 5))
    print("logdet C vs tr(Log C):",
          float(np.log(np.linalg.det(C))), float(np.trace(L).real))

if __name__ == "__main__":
    main()
\end{codepy}

\METRICS{
Diagonal of whitened covariance and difference between $\log\det(C)$ and
$\operatorname{tr}(\Log(C))$.
}
\INTERPRET{
Exponential of negative half-log rescales eigen-directions to unit variance.
}
\NEXTSTEPS{
Use the same transform for Mahalanobis distance and Gaussian modeling.
}

\end{document}
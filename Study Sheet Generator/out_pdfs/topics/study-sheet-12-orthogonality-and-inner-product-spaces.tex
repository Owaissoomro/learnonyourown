% !TeX program = xelatex
\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{microtype,setspace,amsmath,amssymb,mathtools,amsthm,unicode-math}
\setstretch{1.05}
\setmainfont{Latin Modern Roman}
\setmonofont{Latin Modern Mono}
\setmathfont{Latin Modern Math}

% --- Overflow / line-break safety ---
\allowdisplaybreaks[4]
\setlength{\jot}{7pt}
\setlength{\emergencystretch}{8em}
\sloppy

\usepackage{xcolor,fancyhdr,enumitem,inconsolata,listings}
\pagestyle{fancy}\fancyhf{}\lhead{\nouppercase{\leftmark}}\rhead{\thepage}
\setlength{\headheight}{26pt}
\setlength{\parindent}{0pt}
\setlength{\parskip}{8pt plus 2pt minus 1pt}
\raggedbottom

% --- Breakable math helpers (use these in the body) ---
\newenvironment{BreakableEquation}{\begin{equation}\begin{aligned}}{\end{aligned}\end{equation}}
\newenvironment{BreakableEquation*}{\begin{equation*}\begin{aligned}}{\end{aligned}\end{equation*}}
\newenvironment{tightalign}{\begingroup\small\allowdisplaybreaks\begin{align}}{\end{align}\endgroup}

% ---------- Safety shims ----------
% Robust minted → listings shim (no shell-escape; supports [opts]{language})
\providecommand{\enumlistm}{enumitem}
\newenvironment{minted}[2][]{%
  \lstset{style=code,language=#2,#1}\begin{lstlisting}%
}{\end{lstlisting}}

% Fallback for \inputminted (ignore file; keep build unbroken)
\newcommand{\inputminted}[3][]{\begin{lstlisting}\end{lstlisting}}

% ---------- Bulleted lines (no tables) ----------
\newlist{bullets}{itemize}{1}
\setlist[bullets]{label=--,leftmargin=1.6em,itemsep=2pt,topsep=4pt}

% ---------- Variable mapping (lines, no tables) ----------
\newcommand{\varmapStart}{\textbf{VARIABLE MAPPING:}\par\begin{bullets}}
\newcommand{\var}[2]{\item $#1$ — #2}
\newcommand{\varmapEnd}{\end{bullets}}

% ---------- Glossary item with ELI5 and Pitfall/Example ----------
\newcommand{\glossx}[6]{%
  \textbf{#1}\par
  \begin{bullets}
    \item \textbf{What:} #2
    \item \textbf{Why:} #3
    \item \textbf{How:} #4
    \item \textbf{ELI5:} #5
    \item \textbf{Pitfall/Example:} #6
  \end{bullets}
}

% ---------- Theorem structures ----------
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{claim}[theorem]{Claim}
% ---------- Code blocks ----------
\lstdefinestyle{code}{
  basicstyle=\ttfamily\footnotesize,
  backgroundcolor=\color{black!02},
  frame=single,
  numbers=left, numberstyle=\tiny, numbersep=8pt,
  breaklines=true, breakatwhitespace=true,
  tabsize=4, showstringspaces=false,
  upquote=true, keepspaces=true, columns=fullflexible,
  % Unicode safety: map common symbols so listings never chokes
  literate=
    {–}{{-}}1
    {—}{{-}}1
    {…}{{...}}1
    {≤}{{\ensuremath{\le}}}1
    {≥}{{\ensuremath{\ge}}}1
    {≠}{{\ensuremath{\ne}}}1
    {≈}{{\ensuremath{\approx}}}1
    {±}{{\ensuremath{\pm}}}1
    {→}{{\ensuremath{\to}}}1
    {←}{{\ensuremath{\leftarrow}}}1
    {∞}{{\ensuremath{\infty}}}1
    {√}{{\ensuremath{\sqrt{\ }}}}1
    {×}{{\ensuremath{\times}}}1
    {÷}{{\ensuremath{\div}}}1
}

% Main code environment for all Python blocks
\lstnewenvironment{codepy}[1][]%
  {\lstset{style=code,language=Python,#1}}%
  {}

% Inline code; change delimiters if your snippet contains '!'
\newcommand{\inlinecode}[1]{\lstinline[style=code]!#1!}

% ---------- Line-label macros ----------
\newcommand{\LF}[2]{\par\noindent\textbf{#1:}~#2\par}
\newcommand{\WHAT}[1]{\LF{WHAT}{#1}}
\newcommand{\WHY}[1]{\LF{WHY}{#1}}
\newcommand{\HOW}[1]{\LF{HOW}{#1}}
\newcommand{\ELI}[1]{\LF{ELI5}{#1}}
\newcommand{\SCOPE}[1]{\LF{SCOPE}{#1}}
\newcommand{\CONFUSIONS}[1]{\LF{COMMON CONFUSIONS}{#1}}
\newcommand{\APPLICATIONS}[1]{\LF{APPLICATIONS}{#1}}
\newcommand{\FORMULA}[1]{\LF{FORMULA}{#1}}
\newcommand{\CANONICAL}[1]{\LF{CANONICAL FORM}{#1}}
\newcommand{\PRECONDS}[1]{\LF{PRECONDITIONS}{#1}}
\newcommand{\DERIVATION}[1]{\LF{DERIVATION}{#1}}
\newcommand{\EQUIV}[1]{\LF{EQUIVALENT FORMS}{#1}}
\newcommand{\LIMITS}[1]{\LF{LIMIT CASES}{#1}}
\newcommand{\INPUTS}[1]{\LF{INPUTS}{#1}}
\newcommand{\OUTPUTS}[1]{\LF{OUTPUTS}{#1}}
\newcommand{\RESULT}[1]{\LF{RESULT}{#1}}
\newcommand{\INTUITION}[1]{\LF{INTUITION}{#1}}
\newcommand{\PITFALLS}[1]{\LF{PITFALLS}{#1}}
\newcommand{\MODEL}[1]{\LF{CANONICAL MATH MODEL}{#1}}
\newcommand{\ASSUMPTIONS}[1]{\LF{ASSUMPTIONS}{#1}}
\newcommand{\WHICHFORMULA}[1]{\LF{WHICH FORMULA \& WHY}{#1}}
\newcommand{\GOVERN}[1]{\LF{GOVERNING EQUATION(S)}{#1}}
\newcommand{\UNITCHECK}[1]{\LF{UNIT CHECK}{#1}}
\newcommand{\EDGECASES}[1]{\LF{EDGE CASES}{#1}}
\newcommand{\ALTERNATE}[1]{\LF{ALTERNATE APPROACH (sketch)}{#1}}
\newcommand{\PROBLEM}[1]{\LF{PROBLEM}{#1}}
\newcommand{\API}[1]{\LF{API}{#1}}
\newcommand{\COMPLEXITY}[1]{\LF{COMPLEXITY}{#1}}
\newcommand{\FAILMODES}[1]{\LF{FAILURE MODES}{#1}}
\newcommand{\STABILITY}[1]{\LF{NUMERICAL STABILITY}{#1}}
\newcommand{\VALIDATION}[1]{\LF{VALIDATION}{#1}}
\newcommand{\EXPLANATION}[1]{\LF{EXPLANATION}{#1}}
\newcommand{\SCENARIO}[1]{\LF{SCENARIO}{#1}}
\newcommand{\PIPELINE}[1]{\LF{PIPELINE STEPS}{#1}}
\newcommand{\METRICS}[1]{\LF{METRICS}{#1}}
\newcommand{\INTERPRET}[1]{\LF{INTERPRETATION}{#1}}
\newcommand{\NEXTSTEPS}[1]{\LF{LIMITATIONS \& NEXT STEPS}{#1}}

% ---------- Section formatting ----------
\usepackage{titlesec}
\titleformat{\section}{\Large\bfseries}{\thesection}{0.6em}{}
\titlespacing*{\section}{0pt}{*2}{*1}
\usepackage{etoolbox}
\pretocmd{\section}{\clearpage}{}{}

% ---------- Page helpers ----------
\newcommand{\FormulaPage}[2]{%
  \clearpage
  \section*{Formula #1 — #2}%
  \addcontentsline{toc}{section}{Formula #1 — #2}%
}
\newcommand{\ProblemPage}[2]{%
  \clearpage
  \subsection*{Problem #1: #2}%
  \addcontentsline{toc}{subsection}{Problem #1: #2}%
}
\newcommand{\CodeDemoPage}[1]{%
  \clearpage
  \subsection*{Coding Demo: #1}%
  \addcontentsline{toc}{subsection}{Coding Demo: #1}%
}
\newcommand{\DomainPage}[1]{%
  \clearpage
  \subsection*{#1 (End-to-End)}%
  \addcontentsline{toc}{subsection}{#1 (End-to-End)}%
}

\begin{document}
\title{Comprehensive Study Sheet — Orthogonality and Inner Product Spaces}
\date{\today}
\maketitle
\tableofcontents
\clearpage

\section{Concept Overview}
\WHAT{
An inner product space is a vector space $V$ over $\mathbb{F}\in\{\mathbb{R},\mathbb{C}\}$
equipped with a map $\langle\cdot,\cdot\rangle:V\times V\to\mathbb{F}$ that is
linear in its first argument, conjugate symmetric
$\langle x,y\rangle=\overline{\langle y,x\rangle}$, and positive definite
$\langle x,x\rangle>0$ for $x\neq 0$. Orthogonality means
$\langle x,y\rangle=0$. The induced norm is $\|x\|=\sqrt{\langle x,x\rangle}$.}

\WHY{
Inner products quantify angles, lengths, and orthogonality, enabling projection,
decomposition, and optimization. They unify geometry and algebra, power least
squares, Fourier methods, and spectral theory, and underpin numerical stability
and approximation in Hilbert spaces.}

\HOW{
1. Postulate inner product axioms. 
2. Derive norm properties, Cauchy–Schwarz, triangle inequality, and the
parallelogram law. 
3. Use orthogonality to decompose vectors into components via projection
theorems. 
4. Construct orthonormal bases by Gram–Schmidt; represent operators and solve
least squares as orthogonal projections.}

\ELI{
Treat vectors as arrows and the inner product as a similarity score. If the
score is zero, the arrows are at right angles. Projection is dropping a shadow
perpendicularly onto a subspace; Gram–Schmidt builds a set of perfectly
perpendicular arrows.}

\SCOPE{
Valid in any inner product space; projection theorems require completeness when
minimizing over closed subspaces. In non-inner-product normed spaces, angles may
not be well-defined and projection may fail to exist or be unique. Complex
spaces require conjugate symmetry and linearity in one argument only.}

\CONFUSIONS{
Orthogonal vs. orthonormal: orthonormal adds unit norm. Independence vs.
orthogonality: orthogonality implies independence in probability only for
Gaussian variables, not in general. Least squares normal equations hold for the
Euclidean inner product; with weights, modify the inner product accordingly.}

\APPLICATIONS{
\begin{bullets}
\item Mathematical foundations: Hilbert space theory, Fourier series, spectral decompositions.
\item Computational modeling: least squares, Krylov methods, conjugate gradients.
\item Physical interpretations: energy inner products, orthogonal modes, vibrations.
\item Statistical implications: PCA, orthogonal regressors, decorrelation.
\end{bullets}
}

\textbf{ANALYTIC STRUCTURE.}
Inner products induce norms and metrics, making spaces linear and geometric.
Orthogonality implies Pythagorean additivity of energy. Subspaces admit
orthogonal complements; projections are linear, self-adjoint, idempotent.

\textbf{CANONICAL LINKS.}
Cauchy–Schwarz bounds inner products and yields triangle inequality; together
with positivity gives projection existence and uniqueness. Gram–Schmidt uses
orthogonal projections repeatedly; normal equations express orthogonality of
residuals. Parseval identities follow from orthonormal bases.

\textbf{PROBLEM-TYPE RECOGNITION HEURISTICS.}
\begin{bullets}
\item Minimizing a norm over a subspace suggests orthogonal projection.
\item Asking for a basis with pairwise zero inner products calls for Gram–Schmidt.
\item Bounds of the form $|\langle x,y\rangle|\leq\cdots$ trigger Cauchy–Schwarz.
\item Symmetric idempotent matrices signal orthogonal projectors in $\mathbb{R}^n$.
\end{bullets}

\textbf{SOLUTION STRATEGY BLUEPRINT.}
\begin{bullets}
\item Translate geometric statements into inner-product equations.
\item Identify the relevant identity: Cauchy–Schwarz, projection, or orthogonality.
\item Substitute definitions, compute residuals, and enforce orthogonality.
\item Interpret results as lengths, angles, or energies; validate with limits and
invariance under unitary transforms.
\end{bullets}

\textbf{CONCEPTUAL INVARIANTS.}
Norms and angles under unitary or orthogonal transformations; total energy in
orthogonal decompositions; idempotency of projection operators.

\textbf{EDGE INTUITION.}
If $\|y\|\to 0$, then $|\langle x,y\rangle|\to 0$; if $\|x\|\to\infty$ with $y$
fixed, projections scale linearly; as dimension grows, random vectors become
nearly orthogonal with high probability in Euclidean spaces.

\clearpage
\section{Glossary}
\glossx{Inner Product}
{A function $\langle x,y\rangle$ on a vector space satisfying linearity in the
first argument, conjugate symmetry, and positive definiteness.}
{Induces norm, angle, and orthogonality, enabling projections and spectral
methods.}
{Compute $\|x\|=\sqrt{\langle x,x\rangle}$; use $\langle x,y\rangle$ to test
orthogonality and build orthonormal bases.}
{Like a similarity score; zero means perfectly perpendicular.}
{Pitfall: in complex spaces, use conjugate linearity in the second argument when
needed and never forget complex conjugation in $\langle y,x\rangle$.}

\glossx{Orthogonality}
{Vectors $x,y$ are orthogonal if $\langle x,y\rangle=0$.}
{Orthogonality yields Pythagorean additivity and decouples optimization across
components.}
{Check inner product, ensure basis vectors satisfy pairwise zero inner products.}
{Two arrows form a right angle if their dot-like score is zero.}
{Common error: confusing orthogonal with independent or uncorrelated without
context.}

\glossx{Orthogonal Projection}
{Map $P_W:V\to W$ sending $x$ to the unique $w\in W$ with $x-w\perp W$.}
{Solves best approximation problems and expresses least squares solutions.}
{Solve linear equations $\langle x-P_Wx, w\rangle=0$ for a basis $w$ of $W$.}
{Dropping a perpendicular shadow onto a floor.}
{Pitfall: projection onto a non-closed subspace in an incomplete space may not
exist; in finite dimensions it always exists and is unique.}

\glossx{Gram–Schmidt Process}
{Procedure to convert a linearly independent set into an orthonormal basis.}
{Stabilizes computations, decouples coordinates, and prepares QR factorizations.}
{Iteratively subtract projections onto prior orthonormal vectors and normalize.}
{Make each new stick perpendicular to all previous and rescale to length one.}
{Pitfall: numerical cancellation; use modified Gram–Schmidt or QR with pivoting.}

\clearpage
\section{Symbol Ledger}
\varmapStart
\var{V}{Inner product space over $\mathbb{F}\in\{\mathbb{R},\mathbb{C}\}$.}
\var{\langle x,y\rangle}{Inner product on $V$.}
\var{\|x\|}{Norm $\sqrt{\langle x,x\rangle}$.}
\var{W,U}{Linear subspaces of $V$.}
\var{P_W}{Orthogonal projector onto subspace $W$.}
\var{x,y,z}{Vectors in $V$.}
\var{B}{A basis; $Q$ an orthonormal basis matrix.}
\var{X}{Design matrix with full column rank in $\mathbb{R}^{n\times d}$.}
\var{y}{Data vector in $\mathbb{R}^n$.}
\var{\beta}{Coefficient vector in $\mathbb{R}^d$.}
\var{e}{Residual $y-X\beta$.}
\var{n,d}{Dimensions; $n$ samples, $d$ features, or $\dim V$.}
\var{\mathbb{P}}{Orthogonal projector matrix in coordinates.}
\var{\theta}{Angle with $\cos\theta=\frac{\langle x,y\rangle}{\|x\|\|y\|}$.}
\varmapEnd

\clearpage
\section{Formula Canon — One Formula Per Page}
\FormulaPage{1}{Cauchy–Schwarz Inequality}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
For all $x,y$ in an inner product space,
\[
|\langle x,y\rangle|\le \|x\|\|y\|, \quad\text{with equality iff } x\text{ and }y
\text{ are linearly dependent.}
\]
\WHAT{
A universal bound relating the inner product to the product of norms.}
\WHY{
It ensures angle definition, yields triangle inequality, and underpins
projection error bounds and stability estimates.}
\FORMULA{
\[
|\langle x,y\rangle|\le \|x\|\|y\|,\quad
\cos\theta=\frac{\langle x,y\rangle}{\|x\|\|y\|}\in[-1,1].
\]
}
\CANONICAL{
Valid in any real or complex inner product space; complex case uses modulus and
conjugation.}
\PRECONDS{
\begin{bullets}
\item $(V,\langle\cdot,\cdot\rangle)$ is an inner product space.
\item $x,y\in V$ with norms finite (automatic).
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
For fixed $x,y\in V$ and $t\in\mathbb{R}$, the function
$f(t)=\|x-ty\|^2$ is a quadratic with nonnegative minimum, implying
$\langle x,y\rangle^2\le \|x\|^2\|y\|^2$ in real spaces. In complex spaces,
taking $t=\frac{\langle y,x\rangle}{\|y\|^2}$ yields the bound.
\end{lemma}
\begin{proof}
In the real case expand
$f(t)=\langle x-ty,x-ty\rangle=\|x\|^2-2t\langle x,y\rangle+t^2\|y\|^2\ge 0$.
A quadratic $at^2+bt+c$ with $a=\|y\|^2\ge 0$ that is nonnegative for all
$t$ must have discriminant $b^2-4ac\le 0$, hence
$4\langle x,y\rangle^2-4\|x\|^2\|y\|^2\le 0$. For complex spaces let
$t=\langle y,x\rangle/\|y\|^2$ so that
$\|x-ty\|^2=\|x\|^2-|\langle x,y\rangle|^2/\|y\|^2\ge 0$, giving the result.
Equality occurs only when $x-ty=0$, i.e., linear dependence. \qedhere
\end{proof}
\DERIVATION{
\begin{align*}
\text{Step 1: }& f(t)=\|x-ty\|^2\ge 0.\\
\text{Step 2: }& f(t)=\|x\|^2-2\operatorname{Re}(t\langle x,y\rangle)
+|t|^2\|y\|^2.\\
\text{Step 3: }& \text{Choose }t=\frac{\langle y,x\rangle}{\|y\|^2}
\text{ to minimize }f.\\
\text{Step 4: }& 0\le \|x\|^2-\frac{|\langle x,y\rangle|^2}{\|y\|^2}.\\
\text{Step 5: }& |\langle x,y\rangle|\le \|x\|\|y\|.
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Normalize vectors to unit norm to interpret $\langle x,y\rangle$ as cosine.
\item Use discriminant or minimization of $\|x-ty\|$ over $t$.
\item Check equality by testing linear dependence.
\end{bullets}
\EQUIV{
\begin{bullets}
\item $|\langle x,y\rangle|=\|x\|\|y\|$ iff $x=\alpha y$.
\item Triangle inequality via $0\le \|x+y\|^2$ and this inequality.
\item Hölder inequality in $\ell^2$ is an instance of Cauchy–Schwarz.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item If $y=0$, both sides vanish.
\item Scaling: $|\langle \alpha x,\beta y\rangle|=|\alpha||\beta||\langle x,y\rangle|$.
\item Near orthogonality: small inner product implies small cosine angle.
\end{bullets}
}
\INPUTS{$x\in V,\; y\in V$.}
\DERIVATION{
\begin{align*}
\text{With data }x,y:&\quad \text{Compute }\|x\|,\|y\|,\langle x,y\rangle.\\
&\quad \text{Evaluate }|\langle x,y\rangle|\le \|x\|\|y\|.
\end{align*}
}
\RESULT{
Bound on the inner product by the product of norms; equality characterizes
collinearity.}
\UNITCHECK{
Homogeneous of degree two; invariant under unitary transforms.}
\PITFALLS{
\begin{bullets}
\item Forgetting complex conjugation in $\langle y,x\rangle$.
\item Using the inequality to assert independence in statistics without Gaussianity.
\end{bullets}
}
\INTUITION{
Projection of $x$ onto $y$ has length at most $\|x\|$, scaled by $\|y\|$;
the inner product measures that projected length.}
\CANONICAL{
\begin{bullets}
\item Universal inequality in inner product spaces.
\item Encodes the metric geometry needed for angles and projections.
\end{bullets}
}

\FormulaPage{2}{Parallelogram Law and Polarization Identity}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Norms from inner products satisfy
\[
\|x+y\|^2+\|x-y\|^2=2\|x\|^2+2\|y\|^2,
\]
and the inner product is recovered from the norm via polarization:
\[
\langle x,y\rangle=
\begin{cases}
\frac{1}{4}\left(\|x+y\|^2-\|x-y\|^2\right), & \mathbb{F}=\mathbb{R},\\[4pt]
\frac{1}{4}\sum_{k=0}^{3} i^k \|x+i^k y\|^2, & \mathbb{F}=\mathbb{C}.
\end{cases}
\]
\WHAT{
A structural identity linking the norm geometry with vector addition and
recovering the inner product from norms.}
\WHY{
Characterizes inner-product-induced norms and allows reconstruction of the inner
product from observable lengths only.}
\FORMULA{
\[
\|x+y\|^2+\|x-y\|^2=2\|x\|^2+2\|y\|^2,\quad
\langle x,y\rangle=\frac{1}{4}\left(\|x+y\|^2-\|x-y\|^2\right)\ (\mathbb{R}).
\]
}
\CANONICAL{
Holds in any inner product space; conversely, a norm satisfies the parallelogram
law if and only if it comes from an inner product via polarization.}
\PRECONDS{
\begin{bullets}
\item Inner product space for direct derivation.
\item For converse, completeness not required but norm must satisfy the law.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
In an inner product space,
$\|x\pm y\|^2=\|x\|^2\pm 2\operatorname{Re}\langle x,y\rangle+\|y\|^2$.
\end{lemma}
\begin{proof}
Expand $\|x\pm y\|^2=\langle x\pm y,x\pm y\rangle=\|x\|^2+\|y\|^2\pm
\langle x,y\rangle\pm\overline{\langle x,y\rangle}=
\|x\|^2+\|y\|^2\pm 2\operatorname{Re}\langle x,y\rangle$. \qedhere
\end{proof}
\DERIVATION{
\begin{align*}
\text{Add the expansions: }&
\|x+y\|^2+\|x-y\|^2\\
&=2\|x\|^2+2\|y\|^2+2\operatorname{Re}\langle x,y\rangle
-2\operatorname{Re}\langle x,y\rangle\\
&=2\|x\|^2+2\|y\|^2.\\
\text{Subtract: }&
\|x+y\|^2-\|x-y\|^2=4\operatorname{Re}\langle x,y\rangle,
\end{align*}
which yields the real polarization identity. The complex case follows by
combining $\|x\pm y\|^2$ and $\|x\pm i y\|^2$ to isolate real and imaginary
parts of $\langle x,y\rangle$.}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Expand squared norms and collect real parts.
\item To retrieve $\langle x,y\rangle$ use combinations $x\pm y$ and $x\pm i y$.
\item For the converse, define $\langle x,y\rangle$ by polarization and verify axioms.
\end{bullets}
\EQUIV{
\begin{bullets}
\item Parallelogram law $\iff$ norm from inner product.
\item Pythagorean theorem: if $\langle x,y\rangle=0$ then $\|x+y\|^2=\|x\|^2+\|y\|^2$.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item If $y=0$, both sides reduce consistently.
\item Complex case requires combinations with $i$ to recover imaginary part.
\end{bullets}
}
\INPUTS{$x,y\in V$.}
\DERIVATION{
\begin{align*}
\text{Compute }&\|x+y\|^2,\|x-y\|^2\text{ and combine to obtain }
\langle x,y\rangle.
\end{align*}
}
\RESULT{
Exact equivalence between norm geometry and inner product via polarization.}
\UNITCHECK{
Invariant under unitary transforms; homogeneous of degree two.}
\PITFALLS{
\begin{bullets}
\item Forgetting the factor $1/4$ in polarization.
\item Using real polarization in complex spaces without isolating imaginary part.
\end{bullets}
}
\INTUITION{
The sum and difference diagonals of a parallelogram determine side lengths and
the angle, hence the inner product.}
\CANONICAL{
\begin{bullets}
\item Inner product can be reconstructed from the norm.
\item Norms obeying parallelogram law are exactly inner-product norms.
\end{bullets}
}

\FormulaPage{3}{Projection Theorem onto a Closed Subspace}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Let $V$ be a Hilbert space and $W\subset V$ a closed subspace. For each $x\in V$
there is a unique $w^*\in W$ such that $x-w^*\perp W$. The map $P_W:x\mapsto w^*$
is linear, bounded, self-adjoint, and idempotent.
\WHAT{
Existence and uniqueness of best approximation in a closed subspace via
orthogonal projection.}
\WHY{
Solves constrained minimization of distances, justifies least squares, and
decomposes spaces as $V=W\oplus W^\perp$.}
\FORMULA{
\[
P_W x=w^*\in W\quad\text{such that}\quad
\langle x-w^*,w\rangle=0\ \forall\, w\in W.
\]
}
\CANONICAL{
Requires completeness of $V$ and closedness of $W$. In finite dimensions all
subspaces are closed.}
\PRECONDS{
\begin{bullets}
\item $V$ is Hilbert (complete inner product space).
\item $W$ is a closed linear subspace of $V$.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
If $w^*,w\in W$ minimize $\|x-w\|$, then $w^*=w$.
\end{lemma}
\begin{proof}
By the parallelogram law,
$2\|x-\tfrac{w^*+w}{2}\|^2+\tfrac{1}{2}\|w^*-w\|^2=
\|x-w^*\|^2+\|x-w\|^2$. If both are minimizers, then the midpoint is also a
minimizer, forcing $\|w^*-w\|=0$ and $w^*=w$. \qedhere
\end{proof}
\begin{lemma}
If $w^*$ minimizes $\|x-w\|$ over $W$, then $\langle x-w^*,w\rangle=0$ for all
$w\in W$.
\end{lemma}
\begin{proof}
For any $w\in W$ and $t\in\mathbb{R}$, minimality of $w^*$ implies
$g(t)=\|x-(w^*+tw)\|^2$ has minimum at $t=0$. Then $g'(0)=
-2\operatorname{Re}\langle x-w^*,w\rangle=0$, giving orthogonality. \qedhere
\end{proof}
\DERIVATION{
\begin{align*}
\text{Step 1: }& \text{Take a minimizing sequence }w_k\in W
\text{ with }\|x-w_k\|\downarrow \inf.\\
\text{Step 2: }& \text{Show }\{w_k\}\text{ is Cauchy using parallelogram law.}\\
\text{Step 3: }& W\text{ closed }\Rightarrow w_k\to w^*\in W.\\
\text{Step 4: }& \text{By lower-semicontinuity, }w^*\text{ minimizes distance.}\\
\text{Step 5: }& \text{Use the first-order condition to get }x-w^*\perp W.
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Write normal equations $\langle x-P_Wx,w\rangle=0$ for a basis of $W$.
\item Solve for coefficients of $P_Wx$ in that basis.
\item Use $x=P_Wx+(x-P_Wx)$ with orthogonal components to analyze error.
\end{bullets}
\EQUIV{
\begin{bullets}
\item $V=W\oplus W^\perp$ and $P_W$ is the identity on $W$ and zero on $W^\perp$.
\item In coordinates, $P_W=Q Q^\ast$ for an orthonormal basis $Q$ of $W$.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item If $W$ is not closed in an incomplete space, a minimizer may fail to exist.
\item Non-orthogonal projections are not self-adjoint and not energy-minimizing.
\end{bullets}
}
\INPUTS{$x\in V,\; W\subset V$ closed subspace.}
\DERIVATION{
\begin{align*}
\text{Given an orthonormal basis }&\{q_1,\dots,q_m\}\text{ of }W,\\
P_W x&=\sum_{j=1}^m \langle x,q_j\rangle q_j,\\
\|x-P_W x\|^2&=\|x\|^2-\sum_{j=1}^m |\langle x,q_j\rangle|^2.
\end{align*}
}
\RESULT{
Unique decomposition $x=P_W x+(x-P_W x)$ with $P_W x\in W$ and $x-P_W x\in W^\perp$.}
\UNITCHECK{
Energy balance: $\|x\|^2=\|P_W x\|^2+\|x-P_W x\|^2$.}
\PITFALLS{
\begin{bullets}
\item Using non-orthonormal bases without solving the Gram system.
\item Assuming $P_W$ is diagonal in arbitrary bases; it is diagonal only in a
basis adapted to $W\oplus W^\perp$.
\end{bullets}
}
\INTUITION{
Best approximation is the perpendicular foot of $x$ on the subspace.}
\CANONICAL{
\begin{bullets}
\item Orthogonal projection equals best approximation in Hilbert spaces.
\item Self-adjoint idempotents characterize orthogonal projectors.
\end{bullets}
}

\FormulaPage{4}{Gram–Schmidt Orthonormalization}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Given linearly independent $v_1,\dots,v_m$ in an inner product space, define
\[
u_1=v_1,\quad
u_k=v_k-\sum_{j=1}^{k-1}\frac{\langle v_k,q_j\rangle}{\langle q_j,q_j\rangle}q_j,
\quad q_k=\frac{u_k}{\|u_k\|}.
\]
Then $\{q_1,\dots,q_m\}$ is orthonormal and spans the same subspace.
\WHAT{
Construct an orthonormal basis from a linearly independent set.}
\WHY{
Enables stable coordinates, projections, QR factorization, and numerical
methods relying on orthogonality.}
\FORMULA{
\[
q_k=\frac{1}{\|u_k\|}\Bigl(v_k-\sum_{j<k}\langle v_k,q_j\rangle q_j\Bigr),\quad
\langle q_i,q_j\rangle=\delta_{ij}.
\]
}
\CANONICAL{
Works in any inner product space; in finite dimensions, stacks into $A=QR$ with
$Q$ orthonormal columns and upper-triangular $R$.}
\PRECONDS{
\begin{bullets}
\item $\{v_1,\dots,v_m\}$ linearly independent.
\item Nonzero $u_k$ at each step, guaranteed by independence.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
At step $k$, $u_k$ is orthogonal to $q_1,\dots,q_{k-1}$ and $u_k\neq 0$.
\end{lemma}
\begin{proof}
For $i<k$,
$\langle u_k,q_i\rangle=\langle v_k,q_i\rangle-\sum_{j<k}\langle v_k,q_j\rangle
\langle q_j,q_i\rangle=\langle v_k,q_i\rangle-\langle v_k,q_i\rangle=0$.
If $u_k=0$, then $v_k$ lies in $\operatorname{span}\{v_1,\dots,v_{k-1}\}$,
contradicting independence. \qedhere
\end{proof}
\DERIVATION{
\begin{align*}
\text{Step 1: }& q_1=v_1/\|v_1\|.\\
\text{Step 2: }& \text{Subtract projection }u_2=v_2-\langle v_2,q_1\rangle q_1,
\ q_2=u_2/\|u_2\|.\\
\text{Step 3: }& \text{Continue }u_k=v_k-\sum_{j<k}\langle v_k,q_j\rangle q_j.\\
\text{Step 4: }& \text{Normalize }q_k=u_k/\|u_k\|.\\
\text{Step 5: }& \text{Prove orthonormality and spanning by induction.}
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Maintain and reuse inner products $\langle v_k,q_j\rangle$.
\item Use modified Gram–Schmidt for better numerical stability.
\item Assemble $Q=[q_1\ \cdots\ q_m]$ and $R=Q^\ast A$ for $A=[v_1\ \cdots\ v_m]$.
\end{bullets}
\EQUIV{
\begin{bullets}
\item Classical vs. modified Gram–Schmidt are algebraically equivalent in exact arithmetic.
\item Householder QR is a numerically stable orthogonalization alternative.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item Nearly dependent vectors lead to small $\|u_k\|$ and loss of accuracy.
\item In infinite dimensions, process may not converge without additional structure.
\end{bullets}
}
\INPUTS{$v_1,\dots,v_m\in V$.}
\DERIVATION{
\begin{align*}
\text{Example in }\mathbb{R}^3:&\ v_1=(1,1,0),\ v_2=(1,0,1),\ v_3=(0,1,1).\\
q_1&=\tfrac{1}{\sqrt{2}}(1,1,0).\\
u_2&=(1,0,1)-\tfrac{1}{\sqrt{2}}\cdot\tfrac{1}{\sqrt{2}}(1,1,0)
=(\tfrac{1}{2},-\tfrac{1}{2},1).\\
q_2&=\frac{u_2}{\|u_2\|}=\frac{1}{\sqrt{3/2}}(\tfrac{1}{2},-\tfrac{1}{2},1).\\
u_3&=(0,1,1)-\langle v_3,q_1\rangle q_1-\langle v_3,q_2\rangle q_2,\ \text{then }
q_3=u_3/\|u_3\|.
\end{align*}
}
\RESULT{
An orthonormal basis spanning $\operatorname{span}\{v_1,\dots,v_m\}$.}
\UNITCHECK{
Each $q_k$ has unit norm and pairwise zero inner products.}
\PITFALLS{
\begin{bullets}
\item Skipping normalization or using non-updated inner products.
\item Ignoring underflow when $\|u_k\|$ is tiny.
\end{bullets}
}
\INTUITION{
Iteratively remove components along existing directions and normalize the
remainder.}
\CANONICAL{
\begin{bullets}
\item Orthogonalization equals repeated orthogonal projections.
\item QR factorization $A=QR$ with $Q^\ast Q=I$ and $R$ upper triangular.
\end{bullets}
}

\FormulaPage{5}{Orthogonal Projector Matrix and Least Squares}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
For full column rank $X\in\mathbb{R}^{n\times d}$, the matrix
\[
\mathbb{P}=X(X^\top X)^{-1}X^\top
\]
is the orthogonal projector onto $\mathcal{C}(X)$, the column space of $X$. The
least squares solution $\hat\beta=(X^\top X)^{-1}X^\top y$ satisfies
$e=y-X\hat\beta\perp \mathcal{C}(X)$.
\WHAT{
Explicit coordinate formula for orthogonal projection and least squares.}
\WHY{
Connects geometry and computation, making residual orthogonality manifest and
enabling efficient solutions and diagnostics.}
\FORMULA{
\[
\hat\beta=(X^\top X)^{-1}X^\top y,\quad
\widehat{y}=\mathbb{P}y,\quad e=y-\widehat{y},\quad X^\top e=0.
\]
}
\CANONICAL{
Euclidean inner product on $\mathbb{R}^n$; full column rank ensures
invertibility of $X^\top X$.}
\PRECONDS{
\begin{bullets}
\item $X$ has linearly independent columns.
\item Euclidean inner product $\langle u,v\rangle=u^\top v$.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
$\mathbb{P}$ is symmetric and idempotent, hence an orthogonal projector.
\end{lemma}
\begin{proof}
Symmetry: $\mathbb{P}^\top=X(X^\top X)^{-1}X^\top=\mathbb{P}$. Idempotency:
$\mathbb{P}^2=X(X^\top X)^{-1}X^\top X(X^\top X)^{-1}X^\top=\mathbb{P}$.
Therefore $\mathbb{P}$ is the orthogonal projector onto $\mathcal{C}(X)$.
\qedhere
\end{proof}
\DERIVATION{
\begin{align*}
\text{Normal equations: }& \min_\beta \|y-X\beta\|^2
\Rightarrow X^\top (y-X\beta)=0.\\
\text{Solve: }& X^\top X\,\hat\beta=X^\top y
\Rightarrow \hat\beta=(X^\top X)^{-1}X^\top y.\\
\text{Projection: }& \widehat{y}=X\hat\beta=X(X^\top X)^{-1}X^\top y=\mathbb{P}y.
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Form $X^\top X$ and $X^\top y$; solve for $\hat\beta$.
\item Compute $\widehat{y}=X\hat\beta$ and residual $e=y-\widehat{y}$.
\item Verify $X^\top e=0$ to confirm orthogonality.
\end{bullets}
\EQUIV{
\begin{bullets}
\item If $X=QR$ with $Q^\top Q=I$, then $\mathbb{P}=QQ^\top$ and
$\hat\beta=R^{-1}Q^\top y$.
\item Weighted least squares uses inner product $\langle u,v\rangle=u^\top W v$,
giving $\mathbb{P}=X(X^\top W X)^{-1}X^\top W$.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item If $X$ is rank-deficient, use the Moore–Penrose pseudoinverse.
\item Sensitivity grows with condition number $\kappa(X^\top X)$.
\end{bullets}
}
\INPUTS{$X\in\mathbb{R}^{n\times d}$ full column rank, $y\in\mathbb{R}^n$.}
\DERIVATION{
\begin{align*}
\text{Numeric example: }&
X=\begin{bmatrix}1&0\\1&1\\1&2\end{bmatrix},\ 
y=\begin{bmatrix}1\\2\\2\end{bmatrix}.\\
X^\top X&=\begin{bmatrix}3&3\\3&5\end{bmatrix},\
X^\top y=\begin{bmatrix}5\\6\end{bmatrix}.\\
\hat\beta&=(X^\top X)^{-1}X^\top y
=\begin{bmatrix}5&-3\\-3&3\end{bmatrix}\tfrac{1}{6}\begin{bmatrix}5\\6\end{bmatrix}
=\begin{bmatrix}\tfrac{7}{6}\\\tfrac{1}{6}\end{bmatrix}.\\
\widehat{y}&=X\hat\beta=\begin{bmatrix}7/6\\4/3\\3/2\end{bmatrix},\
e=\begin{bmatrix}-1/6\\2/3\\1/2\end{bmatrix},\
X^\top e=\begin{bmatrix}0\\0\end{bmatrix}.
\end{align*}
}
\RESULT{
$\widehat{y}=\mathbb{P}y$ is the orthogonal projection onto $\mathcal{C}(X)$ and
$e\perp \mathcal{C}(X)$.}
\UNITCHECK{
Energy decomposition $\|y\|^2=\|\widehat{y}\|^2+\|e\|^2$ holds.}
\PITFALLS{
\begin{bullets}
\item Forming $(X^\top X)^{-1}$ explicitly is numerically unstable; prefer QR.
\item Forgetting to check rank leads to spurious solutions.
\end{bullets}
}
\INTUITION{
Choose coefficients so that the error is orthogonal to all columns of $X$.}
\CANONICAL{
\begin{bullets}
\item Orthogonal projection as matrix $\mathbb{P}$.
\item Normal equations characterize residual orthogonality.
\end{bullets}
}

\clearpage
\section{10 Exhaustive Problems and Solutions}
\ProblemPage{1}{Gram–Schmidt in $\mathbb{R}^3$ with Worked Numbers}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Construct an orthonormal basis from given vectors and verify orthogonality.
\PROBLEM{
Given $v_1=(1,2,2)$, $v_2=(2,1,2)$, $v_3=(2,2,1)$ in $\mathbb{R}^3$ with the
standard inner product, produce an orthonormal basis $\{q_1,q_2,q_3\}$ via
Gram–Schmidt and verify $Q^\top Q=I$.}
\MODEL{
\[
q_1=\frac{v_1}{\|v_1\|},\quad
u_2=v_2-\langle v_2,q_1\rangle q_1,\ q_2=\frac{u_2}{\|u_2\|},\quad
u_3=v_3-\sum_{j=1}^2\langle v_3,q_j\rangle q_j,\ q_3=\frac{u_3}{\|u_3\|}.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Standard Euclidean inner product on $\mathbb{R}^3$.
\item Vectors are linearly independent.
\end{bullets}
}
\varmapStart
\var{v_i}{Input vectors.}
\var{q_i}{Orthonormal vectors to compute.}
\var{u_i}{Intermediate orthogonalized vectors.}
\varmapEnd
\WHICHFORMULA{
Formula 4 (Gram–Schmidt Orthonormalization).}
\GOVERN{
\[
q_k=\frac{1}{\|u_k\|}\Bigl(v_k-\sum_{j<k}\langle v_k,q_j\rangle q_j\Bigr).
\]
}
\INPUTS{$v_1=(1,2,2)$, $v_2=(2,1,2)$, $v_3=(2,2,1)$.}
\DERIVATION{
\begin{align*}
\|v_1\|&=\sqrt{1+4+4}=3,\ q_1=\tfrac{1}{3}(1,2,2).\\
\langle v_2,q_1\rangle&=\tfrac{1}{3}(2+2+4)=\tfrac{8}{3}.\\
u_2&=(2,1,2)-\tfrac{8}{3}\tfrac{1}{3}(1,2,2)
=(\tfrac{10}{9},-\tfrac{7}{9},\tfrac{2}{9}).\\
\|u_2\|&=\tfrac{1}{9}\sqrt{100+49+4}=\tfrac{\sqrt{153}}{9}.\\
q_2&=\frac{u_2}{\|u_2\|}=
\frac{1}{\sqrt{153}}(10,-7,2).\\
\langle v_3,q_1\rangle&=\tfrac{1}{3}(2+4+2)=\tfrac{8}{3}.\\
\langle v_3,q_2\rangle&=\frac{1}{\sqrt{153}}(20-14+2)=\frac{8}{\sqrt{153}}.\\
u_3&=(2,2,1)-\tfrac{8}{3}q_1-\tfrac{8}{\sqrt{153}}q_2.\\
&=(2,2,1)-\tfrac{8}{9}(1,2,2)
-\tfrac{8}{153}(10,-7,2).\\
&=\Bigl(2-\tfrac{8}{9}-\tfrac{80}{153},\
2-\tfrac{16}{9}+\tfrac{56}{153},\
1-\tfrac{16}{9}-\tfrac{16}{153}\Bigr).\\
&=\Bigl(\tfrac{270-136-80}{153},\
\tfrac{306-272+56}{153},\
\tfrac{153-272-16}{153}\Bigr)\\
&=\tfrac{1}{153}(54,90,-135)=\tfrac{3}{17}(2, \tfrac{10}{3}, -5).\\
\|u_3\|&=\tfrac{3}{17}\sqrt{4+\tfrac{100}{9}+25}=\tfrac{3}{17}\sqrt{\tfrac{429}{9}}
=\tfrac{\sqrt{429}}{17}.\\
q_3&=\frac{u_3}{\|u_3\|}=\frac{1}{\sqrt{429}}(18,30,-45)
=\frac{1}{\sqrt{429}}(6,10,-15).
\end{align*}
}
\RESULT{
$Q=[q_1\ q_2\ q_3]$ has $Q^\top Q=I$.}
\UNITCHECK{
Verify $\|q_i\|=1$ and $\langle q_i,q_j\rangle=0$ for $i\ne j$.}
\EDGECASES{
\begin{bullets}
\item If some $u_k$ is numerically tiny, reorthogonalize.
\item Linear dependence would halt the process.
\end{bullets}
}
\ALTERNATE{
Use Householder reflections to compute $Q$ with superior numerical stability.}
\VALIDATION{
Compute $Q^\top Q$ numerically and check deviation from $I$ within tolerance.}
\INTUITION{
Strip away components along earlier directions to reveal a perpendicular
remainder, then normalize.}
\CANONICAL{
\begin{bullets}
\item Orthogonality enforced by subtracting projections.
\item Energy preserved by unit normalization.
\end{bullets}
}

\ProblemPage{2}{Projection onto a Plane and Error Orthogonality}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Project a vector onto a subspace and verify orthogonality of the residual.
\PROBLEM{
Let $W=\operatorname{span}\{(1,0,0),(1,1,0)\}\subset\mathbb{R}^3$. For
$x=(2,1,3)$, compute $P_W x$ and show $x-P_W x\perp W$.}
\MODEL{
\[
P_W x=Q Q^\top x,\quad Q=\text{orthonormal basis of }W.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Euclidean inner product on $\mathbb{R}^3$.
\item Orthonormal basis exists for $W$.
\end{bullets}
}
\varmapStart
\var{Q}{Orthonormal basis matrix for $W$.}
\var{x}{Vector to project.}
\var{P_W}{Projector $Q Q^\top$.}
\var{e}{Residual $x-P_W x$.}
\varmapEnd
\WHICHFORMULA{
Formula 3 (Projection Theorem) and equivalent matrix form $P_W=Q Q^\top$.}
\GOVERN{
\[
P_W x=\sum_{j=1}^2 \langle x,q_j\rangle q_j,\quad e=x-P_W x\perp W.
\]
}
\INPUTS{$x=(2,1,3)$, $W=\operatorname{span}\{(1,0,0),(1,1,0)\}$.}
\DERIVATION{
\begin{align*}
\text{Orthonormalize }&(1,0,0),(1,1,0).\\
q_1&=(1,0,0),\ \tilde q_2=(1,1,0)-\langle (1,1,0),q_1\rangle q_1=(0,1,0),\\
q_2&=(0,1,0).\\
P_W x&=\langle x,q_1\rangle q_1+\langle x,q_2\rangle q_2=(2,0,0)+(0,1,0)=(2,1,0).\\
e&=x-P_W x=(0,0,3).\\
\langle e,q_1\rangle&=0,\ \langle e,q_2\rangle=0 \Rightarrow e\perp W.
\end{align*}
}
\RESULT{
$P_W x=(2,1,0)$ and $e=(0,0,3)\perp W$.}
\UNITCHECK{
$\|x\|^2=14$, $\|P_W x\|^2=5$, $\|e\|^2=9$, and $14=5+9$.}
\EDGECASES{
\begin{bullets}
\item If $x\in W$, residual vanishes.
\item If $W=\{0\}$, projection is zero vector.
\end{bullets}
}
\ALTERNATE{
Solve for coefficients $a,b$ minimizing $\|(2,1,3)-a(1,0,0)-b(1,1,0)\|^2$;
normal equations give $a=2$, $b=1$.}
\VALIDATION{
Check $Q^\top e=0$ and energy identity numerically.}
\INTUITION{
Projection keeps only the components along the plane directions.}
\CANONICAL{
\begin{bullets}
\item Orthogonal decomposition $x=P_W x+e$.
\item Residual orthogonality characterizes best approximation.
\end{bullets}
}

\ProblemPage{3}{Cauchy–Schwarz Bound with Equality Condition}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Prove the bound and determine when equality holds.
\PROBLEM{
For $x,y\in\mathbb{R}^n$, show $|\langle x,y\rangle|\le \|x\|\|y\|$ and that
equality holds if and only if $x=\alpha y$ for some $\alpha\in\mathbb{R}$.}
\MODEL{
\[
\|x-ty\|^2\ge 0\ \forall t\in\mathbb{R}.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Euclidean inner product.
\item Real field for equality characterization simplicity.
\end{bullets}
}
\varmapStart
\var{x,y}{Vectors under comparison.}
\var{t}{Scalar minimizing $\|x-ty\|^2$.}
\varmapEnd
\WHICHFORMULA{
Formula 1 (Cauchy–Schwarz Inequality).}
\GOVERN{
\[
\|x-ty\|^2=\|x\|^2-2t\langle x,y\rangle+t^2\|y\|^2\ge 0.
\]
}
\INPUTS{$x,y$ arbitrary vectors in $\mathbb{R}^n$.}
\DERIVATION{
\begin{align*}
\text{Discriminant }\Delta&=4\langle x,y\rangle^2-4\|y\|^2\|x\|^2\le 0.\\
\Rightarrow\ &|\langle x,y\rangle|\le \|x\|\|y\|.\\
\text{Equality: }&\Delta=0\ \Rightarrow\ \exists t:\ x-ty=0\Rightarrow x=\alpha y.
\end{align*}
}
\RESULT{
$|\langle x,y\rangle|\le \|x\|\|y\|$, equality iff linear dependence.}
\UNITCHECK{
Scale invariance under $x\mapsto \alpha x$, $y\mapsto \beta y$.}
\EDGECASES{
\begin{bullets}
\item If $y=0$, bound is $0\le 0$ and equality holds vacuously.
\item If $x=0$, same as above.
\end{bullets}
}
\ALTERNATE{
Minimize $\|x-ty\|$ by choosing $t=\langle y,x\rangle/\|y\|^2$.}
\VALIDATION{
Test with numeric examples; verify equality for collinear vectors.}
\INTUITION{
The projection of $x$ onto $y$ can be no longer than $x$ itself.}
\CANONICAL{
\begin{bullets}
\item Fundamental inner-product bound.
\item Equality reveals one-dimensional alignment.
\end{bullets}
}

\ProblemPage{4}{Alice and Bob Discover the Parallelogram Law}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Relate sums and differences of vectors to side lengths of a parallelogram.
\PROBLEM{
Alice measures $\|x+y\|$ and Bob measures $\|x-y\|$ for two vectors $x,y$ in an
inner product space. Show their squared measurements sum to twice the sum of the
squares of the side lengths and explain when this reveals orthogonality.}
\MODEL{
\[
\|x+y\|^2+\|x-y\|^2\stackrel{?}{=}2\|x\|^2+2\|y\|^2.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Inner product axioms hold.
\item Real or complex field.
\end{bullets}
}
\varmapStart
\var{x,y}{Parallelogram sides.}
\var{x\pm y}{Diagonals.}
\varmapEnd
\WHICHFORMULA{
Formula 2 (Parallelogram Law and Polarization Identity).}
\GOVERN{
\[
\|x\pm y\|^2=\|x\|^2+\|y\|^2\pm 2\operatorname{Re}\langle x,y\rangle.
\]
}
\INPUTS{$x,y$ arbitrary.}
\DERIVATION{
\begin{align*}
\|x+y\|^2+\|x-y\|^2&=2\|x\|^2+2\|y\|^2.\\
\text{If }&x\perp y\Rightarrow \langle x,y\rangle=0,\\
&\Rightarrow \|x+y\|^2=\|x\|^2+\|y\|^2.
\end{align*}
}
\RESULT{
Parallelogram law holds; equality $\|x+y\|^2=\|x\|^2+\|y\|^2$ indicates
orthogonality.}
\UNITCHECK{
Each term has units of squared norm; invariance under rotations.}
\EDGECASES{
\begin{bullets}
\item If $x=0$ or $y=0$, the identity reduces to trivial equality.
\item If $x=y$, then $\|2x\|^2+0=4\|x\|^2$.
\end{bullets}
}
\ALTERNATE{
Use polarization identity to solve for $\langle x,y\rangle$ from the two
measurements and detect orthogonality by testing if it is zero.}
\VALIDATION{
Pick numerical vectors and compute both sides to verify equality.}
\INTUITION{
Diagonal lengths encode the angle between sides; summing squares removes the
angle dependence.}
\CANONICAL{
\begin{bullets}
\item Norm arises from inner product iff the parallelogram law holds.
\item Pythagorean theorem is the orthogonal case.
\end{bullets}
}

\ProblemPage{5}{Coin-Flip Random Signs and Expected Inner Products}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Compute expectation of squared inner products with random sign vectors.
\PROBLEM{
Let $\varepsilon\in\{\pm1\}^n$ have independent fair signs. For fixed $a\in
\mathbb{R}^n$, compute $\mathbb{E}\,\langle a,\varepsilon\rangle^2$ and deduce a
bound on $|\langle a,\varepsilon\rangle|$.}
\MODEL{
\[
\langle a,\varepsilon\rangle=\sum_{i=1}^n a_i \varepsilon_i.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $\mathbb{E}\,\varepsilon_i=0$, $\mathbb{E}\,\varepsilon_i\varepsilon_j=
\delta_{ij}$.
\item Euclidean inner product.
\end{bullets}
}
\varmapStart
\var{a}{Fixed vector.}
\var{\varepsilon}{Random sign vector.}
\varmapEnd
\WHICHFORMULA{
Cauchy–Schwarz (Formula 1) and orthogonality in expectation.}
\GOVERN{
\[
\mathbb{E}\,\langle a,\varepsilon\rangle^2=
\sum_{i,j}a_i a_j \mathbb{E}\,\varepsilon_i\varepsilon_j.
\]
}
\INPUTS{$a\in\mathbb{R}^n$, $\varepsilon$ i.i.d. Rademacher.}
\DERIVATION{
\begin{align*}
\mathbb{E}\,\langle a,\varepsilon\rangle^2
&=\sum_{i,j} a_i a_j \mathbb{E}\,\varepsilon_i\varepsilon_j
=\sum_{i} a_i^2=\|a\|^2.\\
\text{Hence }&\mathbb{P}\{|\langle a,\varepsilon\rangle|\ge t\}
\le \frac{\mathbb{E}\langle a,\varepsilon\rangle^2}{t^2}
=\frac{\|a\|^2}{t^2}.
\end{align*}
}
\RESULT{
$\mathbb{E}\,\langle a,\varepsilon\rangle^2=\|a\|^2$ and a Chebyshev tail bound.}
\UNITCHECK{
Both sides have units of squared norm.}
\EDGECASES{
\begin{bullets}
\item If $a=0$, the inner product is almost surely zero.
\item Heavy-tailed bounds improve with sharper inequalities but remain consistent.
\end{bullets}
}
\ALTERNATE{
Use orthonormal basis where $a=\|a\|e_1$; then
$\langle a,\varepsilon\rangle=\|a\|\varepsilon_1$.}
\VALIDATION{
Monte Carlo with fixed seed confirms $\mathbb{E}\approx\|a\|^2$.}
\INTUITION{
Random signs are orthogonal on average, leaving only the energy of $a$.}
\CANONICAL{
\begin{bullets}
\item Expected orthogonality simplifies quadratic forms.
\item Energy conservation in expectation.
\end{bullets}
}

\ProblemPage{6}{Proof: Polarization Recovers the Inner Product}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Show that a norm satisfying the parallelogram law comes from an inner product.
\PROBLEM{
Let $\|\cdot\|$ satisfy $\|x+y\|^2+\|x-y\|^2=2\|x\|^2+2\|y\|^2$ on a real vector
space. Define $\langle x,y\rangle=\frac{1}{4}(\|x+y\|^2-\|x-y\|^2)$. Prove that
$\langle\cdot,\cdot\rangle$ is an inner product inducing $\|\cdot\|$.}
\MODEL{
\[
\langle x,y\rangle=\frac{1}{4}(\|x+y\|^2-\|x-y\|^2).
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Real vector space with norm satisfying the parallelogram law.
\end{bullets}
}
\varmapStart
\var{\|\cdot\|}{Given norm.}
\var{\langle\cdot,\cdot\rangle}{Candidate inner product.}
\varmapEnd
\WHICHFORMULA{
Formula 2 (Parallelogram Law and Polarization Identity).}
\GOVERN{
\[
\|x\|^2=\langle x,x\rangle.
\]
}
\INPUTS{$x,y,z\in V$, scalars $\alpha,\beta\in\mathbb{R}$.}
\DERIVATION{
\begin{align*}
\text{Symmetry: }&\langle x,y\rangle=\langle y,x\rangle\ \text{by definition.}\\
\text{Bilinearity: }&
\langle \alpha x+\beta y,z\rangle\\
&=\tfrac{1}{4}\bigl(\|\alpha x+\beta y+z\|^2-\|\alpha x+\beta y-z\|^2\bigr)\\
&=\alpha \tfrac{1}{4}\bigl(\|x+z\|^2-\|x-z\|^2\bigr)
+\beta \tfrac{1}{4}\bigl(\|y+z\|^2-\|y-z\|^2\bigr)\\
&=\alpha\langle x,z\rangle+\beta\langle y,z\rangle,\\
\text{using the parallelogram law algebra.}\\
\text{Positive definite: }&\langle x,x\rangle=\tfrac{1}{4}(4\|x\|^2)=\|x\|^2>0
\text{ for }x\ne 0.
\end{align*}
}
\RESULT{
$\langle\cdot,\cdot\rangle$ is an inner product and induces the norm.}
\UNITCHECK{
$\|x\|^2=\langle x,x\rangle$ by construction.}
\EDGECASES{
\begin{bullets}
\item If the parallelogram law fails, polarization need not define bilinearity.
\end{bullets}
}
\ALTERNATE{
In complex spaces use the complex polarization formula to recover sesquilinearity.}
\VALIDATION{
Check on a basis and extend by linearity; verify Cauchy–Schwarz holds.}
\INTUITION{
Lengths along sums and differences encode angles, hence the inner product.}
\CANONICAL{
\begin{bullets}
\item Equivalence of inner-product norms and parallelogram law.
\item Polarization reconstructs the inner product.
\end{bullets}
}

\ProblemPage{7}{Proof: Orthogonal Projectors are Self-Adjoint Idempotents}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Characterize orthogonal projectors by algebraic properties.
\PROBLEM{
Let $P$ be a bounded linear operator on a Hilbert space $V$. Prove that $P$ is
the orthogonal projector onto $\mathcal{R}(P)$ if and only if $P^2=P$ and
$P=P^\ast$.}
\MODEL{
\[
P^2=P,\quad P=P^\ast \iff P=P_W \text{ for }W=\mathcal{R}(P).
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $V$ is Hilbert.
\item $P$ is bounded and linear.
\end{bullets}
}
\varmapStart
\var{P}{Operator under test.}
\var{W}{Candidate range subspace $\mathcal{R}(P)$.}
\varmapEnd
\WHICHFORMULA{
Formula 3 (Projection Theorem) and self-adjointness of orthogonal projectors.}
\GOVERN{
\[
x-Px\perp \mathcal{R}(P)\ \text{ iff }\ \langle x-Px, Pw\rangle=0\ \forall w.
\]
}
\INPUTS{$P$ with $P^2=P$ and $P=P^\ast$.}
\DERIVATION{
\begin{align*}
\text{If }&P=P_W,\ \text{then }P^2=P,\ P=P^\ast\ \text{are standard.}\\
\text{Conversely, }&\text{take }W=\mathcal{R}(P).\\
\text{For any }&x\in V,\ w\in V:\\
\langle x-Px, Pw\rangle&=\langle x, Pw\rangle-\langle Px, Pw\rangle\\
&=\langle P^\ast x, w\rangle-\langle P^2 x, w\rangle\\
&=\langle Px-Px, w\rangle=0.\\
\Rightarrow&\ x-Px\perp W, \text{ so }Px=P_W x.
\end{align*}
}
\RESULT{
Orthogonal projectors are exactly the self-adjoint idempotents.}
\UNITCHECK{
$P$ preserves units of length along $W$ and annihilates along $W^\perp$.}
\EDGECASES{
\begin{bullets}
\item If $P^2=P$ but $P\ne P^\ast$, $P$ is an oblique projector.
\end{bullets}
}
\ALTERNATE{
Diagonalize $P$ via spectral theorem; eigenvalues in $\{0,1\}$ and the operator
is a projection onto its 1-eigenspace iff self-adjoint.}
\VALIDATION{
In finite dimensions check $P^\top=P$ and $P^2=P$ numerically.}
\INTUITION{
Self-adjointness ensures perpendicularity of the dropped component; idempotence
ensures projecting twice changes nothing.}
\CANONICAL{
\begin{bullets}
\item Geometric orthogonality equals algebraic symmetry plus idempotence.
\item Range and nullspace are orthogonal complements.
\end{bullets}
}

\ProblemPage{8}{Combo: QR-Based Least Squares and Orthogonality}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Solve least squares via QR and verify residual orthogonality.
\PROBLEM{
For $X\in\mathbb{R}^{5\times 2}$ with full column rank and $y\in\mathbb{R}^5$,
solve $\min_\beta\|y-X\beta\|_2$ using QR and verify $Q^\top e=0$.}
\MODEL{
\[
X=QR,\ Q^\top Q=I,\ \beta=R^{-1}Q^\top y,\ e=y-X\beta.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $X$ has full column rank.
\item Euclidean inner product.
\end{bullets}
}
\varmapStart
\var{Q}{Matrix with orthonormal columns spanning $\mathcal{C}(X)$.}
\var{R}{Upper triangular.}
\var{\beta}{Least squares coefficients.}
\var{e}{Residual orthogonal to $\mathcal{C}(X)$.}
\varmapEnd
\WHICHFORMULA{
Formula 5 (Orthogonal Projector and Least Squares) and QR equivalence.}
\GOVERN{
\[
\widehat{y}=QQ^\top y,\ e=(I-QQ^\top)y,\ Q^\top e=0.
\]
}
\INPUTS{$X=\begin{bmatrix}1&0\\1&1\\1&2\\1&3\\1&4\end{bmatrix}$,
$y=(1,2,2,3,3)^\top$.}
\DERIVATION{
\begin{align*}
\text{Compute }&Q\text{ from Gram–Schmidt on columns of }X.\\
x_1&=(1,1,1,1,1)^\top,\ \|x_1\|=\sqrt{5},\ q_1=x_1/\sqrt{5}.\\
x_2&=(0,1,2,3,4)^\top,\ \langle x_2,q_1\rangle= \tfrac{10}{\sqrt{5}}=2\sqrt{5}.\\
u_2&=x_2-2\sqrt{5}\,q_1=(-2,-1,0,1,2)^\top,\\
\|u_2\|&=\sqrt{10},\ q_2=u_2/\sqrt{10}.\\
\widehat{y}&=QQ^\top y=(\langle y,q_1\rangle)q_1+(\langle y,q_2\rangle)q_2.\\
\langle y,q_1\rangle&=\tfrac{11}{\sqrt{5}},\
\langle y,q_2\rangle=\tfrac{3}{\sqrt{10}}.\\
\widehat{y}&=\tfrac{11}{5}x_1+\tfrac{3}{10}u_2
=\tfrac{11}{5}(1,1,1,1,1)^\top+\tfrac{3}{10}(-2,-1,0,1,2)^\top.\\
&=(1.7,1.9,2.2,2.5,2.8)^\top.\\
e&=y-\widehat{y}=(-0.7,0.1,-0.2,0.5,0.2)^\top,\\
Q^\top e&=(0,0)^\top.
\end{align*}
}
\RESULT{
$\beta=R^{-1}Q^\top y$ and $Q^\top e=0$; projection $\widehat{y}=QQ^\top y$.}
\UNITCHECK{
$\|y\|^2=\|\widehat{y}\|^2+\|e\|^2$ numerically verified.}
\EDGECASES{
\begin{bullets}
\item If columns nearly dependent, $R$ ill-conditioned; prefer pivoting.
\end{bullets}
}
\ALTERNATE{
Solve normal equations and compare to QR result.}
\VALIDATION{
Check that $Q^\top Q=I$ and compute $Q^\top e$ numerically to be zero.}
\INTUITION{
Project onto the column space with $Q$ and measure remaining orthogonal error.}
\CANONICAL{
\begin{bullets}
\item QR realizes projection via $QQ^\top$.
\item Residual orthogonality encodes optimality.
\end{bullets}
}

\ProblemPage{9}{Narrative: Bob's Best Approximation on a Line}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Find the closest point on a line through the origin.
\PROBLEM{
Bob must choose $w=\alpha u$ on the line $W=\operatorname{span}\{u\}$ to
approximate a given $x$. Determine $\alpha$ and show $x-\alpha u\perp u$.}
\MODEL{
\[
\alpha=\arg\min_{t\in\mathbb{F}}\|x-tu\|^2.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $u\ne 0$ in an inner product space over $\mathbb{F}$.
\end{bullets}
}
\varmapStart
\var{x}{Given vector to approximate.}
\var{u}{Direction of the line.}
\var{\alpha}{Optimal scalar coefficient.}
\varmapEnd
\WHICHFORMULA{
Projection theorem specialized to one-dimensional subspace.}
\GOVERN{
\[
\alpha=\frac{\langle x,u\rangle}{\langle u,u\rangle},\quad
x-\alpha u\perp u.
\]
}
\INPUTS{$x,u$ with $u\ne 0$.}
\DERIVATION{
\begin{align*}
\|x-tu\|^2&=\|x\|^2-2\operatorname{Re}(t\langle x,u\rangle)
+|t|^2\|u\|^2,\\
\text{minimizer }&t=\frac{\langle x,u\rangle}{\langle u,u\rangle}=\alpha.\\
\langle x-\alpha u,u\rangle&=\langle x,u\rangle-\alpha\langle u,u\rangle=0.
\end{align*}
}
\RESULT{
$w=\alpha u$ with $\alpha=\langle x,u\rangle/\langle u,u\rangle$.}
\UNITCHECK{
Scale-invariant after normalizing $u$ to unit length.}
\EDGECASES{
\begin{bullets}
\item If $x\perp u$, then $\alpha=0$ and the best approximation is the origin.
\end{bullets}
}
\ALTERNATE{
Normalize $u$ to $q=u/\|u\|$ and take $\alpha=\langle x,q\rangle$.}
\VALIDATION{
Check numerically that $\|x-\alpha u\|\le \|x-tu\|$ for nearby $t$.}
\INTUITION{
Drop a perpendicular from $x$ to the line.}
\CANONICAL{
\begin{bullets}
\item One-dimensional projection formula.
\item Orthogonality condition as first-order optimality.
\end{bullets}
}

\ProblemPage{10}{Combo: PCA Directions are Orthogonal}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Show principal components are orthonormal eigenvectors of the covariance.
\PROBLEM{
Given centered data matrix $Z\in\mathbb{R}^{n\times d}$, prove that the
principal component directions are orthonormal and maximize projected variance,
and compute the first component for a small numeric example.}
\MODEL{
\[
S=\tfrac{1}{n}Z^\top Z,\quad v_1=\arg\max_{\|v\|=1} v^\top S v.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Centered data; Euclidean inner product.
\item $S$ is symmetric positive semidefinite.
\end{bullets}
}
\varmapStart
\var{S}{Covariance matrix.}
\var{v_i}{Principal directions, $\|v_i\|=1$.}
\var{\lambda_i}{Eigenvalues, variances along $v_i$.}
\varmapEnd
\WHICHFORMULA{
Rayleigh quotient maximization with orthonormal constraints uses inner products;
orthogonality arises from spectral theorem.}
\GOVERN{
\[
\max_{\|v\|=1} v^\top S v=\lambda_{\max},\ v_1\ \text{eigenvector.}
\]
}
\INPUTS{$Z=\begin{bmatrix}-1&0\\0&1\\1&2\end{bmatrix}$.}
\DERIVATION{
\begin{align*}
S&=\tfrac{1}{3}\begin{bmatrix}(-1)^2+0^2+1^2&(-1)\cdot 0+0\cdot 1+1\cdot 2\\
0\cdot(-1)+1\cdot 0+2\cdot 1&0^2+1^2+2^2\end{bmatrix}
=\tfrac{1}{3}\begin{bmatrix}2&2\\2&5\end{bmatrix}.\\
\text{Eigenvalues: }&\det(S-\lambda I)=0\Rightarrow
\lambda=\tfrac{7\pm\sqrt{33}}{6}.\\
v_1&\propto \begin{bmatrix}2\\ \tfrac{7+\sqrt{33}}{2}-\tfrac{5}{3}\end{bmatrix}
\propto (1,\ \tfrac{1+\sqrt{33}}{4})^\top,\ \text{normalize}.
\end{align*}
}
\RESULT{
Principal directions are orthonormal and correspond to eigenvectors of $S$.}
\UNITCHECK{
$v^\top v=1$; projected variance equals eigenvalue.}
\EDGECASES{
\begin{bullets}
\item Equal eigenvalues yield non-unique directions within eigenspaces.
\end{bullets}
}
\ALTERNATE{
Compute SVD $Z=U\Sigma V^\top$; columns of $V$ are orthonormal PCs.}
\VALIDATION{
Verify $S v_i=\lambda_i v_i$ and $v_i^\top v_j=0$ for $i\ne j$.}
\INTUITION{
Orthogonality of directions prevents redundancy; each component captures new
variance independently.}
\CANONICAL{
\begin{bullets}
\item Spectral orthogonality of symmetric operators.
\item Orthonormal basis diagonalizes $S$.
\end{bullets}
}

\clearpage
\section{Coding Demonstrations}
\CodeDemoPage{Cauchy–Schwarz Verification and Equality Case}
\PROBLEM{
Compute $|\langle x,y\rangle|$ and compare to $\|x\|\|y\|$ for deterministic
vectors; construct an equality case by alignment.}
\API{
\begin{bullets}
\item \inlinecode{def read_input(s) -> tuple} — parse two vectors.
\item \inlinecode{def solve_case(x,y) -> tuple} — compute lhs, rhs, equal flag.
\item \inlinecode{def validate() -> None} — assertions including equality case.
\item \inlinecode{def main() -> None} — orchestrate demo.
\end{bullets}
}
\INPUTS{
Two whitespace-separated lines of floats for $x$ and $y$.}
\OUTPUTS{
Tuple $(\text{lhs},\text{rhs},\text{eq})$ with floats and boolean equality flag.}
\FORMULA{
\[
\text{lhs}=|\langle x,y\rangle|,\quad \text{rhs}=\|x\|\|y\|,\quad
\text{eq}=(\text{lhs}\approx \text{rhs}).
\]
}
\textbf{SOLUTION A — From Scratch (Mathematically Explicit Implementation)}
\begin{codepy}
import math

def read_input(s):
    a, b = s.strip().split("\n")
    x = [float(t) for t in a.split()]
    y = [float(t) for t in b.split()]
    return x, y

def dot(x, y):
    return sum(xi*yi for xi, yi in zip(x, y))

def norm(x):
    return math.sqrt(dot(x, x))

def solve_case(x, y):
    lhs = abs(dot(x, y))
    rhs = norm(x) * norm(y)
    eq = abs(lhs - rhs) < 1e-12
    return lhs, rhs, eq

def validate():
    x = [1.0, 2.0, -1.0]
    y = [2.0, -1.0, 0.0]
    lhs, rhs, eq = solve_case(x, y)
    assert lhs <= rhs + 1e-12
    # Equality case: y is scalar multiple of x
    y2 = [3.0*xi for xi in x]
    lhs2, rhs2, eq2 = solve_case(x, y2)
    assert abs(lhs2 - rhs2) < 1e-12 and eq2

def main():
    validate()
    x, y = [1.0, 2.0], [3.0, 4.0]
    lhs, rhs, eq = solve_case(x, y)
    print("lhs", round(lhs, 6), "rhs", round(rhs, 6), "eq", eq)

if __name__ == "__main__":
    main()
\end{codepy}
\textbf{SOLUTION B — Library-Based (Validated Computational Shortcut)}
\begin{codepy}
import numpy as np

def read_input(s):
    a, b = s.strip().split("\n")
    x = np.array([float(t) for t in a.split()], dtype=float)
    y = np.array([float(t) for t in b.split()], dtype=float)
    return x, y

def solve_case(x, y):
    lhs = float(abs(x @ y))
    rhs = float(np.linalg.norm(x) * np.linalg.norm(y))
    eq = abs(lhs - rhs) < 1e-12
    return lhs, rhs, eq

def validate():
    x = np.array([1.0, 2.0, -1.0])
    y = np.array([2.0, -1.0, 0.0])
    lhs, rhs, eq = solve_case(x, y)
    assert lhs <= rhs + 1e-12
    y2 = 3.0 * x
    lhs2, rhs2, eq2 = solve_case(x, y2)
    assert abs(lhs2 - rhs2) < 1e-12 and eq2

def main():
    validate()
    x, y = np.array([1.0, 2.0]), np.array([3.0, 4.0])
    lhs, rhs, eq = solve_case(x, y)
    print("lhs", round(lhs, 6), "rhs", round(rhs, 6), "eq", eq)

if __name__ == "__main__":
    main()
\end{codepy}
\COMPLEXITY{
Time $\mathcal{O}(n)$, space $\mathcal{O}(1)$ beyond inputs for both variants.}
\FAILMODES{
\begin{bullets}
\item Empty vectors or mismatched lengths; guard by zipping and size checks.
\item Overflow for huge values; scale inputs if needed.
\end{bullets}
}
\STABILITY{
\begin{bullets}
\item Catastrophic cancellation is minimal in dot products with moderate scales.
\item Use extended precision if needed for extreme magnitudes.
\end{bullets}
}
\VALIDATION{
\begin{bullets}
\item Analytical equality case confirms tightness.
\item Random tests with fixed seeds can be added for coverage.
\end{bullets}
}
\RESULT{
Both implementations agree and detect equality when vectors are aligned.}
\EXPLANATION{
Computes the inner product, norms, and verifies the Cauchy–Schwarz bound, which
is Formula 1.}

\CodeDemoPage{Gram–Schmidt and QR Factorization Verification}
\PROBLEM{
Implement Gram–Schmidt to produce $Q$ with orthonormal columns and $R=Q^\top A$;
verify $A=QR$ and $Q^\top Q=I$.}
\API{
\begin{bullets}
\item \inlinecode{def gs(A) -> (Q,R)} — classical Gram–Schmidt.
\item \inlinecode{def validate() -> None} — checks identities on a test matrix.
\item \inlinecode{def main() -> None} — run validation and print norms.
\end{bullets}
}
\INPUTS{
Matrix $A\in\mathbb{R}^{n\times m}$ with full column rank.}
\OUTPUTS{
$Q$ with orthonormal columns and $R$ upper triangular so that $A\approx QR$.}
\FORMULA{
\[
q_k=\frac{1}{\|u_k\|}\Bigl(a_k-\sum_{j<k}\langle a_k,q_j\rangle q_j\Bigr),\ 
R=Q^\top A.
\]
}
\textbf{SOLUTION A — From Scratch (Mathematically Explicit Implementation)}
\begin{codepy}
import math

def gs(A):
    n, m = len(A), len(A[0])
    Q = [[0.0]*m for _ in range(n)]
    R = [[0.0]*m for _ in range(m)]
    for k in range(m):
        u = [A[i][k] for i in range(n)]
        for j in range(k):
            qj = [Q[i][j] for i in range(n)]
            rjk = sum(qj[i]*A[i][k] for i in range(n))
            R[j][k] = rjk
            u = [u[i] - rjk*qj[i] for i in range(n)]
        rk = math.sqrt(sum(ui*ui for ui in u))
        R[k][k] = rk
        qk = [ui/rk for ui in u]
        for i in range(n):
            Q[i][k] = qk[i]
    return Q, R

def matmul(A, B):
    n, m, p = len(A), len(B), len(B[0])
    return [[sum(A[i][k]*B[k][j] for k in range(m)) for j in range(p)]
            for i in range(n)]

def transpose(A):
    return list(map(list, zip(*A)))

def frob_norm(A):
    return math.sqrt(sum(x*x for row in A for x in row))

def validate():
    A = [[1.0, 1.0], [1.0, 2.0], [1.0, 3.0]]
    Q, R = gs(A)
    QtQ = matmul(transpose(Q), Q)
    I = [[1.0, 0.0], [0.0, 1.0]]
    err_orth = frob_norm([[QtQ[i][j]-I[i][j] for j in range(2)] for i in range(2)])
    QR = matmul(Q, R)
    diff = [[QR[i][j]-A[i][j] for j in range(2)] for i in range(3)]
    err_fact = frob_norm(diff)
    assert err_orth < 1e-10 and err_fact < 1e-10

def main():
    validate()
    print("ok")

if __name__ == "__main__":
    main()
\end{codepy}
\textbf{SOLUTION B — Library-Based (Validated Computational Shortcut)}
\begin{codepy}
import numpy as np

def validate():
    A = np.array([[1.0, 1.0], [1.0, 2.0], [1.0, 3.0]])
    Q, R = np.linalg.qr(A, mode="reduced")
    err_orth = np.linalg.norm(Q.T @ Q - np.eye(2))
    err_fact = np.linalg.norm(Q @ R - A)
    assert err_orth < 1e-10 and err_fact < 1e-10

def main():
    validate()
    print("ok")

if __name__ == "__main__":
    main()
\end{codepy}
\COMPLEXITY{
Time $\mathcal{O}(nm^2)$ and space $\mathcal{O}(nm)$ for classical Gram–Schmidt.}
\FAILMODES{
\begin{bullets}
\item Near collinearity causes small $R_{kk}$; reorthogonalize if needed.
\item Zero column implies rank deficiency; detect and stop.
\end{bullets}
}
\STABILITY{
\begin{bullets}
\item Modified Gram–Schmidt or Householder QR improves conditioning.
\end{bullets}
}
\VALIDATION{
\begin{bullets}
\item Check $Q^\top Q=I$ and $QR=A$ with small Frobenius error.
\end{bullets}
}
\RESULT{
Both methods yield orthonormal $Q$ and accurate $QR$ factorization.}
\EXPLANATION{
Implements Formula 4 to produce orthonormal bases and demonstrates QR structure.}

\CodeDemoPage{Least Squares Projection and Residual Orthogonality}
\PROBLEM{
Compute $\hat\beta=(X^\top X)^{-1}X^\top y$ and verify $X^\top e=0$ and
$\widehat{y}=\mathbb{P}y$.}
\API{
\begin{bullets}
\item \inlinecode{def solve_case(X,y) -> tuple} — $\hat\beta,\widehat{y},e$.
\item \inlinecode{def validate() -> None} — assertions on orthogonality.
\item \inlinecode{def main() -> None} — run validation and print results.
\end{bullets}
}
\INPUTS{
$X\in\mathbb{R}^{n\times d}$ full column rank, $y\in\mathbb{R}^n$.}
\OUTPUTS{
$\hat\beta$, fitted $\widehat{y}$, and residual $e$.}
\FORMULA{
\[
\hat\beta=(X^\top X)^{-1}X^\top y,\quad \widehat{y}=X\hat\beta,\quad e=y-\widehat{y}.
\]
}
\textbf{SOLUTION A — From Scratch (Mathematically Explicit Implementation)}
\begin{codepy}
import numpy as np

def solve_case(X, y):
    XtX = X.T @ X
    Xty = X.T @ y
    beta = np.linalg.inv(XtX) @ Xty
    yhat = X @ beta
    e = y - yhat
    return beta, yhat, e

def validate():
    X = np.array([[1,0],[1,1],[1,2]], dtype=float)
    y = np.array([1,2,2], dtype=float)
    beta, yhat, e = solve_case(X, y)
    assert np.allclose(X.T @ e, np.zeros(2))
    P = X @ np.linalg.inv(X.T @ X) @ X.T
    assert np.allclose(yhat, P @ y)

def main():
    validate()
    print("ok")

if __name__ == "__main__":
    main()
\end{codepy}
\textbf{SOLUTION B — Library-Based (Validated Computational Shortcut)}
\begin{codepy}
import numpy as np

def solve_case(X, y):
    beta, *_ = np.linalg.lstsq(X, y, rcond=None)
    yhat = X @ beta
    e = y - yhat
    return beta, yhat, e

def validate():
    X = np.array([[1,0],[1,1],[1,2]], dtype=float)
    y = np.array([1,2,2], dtype=float)
    beta, yhat, e = solve_case(X, y)
    assert np.allclose(X.T @ e, np.zeros(2))
    P = X @ np.linalg.pinv(X)  # equals X(X^T X)^{-1}X^T for full rank
    assert np.allclose(yhat, P @ y)

def main():
    validate()
    print("ok")

if __name__ == "__main__":
    main()
\end{codepy}
\COMPLEXITY{
Time $\mathcal{O}(nd^2)$ for normal equations, $\mathcal{O}(nd^2)$ for lstsq; 
space $\mathcal{O}(d^2)$.}
\FAILMODES{
\begin{bullets}
\item Rank deficiency; use pseudoinverse or QR with pivoting.
\item Poor conditioning; avoid explicit inverse in practice.
\end{bullets}
}
\STABILITY{
\begin{bullets}
\item Prefer QR or SVD for stable solves when $X^\top X$ is ill-conditioned.
\end{bullets}
}
\VALIDATION{
\begin{bullets}
\item Check $X^\top e=0$ and $yhat=\mathbb{P}y$.
\end{bullets}
}
\RESULT{
Both variants produce identical fitted values and orthogonal residuals.}
\EXPLANATION{
Implements Formula 5 and verifies projection and orthogonality properties.}

\clearpage
\section{Applied Domains — Detailed End-to-End Scenarios}
\DomainPage{Machine Learning}
\SCENARIO{
Fit linear regression by interpreting it as orthogonal projection of $y$ onto
$\mathcal{C}(X)$; verify residual orthogonality and energy decomposition.}
\ASSUMPTIONS{
\begin{bullets}
\item Euclidean inner product on $\mathbb{R}^n$.
\item Full column rank design matrix $X$.
\end{bullets}
}
\WHICHFORMULA{
Orthogonal projector and normal equations (Formula 5):
$\hat\beta=(X^\top X)^{-1}X^\top y$, $\widehat{y}=X\hat\beta$, $X^\top e=0$.}
\varmapStart
\var{X}{Design matrix $(n,d)$ including bias.}
\var{y}{Response vector length $n$.}
\var{\beta}{Coefficients minimizing squared error.}
\var{e}{Residual orthogonal to $\mathcal{C}(X)$.}
\varmapEnd
\PIPELINE{
\begin{bullets}
\item Generate synthetic data with known $\beta$.
\item Compute $\hat\beta$ via normal equations and via QR.
\item Verify $X^\top e=0$ and compare energies.
\end{bullets}
}
\textbf{Implementation (From Scratch)}
\begin{codepy}
import numpy as np

def generate(n=50, noise=0.1, seed=0):
    rng = np.random.RandomState(seed)
    x = np.linspace(0.0, 1.0, n)
    X = np.column_stack([np.ones(n), x])
    beta_true = np.array([1.0, 2.0])
    y = X @ beta_true + rng.randn(n) * noise
    return X, y, beta_true

def fit_normal(X, y):
    return np.linalg.inv(X.T @ X) @ (X.T @ y)

def fit_qr(X, y):
    Q, R = np.linalg.qr(X)
    return np.linalg.solve(R, Q.T @ y)

def main():
    X, y, b = generate()
    b1 = fit_normal(X, y)
    b2 = fit_qr(X, y)
    yhat = X @ b1
    e = y - yhat
    print("beta", np.round(b1, 3))
    print("orth", np.linalg.norm(X.T @ e))
    print("energy", round(np.dot(y, y) - np.dot(yhat, yhat) - np.dot(e, e), 9))

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{Residual orthogonality $\|X^\top e\|$ and energy split $\|y\|^2=\|\widehat{y}\|^2+\|e\|^2$.}
\INTERPRET{Least squares is an orthogonal projection; the residual is perpendicular to the span of features.}
\NEXTSTEPS{Use weighted inner products for heteroskedastic noise.}

\DomainPage{Quantitative Finance}
\SCENARIO{
Construct orthogonal risk factors from raw returns via Gram–Schmidt and compute
portfolio exposures to orthonormal factors.}
\ASSUMPTIONS{
\begin{bullets}
\item Returns matrix $R\in\mathbb{R}^{n\times d}$ is centered.
\item Euclidean inner product across time samples.
\end{bullets}
}
\WHICHFORMULA{
Gram–Schmidt (Formula 4) to build orthonormal factors $Q$; exposures are
projections $\langle r, q_j\rangle$.}
\varmapStart
\var{R}{Matrix of returns (time by asset).}
\var{Q}{Orthonormal factor matrix.}
\var{w}{Portfolio weights.}
\var{f}{Factor returns $Q^\top R$.}
\varmapEnd
\PIPELINE{
\begin{bullets}
\item Center returns and orthonormalize columns to get $Q$.
\item Project returns onto $Q$ to obtain factor series.
\item Compute portfolio factor exposures via inner products.
\end{bullets}
}
\textbf{Implementation (Full Pipeline)}
\begin{codepy}
import numpy as np

def center(R):
    return R - R.mean(axis=0, keepdims=True)

def gram_schmidt(A):
    n, d = A.shape
    Q = np.zeros_like(A)
    for k in range(d):
        u = A[:, k].copy()
        for j in range(k):
            u -= np.dot(Q[:, j], A[:, k]) * Q[:, j]
        u_norm = np.linalg.norm(u)
        Q[:, k] = u / u_norm
    return Q

def main():
    np.random.seed(0)
    R = np.random.randn(100, 3)
    R = center(R)
    Q = gram_schmidt(R)
    f = Q.T @ R
    w = np.array([0.5, 0.3, 0.2])
    r_p = R @ w
    expos = Q.T @ r_p
    print("orth", np.linalg.norm(Q.T @ Q - np.eye(3)))
    print("expos", np.round(expos, 3))

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{Orthonormality error of $Q$ and portfolio factor exposures.}
\INTERPRET{Orthogonal factors decouple risk; exposures read off as projections.}
\NEXTSTEPS{Use PCA for maximal-variance orthonormal factors.}

\DomainPage{Deep Learning}
\SCENARIO{
Enforce orthonormality of a linear layer by projecting its weight matrix onto
the Stiefel manifold using QR after gradient steps.}
\ASSUMPTIONS{
\begin{bullets}
\item Linear layer with weight $W\in\mathbb{R}^{d\times k}$ and $d\ge k$.
\item Euclidean inner product; columns of $W$ constrained to be orthonormal.
\end{bullets}
}
\WHICHFORMULA{
Orthogonalization via QR: $W\leftarrow \operatorname{qf}(W)$ where
$\operatorname{qf}$ returns the $Q$ factor with orthonormal columns.}
\PIPELINE{
\begin{bullets}
\item Initialize $W$ and perform a few gradient-like updates.
\item Project $W$ to orthonormal columns by QR.
\item Verify $W^\top W=I$ and compute projection error.
\end{bullets}
}
\textbf{Implementation (End-to-End)}
\begin{codepy}
import numpy as np

def project_orthonormal(W):
    Q, _ = np.linalg.qr(W)
    return Q

def main():
    np.random.seed(0)
    d, k = 6, 3
    W = np.random.randn(d, k)
    for _ in range(5):
        grad = np.sign(W)  # dummy gradient step
        W = W - 0.1 * grad
        W = project_orthonormal(W)
    err = np.linalg.norm(W.T @ W - np.eye(k))
    print("orth_error", err)

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{Orthogonality error $\|W^\top W-I\|$.}
\INTERPRET{Column orthonormality stabilizes training and preserves energy.}
\NEXTSTEPS{Use manifold optimization respecting orthogonality constraints.}

\DomainPage{Kaggle / Data Analytics}
\SCENARIO{
Perform PCA on synthetic data and verify that principal components form an
orthonormal basis with respect to the Euclidean inner product.}
\ASSUMPTIONS{
\begin{bullets}
\item Data centered; covariance matrix symmetric positive semidefinite.
\end{bullets}
}
\WHICHFORMULA{
SVD $X=U\Sigma V^\top$ with $V$ orthonormal; columns of $V$ are PCs.}
\PIPELINE{
\begin{bullets}
\item Generate correlated features and center them.
\item Compute SVD and verify $V^\top V=I$.
\item Project data onto top components and measure variance captured.
\end{bullets}
}
\textbf{Implementation (Complete EDA Pipeline)}
\begin{codepy}
import numpy as np

def make_data(n=200, seed=0):
    rng = np.random.RandomState(seed)
    x1 = rng.randn(n)
    x2 = 0.8 * x1 + rng.randn(n) * 0.2
    x3 = rng.randn(n) * 0.5
    X = np.column_stack([x1, x2, x3])
    X -= X.mean(axis=0, keepdims=True)
    return X

def main():
    X = make_data()
    U, S, Vt = np.linalg.svd(X, full_matrices=False)
    V = Vt.T
    orth_err = np.linalg.norm(V.T @ V - np.eye(3))
    var_total = np.linalg.norm(X, "fro")**2
    var_pc12 = (S[0]**2 + S[1]**2)
    print("orth_err", orth_err)
    print("var_ratio", round(var_pc12 / var_total, 3))

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{Orthogonality error of $V$ and variance ratio captured by top PCs.}
\INTERPRET{Orthonormal PCs decompose variance without redundancy.}
\NEXTSTEPS{Whiten data by scaling by singular values to obtain orthonormal scores.}

\end{document}
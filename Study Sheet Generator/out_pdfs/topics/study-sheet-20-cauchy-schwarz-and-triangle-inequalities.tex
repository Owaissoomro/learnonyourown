% !TeX program = xelatex
\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{microtype,setspace,amsmath,amssymb,mathtools,amsthm,unicode-math}
\setstretch{1.05}
\setmainfont{Latin Modern Roman}
\setmonofont{Latin Modern Mono}
\setmathfont{Latin Modern Math}

\allowdisplaybreaks[4]
\setlength{\jot}{7pt}
\setlength{\emergencystretch}{8em}
\sloppy

\usepackage{xcolor,fancyhdr,enumitem,inconsolata,listings}
\pagestyle{fancy}\fancyhf{}\lhead{\nouppercase{\leftmark}}\rhead{\thepage}
\setlength{\headheight}{26pt}
\setlength{\parindent}{0pt}
\setlength{\parskip}{8pt plus 2pt minus 1pt}
\raggedbottom

\newenvironment{BreakableEquation}{\begin{equation}\begin{aligned}}{\end{aligned}\end{equation}}
\newenvironment{BreakableEquation*}{\begin{equation*}\begin{aligned}}{\end{aligned}\end{equation*}}
\newenvironment{tightalign}{\begingroup\small\allowdisplaybreaks\begin{align}}{\end{align}\endgroup}

\providecommand{\enumlistm}{enumitem}
\newenvironment{minted}[2][]{%
  \lstset{style=code,language=#2,#1}\begin{lstlisting}%
}{\end{lstlisting}}

\newcommand{\inputminted}[3][]{\begin{lstlisting}\end{lstlisting}}

\newlist{bullets}{itemize}{1}
\setlist[bullets]{label=--,leftmargin=1.6em,itemsep=2pt,topsep=4pt}

\newcommand{\varmapStart}{\textbf{VARIABLE MAPPING:}\par\begin{bullets}}
\newcommand{\var}[2]{\item $#1$ — #2}
\newcommand{\varmapEnd}{\end{bullets}}

\newcommand{\glossx}[6]{%
  \textbf{#1}\par
  \begin{bullets}
    \item \textbf{What:} #2
    \item \textbf{Why:} #3
    \item \textbf{How:} #4
    \item \textbf{ELI5:} #5
    \item \textbf{Pitfall/Example:} #6
  \end{bullets}
}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{claim}[theorem]{Claim}

\lstdefinestyle{code}{
  basicstyle=\ttfamily\footnotesize,
  backgroundcolor=\color{black!02},
  frame=single,
  numbers=left, numberstyle=\tiny, numbersep=8pt,
  breaklines=true, breakatwhitespace=true,
  tabsize=4, showstringspaces=false,
  upquote=true, keepspaces=true, columns=fullflexible,
  literate=
    {–}{{-}}1
    {—}{{-}}1
    {…}{{...}}1
    {≤}{{\ensuremath{\le}}}1
    {≥}{{\ensuremath{\ge}}}1
    {≠}{{\ensuremath{\ne}}}1
    {≈}{{\ensuremath{\approx}}}1
    {±}{{\ensuremath{\pm}}}1
    {→}{{\ensuremath{\to}}}1
    {←}{{\ensuremath{\leftarrow}}}1
    {∞}{{\ensuremath{\infty}}}1
    {√}{{\ensuremath{\sqrt{\ }}}}1
    {×}{{\ensuremath{\times}}}1
    {÷}{{\ensuremath{\div}}}1
}

\lstnewenvironment{codepy}[1][]%
  {\lstset{style=code,language=Python,#1}}%
  {}

\newcommand{\inlinecode}[1]{\lstinline[style=code]!#1!}

\newcommand{\LF}[2]{\par\noindent\textbf{#1:}~#2\par}
\newcommand{\WHAT}[1]{\LF{WHAT}{#1}}
\newcommand{\WHY}[1]{\LF{WHY}{#1}}
\newcommand{\HOW}[1]{\LF{HOW}{#1}}
\newcommand{\ELI}[1]{\LF{ELI5}{#1}}
\newcommand{\SCOPE}[1]{\LF{SCOPE}{#1}}
\newcommand{\CONFUSIONS}[1]{\LF{COMMON CONFUSIONS}{#1}}
\newcommand{\APPLICATIONS}[1]{\LF{APPLICATIONS}{#1}}
\newcommand{\FORMULA}[1]{\LF{FORMULA}{#1}}
\newcommand{\CANONICAL}[1]{\LF{CANONICAL FORM}{#1}}
\newcommand{\PRECONDS}[1]{\LF{PRECONDITIONS}{#1}}
\newcommand{\DERIVATION}[1]{\LF{DERIVATION}{#1}}
\newcommand{\EQUIV}[1]{\LF{EQUIVALENT FORMS}{#1}}
\newcommand{\LIMITS}[1]{\LF{LIMIT CASES}{#1}}
\newcommand{\INPUTS}[1]{\LF{INPUTS}{#1}}
\newcommand{\OUTPUTS}[1]{\LF{OUTPUTS}{#1}}
\newcommand{\RESULT}[1]{\LF{RESULT}{#1}}
\newcommand{\INTUITION}[1]{\LF{INTUITION}{#1}}
\newcommand{\PITFALLS}[1]{\LF{PITFALLS}{#1}}
\newcommand{\MODEL}[1]{\LF{CANONICAL MATH MODEL}{#1}}
\newcommand{\ASSUMPTIONS}[1]{\LF{ASSUMPTIONS}{#1}}
\newcommand{\WHICHFORMULA}[1]{\LF{WHICH FORMULA \& WHY}{#1}}
\newcommand{\GOVERN}[1]{\LF{GOVERNING EQUATION(S)}{#1}}
\newcommand{\UNITCHECK}[1]{\LF{UNIT CHECK}{#1}}
\newcommand{\EDGECASES}[1]{\LF{EDGE CASES}{#1}}
\newcommand{\ALTERNATE}[1]{\LF{ALTERNATE APPROACH (sketch)}{#1}}
\newcommand{\PROBLEM}[1]{\LF{PROBLEM}{#1}}
\newcommand{\API}[1]{\LF{API}{#1}}
\newcommand{\COMPLEXITY}[1]{\LF{COMPLEXITY}{#1}}
\newcommand{\FAILMODES}[1]{\LF{FAILURE MODES}{#1}}
\newcommand{\STABILITY}[1]{\LF{NUMERICAL STABILITY}{#1}}
\newcommand{\VALIDATION}[1]{\LF{VALIDATION}{#1}}
\newcommand{\EXPLANATION}[1]{\LF{EXPLANATION}{#1}}
\newcommand{\SCENARIO}[1]{\LF{SCENARIO}{#1}}
\newcommand{\PIPELINE}[1]{\LF{PIPELINE STEPS}{#1}}
\newcommand{\METRICS}[1]{\LF{METRICS}{#1}}
\newcommand{\INTERPRET}[1]{\LF{INTERPRETATION}{#1}}
\newcommand{\NEXTSTEPS}[1]{\LF{LIMITATIONS \& NEXT STEPS}{#1}}

\usepackage{titlesec}
\titleformat{\section}{\Large\bfseries}{\thesection}{0.6em}{}
\titlespacing*{\section}{0pt}{*2}{*1}
\usepackage{etoolbox}
\pretocmd{\section}{\clearpage}{}{}

\newcommand{\FormulaPage}[2]{%
  \clearpage
  \section*{Formula #1 — #2}%
  \addcontentsline{toc}{section}{Formula #1 — #2}%
}
\newcommand{\ProblemPage}[2]{%
  \clearpage
  \subsection*{Problem #1: #2}%
  \addcontentsline{toc}{subsection}{Problem #1: #2}%
}
\newcommand{\CodeDemoPage}[1]{%
  \clearpage
  \subsection*{Coding Demo: #1}%
  \addcontentsline{toc}{subsection}{Coding Demo: #1}%
}
\newcommand{\DomainPage}[1]{%
  \clearpage
  \subsection*{#1 (End-to-End)}%
  \addcontentsline{toc}{subsection}{#1 (End-to-End)}%
}

\begin{document}
\title{Comprehensive Study Sheet — Cauchy-Schwarz and Triangle Inequalities}
\date{\today}
\maketitle
\tableofcontents
\clearpage

\section{Concept Overview}
\WHAT{
Let $(V,\langle\cdot,\cdot\rangle)$ be an inner product space over $\mathbb{F}$,
$\mathbb{F}\in\{\mathbb{R},\mathbb{C}\}$, with induced norm
$\lVert x\rVert=\sqrt{\langle x,x\rangle}$. The Cauchy--Schwarz inequality states
that $|\langle x,y\rangle|\le \lVert x\rVert\,\lVert y\rVert$ for all $x,y\in V$,
with equality iff $x$ and $y$ are linearly dependent. The triangle inequality
states that $\lVert x+y\rVert\le \lVert x\rVert+\lVert y\rVert$ for all $x,y\in V$.
}
\WHY{
They are foundational for geometry in vector spaces, ensure that norms behave
like lengths, justify angle and cosine, bound errors and correlations, and are
used in analysis, probability, optimization, and numerical linear algebra.
}
\HOW{
1. Assume inner product axioms: conjugate symmetry, linearity in first slot,
positive-definiteness. 2. Consider the quadratic $q(t)=\lVert x-ty\rVert^2\ge0$.
3. Nonnegativity of $q(t)$ forces discriminant $\le0$, yielding Cauchy--Schwarz.
4. Expand $\lVert x+y\rVert^2$ and apply Cauchy--Schwarz to get triangle
inequality. Interpret via projection and angle between vectors.
}
\ELI{
Cauchy--Schwarz says the shadow of one vector on another is never longer than
their lengths multiplied. Triangle inequality says going directly is never
longer than going in two legs. Together, they formalize length and angle.
}
\SCOPE{
Valid for all inner product spaces (finite or infinite dimensional) and for
$L^2$ spaces of square-integrable functions. Triangle inequality holds for any
normed space. Equality in Cauchy--Schwarz requires collinearity. In triangle
inequality, equality requires same direction (nonnegative multiple).
}
\CONFUSIONS{
Cauchy--Schwarz vs. Hölder: CS is the $p=q=2$ case. Triangle inequality vs.
reverse triangle inequality: the latter is $\lVert x-y\rVert\ge\big|\lVert x\rVert
-\lVert y\rVert\big|$. Inner product norm vs. arbitrary norm: not every norm
comes from an inner product.
}
\APPLICATIONS{
\begin{bullets}
\item Bounding correlations and variances in probability and statistics.
\item Proving convergence and stability in numerical methods and PDEs.
\item Deriving error bounds in optimization and machine learning.
\item Geometric interpretations: angles, projections, and orthogonality.
\end{bullets}
}
\textbf{ANALYTIC STRUCTURE.}
Inner products define a symmetric positive bilinear (or sesquilinear) form,
generating a convex, absolutely homogeneous norm. Cauchy--Schwarz encodes
positive semidefiniteness and induces cosine bounded in $[-1,1]$.

\textbf{CANONICAL LINKS.}
Cauchy--Schwarz implies triangle inequality for inner-product norms; triangle
inequality implies reverse triangle inequality; CS underlies covariance bounds
$|\mathrm{Cov}(X,Y)|\le \sigma_X\sigma_Y$; Minkowski for $L^2$ is triangle
inequality in function space.

\textbf{PROBLEM-TYPE RECOGNITION HEURISTICS.}
\begin{bullets}
\item Questions bounding dot products or integrals of products.
\item Norm sums or distances between sums of vectors or functions.
\item Correlation or cosine similarity constraints in $[-1,1]$.
\item Projections, least squares, and orthogonality conditions.
\end{bullets}

\textbf{SOLUTION STRATEGY BLUEPRINT.}
\begin{bullets}
\item Translate quantities into inner products and norms.
\item Invoke Cauchy--Schwarz to bound products; use triangle inequality for sums.
\item Expand squares to expose cross terms; estimate with CS.
\item Interpret results as lengths/angles; check equality cases.
\end{bullets}

\textbf{CONCEPTUAL INVARIANTS.}
Nonnegativity of squared norms; orthogonal decomposition; projection minimizes
distance; cosine of angle lies in $[-1,1]$.

\textbf{EDGE INTUITION.}
If one vector is zero, both inequalities are equalities. As vectors align,
the bounds tighten to equalities. As vectors become orthogonal, cross terms
vanish, and Pythagoras emerges.

\clearpage
\section{Glossary}
\glossx{Inner Product}{
A map $\langle\cdot,\cdot\rangle:V\times V\to\mathbb{F}$ that is linear in the
first argument, conjugate symmetric, and positive definite.
}{
Defines geometry: angles, lengths, orthogonality; enables projections and
least-squares.
}{
Check axioms, compute $\langle x,y\rangle$ by sum/integral. Induce norm
$\lVert x\rVert=\sqrt{\langle x,x\rangle}$.
}{
Like measuring how much one arrow points along another.
}{
Pitfall: over $\mathbb{C}$, linearity is in one slot and conjugate linear in
the other; keep conventions consistent.
}
\glossx{Cauchy--Schwarz Inequality}{
$|\langle x,y\rangle|\le \lVert x\rVert\,\lVert y\rVert$.
}{
Bounds inner products; ensures cosine is in $[-1,1]$; central in analysis.
}{
Consider $\lVert x-ty\rVert^2\ge0$ for all $t$, analyze discriminant.
}{
The shadow length can not exceed the product of lengths.
}{
Equality iff $x$ and $y$ are linearly dependent; common mistake: forgetting
absolute value or complex conjugation.
}
\glossx{Triangle Inequality}{
$\lVert x+y\rVert\le \lVert x\rVert+\lVert y\rVert$ for a norm.
}{
Fundamental property of distance; underlies metric spaces and convergence.
}{
Expand $\lVert x+y\rVert^2$ and bound cross term with Cauchy--Schwarz.
}{
Two-leg path is never shorter than direct path.
}{
Equality requires same direction for inner-product norms; otherwise strict.
}
\glossx{Projection}{
$\mathrm{proj}_y(x)=\frac{\langle x,y\rangle}{\langle y,y\rangle}y$ for $y\ne0$.
}{
Gives closest point to $x$ on span$\{y\}$; used in least squares.
}{
Minimizes $\lVert x-ty\rVert$ over $t\in\mathbb{F}$; derived via CS/orthogonality.
}{
Drop a perpendicular to the line of $y$.
}{
Pitfall: undefined for $y=0$; check denominators and orthogonality.
}

\clearpage
\section{Symbol Ledger}
\varmapStart
\var{V}{Inner product space over $\mathbb{F}\in\{\mathbb{R},\mathbb{C}\}$.}
\var{\langle x,y\rangle}{Inner product of $x,y\in V$.}
\var{\lVert x\rVert}{Norm induced by the inner product.}
\var{x,y,z}{Generic vectors in $V$.}
\var{t,s}{Scalars in $\mathbb{F}$, used in quadratics/projections.}
\var{\theta}{Angle defined via $\cos\theta=\frac{\langle x,y\rangle}{\lVert x\rVert\lVert y\rVert}$.}
\var{f,g}{Functions in $L^2(\Omega,\mu)$.}
\var{\Omega}{Measurable space with measure $\mu$.}
\var{\mathbb{E}}{Expectation with respect to a probability measure.}
\var{\mathrm{Cov}}{Covariance bilinear form $\mathbb{E}[(X-\mu_X)(Y-\mu_Y)]$.}
\var{\sigma_X}{Standard deviation of random variable $X$.}
\var{n}{Dimension in $\mathbb{R}^n$ when applicable.}
\var{\rho}{Pearson correlation coefficient.}
\var{P}{Projection operator onto a subspace.}
\varmapEnd

\section{Formula Canon — One Formula Per Page}

\FormulaPage{1}{Cauchy--Schwarz Inequality in Inner Product Spaces}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
For all $x,y\in V$, $|\langle x,y\rangle|\le \lVert x\rVert\,\lVert y\rVert$,
with equality iff $x$ and $y$ are linearly dependent.

\WHAT{
Bounds the magnitude of the inner product by the product of norms; quantifies
alignment and ensures cosine similarity is well-defined and bounded.
}
\WHY{
It is the backbone of many estimates, guarantees the norm from an inner product
satisfies triangle inequality, and controls errors in projections and sums.
}
\FORMULA{
\[
\forall x,y\in V:\quad |\langle x,y\rangle|\le \lVert x\rVert\,\lVert y\rVert.
\]
Equality holds iff $\exists \lambda\in\mathbb{F}$ with $x=\lambda y$ or $y=0$.
}
\CANONICAL{
$V$ any inner product space over $\mathbb{F}$; inner product linear in first
argument, conjugate symmetric, positive definite. Norm is
$\lVert x\rVert=\sqrt{\langle x,x\rangle}$.
}
\PRECONDS{
\begin{bullets}
\item Inner product axioms hold.
\item If $y=0$ or $x=0$, the inequality is trivial and equality holds.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
For any $x,y\in V$, the quadratic $q(t)=\lVert x-ty\rVert^2$ in $t\in\mathbb{R}$
(for $\mathbb{F}=\mathbb{R}$) or $t\in\mathbb{C}$ (restrict to real $t$ via
phase) is nonnegative for all $t$.
\end{lemma}
\begin{proof}
By definition of norm, $\lVert x-ty\rVert^2=\langle x-ty,x-ty\rangle\ge0$ for
all $t$. This is immediate from positive definiteness. \qedhere
\end{proof}
\DERIVATION{
\begin{align*}
\text{Step 1: }&q(t)=\lVert x-ty\rVert^2=\langle x-ty,x-ty\rangle\ge0.\\
\text{Step 2: }&q(t)=\langle x,x\rangle-2\Re\big(t\langle y,x\rangle\big)
+|t|^2\langle y,y\rangle.\\
\text{Step 3: }&\text{For $\mathbb{R}$, }q(t)=\lVert x\rVert^2-2t\langle x,y\rangle
+t^2\lVert y\rVert^2.\\
\text{Step 4: }&\text{Minimize quadratic: }t^\ast=\frac{\langle x,y\rangle}
{\lVert y\rVert^2}\ \ (\lVert y\rVert\ne0).\\
\text{Step 5: }&0\le q(t^\ast)=\lVert x\rVert^2-
\frac{|\langle x,y\rangle|^2}{\lVert y\rVert^2}.\\
\text{Step 6: }&|\langle x,y\rangle|^2\le \lVert x\rVert^2\lVert y\rVert^2
\implies |\langle x,y\rangle|\le \lVert x\rVert\lVert y\rVert.\\
\text{Step 7: }&\text{Equality iff }q(t^\ast)=0\iff x=t^\ast y,
\text{ i.e., linear dependence.}
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Express the target bound as an inner product.
\item Form $\lVert x-ty\rVert^2$ and choose optimal $t$ to minimize it.
\item Conclude bound and check equality via collinearity.
\end{bullets}
\EQUIV{
\begin{bullets}
\item $|\langle x,y\rangle|^2\le \langle x,x\rangle\langle y,y\rangle$.
\item In $\mathbb{R}^n$: $|(x\cdot y)|\le \lVert x\rVert_2\lVert y\rVert_2$.
\item In $L^2$: $\left|\int fg\,d\mu\right|\le
\sqrt{\int f^2}\sqrt{\int g^2}$.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item If $x=0$ or $y=0$, both sides are $0$; equality holds.
\item When $x\perp y$, the bound is $0\le \lVert x\rVert\lVert y\rVert$
with $|\langle x,y\rangle|=0$.
\end{bullets}
}
\INPUTS{$x,y\in V$.}
\DERIVATION{
\begin{align*}
\text{Compute: }&t^\ast=\frac{\langle x,y\rangle}{\lVert y\rVert^2},\ 
q(t^\ast)=\lVert x\rVert^2-\frac{|\langle x,y\rangle|^2}{\lVert y\rVert^2}.\\
\text{Conclude: }&|\langle x,y\rangle|\le \lVert x\rVert\lVert y\rVert.
\end{align*}
}
\RESULT{
$|\langle x,y\rangle|\le \lVert x\rVert\lVert y\rVert$, equality iff $x=\lambda y$
for some $\lambda$ or one vector is zero.
}
\UNITCHECK{
Both sides have units of length$^2$ after squaring; invariant under scaling
$x\mapsto \alpha x$ and $y\mapsto \beta y$.
}
\PITFALLS{
\begin{bullets}
\item Over $\mathbb{C}$, take modulus and real part appropriately.
\item Forgetting equality condition requires linear dependence, not just
proportional norms.
\end{bullets}
}
\INTUITION{
Project $x$ onto the line of $y$; the projection length is at most the product
of lengths times cosine; cosine is bounded by $1$.
}
\CANONICAL{
\begin{bullets}
\item Universal inequality for positive semidefinite forms.
\item Equivalent to positivity of Gram matrices.
\end{bullets}
}

\FormulaPage{2}{Triangle Inequality from Cauchy--Schwarz}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
For any $x,y\in V$, $\lVert x+y\rVert\le \lVert x\rVert+\lVert y\rVert$.

\WHAT{
Establishes subadditivity of norm; ensures metric structure via
$d(x,y)=\lVert x-y\rVert$.
}
\WHY{
Defines distance and convergence; cornerstone for analysis in normed spaces,
error accumulation, and stability bounds.
}
\FORMULA{
\[
\forall x,y\in V:\quad \lVert x+y\rVert\le \lVert x\rVert+\lVert y\rVert.
\]
}
\CANONICAL{
In inner product spaces, expand $\lVert x+y\rVert^2$ and apply CS to the cross
term $\langle x,y\rangle$.
}
\PRECONDS{
\begin{bullets}
\item Valid in any normed space; here we prove for inner-product norms.
\item Cauchy--Schwarz available.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
For $x,y\in V$, $\lVert x+y\rVert^2=\lVert x\rVert^2+\lVert y\rVert^2
+2\Re\langle x,y\rangle$.
\end{lemma}
\begin{proof}
By bilinearity and conjugate symmetry,
$\langle x+y,x+y\rangle=\langle x,x\rangle+\langle y,y\rangle+\langle x,y\rangle
+\langle y,x\rangle=\lVert x\rVert^2+\lVert y\rVert^2+2\Re\langle x,y\rangle$.
\qedhere
\end{proof}
\DERIVATION{
\begin{align*}
\text{Step 1: }&\lVert x+y\rVert^2=\lVert x\rVert^2+\lVert y\rVert^2
+2\Re\langle x,y\rangle.\\
\text{Step 2: }&\Re\langle x,y\rangle\le |\langle x,y\rangle|\le
\lVert x\rVert\lVert y\rVert\ \ (\text{by CS}).\\
\text{Step 3: }&\lVert x+y\rVert^2\le \lVert x\rVert^2+\lVert y\rVert^2
+2\lVert x\rVert\lVert y\rVert=(\lVert x\rVert+\lVert y\rVert)^2.\\
\text{Step 4: }&\lVert x+y\rVert\le \lVert x\rVert+\lVert y\rVert.
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Expand $\lVert \sum_i x_i\rVert^2$ to expose pairwise inner products.
\item Bound cross terms via CS and additivity of norms.
\item Take square roots carefully, preserving inequality direction.
\end{bullets}
\EQUIV{
\begin{bullets}
\item Reverse triangle: $\lVert x-y\rVert\ge \big|\lVert x\rVert-\lVert y\rVert\big|$.
\item Minkowski in $L^2$: $\lVert f+g\rVert_2\le \lVert f\rVert_2+\lVert g\rVert_2$.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item Equality iff $x$ and $y$ are nonnegative multiples of each other.
\item If $x\perp y$, then $\lVert x+y\rVert^2=\lVert x\rVert^2+\lVert y\rVert^2$
(Pythagoras).
\end{bullets}
}
\INPUTS{$x,y\in V$.}
\DERIVATION{
\begin{align*}
\lVert x+y\rVert^2&=\lVert x\rVert^2+\lVert y\rVert^2+2\Re\langle x,y\rangle\\
&\le \lVert x\rVert^2+\lVert y\rVert^2+2\lVert x\rVert\lVert y\rVert\\
&=(\lVert x\rVert+\lVert y\rVert)^2.
\end{align*}
}
\RESULT{
$\lVert x+y\rVert\le \lVert x\rVert+\lVert y\rVert$; equality characterization
as above.
}
\UNITCHECK{
All terms are lengths squared before taking square roots, ensuring consistency.
}
\PITFALLS{
\begin{bullets}
\item Dropping the real part over $\mathbb{C}$ can lead to sign mistakes.
\item Taking square roots: maintain nonnegativity and inequality direction.
\end{bullets}
}
\INTUITION{
The cross term cannot exceed the product of lengths; thus the diagonal of a
triangle is shorter than or equal to the sum of sides.
}
\CANONICAL{
\begin{bullets}
\item Subadditivity of norms from CS.
\item Leads to metric $d(x,y)=\lVert x-y\rVert$.
\end{bullets}
}

\FormulaPage{3}{Angle and Cosine via Cauchy--Schwarz}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
For nonzero $x,y\in V$, define
$\cos\theta=\dfrac{\Re\langle x,y\rangle}{\lVert x\rVert\lVert y\rVert}$ in
$\mathbb{C}$ or $\cos\theta=\dfrac{\langle x,y\rangle}{\lVert x\rVert\lVert
y\rVert}$ in $\mathbb{R}$. Then $\theta\in[0,\pi]$ is well-defined.

\WHAT{
Establishes boundedness of cosine similarity and defines the angle between
vectors in inner product spaces.
}
\WHY{
Angles enable geometric reasoning, projections, and interpretability of
similarity measures across data and functions.
}
\FORMULA{
\[
-1\le \frac{\Re\langle x,y\rangle}{\lVert x\rVert\lVert y\rVert}\le 1.
\]
}
\CANONICAL{
Use Cauchy--Schwarz: $|\langle x,y\rangle|\le \lVert x\rVert\lVert y\rVert$ and
$|\Re z|\le |z|$ for $z\in\mathbb{C}$.
}
\PRECONDS{
\begin{bullets}
\item $x\ne0$, $y\ne0$ to avoid division by zero.
\item Inner product axioms.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
For any $z\in\mathbb{C}$, $|\Re z|\le |z|$.
\end{lemma}
\begin{proof}
Write $z=a+ib$. Then $|z|=\sqrt{a^2+b^2}\ge |a|=|\Re z|$. \qedhere
\end{proof}
\DERIVATION{
\begin{align*}
\text{Step 1: }&|\langle x,y\rangle|\le \lVert x\rVert\lVert y\rVert\quad
\text{(CS)}.\\
\text{Step 2: }&|\Re\langle x,y\rangle|\le |\langle x,y\rangle|\le
\lVert x\rVert\lVert y\rVert.\\
\text{Step 3: }&-1\le \frac{\Re\langle x,y\rangle}{\lVert x\rVert\lVert y\rVert}
\le 1.\\
\text{Step 4: }&\text{Define }\theta=\arccos\Big(\frac{\Re\langle x,y\rangle}
{\lVert x\rVert\lVert y\rVert}\Big)\in[0,\pi].
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Compute inner product and norms.
\item Form cosine similarity by normalization.
\item Interpret $\theta$; check orthogonality when numerator is zero.
\end{bullets}
\EQUIV{
\begin{bullets}
\item In $\mathbb{R}^n$, $\cos\theta=\dfrac{x\cdot y}{\lVert x\rVert\lVert y\rVert}$.
\item Orthogonality iff $\theta=\frac{\pi}{2}$ iff $\langle x,y\rangle=0$.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item If $x$ or $y$ is zero, angle undefined; treat separately.
\item If $x$ proportional to $y$, $\theta=0$ or $\pi$ depending on sign/phase.
\end{bullets}
}
\INPUTS{$x,y\in V$, $x\ne0$, $y\ne0$.}
\DERIVATION{
\begin{align*}
c&=\frac{\Re\langle x,y\rangle}{\lVert x\rVert\lVert y\rVert},\quad |c|\le 1,\\
\theta&=\arccos(c).
\end{align*}
}
\RESULT{
A well-defined angle $\theta\in[0,\pi]$; cosine similarity lies in $[-1,1]$.
}
\UNITCHECK{
Cosine is dimensionless; normalization by norms cancels units.
}
\PITFALLS{
\begin{bullets}
\item Over $\mathbb{C}$, use real part to keep angle real.
\item Numerical underflow/overflow when norms are very small/large.
\end{bullets}
}
\INTUITION{
Normalize vectors to the unit sphere; their inner product equals the cosine of
the angle between them.
}
\CANONICAL{
\begin{bullets}
\item Normalized inner product encodes geometry on the unit sphere.
\item Boundedness is exactly Cauchy--Schwarz.
\end{bullets}
}

\FormulaPage{4}{Minkowski (Triangle) Inequality in $L^2$}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
For $f,g\in L^2(\Omega,\mu)$,
$\lVert f+g\rVert_2\le \lVert f\rVert_2+\lVert g\rVert_2$, where
$\lVert f\rVert_2=\big(\int |f|^2\,d\mu\big)^{1/2}$.

\WHAT{
Triangle inequality specialized to $L^2$ function space; distance between
functions respects addition.
}
\WHY{
Fundamental to Fourier analysis, PDEs, and probability; ensures $L^2$ is a
normed (indeed Hilbert) space.
}
\FORMULA{
\[
\left(\int |f+g|^2\,d\mu\right)^{1/2}\le
\left(\int |f|^2\,d\mu\right)^{1/2}+
\left(\int |g|^2\,d\mu\right)^{1/2}.
\]
}
\CANONICAL{
Use inner product $\langle f,g\rangle=\int f\overline{g}\,d\mu$ and apply the
finite-dimensional derivation with integrals replacing sums.
}
\PRECONDS{
\begin{bullets}
\item $f,g\in L^2(\Omega,\mu)$ so integrals of squares are finite.
\item $\mu$ is a measure; Fubini/Tonelli as needed for rearrangements.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
(Cauchy--Schwarz in $L^2$) For $f,g\in L^2$, $|\int f\overline{g}\,d\mu|
\le \lVert f\rVert_2\lVert g\rVert_2$.
\end{lemma}
\begin{proof}
Apply Formula 1 with $V=L^2(\Omega,\mu)$ and inner product
$\langle f,g\rangle=\int f\overline{g}\,d\mu$. \qedhere
\end{proof}
\DERIVATION{
\begin{align*}
\lVert f+g\rVert_2^2&=\int |f+g|^2\,d\mu\\
&=\int (|f|^2+|g|^2+2\Re(f\overline{g}))\,d\mu\\
&=\lVert f\rVert_2^2+\lVert g\rVert_2^2+2\Re\int f\overline{g}\,d\mu\\
&\le \lVert f\rVert_2^2+\lVert g\rVert_2^2+2\lVert f\rVert_2\lVert g\rVert_2\\
&=(\lVert f\rVert_2+\lVert g\rVert_2)^2.\\
\Rightarrow\ \lVert f+g\rVert_2&\le \lVert f\rVert_2+\lVert g\rVert_2.
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Express integrals via $L^2$ inner products.
\item Bound mixed term by Cauchy--Schwarz.
\item Take square roots to finish.
\end{bullets}
\EQUIV{
\begin{bullets}
\item Reverse inequality: $\lVert f-g\rVert_2\ge \big|\lVert f\rVert_2-\lVert g\rVert_2\big|$.
\item Orthogonality: if $\int f\overline{g}=0$, then $\lVert f+g\rVert_2^2=
\lVert f\rVert_2^2+\lVert g\rVert_2^2$.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item If one function is zero, equality holds.
\item Equality requires nonnegative proportionality a.e.
\end{bullets}
}
\INPUTS{$f,g\in L^2(\Omega,\mu)$.}
\DERIVATION{
\begin{align*}
\int |f+g|^2&=\int |f|^2+\int |g|^2+2\Re\int f\overline{g}\\
&\le (\lVert f\rVert_2+\lVert g\rVert_2)^2.
\end{align*}
}
\RESULT{
Minkowski inequality in $L^2$: $\lVert f+g\rVert_2\le \lVert f\rVert_2+
\lVert g\rVert_2$ with equality conditions as above.
}
\UNITCHECK{
Each $\lVert \cdot\rVert_2$ has units of the square root of the integral of
squared units, consistent across terms.
}
\PITFALLS{
\begin{bullets}
\item Ensure $f,g\in L^2$; otherwise integrals may diverge.
\item Complex case requires real part for mixed term.
\end{bullets}
}
\INTUITION{
Pointwise, $|f+g|\le |f|+|g|$; integrating and squaring coheres with the
geometry of $L^2$.
}
\CANONICAL{
\begin{bullets}
\item Triangle inequality in Hilbert spaces.
\item Derived via CS plus polarization of $\lVert \cdot\rVert_2^2$.
\end{bullets}
}

\section{10 Exhaustive Problems and Solutions}

\ProblemPage{1}{Bounding a Dot Product and Equality Case}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
In $\mathbb{R}^3$, bound $x\cdot y$ given $\lVert x\rVert_2=3$, $\lVert y\rVert_2=4$
and determine when equality holds.

\PROBLEM{
Find the maximum and minimum possible values of $x\cdot y$ under the constraints
$\lVert x\rVert_2=3$ and $\lVert y\rVert_2=4$. Provide an explicit pair $(x,y)$
achieving each extremum.
}
\MODEL{
\[
V=\mathbb{R}^3,\ \langle x,y\rangle=x^\top y,\ \lVert x\rVert_2=\sqrt{x^\top x}.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Standard Euclidean inner product.
\item Norm constraints are exact.
\end{bullets}
}
\varmapStart
\var{x,y}{Vectors in $\mathbb{R}^3$.}
\var{r,s}{Given norms $r=3$, $s=4$.}
\var{\theta}{Angle between $x$ and $y$.}
\varmapEnd
\WHICHFORMULA{
Cauchy--Schwarz: $|x\cdot y|\le \lVert x\rVert_2\lVert y\rVert_2$ and
$x\cdot y=\lVert x\rVert_2\lVert y\rVert_2\cos\theta$.
}
\GOVERN{
\[
x\cdot y=rs\cos\theta,\quad -1\le \cos\theta\le 1.
\]
}
\INPUTS{$r=3$, $s=4$.}
\DERIVATION{
\begin{align*}
\text{Step 1: }&|x\cdot y|\le rs=12.\\
\text{Step 2: }&x\cdot y\in[-12,12].\\
\text{Step 3: }&\text{Max at }\theta=0:\ x=3u,\ y=4u\Rightarrow x\cdot y=12.\\
\text{Step 4: }&\text{Min at }\theta=\pi:\ x=3u,\ y=-4u\Rightarrow x\cdot y=-12,
\end{align*}
with $u$ any unit vector, e.g., $u=(1,0,0)$.
}
\RESULT{
$x\cdot y\in[-12,12]$. Achieved by parallel or antiparallel vectors of the
given lengths.
}
\UNITCHECK{
All terms are scalar products with units length$^2$; consistent with $rs$.
}
\EDGECASES{
\begin{bullets}
\item If $r=0$ or $s=0$, then $x\cdot y=0$ uniquely.
\item If $\theta=\frac{\pi}{2}$, $x\cdot y=0$.
\end{bullets}
}
\ALTERNATE{
Use Lagrange multipliers to maximize/minimize $x^\top y$ under norm constraints;
solution yields collinearity.
}
\VALIDATION{
\begin{bullets}
\item Take $x=(3,0,0)$, $y=(4,0,0)$, compute $x\cdot y=12$.
\item Take $y=(-4,0,0)$, compute $x\cdot y=-12$.
\end{bullets}
}
\INTUITION{
Inner product equals product of lengths times cosine; extremes at aligned or
opposite directions.
}
\CANONICAL{
\begin{bullets}
\item $|x\cdot y|\le \lVert x\rVert_2\lVert y\rVert_2$.
\item Equality iff linear dependence.
\end{bullets}
}

\ProblemPage{2}{Proving Triangle Inequality and Equality Condition}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Show $\lVert x+y\rVert_2\le \lVert x\rVert_2+\lVert y\rVert_2$ in $\mathbb{R}^n$
and find when equality holds.

\PROBLEM{
Prove the triangle inequality using Cauchy--Schwarz and characterize equality.
Provide a numeric example with $x=(1,2)$, $y=(3,-1)$.
}
\MODEL{
\[
\lVert x+y\rVert_2^2=\lVert x\rVert_2^2+\lVert y\rVert_2^2+2x^\top y.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Standard Euclidean inner product.
\end{bullets}
}
\varmapStart
\var{x,y}{Vectors in $\mathbb{R}^n$.}
\var{n}{Dimension.}
\varmapEnd
\WHICHFORMULA{
Triangle inequality from Formula 2; cross term bounded by CS (Formula 1).
}
\GOVERN{
\[
x^\top y\le \lVert x\rVert_2\lVert y\rVert_2.
\]
}
\INPUTS{$x=(1,2)$, $y=(3,-1)$ in $\mathbb{R}^2$.}
\DERIVATION{
\begin{align*}
\lVert x+y\rVert_2^2&=\lVert x\rVert_2^2+\lVert y\rVert_2^2+2x^\top y\\
&\le \lVert x\rVert_2^2+\lVert y\rVert_2^2+2\lVert x\rVert_2\lVert y\rVert_2\\
&=(\lVert x\rVert_2+\lVert y\rVert_2)^2.\\
\text{Numeric: }&x+y=(4,1),\ \lVert x+y\rVert_2=\sqrt{17}.\\
&\lVert x\rVert_2=\sqrt{5},\ \lVert y\rVert_2=\sqrt{10}.\\
&\sqrt{17}\le \sqrt{5}+\sqrt{10}\ \text{(true)}.
\end{align*}
}
\RESULT{
Triangle inequality holds; equality iff $x=\lambda y$ with $\lambda\ge0$.
}
\UNITCHECK{
Both sides are lengths; taking squares shows consistent units.
}
\EDGECASES{
\begin{bullets}
\item If $x=-y$, then $\lVert x+y\rVert_2=0$ and inequality is strict unless
both zero.
\item If $x,y$ are positive multiples, equality holds.
\end{bullets}
}
\ALTERNATE{
Use convexity of norm: $\lVert x+y\rVert=\lVert 2\cdot \tfrac{x+y}{2}\rVert
\le 2\left\lVert \tfrac{x+y}{2}\right\rVert\le \lVert x\rVert+\lVert y\rVert$
via triangle inequality itself in a bootstrapped proof from CS.
}
\VALIDATION{
\begin{bullets}
\item Compute $x^\top y=1\cdot3+2\cdot(-1)=1$.
\item Check $\lVert x+y\rVert_2^2=17\le 5+10+2\sqrt{50}=5+10+14.142\ldots$.
\end{bullets}
}
\INTUITION{
Cross term can only help up to product of lengths; the sum length bounded by
sum of lengths.
}
\CANONICAL{
\begin{bullets}
\item $\lVert x+y\rVert\le \lVert x\rVert+\lVert y\rVert$.
\item Equality iff aligned in same direction.
\end{bullets}
}

\ProblemPage{3}{Reverse Triangle Inequality}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Prove $\big|\lVert x\rVert-\lVert y\rVert\big|\le \lVert x-y\rVert$ in any
inner product space and give a numeric example.

\PROBLEM{
Provide a direct proof from the triangle inequality, and verify with
$x=(2,1)$, $y=(1,-1)$.
}
\MODEL{
\[
\lVert x\rVert=\lVert (x-y)+y\rVert\le \lVert x-y\rVert+\lVert y\rVert.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Triangle inequality holds (Formula 2).
\end{bullets}
}
\varmapStart
\var{x,y}{Vectors in $V$.}
\varmapEnd
\WHICHFORMULA{
Triangle inequality applied twice, to $x=(x-y)+y$ and $y=(y-x)+x$.
}
\GOVERN{
\[
\lVert x\rVert\le \lVert x-y\rVert+\lVert y\rVert,\quad
\lVert y\rVert\le \lVert x-y\rVert+\lVert x\rVert.
\]
}
\INPUTS{$x=(2,1)$, $y=(1,-1)$.}
\DERIVATION{
\begin{align*}
\text{Step 1: }&\lVert x\rVert-\lVert y\rVert\le \lVert x-y\rVert.\\
\text{Step 2: }&\lVert y\rVert-\lVert x\rVert\le \lVert x-y\rVert.\\
\text{Step 3: }&\big|\lVert x\rVert-\lVert y\rVert\big|\le \lVert x-y\rVert.\\
\text{Numeric: }&\lVert x\rVert=\sqrt{5},\ \lVert y\rVert=\sqrt{2}.\\
&\lVert x-y\rVert=\lVert(1,2)\rVert=\sqrt{5}.\\
&|\sqrt{5}-\sqrt{2}|\le \sqrt{5}\ \text{(true)}.
\end{align*}
}
\RESULT{
Reverse triangle inequality holds with equality in degenerate alignments.
}
\UNITCHECK{
All quantities are lengths; units consistent.
}
\EDGECASES{
\begin{bullets}
\item If $x=\lambda y$ with $\lambda\ge0$, then $\lVert x-y\rVert=
\big|\lVert x\rVert-\lVert y\rVert\big|$.
\end{bullets}
}
\ALTERNATE{
Square both sides and expand with CS to obtain the same inequality.
}
\VALIDATION{
\begin{bullets}
\item Check both directions by symmetry: swap $x,y$.
\end{bullets}
}
\INTUITION{
The distance between lengths cannot exceed the length of the difference vector.
}
\CANONICAL{
\begin{bullets}
\item Immediate corollary of triangle inequality.
\end{bullets}
}

\ProblemPage{4}{Covariance Bound via Cauchy--Schwarz}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
For random variables with finite variance, $|\mathrm{Cov}(X,Y)|\le \sigma_X\sigma_Y$.

\PROBLEM{
Let $X,Y$ be square-integrable real random variables. Prove the covariance bound
and identify equality conditions. Provide a numeric check on a simple two-point
distribution.
}
\MODEL{
\[
\langle f,g\rangle=\mathbb{E}[fg],\ f=X-\mathbb{E}X,\ g=Y-\mathbb{E}Y.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $\mathbb{E}[X^2],\mathbb{E}[Y^2]<\infty$.
\end{bullets}
}
\varmapStart
\var{X,Y}{Square-integrable random variables.}
\var{\sigma_X,\sigma_Y}{Standard deviations.}
\var{\mathrm{Cov}(X,Y)}{Expectation $\mathbb{E}[(X-\mu_X)(Y-\mu_Y)]$.}
\varmapEnd
\WHICHFORMULA{
Cauchy--Schwarz in $L^2(\Omega,\mathbb{P})$ (Formula 1).
}
\GOVERN{
\[
|\mathbb{E}[(X-\mu_X)(Y-\mu_Y)]|\le
\sqrt{\mathbb{E}[(X-\mu_X)^2]}\sqrt{\mathbb{E}[(Y-\mu_Y)^2]}.
\]
}
\INPUTS{Two-point distribution: $\mathbb{P}(X=0,Y=0)=\mathbb{P}(X=1,Y=1)=\tfrac12$.}
\DERIVATION{
\begin{align*}
\mathrm{Cov}(X,Y)&=\mathbb{E}[(X-\mu_X)(Y-\mu_Y)]=\langle f,g\rangle.\\
|\mathrm{Cov}(X,Y)|&\le \lVert f\rVert_2\lVert g\rVert_2=\sigma_X\sigma_Y.\\
\text{Numeric: }&\mu_X=\mu_Y=\tfrac12,\ \sigma_X=\sigma_Y=\tfrac12.\\
&\mathrm{Cov}(X,Y)=\mathbb{E}[(X-\tfrac12)(Y-\tfrac12)]=\tfrac14.\\
&|\mathrm{Cov}|\le \sigma_X\sigma_Y=\tfrac14\ \text{(equality)}.
\end{align*}
}
\RESULT{
$|\mathrm{Cov}(X,Y)|\le \sigma_X\sigma_Y$ with equality iff $Y-\mu_Y$ is a
scalar multiple of $X-\mu_X$ a.s.
}
\UNITCHECK{
Both sides have units of product of variables; standard deviations multiply.
}
\EDGECASES{
\begin{bullets}
\item If $\sigma_X=0$ or $\sigma_Y=0$, both sides are $0$.
\end{bullets}
}
\ALTERNATE{
Regress $Y$ on $X$: $\mathrm{Cov}(X,Y)=\beta\mathrm{Var}(X)$ with
$\beta=\frac{\mathrm{Cov}(X,Y)}{\mathrm{Var}(X)}$ and use
$\mathrm{Var}(Y)\ge \beta^2\mathrm{Var}(X)$.
}
\VALIDATION{
\begin{bullets}
\item Simulate finite sample; sample covariance magnitude bounded by product of
sample standard deviations.
\end{bullets}
}
\INTUITION{
Centered variables are vectors; their inner product is covariance, bounded by
the product of their lengths.
}
\CANONICAL{
\begin{bullets}
\item A direct instance of Cauchy--Schwarz in $L^2$.
\end{bullets}
}

\ProblemPage{5}{Projection Minimizes Distance}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
The point on the line spanned by $y\ne0$ closest to $x$ is
$\mathrm{proj}_y(x)=\frac{\langle x,y\rangle}{\langle y,y\rangle}y$.

\PROBLEM{
Show that for any $t\in\mathbb{F}$,
$\lVert x-ty\rVert\ge \lVert x-\mathrm{proj}_y(x)\rVert$ with equality only at
$t=\frac{\langle x,y\rangle}{\langle y,y\rangle}$.
}
\MODEL{
\[
\phi(t)=\lVert x-ty\rVert^2=\lVert x\rVert^2-2\Re(t\langle y,x\rangle)
+|t|^2\lVert y\rVert^2.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $y\ne0$; inner product axioms hold.
\end{bullets}
}
\varmapStart
\var{x,y}{Vectors in $V$ with $y\ne0$.}
\var{t}{Scalar parameter.}
\var{P}{Projection operator.}
\varmapEnd
\WHICHFORMULA{
Quadratic minimization using CS as in Formula 1; derivative or completing square.
}
\GOVERN{
\[
\phi(t)=\lVert y\rVert^2\left|t-\frac{\langle x,y\rangle}{\lVert y\rVert^2}
\right|^2+\lVert x\rVert^2-\frac{|\langle x,y\rangle|^2}{\lVert y\rVert^2}.
\]
}
\INPUTS{$x=(1,2)$, $y=(2,0)$ in $\mathbb{R}^2$.}
\DERIVATION{
\begin{align*}
t^\ast&=\frac{\langle x,y\rangle}{\lVert y\rVert^2}
=\frac{1\cdot2+2\cdot0}{4}=0.5.\\
\mathrm{proj}_y(x)&=0.5\cdot(2,0)=(1,0).\\
\lVert x-\mathrm{proj}_y(x)\rVert&=\lVert(0,2)\rVert=2.\\
\text{Any }t:&\ \lVert x-ty\rVert^2=\lVert(1-2t,2)\rVert^2=(1-2t)^2+4\\
&\ge \min_t((1-2t)^2)+4=0+4=4.
\end{align*}
}
\RESULT{
Projection minimizes distance; residual $x-\mathrm{proj}_y(x)$ is orthogonal
to $y$.
}
\UNITCHECK{
All terms are squared lengths; units consistent.
}
\EDGECASES{
\begin{bullets}
\item If $x$ is already in span$\{y\}$, the residual is zero.
\end{bullets}
}
\ALTERNATE{
Use orthogonal decomposition: write $x=\alpha y+z$ with $z\perp y$ via CS;
then $\lVert x-ty\rVert^2=\lVert (\alpha-t)y\rVert^2+\lVert z\rVert^2$.
}
\VALIDATION{
\begin{bullets}
\item Numeric example gives minimal value at $t=0.5$ as computed.
\end{bullets}
}
\INTUITION{
Drop a perpendicular from $x$ to the line through $y$; the foot is the closest
point.
}
\CANONICAL{
\begin{bullets}
\item Projection formula from CS-based minimization.
\end{bullets}
}

\ProblemPage{6}{Sum of Many Vectors: Generalized Triangle Inequality}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Show $\left\lVert \sum_{i=1}^m x_i\right\rVert\le \sum_{i=1}^m \lVert x_i\rVert$.

\PROBLEM{
Prove by induction using triangle inequality and apply to $x_1=(1,0)$,
$x_2=(0,2)$, $x_3=(-1,1)$.
}
\MODEL{
\[
\lVert u+v\rVert\le \lVert u\rVert+\lVert v\rVert.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Norm satisfies triangle inequality (Formula 2).
\end{bullets}
}
\varmapStart
\var{x_i}{Vectors in $V$.}
\var{m}{Number of terms.}
\varmapEnd
\WHICHFORMULA{
Triangle inequality applied iteratively.
}
\GOVERN{
\[
\left\lVert \sum_{i=1}^m x_i\right\rVert\le
\left\lVert \sum_{i=1}^{m-1} x_i\right\rVert+\lVert x_m\rVert\le
\sum_{i=1}^m \lVert x_i\rVert.
\]
}
\INPUTS{$x_1=(1,0)$, $x_2=(0,2)$, $x_3=(-1,1)$.}
\DERIVATION{
\begin{align*}
\sum_{i=1}^3 x_i&=(1,0)+(0,2)+(-1,1)=(0,3).\\
\left\lVert \sum x_i\right\rVert&=\lVert(0,3)\rVert=3.\\
\sum \lVert x_i\rVert&=1+2+\sqrt{2}=3+\sqrt{2}.\\
&3\le 3+\sqrt{2}\ \text{(true)}.
\end{align*}
}
\RESULT{
Generalized triangle inequality holds; numeric example verified.
}
\UNITCHECK{
Lengths on both sides; consistent units.
}
\EDGECASES{
\begin{bullets}
\item If vectors align positively, equality holds.
\item If many cancellations occur, left side can be much smaller.
\end{bullets}
}
\ALTERNATE{
Use convexity: $\lVert \sum x_i\rVert\le \sum \lVert x_i\rVert$ by repeated
application or Minkowski for sums.
}
\VALIDATION{
\begin{bullets}
\item Reorder summation; inequality invariant under permutation.
\end{bullets}
}
\INTUITION{
Walking multiple segments, total direct distance does not exceed the sum of
segment lengths.
}
\CANONICAL{
\begin{bullets}
\item Iterated triangle inequality.
\end{bullets}
}

\ProblemPage{7}{Narrative: Alice and Bob Combine Signals}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Alice has signal $a$, Bob has $b$ in $L^2[0,1]$. Their combined signal is
$s=a+b$. Show the energy of $s$ is at most the sum of energies plus cross
coupling bounded by CS, and deduce a triangle-type bound.

\PROBLEM{
Prove $\lVert s\rVert_2\le \lVert a\rVert_2+\lVert b\rVert_2$ and identify
when energy equals sum of energies. Give a concrete pair achieving equality.
}
\MODEL{
\[
\lVert s\rVert_2^2=\lVert a\rVert_2^2+\lVert b\rVert_2^2+2\Re\langle a,b\rangle.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $a,b\in L^2[0,1]$ real-valued.
\end{bullets}
}
\varmapStart
\var{a,b}{Signals in $L^2[0,1]$.}
\var{s}{Sum $a+b$.}
\varmapEnd
\WHICHFORMULA{
Formula 4 (Minkowski) with CS to bound cross term.
}
\GOVERN{
\[
\lVert s\rVert_2\le \lVert a\rVert_2+\lVert b\rVert_2.
\]
}
\INPUTS{$a(t)=2t$, $b(t)=t$ on $[0,1]$.}
\DERIVATION{
\begin{align*}
\lVert a\rVert_2^2&=\int_0^1 (2t)^2dt=\tfrac{4}{3}.\\
\lVert b\rVert_2^2&=\int_0^1 t^2dt=\tfrac{1}{3}.\\
\langle a,b\rangle&=\int_0^1 2t\cdot t\,dt=\tfrac{2}{3}.\\
\lVert s\rVert_2^2&=\tfrac{4}{3}+\tfrac{1}{3}+2\cdot\tfrac{2}{3}
=\tfrac{9}{3}=3.\\
\lVert s\rVert_2&=\sqrt{3},\ \lVert a\rVert_2+\lVert b\rVert_2=
\sqrt{\tfrac{4}{3}}+\sqrt{\tfrac{1}{3}}=\sqrt{3}.\\
\text{Equality: }&b=\tfrac12 a\ \text{(nonnegative multiple).}
\end{align*}
}
\RESULT{
Triangle inequality holds with equality when $a,b$ are nonnegative multiples
almost everywhere.
}
\UNITCHECK{
Energy units consistent across integrals and norms.
}
\EDGECASES{
\begin{bullets}
\item If $a\perp b$, then $\lVert s\rVert_2^2=\lVert a\rVert_2^2+\lVert b\rVert_2^2$.
\end{bullets}
}
\ALTERNATE{
Normalize $a,b$ and use angle interpretation to relate energies via cosine.
}
\VALIDATION{
\begin{bullets}
\item Compute numerically via discretization to confirm the integrals.
\end{bullets}
}
\INTUITION{
Aligned signals add constructively; misaligned ones add less efficiently.
}
\CANONICAL{
\begin{bullets}
\item Minkowski in $L^2$ with equality characterization.
\end{bullets}
}

\ProblemPage{8}{Narrative: Bob Bounds Error Accumulation}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Bob accumulates two independent error vectors $e_1,e_2\in\mathbb{R}^n$. Bound
the worst-case combined error magnitude.

\PROBLEM{
Show $\lVert e_1+e_2\rVert_2\le \lVert e_1\rVert_2+\lVert e_2\rVert_2$ and
explain when this is tight. Provide a concrete numeric sample for $n=3$.
}
\MODEL{
\[
\lVert e_1+e_2\rVert_2^2=\lVert e_1\rVert_2^2+\lVert e_2\rVert_2^2
+2e_1^\top e_2.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item No distributional assumptions; worst-case bounding.
\end{bullets}
}
\varmapStart
\var{e_1,e_2}{Error vectors in $\mathbb{R}^n$.}
\varmapEnd
\WHICHFORMULA{
Triangle inequality (Formula 2).
}
\GOVERN{
\[
\lVert e_1+e_2\rVert_2\le \lVert e_1\rVert_2+\lVert e_2\rVert_2.
\]
}
\INPUTS{$e_1=(1,2,2)$, $e_2=(2,1,2)$.}
\DERIVATION{
\begin{align*}
\lVert e_1\rVert_2&=\sqrt{1+4+4}=3.\\
\lVert e_2\rVert_2&=\sqrt{4+1+4}=3.\\
\lVert e_1+e_2\rVert_2&=\lVert(3,3,4)\rVert=\sqrt{9+9+16}=\sqrt{34}.\\
\lVert e_1\rVert_2+\lVert e_2\rVert_2&=6,\ \sqrt{34}\le 6.
\end{align*}
}
\RESULT{
Bound verified; tight when $e_1$ and $e_2$ are positively collinear.
}
\UNITCHECK{
All are lengths; units consistent.
}
\EDGECASES{
\begin{bullets}
\item If $e_2=-e_1$, then combined error is zero.
\end{bullets}
}
\ALTERNATE{
Use CS on $\lVert e_1+e_2\rVert_2^2$ to bound cross term.
}
\VALIDATION{
\begin{bullets}
\item Compute dot product $e_1^\top e_2=1\cdot2+2\cdot1+2\cdot2=8$ and verify
square inequality: $34\le 9+9+2\cdot3\cdot3=36$.
\end{bullets}
}
\INTUITION{
Worst-case alignment adds magnitudes; misalignment helps cancel errors.
}
\CANONICAL{
\begin{bullets}
\item Triangle inequality as worst-case bound for sums.
\end{bullets}
}

\ProblemPage{9}{Expectation Puzzle: Dice and CS}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Let $X_i$ be independent fair die outcomes centered by their mean. Bound
$|\mathbb{E}[S T]|$ where $S=\sum_{i=1}^n X_i'$ and $T=\sum_{i=1}^n a_i X_i'$,
with $X_i'=X_i-\mathbb{E}X$.

\PROBLEM{
Show $|\mathbb{E}[S T]|\le \sqrt{\mathbb{E}[S^2]}\sqrt{\mathbb{E}[T^2]}$ and
compute both sides explicitly in terms of $n$ and $a_i$.
}
\MODEL{
\[
\langle U,V\rangle=\mathbb{E}[UV],\ U=S,\ V=T\ \text{in }L^2.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $X_i$ i.i.d. uniform on $\{1,2,3,4,5,6\}$.
\end{bullets}
}
\varmapStart
\var{X_i'}{Centered die: $X_i-\mu$, $\mu=3.5$.}
\var{a_i}{Real coefficients.}
\var{S,T}{Centered sums.}
\var{n}{Number of dice.}
\varmapEnd
\WHICHFORMULA{
Cauchy--Schwarz in $L^2$ (Formula 1).
}
\GOVERN{
\[
|\mathbb{E}[S T]|\le \sqrt{\mathbb{E}[S^2]}\sqrt{\mathbb{E}[T^2]}.
\]
}
\INPUTS{$\mathbb{E}[X_i']=0$, $\mathrm{Var}(X_i)=\sigma^2=\tfrac{35}{12}$.}
\DERIVATION{
\begin{align*}
\mathbb{E}[S^2]&=\sum_{i=1}^n \mathbb{E}[(X_i')^2]=n\sigma^2.\\
\mathbb{E}[T^2]&=\sum_{i=1}^n a_i^2\mathbb{E}[(X_i')^2]=\sigma^2\sum a_i^2.\\
\mathbb{E}[S T]&=\sum_{i=1}^n a_i\mathbb{E}[(X_i')^2]=\sigma^2\sum a_i.\\
|\mathbb{E}[S T]|&=\sigma^2\left|\sum a_i\right|
\le \sqrt{n\sigma^2}\sqrt{\sigma^2\sum a_i^2}\\
&=\sigma^2\sqrt{n\sum a_i^2}.
\end{align*}
}
\RESULT{
$|\mathbb{E}[S T]|\le \sigma^2\sqrt{n\sum a_i^2}$ with equality iff all $a_i$
are equal (up to sign) so that $T$ is proportional to $S$.
}
\UNITCHECK{
Both sides have units of variance; consistent scaling by $\sigma^2$.
}
\EDGECASES{
\begin{bullets}
\item If $\sum a_i=0$, then $\mathbb{E}[S T]=0$.
\end{bullets}
}
\ALTERNATE{
View vectors $u=(1,\dots,1)$, $v=(a_1,\dots,a_n)$; then
$|\sum a_i|\le \sqrt{n}\sqrt{\sum a_i^2}$ is CS in $\mathbb{R}^n$.
}
\VALIDATION{
\begin{bullets}
\item For $a_i=1$, both sides equal $\sigma^2 n$, achieving equality.
\end{bullets}
}
\INTUITION{
Centered sums act like vectors; correlation bounded by product of lengths.
}
\CANONICAL{
\begin{bullets}
\item Discrete CS on coefficient vectors or $L^2$ on random variables.
\end{bullets}
}

\ProblemPage{10}{Proof: Gram Matrix Positivity and CS}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Show that the $2\times2$ Gram matrix $G=\begin{pmatrix}
\langle x,x\rangle & \langle x,y\rangle\\
\langle y,x\rangle & \langle y,y\rangle
\end{pmatrix}$ is positive semidefinite and deduce CS.

\PROBLEM{
Prove $\det G\ge 0$ and hence $|\langle x,y\rangle|^2\le \langle x,x\rangle
\langle y,y\rangle$.
}
\MODEL{
\[
\forall (a,b)\in\mathbb{F}^2:\ (a\overline{a},b\overline{b})G
\begin{pmatrix}a\\b\end{pmatrix}\ge 0.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Inner product axioms.
\end{bullets}
}
\varmapStart
\var{x,y}{Vectors in $V$.}
\var{G}{Gram matrix of $\{x,y\}$.}
\varmapEnd
\WHICHFORMULA{
Positivity of $\lVert ax+by\rVert^2$ for all $a,b$; determinant nonnegative
for Hermitian $2\times2$.
}
\GOVERN{
\[
\lVert ax+by\rVert^2=\begin{pmatrix}a\\b\end{pmatrix}^\ast
G\begin{pmatrix}a\\b\end{pmatrix}\ge0.
\]
}
\INPUTS{Choose $a=1$, $b=-t$ with $t\in\mathbb{R}$.}
\DERIVATION{
\begin{align*}
\det G&=\langle x,x\rangle\langle y,y\rangle-|\langle x,y\rangle|^2\ge 0.\\
&\Rightarrow |\langle x,y\rangle|^2\le \langle x,x\rangle\langle y,y\rangle.\\
\text{Alternate: }&\lVert x-ty\rVert^2\ge 0\ \forall t\in\mathbb{R}\\
&\Rightarrow \text{discriminant }\le 0\ \text{(Formula 1 derivation)}.
\end{align*}
}
\RESULT{
CS inequality holds; Gram matrix is positive semidefinite.
}
\UNITCHECK{
Each term has units of length$^4$ after squaring; consistent.
}
\EDGECASES{
\begin{bullets}
\item If $\det G=0$, then $x,y$ are linearly dependent.
\end{bullets}
}
\ALTERNATE{
Use spectral theorem: Hermitian $G$ has nonnegative eigenvalues iff positive
semidefinite; compute determinant as product of eigenvalues.
}
\VALIDATION{
\begin{bullets}
\item Numeric examples in $\mathbb{R}^2$ satisfy determinant $\ge0$.
\end{bullets}
}
\INTUITION{
Areas of parallelograms square to determinants; nonnegative area implies bound
on the cross term.
}
\CANONICAL{
\begin{bullets}
\item CS is equivalent to positivity of all $2\times2$ Gram determinants.
\end{bullets}
}

\section{Coding Demonstrations}

\CodeDemoPage{Numerical Verification of Cauchy--Schwarz in $\mathbb{R}^n$}
\PROBLEM{
Generate deterministic vectors and verify $|x^\top y|\le \lVert x\rVert
\lVert y\rVert$ and equality when vectors are proportional.
}
\API{
\begin{bullets}
\item \inlinecode{def read_input(s) -> tuple} — parse n and seed.
\item \inlinecode{def solve_case(n, seed) -> dict} — compute lhs, rhs, gap.
\item \inlinecode{def validate() -> None} — assertions for correctness.
\item \inlinecode{def main() -> None} — orchestrate, print summary.
\end{bullets}
}
\INPUTS{
$n$ (int), seed (int) for reproducible vectors.
}
\OUTPUTS{
Dictionary with keys: lhs, rhs, gap, eq\_case, cos.
}
\FORMULA{
\[
|x^\top y|\le \lVert x\rVert_2\lVert y\rVert_2,\quad
\cos\theta=\frac{x^\top y}{\lVert x\rVert_2\lVert y\rVert_2}\in[-1,1].
\]
}
\textbf{SOLUTION A — From Scratch (Mathematically Explicit Implementation)}
\begin{codepy}
import math, random

def read_input(s):
    parts = s.strip().split()
    n = int(parts[0]); seed = int(parts[1])
    return n, seed

def gen_vec(n, rng):
    return [rng.uniform(-1.0, 1.0) for _ in range(n)]

def dot(u, v):
    return sum(ui*vi for ui, vi in zip(u, v))

def norm(u):
    return math.sqrt(sum(ui*ui for ui in u))

def solve_case(n, seed):
    rng = random.Random(seed)
    x = gen_vec(n, rng)
    y = gen_vec(n, rng)
    lhs = abs(dot(x, y))
    rhs = norm(x)*norm(y)
    cos = 0.0 if rhs == 0 else dot(x, y)/rhs
    # equality case: y is scalar multiple of x
    lam = 2.5
    y_eq = [lam*xi for xi in x]
    lhs_eq = abs(dot(x, y_eq))
    rhs_eq = norm(x)*norm(y_eq)
    gap = rhs - lhs
    return {"lhs": lhs, "rhs": rhs, "gap": gap,
            "eq_case": abs(lhs_eq - rhs_eq), "cos": cos}

def validate():
    d = solve_case(5, 0)
    assert d["lhs"] <= d["rhs"] + 1e-12
    assert d["gap"] >= -1e-12
    assert abs(d["eq_case"]) < 1e-9
    assert -1.0 - 1e-12 <= d["cos"] <= 1.0 + 1e-12

def main():
    validate()
    n, seed = read_input("7 42")
    d = solve_case(n, seed)
    print("lhs", round(d["lhs"],6), "rhs", round(d["rhs"],6),
          "gap", round(d["gap"],6), "cos", round(d["cos"],6))

if __name__ == "__main__":
    main()
\end{codepy}
\textbf{SOLUTION B — Library-Based (Validated Computational Shortcut)}
\begin{codepy}
import numpy as np

def read_input(s):
    n, seed = map(int, s.strip().split())
    return n, seed

def solve_case(n, seed):
    rng = np.random.default_rng(seed)
    x = rng.uniform(-1.0, 1.0, size=n)
    y = rng.uniform(-1.0, 1.0, size=n)
    lhs = float(abs(np.dot(x, y)))
    rhs = float(np.linalg.norm(x)*np.linalg.norm(y))
    cos = 0.0 if rhs == 0 else float(np.dot(x, y)/rhs)
    lam = 2.5
    y_eq = lam*x
    lhs_eq = float(abs(np.dot(x, y_eq)))
    rhs_eq = float(np.linalg.norm(x)*np.linalg.norm(y_eq))
    gap = rhs - lhs
    return {"lhs": lhs, "rhs": rhs, "gap": gap,
            "eq_case": abs(lhs_eq - rhs_eq), "cos": cos}

def validate():
    d = solve_case(5, 0)
    assert d["lhs"] <= d["rhs"] + 1e-12
    assert d["gap"] >= -1e-12
    assert abs(d["eq_case"]) < 1e-9
    assert -1.0 - 1e-12 <= d["cos"] <= 1.0 + 1e-12

def main():
    validate()
    n, seed = read_input("7 42")
    d = solve_case(n, seed)
    print("lhs", round(d["lhs"],6), "rhs", round(d["rhs"],6),
          "gap", round(d["gap"],6), "cos", round(d["cos"],6))

if __name__ == "__main__":
    main()
\end{codepy}
\COMPLEXITY{
Time $\mathcal{O}(n)$, space $\mathcal{O}(1)$ beyond input storage for both.
}
\FAILMODES{
\begin{bullets}
\item Zero vectors lead to $0/0$ in cosine; guard with check.
\item Very large values may overflow in naive norm; use stable types.
\end{bullets}
}
\STABILITY{
\begin{bullets}
\item Summation error bounded; Kahan summation could reduce roundoff.
\item Normalization scales reduce overflow risk.
\end{bullets}
}
\VALIDATION{
\begin{bullets}
\item Equality test with proportional vectors.
\item Assert cosine within $[-1,1]$ to catch numerical drift.
\end{bullets}
}
\RESULT{
Both implementations produce lhs $\le$ rhs with zero gap in equality case; cosine
lies within bounds as guaranteed by CS.
}
\EXPLANATION{
The code computes inner products and norms to evaluate the CS inequality and
cosine similarity, matching Formula 1 and 3.
}
\EXTENSION{
Vectorize multiple trials and compute empirical distribution of cosines.
}

\CodeDemoPage{Triangle Inequality and Reverse Inequality in $\mathbb{R}^n$}
\PROBLEM{
Verify $\lVert x+y\rVert_2\le \lVert x\rVert_2+\lVert y\rVert_2$ and
$|\lVert x\rVert_2-\lVert y\rVert_2|\le \lVert x-y\rVert_2$ deterministically.
}
\API{
\begin{bullets}
\item \inlinecode{def read_input(s) -> tuple} — parse n and seed.
\item \inlinecode{def solve_case(n, seed) -> dict} — compute values.
\item \inlinecode{def validate() -> None} — run assertions.
\item \inlinecode{def main() -> None} — orchestrate.
\end{bullets}
}
\INPUTS{
$n$ (int), seed (int).
}
\OUTPUTS{
Dictionary with norms and boolean flags for both inequalities.
}
\FORMULA{
\[
\lVert x+y\rVert\le \lVert x\rVert+\lVert y\rVert,\quad
|\lVert x\rVert-\lVert y\rVert|\le \lVert x-y\rVert.
\]
}
\textbf{SOLUTION A — From Scratch (Mathematically Explicit Implementation)}
\begin{codepy}
import math, random

def read_input(s):
    n, seed = map(int, s.strip().split())
    return n, seed

def gen(n, rng):
    return [rng.uniform(-2.0, 2.0) for _ in range(n)]

def norm(u):
    return math.sqrt(sum(ui*ui for ui in u))

def add(u, v):
    return [ui+vi for ui, vi in zip(u, v)]

def sub(u, v):
    return [ui-vi for ui, vi in zip(u, v)]

def solve_case(n, seed):
    rng = random.Random(seed)
    x = gen(n, rng); y = gen(n, rng)
    nx, ny = norm(x), norm(y)
    nsum = norm(add(x, y)); ndiff = norm(sub(x, y))
    tri_ok = nsum <= nx + ny + 1e-12
    rev_ok = abs(nx - ny) <= ndiff + 1e-12
    return {"nx": nx, "ny": ny, "nsum": nsum, "ndiff": ndiff,
            "tri_ok": tri_ok, "rev_ok": rev_ok}

def validate():
    d = solve_case(8, 123)
    assert d["tri_ok"]
    assert d["rev_ok"]

def main():
    validate()
    n, seed = read_input("8 123")
    d = solve_case(n, seed)
    print("nsum", round(d["nsum"],6), "nx+ny", round(d["nx"]+d["ny"],6),
          "ndiff", round(d["ndiff"],6), "|nx-ny|", round(abs(d["nx"]-d["ny"]),6))

if __name__ == "__main__":
    main()
\end{codepy}
\textbf{SOLUTION B — Library-Based (Validated Computational Shortcut)}
\begin{codepy}
import numpy as np

def read_input(s):
    n, seed = map(int, s.strip().split())
    return n, seed

def solve_case(n, seed):
    rng = np.random.default_rng(seed)
    x = rng.uniform(-2.0, 2.0, size=n)
    y = rng.uniform(-2.0, 2.0, size=n)
    nx = float(np.linalg.norm(x))
    ny = float(np.linalg.norm(y))
    nsum = float(np.linalg.norm(x + y))
    ndiff = float(np.linalg.norm(x - y))
    tri_ok = nsum <= nx + ny + 1e-12
    rev_ok = abs(nx - ny) <= ndiff + 1e-12
    return {"nx": nx, "ny": ny, "nsum": nsum, "ndiff": ndiff,
            "tri_ok": tri_ok, "rev_ok": rev_ok}

def validate():
    d = solve_case(8, 123)
    assert d["tri_ok"]
    assert d["rev_ok"]

def main():
    validate()
    n, seed = read_input("8 123")
    d = solve_case(n, seed)
    print("nsum", round(d["nsum"],6), "nx+ny", round(d["nx"]+d["ny"],6),
          "ndiff", round(d["ndiff"],6), "|nx-ny|", round(abs(d["nx"]-d["ny"]),6))

if __name__ == "__main__":
    main()
\end{codepy}
\COMPLEXITY{
Time $\mathcal{O}(n)$, space $\mathcal{O}(1)$ beyond input arrays for both.
}
\FAILMODES{
\begin{bullets}
\item Degenerate zero vectors; handled naturally.
\end{bullets}
}
\STABILITY{
\begin{bullets}
\item Squared norm sums may slightly drift; tolerances used in asserts.
\end{bullets}
}
\VALIDATION{
\begin{bullets}
\item Deterministic seeds ensure reproducibility; assertions verify both
inequalities.
\end{bullets}
}
\RESULT{
Both inequalities numerically verified across random vectors.
}
\EXPLANATION{
Direct computation of norms and sums reflects Formula 2 and its reverse corollary.
}
\EXTENSION{
Test orthogonal pairs to observe Pythagorean equality.
}

\CodeDemoPage{Minkowski in $L^2$ via Discrete Approximation}
\PROBLEM{
Approximate functions on a grid and verify $\lVert f+g\rVert_2\le \lVert f\rVert_2
+\lVert g\rVert_2$ using Riemann sums.
}
\API{
\begin{bullets}
\item \inlinecode{def read_input(s) -> int} — parse grid size.
\item \inlinecode{def solve_case(n) -> dict} — compute discrete norms.
\item \inlinecode{def validate() -> None} — run asserts.
\item \inlinecode{def main() -> None} — orchestrate.
\end{bullets}
}
\INPUTS{
$n$ (int): number of grid points on $[0,1]$.
}
\OUTPUTS{
Discrete norms of $f$, $g$, $f+g$ and triangle inequality flag.
}
\FORMULA{
\[
\lVert f\rVert_{2,h}=\sqrt{h\sum_i f(t_i)^2},\quad
\lVert f+g\rVert_{2,h}\le \lVert f\rVert_{2,h}+\lVert g\rVert_{2,h}.
\]
}
\textbf{SOLUTION A — From Scratch (Mathematically Explicit Implementation)}
\begin{codepy}
import math

def read_input(s):
    return int(s.strip())

def f(t):
    return 2.0*t

def g(t):
    return 1.0 - t

def solve_case(n):
    h = 1.0/n
    ts = [(i+0.5)*h for i in range(n)]
    fsq = sum(f(t)**2 for t in ts)
    gsq = sum(g(t)**2 for t in ts)
    ssum = sum((f(t)+g(t))**2 for t in ts)
    nf = math.sqrt(h*fsq)
    ng = math.sqrt(h*gsq)
    nfg = math.sqrt(h*ssum)
    tri_ok = nfg <= nf + ng + 1e-12
    return {"nf": nf, "ng": ng, "nfg": nfg, "tri_ok": tri_ok}

def validate():
    d = solve_case(1000)
    assert d["tri_ok"]

def main():
    validate()
    d = solve_case(read_input("200"))
    print("nfg", round(d["nfg"],6), "nf+ng", round(d["nf"]+d["ng"],6))

if __name__ == "__main__":
    main()
\end{codepy}
\textbf{SOLUTION B — Library-Based (Validated Computational Shortcut)}
\begin{codepy}
import numpy as np

def read_input(s):
    return int(s.strip())

def f(t):
    return 2.0*t

def g(t):
    return 1.0 - t

def solve_case(n):
    h = 1.0/n
    ts = (np.arange(n) + 0.5)*h
    nf = float(np.sqrt(h*np.sum(f(ts)**2)))
    ng = float(np.sqrt(h*np.sum(g(ts)**2)))
    nfg = float(np.sqrt(h*np.sum((f(ts)+g(ts))**2)))
    tri_ok = nfg <= nf + ng + 1e-12
    return {"nf": nf, "ng": ng, "nfg": nfg, "tri_ok": tri_ok}

def validate():
    d = solve_case(1000)
    assert d["tri_ok"]

def main():
    validate()
    d = solve_case(read_input("200"))
    print("nfg", round(d["nfg"],6), "nf+ng", round(d["nf"]+d["ng"],6))

if __name__ == "__main__":
    main()
\end{codepy}
\COMPLEXITY{
Time $\mathcal{O}(n)$, space $\mathcal{O}(1)$ for scalar code and $\mathcal{O}(n)$
for vectorized arrays.
}
\FAILMODES{
\begin{bullets}
\item Small $n$ leads to coarse approximation; increase grid for accuracy.
\end{bullets}
}
\STABILITY{
\begin{bullets}
\item Squaring magnifies errors; maintain double precision.
\end{bullets}
}
\VALIDATION{
\begin{bullets}
\item Convergence to analytic equality case since $g=\tfrac12 a$ type.
\end{bullets}
}
\RESULT{
Triangle inequality holds discretely and approximates the continuous case.
}
\EXPLANATION{
Discrete inner product approximates $L^2$; CS and Minkowski carry over to
finite sums.
}

\section{Applied Domains — Detailed End-to-End Scenarios}

\DomainPage{Machine Learning}
\SCENARIO{
Bound the absolute Pearson correlation between features $X$ and target $y$
by $1$ via Cauchy--Schwarz; compute correlation and verify the bound.
}
\ASSUMPTIONS{
\begin{bullets}
\item Finite second moments; centered data.
\item Correlation $\rho=\frac{\mathbb{E}[XY]}{\sigma_X\sigma_Y}$.
\end{bullets}
}
\WHICHFORMULA{
$|\mathrm{Cov}(X,Y)|\le \sigma_X\sigma_Y$ (Formula 4 specialization), so
$|\rho|\le 1$.
}
\varmapStart
\var{X}{Feature random variable.}
\var{y}{Target random variable.}
\var{\sigma_X,\sigma_y}{Standard deviations.}
\var{\rho}{Pearson correlation.}
\varmapEnd
\PIPELINE{
\begin{bullets}
\item Generate synthetic $(X,y)$ with linear relation plus noise.
\item Compute sample correlation.
\item Verify $|\hat\rho|\le 1$ and approach to theoretical value.
\end{bullets}
}
\textbf{Implementation (From Scratch)}
\begin{codepy}
import random, math

def gen(n, seed=0):
    rng = random.Random(seed)
    X = [rng.uniform(-2.0, 2.0) for _ in range(n)]
    y = [1.5*x + rng.gauss(0.0, 0.5) for x in X]
    return X, y

def mean(v):
    return sum(v)/len(v)

def std(v):
    m = mean(v)
    return math.sqrt(sum((x-m)**2 for x in v)/len(v))

def cov(x, y):
    mx, my = mean(x), mean(y)
    return sum((xi-mx)*(yi-my) for xi, yi in zip(x, y))/len(x)

def corr(x, y):
    sx, sy = std(x), std(y)
    return 0.0 if sx*sy == 0 else cov(x, y)/(sx*sy)

def main():
    X, y = gen(500, seed=7)
    r = corr(X, y)
    print("corr", round(r, 6), "bound", 1.0)
    assert abs(r) <= 1.0 + 1e-12

if __name__ == "__main__":
    main()
\end{codepy}
\textbf{Implementation (Library Version)}
\begin{codepy}
import numpy as np
import pandas as pd

def main():
    rng = np.random.default_rng(7)
    X = rng.uniform(-2.0, 2.0, size=500)
    y = 1.5*X + rng.normal(0.0, 0.5, size=500)
    df = pd.DataFrame({"X": X, "y": y})
    r = float(df.corr().loc["X", "y"])
    print("corr", round(r, 6), "bound", 1.0)
    assert abs(r) <= 1.0 + 1e-12

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{Sample correlation magnitude bounded by $1$; point estimate near truth.}
\INTERPRET{CS ensures correlation is a cosine between centered variables.}
\NEXTSTEPS{Use orthogonal projections for least squares and derive $R^2$ bounds.}

\DomainPage{Quantitative Finance}
\SCENARIO{
Bound the standard deviation of a two-asset portfolio by the sum of individual
standard deviations: $\sigma_{X+Y}\le \sigma_X+\sigma_Y$, via triangle inequality
in $L^2$.
}
\ASSUMPTIONS{
\begin{bullets}
\item Square-integrable returns; no assumption on dependence for the bound.
\end{bullets}
}
\WHICHFORMULA{
Triangle inequality in $L^2$ (Formula 4) applied to centered returns.
}
\varmapStart
\var{X,Y}{Centered asset returns (random variables).}
\var{\sigma_X,\sigma_Y}{Standard deviations.}
\var{\sigma_{X+Y}}{Std. dev. of the sum.}
\varmapEnd
\PIPELINE{
\begin{bullets}
\item Simulate correlated returns.
\item Compute empirical std deviations and verify the bound.
\end{bullets}
}
\textbf{Implementation (Full Pipeline)}
\begin{codepy}
import numpy as np

def simulate(n=10000, rho=0.6, seed=0):
    rng = np.random.default_rng(seed)
    cov = np.array([[1.0, rho],[rho, 1.0]])
    R = rng.multivariate_normal([0.0, 0.0], cov, size=n)
    return R[:,0], R[:,1]

def bound_std(X, Y):
    sX = float(np.std(X))
    sY = float(np.std(Y))
    sXY = float(np.std(X + Y))
    return sX, sY, sXY, sXY <= sX + sY + 1e-12

def main():
    X, Y = simulate()
    sX, sY, sXY, ok = bound_std(X, Y)
    print("sXY", round(sXY,4), "sX+sY", round(sX+sY,4))
    assert ok

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{Std. dev. of sum vs. sum of std. devs; inequality must hold.}
\INTERPRET{Worst-case correlation gives the upper bound; CS guarantees it.}
\NEXTSTEPS{Use exact formula $\sigma_{X+Y}^2=\sigma_X^2+\sigma_Y^2+2\rho\sigma_X\sigma_Y$.}

\DomainPage{Deep Learning}
\SCENARIO{
Bound gradient inner product with parameter update using CS: for batch gradient
$g$, $|\langle g,\Delta w\rangle|\le \lVert g\rVert\lVert \Delta w\rVert$ and
bound loss change by triangle inequality in linear model approximation.
}
\ASSUMPTIONS{
\begin{bullets}
\item First-order Taylor: $\Delta L\approx \langle g,\Delta w\rangle$.
\item Finite-dimensional parameter vector.
\end{bullets}
}
\WHICHFORMULA{
Cauchy--Schwarz bounds work done by the update; triangle inequality bounds
accumulated updates.
}
\varmapStart
\var{g}{Gradient vector.}
\var{\Delta w}{Parameter update.}
\var{\Delta L}{First-order loss change.}
\varmapEnd
\PIPELINE{
\begin{bullets}
\item Compute gradient for a synthetic linear regression.
\item Verify CS bound on $\Delta L$.
\item Verify triangle bound on cumulative updates.
\end{bullets}
}
\textbf{Implementation (End-to-End)}
\begin{codepy}
import numpy as np

def gen(n=200, d=3, seed=0):
    rng = np.random.default_rng(seed)
    X = rng.normal(0.0, 1.0, size=(n, d))
    w_true = np.arange(1, d+1, dtype=float)
    y = X @ w_true + rng.normal(0.0, 0.1, size=n)
    return X, y, w_true

def grad(X, y, w):
    r = X @ w - y
    return (2.0/len(y)) * (X.T @ r)

def verify_cs(X, y, w, dw):
    g = grad(X, y, w)
    dl = float(abs(np.dot(g, dw)))
    rhs = float(np.linalg.norm(g)*np.linalg.norm(dw))
    return dl, rhs, dl <= rhs + 1e-12

def main():
    X, y, w_true = gen()
    w = np.zeros_like(w_true)
    dw = np.array([0.1, -0.2, 0.05])
    dl, rhs, ok = verify_cs(X, y, w, dw)
    print("abs(<g,dw>)", round(dl,6), "bound", round(rhs,6))
    assert ok

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{Compare $|\langle g,\Delta w\rangle|$ to $\lVert g\rVert\lVert \Delta w\rVert$.}
\INTERPRET{Update effectiveness bounded by lengths; alignment (cosine) matters.}
\NEXTSTEPS{Use cosine annealing or align updates to gradients to maximize descent.}

\DomainPage{Kaggle / Data Analytics}
\SCENARIO{
Standardize features and compute correlation matrix; verify each entry is in
$[-1,1]$ by CS and apply triangle inequality to standardized sums.
}
\ASSUMPTIONS{
\begin{bullets}
\item Numerical features with finite variance.
\end{bullets}
}
\WHICHFORMULA{
$|\rho_{ij}|\le 1$ by CS; for standardized $z$, $\lVert z_i+z_j\rVert
\le \lVert z_i\rVert+\lVert z_j\rVert$ by triangle inequality.
}
\PIPELINE{
\begin{bullets}
\item Create synthetic correlated features.
\item Standardize columns.
\item Compute correlations and verify bounds.
\end{bullets}
}
\textbf{Implementation (Complete EDA Pipeline)}
\begin{codepy}
import numpy as np
import pandas as pd

def synth(n=300, seed=0):
    rng = np.random.default_rng(seed)
    A = rng.normal(size=(3, 3))
    cov = A @ A.T
    X = rng.multivariate_normal(np.zeros(3), cov, size=n)
    df = pd.DataFrame(X, columns=["A","B","C"])
    return df

def standardize(df):
    return (df - df.mean())/df.std(ddof=0)

def main():
    df = synth()
    Z = standardize(df)
    C = Z.corr()
    bounds = C.abs().values <= 1.0 + 1e-12
    print("all |corr|<=1:", bool(bounds.all()))
    assert bounds.all()

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{Absolute correlations all $\le 1$.}
\INTERPRET{CS ensures correlations are cosines between standardized columns.}
\NEXTSTEPS{Leverage orthogonalization (PCA) to decorrelate features.}

\end{document}
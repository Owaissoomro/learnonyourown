% !TeX program = xelatex
\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{microtype,setspace,amsmath,amssymb,mathtools,amsthm,unicode-math}
\setstretch{1.05}
\setmainfont{Latin Modern Roman}
\setmonofont{Latin Modern Mono}
\setmathfont{Latin Modern Math}
\allowdisplaybreaks[4]
\setlength{\jot}{7pt}
\setlength{\emergencystretch}{8em}
\sloppy
\usepackage{xcolor,fancyhdr,enumitem,inconsolata,listings}
\pagestyle{fancy}\fancyhf{}\lhead{\nouppercase{\leftmark}}\rhead{\thepage}
\setlength{\headheight}{26pt}
\setlength{\parindent}{0pt}
\setlength{\parskip}{8pt plus 2pt minus 1pt}
\raggedbottom
\newenvironment{BreakableEquation}{\begin{equation}\begin{aligned}}{\end{aligned}\end{equation}}
\newenvironment{BreakableEquation*}{\begin{equation*}\begin{aligned}}{\end{aligned}\end{equation*}}
\newenvironment{tightalign}{\begingroup\small\allowdisplaybreaks\begin{align}}{\end{align}\endgroup}
\providecommand{\enumlistm}{enumitem}
\newenvironment{minted}[2][]{%
  \lstset{style=code,language=#2,#1}\begin{lstlisting}%
}{\end{lstlisting}}
\newcommand{\inputminted}[3][]{\begin{lstlisting}\end{lstlisting}}
\newlist{bullets}{itemize}{1}
\setlist[bullets]{label=--,leftmargin=1.6em,itemsep=2pt,topsep=4pt}
\newcommand{\varmapStart}{\textbf{VARIABLE MAPPING:}\par\begin{bullets}}
\newcommand{\var}[2]{\item $#1$ — #2}
\newcommand{\varmapEnd}{\end{bullets}}
\newcommand{\glossx}[6]{%
  \textbf{#1}\par
  \begin{bullets}
    \item \textbf{What:} #2
    \item \textbf{Why:} #3
    \item \textbf{How:} #4
    \item \textbf{ELI5:} #5
    \item \textbf{Pitfall/Example:} #6
  \end{bullets}
}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{claim}[theorem]{Claim}
\lstdefinestyle{code}{
  basicstyle=\ttfamily\footnotesize,
  backgroundcolor=\color{black!02},
  frame=single,
  numbers=left, numberstyle=\tiny, numbersep=8pt,
  breaklines=true, breakatwhitespace=true,
  tabsize=4, showstringspaces=false,
  upquote=true, keepspaces=true, columns=fullflexible,
  literate=
    {–}{{-}}1
    {—}{{-}}1
    {…}{{...}}1
    {≤}{{\ensuremath{\le}}}1
    {≥}{{\ensuremath{\ge}}}1
    {≠}{{\ensuremath{\ne}}}1
    {≈}{{\ensuremath{\approx}}}1
    {±}{{\ensuremath{\pm}}}1
    {→}{{\ensuremath{\to}}}1
    {←}{{\ensuremath{\leftarrow}}}1
    {∞}{{\ensuremath{\infty}}}1
    {√}{{\ensuremath{\sqrt{\ }}}}1
    {×}{{\ensuremath{\times}}}1
    {÷}{{\ensuremath{\div}}}1
}
\lstnewenvironment{codepy}[1][]%
  {\lstset{style=code,language=Python,#1}}%
  {}
\newcommand{\inlinecode}[1]{\lstinline[style=code]!#1!}
\newcommand{\LF}[2]{\par\noindent\textbf{#1:}~#2\par}
\newcommand{\WHAT}[1]{\LF{WHAT}{#1}}
\newcommand{\WHY}[1]{\LF{WHY}{#1}}
\newcommand{\HOW}[1]{\LF{HOW}{#1}}
\newcommand{\ELI}[1]{\LF{ELI5}{#1}}
\newcommand{\SCOPE}[1]{\LF{SCOPE}{#1}}
\newcommand{\CONFUSIONS}[1]{\LF{COMMON CONFUSIONS}{#1}}
\newcommand{\APPLICATIONS}[1]{\LF{APPLICATIONS}{#1}}
\newcommand{\FORMULA}[1]{\LF{FORMULA}{#1}}
\newcommand{\CANONICAL}[1]{\LF{CANONICAL FORM}{#1}}
\newcommand{\PRECONDS}[1]{\LF{PRECONDITIONS}{#1}}
\newcommand{\DERIVATION}[1]{\LF{DERIVATION}{#1}}
\newcommand{\EQUIV}[1]{\LF{EQUIVALENT FORMS}{#1}}
\newcommand{\LIMITS}[1]{\LF{LIMIT CASES}{#1}}
\newcommand{\INPUTS}[1]{\LF{INPUTS}{#1}}
\newcommand{\OUTPUTS}[1]{\LF{OUTPUTS}{#1}}
\newcommand{\RESULT}[1]{\LF{RESULT}{#1}}
\newcommand{\INTUITION}[1]{\LF{INTUITION}{#1}}
\newcommand{\PITFALLS}[1]{\LF{PITFALLS}{#1}}
\newcommand{\MODEL}[1]{\LF{CANONICAL MATH MODEL}{#1}}
\newcommand{\ASSUMPTIONS}[1]{\LF{ASSUMPTIONS}{#1}}
\newcommand{\WHICHFORMULA}[1]{\LF{WHICH FORMULA \& WHY}{#1}}
\newcommand{\GOVERN}[1]{\LF{GOVERNING EQUATION(S)}{#1}}
\newcommand{\UNITCHECK}[1]{\LF{UNIT CHECK}{#1}}
\newcommand{\EDGECASES}[1]{\LF{EDGE CASES}{#1}}
\newcommand{\ALTERNATE}[1]{\LF{ALTERNATE APPROACH (sketch)}{#1}}
\newcommand{\PROBLEM}[1]{\LF{PROBLEM}{#1}}
\newcommand{\API}[1]{\LF{API}{#1}}
\newcommand{\COMPLEXITY}[1]{\LF{COMPLEXITY}{#1}}
\newcommand{\FAILMODES}[1]{\LF{FAILURE MODES}{#1}}
\newcommand{\STABILITY}[1]{\LF{NUMERICAL STABILITY}{#1}}
\newcommand{\VALIDATION}[1]{\LF{VALIDATION}{#1}}
\newcommand{\EXPLANATION}[1]{\LF{EXPLANATION}{#1}}
\newcommand{\SCENARIO}[1]{\LF{SCENARIO}{#1}}
\newcommand{\PIPELINE}[1]{\LF{PIPELINE STEPS}{#1}}
\newcommand{\METRICS}[1]{\LF{METRICS}{#1}}
\newcommand{\INTERPRET}[1]{\LF{INTERPRETATION}{#1}}
\newcommand{\NEXTSTEPS}[1]{\LF{LIMITATIONS \& NEXT STEPS}{#1}}
\usepackage{titlesec}
\titleformat{\section}{\Large\bfseries}{\thesection}{0.6em}{}
\titlespacing*{\section}{0pt}{*2}{*1}
\usepackage{etoolbox}
\pretocmd{\section}{\clearpage}{}{}
\newcommand{\FormulaPage}[2]{%
  \clearpage
  \section*{Formula #1 — #2}%
  \addcontentsline{toc}{section}{Formula #1 — #2}%
}
\newcommand{\ProblemPage}[2]{%
  \clearpage
  \subsection*{Problem #1: #2}%
  \addcontentsline{toc}{subsection}{Problem #1: #2}%
}
\newcommand{\CodeDemoPage}[1]{%
  \clearpage
  \subsection*{Coding Demo: #1}%
  \addcontentsline{toc}{subsection}{Coding Demo: #1}%
}
\newcommand{\DomainPage}[1]{%
  \clearpage
  \subsection*{#1 (End-to-End)}%
  \addcontentsline{toc}{subsection}{#1 (End-to-End)}%
}
\begin{document}
\title{Comprehensive Study Sheet — Conjugate Gradient and GMRES Methods}
\date{\today}
\maketitle
\tableofcontents
\clearpage
\section{Concept Overview}
\WHAT{
We study iterative Krylov subspace methods for solving linear systems
$A x = b$ with $A \in \mathbb{R}^{n \times n}$.
Conjugate Gradient (CG) applies to symmetric positive definite (SPD) $A$
and computes $x_k \in x_0 + \mathcal{K}_k(A,r_0)$ minimizing the $A$-norm
of the error. GMRES applies to general nonsingular $A$ and computes
$x_k \in x_0 + \mathcal{K}_k(A,r_0)$ minimizing $\|r_k\|_2$.
Here $r_0=b-Ax_0$ and $\mathcal{K}_k(A,r_0)=\mathrm{span}\{r_0,Ar_0,\dots,
A^{k-1}r_0\}$.}
\WHY{
Direct methods can be expensive or ill-suited for large sparse systems.
CG and GMRES exploit the Krylov subspace structure to provide monotone
convergence under mild spectral conditions, often at near-linear time per
iteration with short recurrence (CG) or robust residual minimization
(GMRES). They are fundamental in numerical linear algebra, PDE solvers,
optimization, and scientific computing.}
\HOW{
1. Assume $A$ is SPD (CG) or general (GMRES).
2. Build a Krylov subspace from $r_0$; impose optimality conditions:
   $A$-orthogonality of error (CG) or residual norm minimality (GMRES).
3. Derive recurrences: CG uses three-term short recurrences; GMRES uses
   Arnoldi orthonormalization and a small least-squares problem.
4. Interpret: CG is optimal over polynomials constrained by $p(0)=1$ in
   the $A$-norm; GMRES is optimal in 2-norm residual via Arnoldi.}
\ELI{
Think of $A$ as a landscape and $f(x)=\tfrac12 x^\top A x - b^\top x$ as
height. CG walks along directions that never fight each other under $A$,
reaching the valley fast. GMRES tries steps that make the current mismatch
(residual) as small as possible within a growing toolbox of directions.}
\SCOPE{
CG requires $A$ SPD and exact arithmetic yields termination in $\le n$
steps. GMRES applies to any nonsingular $A$ but has growing memory cost;
restarts reduce memory but may hurt convergence. Preconditioning modifies
the geometry to accelerate convergence. Breakdown can occur with loss of
orthogonality in finite precision; remedies include reorthogonalization.}
\CONFUSIONS{
CG vs. steepest descent: CG conjugates directions with respect to $A$,
not just orthogonal residuals. GMRES vs. FOM: GMRES minimizes residual
norm; FOM enforces Galerkin orthogonality. CG on normal equations is not
the same as LSQR. Preconditioning side (left/right/split) changes residual
definitions.}
\APPLICATIONS{
\begin{bullets}
\item Mathematical foundations: polynomial approximation and spectra.
\item Computational modeling: PDE discretizations and implicit time-stepping.
\item Engineering: structural analysis, circuit simulation, tomography.
\item Algorithms: machine learning least squares, PageRank, Gaussian process.
\end{bullets}
}
\textbf{ANALYTIC STRUCTURE.}
CG lives on an SPD inner-product space with $A$-conjugacy and three-term
recurrences. GMRES uses orthogonal projections via Arnoldi and Hessenberg
structures. Both are polynomial methods: $r_k = p_k(A) r_0$, $p_k(0)=1$.
\textbf{CANONICAL LINKS.}
CG error bounds use Chebyshev polynomials on $[\lambda_{\min},\lambda_{\max}]$.
GMRES uses Arnoldi: $A V_k = V_{k+1} \bar H_k$ and residual minimization as a
small least squares problem. Preconditioning links to spectral clustering.
\textbf{PROBLEM-TYPE RECOGNITION HEURISTICS.}
\begin{bullets}
\item SPD matrix, quadratic functional, $A$-norm: suspect CG.
\item Nonsymmetric or indefinite matrix, need robust residual decrease:
GMRES.
\item Mentions Krylov subspace, Arnoldi, Hessenberg: GMRES/FOM.
\item Bounds involving condition number $\kappa(A)$: CG Chebyshev bounds.
\end{bullets}
\textbf{SOLUTION STRATEGY BLUEPRINT.}
\begin{bullets}
\item Translate $A x=b$ to Krylov form with $r_0=b-Ax_0$.
\item Pick method: CG for SPD, GMRES for general $A$.
\item Apply recurrences or Arnoldi, compute step via scalars or LS.
\item Interpret residual decrease and check stopping criteria.
\item Validate with spectral insight or polynomial bounds.
\end{bullets}
\textbf{CONCEPTUAL INVARIANTS.}
CG: $r_i^\top r_j=0$ for $i\ne j$, $p_i^\top A p_j=0$ for $i\ne j$,
and $x_k$ minimizes $f$ over $x_0+\mathcal{K}_k$. GMRES: residual
orthogonal to $A\mathcal{K}_k$, monotone $\|r_k\|_2$.
\textbf{EDGE INTUITION.}
If eigenvalues cluster, polynomials cancel errors quickly. As
$\kappa(A)\to\infty$, CG slows, but preconditioning can re-cluster.
GMRES excels when nonnormality is mild or pseudospectra tight; otherwise,
restarts can stagnate. 
\clearpage
\section{Glossary}
\glossx{Krylov Subspace}{
$\mathcal{K}_k(A,r_0)=\mathrm{span}\{r_0,Ar_0,\dots,A^{k-1}r_0\}$.}{
It captures all degree-$(k-1)$ actions of $A$ on $r_0$, the search space
for many optimal iterative methods.}{
Build sequentially by multiplying by $A$ and orthogonalizing.}{
Like stirring paint: each stir (multiply by $A$) mixes in a new pattern;
after $k$ stirs, you span many mixtures.}{
Pitfall: basis vectors become nearly dependent; stabilize via
orthonormalization and reorthogonalization.}
\glossx{$A$-Conjugacy}{
Two vectors $p,q$ are $A$-conjugate if $p^\top A q=0$.}{
Enables independent progress along directions in quadratic minimization,
yielding short recurrences in CG.}{
Inductively build $p_k$ so $p_i^\top A p_j=0$ for $i\ne j$.}{
Like navigating a city with streets at right angles in the $A$-geometry.}{
Example: using Euclidean orthogonality instead of $A$-conjugacy breaks CG
optimality.}
\glossx{Arnoldi Process}{
Orthogonalizes $\{r_0,Ar_0,\dots\}$ to produce $V_{k+1}$ and upper
Hessenberg $\bar H_k$ so that $A V_k = V_{k+1}\bar H_k$.}{
Forms the backbone of GMRES, turning a large problem into a small least
squares.}{
Gram-Schmidt orthogonalization on $A v_j$, normalizing to build $v_{j+1}$.}{
Make a set of perfectly straight, orthogonal rods to describe a complex
shape.}{
Pitfall: loss of orthogonality causes slowdowns; use modified Gram-Schmidt
or reorthogonalization.}
\glossx{Residual Polynomial}{
A polynomial $p_k$ with $p_k(0)=1$ such that $r_k = p_k(A) r_0$.}{
Connects convergence to polynomial approximation on the spectrum of $A$.}{
Characterize the method as choosing $p_k$ to minimize $A$-norm (CG) or
2-norm (GMRES) of $r_k$.}{
Like a filter applied repeatedly to mute spectral components of the error.}{
Pitfall: ignoring nonnormality; eigenvalues alone do not determine GMRES
rates when $A$ is highly nonnormal.}
\clearpage
\section{Symbol Ledger}
\varmapStart
\var{A \in \mathbb{R}^{n\times n}}{System matrix; SPD for CG, general for GMRES.}
\var{b \in \mathbb{R}^n}{Right-hand side vector.}
\var{x^\ast}{Exact solution satisfying $A x^\ast=b$.}
\var{x_k}{Iterate at step $k$.}
\var{r_k=b-Ax_k}{Residual at step $k$.}
\var{e_k=x^\ast-x_k}{Error at step $k$.}
\var{\mathcal{K}_k(A,r_0)}{Krylov subspace of dimension $k$.}
\var{p_k}{CG search direction at step $k$.}
\var{\alpha_k,\beta_k}{CG scalar step and recurrence coefficients.}
\var{V_k}{Arnoldi basis columns $v_1,\dots,v_k$.}
\var{\bar H_k}{$(k+1)\times k$ upper Hessenberg from Arnoldi.}
\var{H_k}{Leading $k\times k$ block of $\bar H_k$.}
\var{e_1}{First coordinate vector.}
\var{\beta=\|r_0\|_2}{Initial residual norm.}
\var{M}{Preconditioner (SPD for left PCG).}
\var{\kappa(A)}{Condition number $\lambda_{\max}/\lambda_{\min}$ for SPD $A$.}
\var{\lambda_{\min},\lambda_{\max}}{Extreme eigenvalues of SPD $A$.}
\varmapEnd
\clearpage
\section{Formula Canon — One Formula Per Page}
\FormulaPage{1}{Conjugate Gradient (CG) Optimality and Recurrence}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
CG for SPD $A$ produces $x_k \in x_0+\mathcal{K}_k(A,r_0)$ minimizing
$f(x)=\tfrac12 x^\top A x - b^\top x$ and equivalently minimizing
$\|e_k\|_A=\sqrt{e_k^\top A e_k}$ over that affine space, with short
recurrences yielding $A$-conjugate search directions and orthogonal
residuals.
\WHAT{
Compute $x_k$ such that $x_k=\arg\min_{x\in x_0+\mathcal{K}_k} f(x)$ for
SPD $A$, using three-term recurrences.}
\WHY{
This yields rapid convergence with minimal memory and arithmetic per step,
terminating in at most $n$ steps in exact arithmetic.}
\FORMULA{
\[
\begin{aligned}
&x_{k+1}=x_k+\alpha_k p_k,\quad r_k=b-Ax_k,\quad p_0=r_0,\\
&\alpha_k=\frac{r_k^\top r_k}{p_k^\top A p_k},\quad
r_{k+1}=r_k-\alpha_k A p_k,\\
&\beta_{k+1}=\frac{r_{k+1}^\top r_{k+1}}{r_k^\top r_k},\quad
p_{k+1}=r_{k+1}+\beta_{k+1} p_k.
\end{aligned}
\]
Residuals satisfy $r_i^\top r_j=0$ for $i\ne j$, directions satisfy
$p_i^\top A p_j=0$ for $i\ne j$, and $x_k$ minimizes $f$ over
$x_0+\mathcal{K}_k(A,r_0)$.
}
\CANONICAL{
SPD $A$; inner product $\langle u,v\rangle_A=u^\top A v$; quadratic model
$f(x)$. The Krylov subspace $\mathcal{K}_k(A,r_0)$ contains all iterates.}
\PRECONDS{
\begin{bullets}
\item $A$ is symmetric positive definite.
\item $r_0\ne 0$ unless at solution.
\item Arithmetic exactness assumed for theoretical identities.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
Let $A$ be SPD and $\{p_i\}_{i=0}^{k-1}$ be $A$-conjugate.
Then $x_k=x_0+\sum_{i=0}^{k-1}\alpha_i p_i$ with
$\alpha_i=\frac{p_i^\top r_i}{p_i^\top A p_i}$ minimizes $f(x)$ over
$x_0+\mathrm{span}\{p_0,\dots,p_{k-1}\}$.
\end{lemma}
\begin{proof}
Let $x=x_0+\sum_{i} \gamma_i p_i$. The gradient is
$\nabla f(x)=Ax-b=-r(x)$. Optimality requires orthogonality of the
residual to the subspace:
$p_j^\top r(x)=0$ for all $j$. Since $r(x)=r_0-A\sum_i \gamma_i p_i$,
\[
p_j^\top r(x)=p_j^\top r_0-\sum_i \gamma_i p_j^\top A p_i
=p_j^\top r_0-\gamma_j p_j^\top A p_j,
\]
by $A$-conjugacy. Therefore
$\gamma_j=\frac{p_j^\top r_0}{p_j^\top A p_j}$. In the recurrence form,
$r_i$ is the residual after $i$ steps, and
$\alpha_i=\frac{p_i^\top r_i}{p_i^\top A p_i}$ enforces the same
orthogonality at step $i$. Hence the stated $x_k$ minimizes $f$ on the
affine subspace. \qedhere
\end{proof}
\DERIVATION{
\begin{align*}
\text{Step 1: }& \text{Initialize } x_0,\ r_0=b-Ax_0,\ p_0=r_0.\\
\text{Step 2: }& \alpha_k=\arg\min_{\alpha} f(x_k+\alpha p_k)\\
&= \arg\min_{\alpha}\ \tfrac12 (x_k+\alpha p_k)^\top A (x_k+\alpha p_k)
- b^\top(x_k+\alpha p_k).\\
\text{Step 3: }& \frac{d}{d\alpha}f=\alpha p_k^\top A p_k - p_k^\top r_k=0
\Rightarrow \alpha_k=\frac{p_k^\top r_k}{p_k^\top A p_k}.\\
\text{Step 4: }& r_{k+1}=r_k-\alpha_k A p_k;\ \text{orthogonality }
r_{k+1}^\top r_k=0.\\
\text{Step 5: }& \beta_{k+1}=\frac{r_{k+1}^\top r_{k+1}}{r_k^\top r_k}
\text{ ensures } p_{k+1}=r_{k+1}+\beta_{k+1} p_k \\
&\text{$A$-conjugate to all previous $p_i$ by induction.}
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Verify SPD and compute $r_0$.
\item Iterate with the CG recurrences until tolerance met.
\item Optionally monitor $\|r_k\|_2$ or $\|e_k\|_A$ surrogates.
\item Check termination or stagnation and adjust preconditioning if needed.
\end{bullets}
\EQUIV{
\begin{bullets}
\item CG equals exact line-search steepest descent with $A$-conjugate
direction update (Fletcher-Reeves form).
\item Equivalent to minimizing $f$ over $x_0+\mathcal{K}_k$.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item If $A$ is ill-conditioned, convergence slows as
$\kappa(A)\to\infty$.
\item Breakdown only at solution in exact arithmetic; rounding may cause
loss of orthogonality.
\end{bullets}
}
\INPUTS{$A\in\mathbb{R}^{n\times n}$ SPD, $b\in\mathbb{R}^n$, $x_0\in\mathbb{R}^n$.}
\DERIVATION{
\begin{align*}
\text{Substitute: }& \alpha_k=\frac{r_k^\top r_k}{p_k^\top A p_k},\ 
\beta_{k+1}=\frac{r_{k+1}^\top r_{k+1}}{r_k^\top r_k}.\\
\text{Compute: }& x_{k+1}=x_k+\alpha_k p_k,\ 
r_{k+1}=r_k-\alpha_k A p_k,\ 
p_{k+1}=r_{k+1}+\beta_{k+1} p_k.\\
\text{Finalize: }& \text{Stop when }\|r_{k+1}\|_2\le \varepsilon\|b\|_2.
\end{align*}
}
\RESULT{
A sequence $\{x_k\}$ with strictly decreasing $f(x_k)$ and orthogonal
residuals, converging to $x^\ast$ in at most $n$ steps in exact arithmetic.}
\UNITCHECK{
All terms are scalar or vector in consistent inner products; denominators
$p_k^\top A p_k>0$ by SPD, ensuring well-defined steps.}
\PITFALLS{
\begin{bullets}
\item Using Euclidean orthogonality instead of $A$-conjugacy breaks
short recurrences.
\item Forgetting to update $p_{k+1}$ with the new $\beta_{k+1}$ causes
loss of optimality.
\end{bullets}
}
\INTUITION{
Each step erases error components along one $A$-conjugate direction;
after $k$ steps, all components in a $k$-dimensional subspace are
eliminated.}
\CANONICAL{
\begin{bullets}
\item $x_k=\arg\min_{x\in x_0+\mathcal{K}_k}\|x-x^\ast\|_A$.
\item $r_k\perp \mathcal{K}_k$ and $p_i^\top A p_j=0$ for $i\ne j$.
\end{bullets}
}
\FormulaPage{2}{CG Chebyshev Error Bound}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
For SPD $A$ with eigenvalues in $[\lambda_{\min},\lambda_{\max}]$,
the CG error satisfies
$\|e_k\|_A \le 2\left(\frac{\sqrt{\kappa}-1}{\sqrt{\kappa}+1}\right)^k
\|e_0\|_A$ with $\kappa=\kappa(A)=\lambda_{\max}/\lambda_{\min}$.
\WHAT{
Bound the $A$-norm error of CG iterates using minimax polynomials.}
\WHY{
Links convergence to spectral conditioning and motivates preconditioning
to cluster eigenvalues.}
\FORMULA{
\[
\|e_k\|_A \le \min_{p\in\mathcal{P}_k,\ p(0)=1} 
\max_{\lambda\in[\lambda_{\min},\lambda_{\max}]} |p(\lambda)|\ \|e_0\|_A
\le 2\left(\frac{\sqrt{\kappa}-1}{\sqrt{\kappa}+1}\right)^k \|e_0\|_A.
\]
}
\CANONICAL{
$\mathcal{P}_k$ is the set of polynomials of degree $\le k$.
Map $[\lambda_{\min},\lambda_{\max}]$ to $[-1,1]$ and use Chebyshev
polynomials $T_k$.}
\PRECONDS{
\begin{bullets}
\item $A$ is SPD with spectrum in $[\lambda_{\min},\lambda_{\max}]$.
\item Exact arithmetic CG or the polynomial characterization holds.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
Let $0<\lambda_{\min}\le \lambda_{\max}$ and define
$\theta=\frac{\sqrt{\kappa}-1}{\sqrt{\kappa}+1}$ with
$\kappa=\lambda_{\max}/\lambda_{\min}$. Then
$\min_{p\in\mathcal{P}_k,\ p(0)=1}\ \max_{\lambda\in[\lambda_{\min},
\lambda_{\max}]} |p(\lambda)| \le 2\theta^k$.
\end{lemma}
\begin{proof}
Affine-map $\lambda \mapsto t=\frac{2\lambda-(\lambda_{\max}+\lambda_{\min})}
{\lambda_{\max}-\lambda_{\min}} \in [-1,1]$. Consider
$q_k(t)=\frac{T_k(t)}{T_k(t_0)}$ where $t_0=\frac{-(\lambda_{\max}+
\lambda_{\min})}{\lambda_{\max}-\lambda_{\min}}<-1$ so that
$q_k(1)=\frac{T_k(1)}{T_k(t_0)}=\frac{1}{T_k(t_0)}$ and $q_k(0)=1$ in
the $\lambda$-variable. Using $|T_k(t)|\le 1$ for $t\in[-1,1]$ and
$T_k(\cosh \xi)=\cosh(k\xi)$, set $t_0=-\cosh \xi$ with
$\cosh \xi=\frac{\kappa+1}{\kappa-1}$. Then
$|T_k(t_0)|=\cosh(k\xi)=\frac{(e^\xi)^k+(e^{-\xi})^k}{2}\ge \frac12 e^{k\xi}$.
Moreover, $e^\xi=\sqrt{\kappa}+\sqrt{\kappa-1} \le \sqrt{\kappa}+(\sqrt{\kappa}-1)
=\ 2\sqrt{\kappa}-1$, and the classical derivation yields
$\frac{1}{|T_k(t_0)|}\le 2\left(\frac{\sqrt{\kappa}-1}{\sqrt{\kappa}+1}\right)^k$.
Thus the minimax value is bounded by $2\theta^k$. \qedhere
\end{proof}
\DERIVATION{
\begin{align*}
\text{Step 1: }& e_k=p_k(A)e_0,\ p_k(0)=1,\ \text{for some }p_k\in\mathcal{P}_k.\\
\text{Step 2: }& \|e_k\|_A^2=e_k^\top A e_k=e_0^\top p_k(A)^\top A p_k(A) e_0.\\
\text{Step 3: }& \text{Diagonalize }A=Q\Lambda Q^\top,\ \|e_k\|_A \le
\max_i |p_k(\lambda_i)|\ \|e_0\|_A.\\
\text{Step 4: }& \min_{p_k}\max_{\lambda\in[\lambda_{\min},\lambda_{\max}]}
|p_k(\lambda)| \le 2\theta^k\ \text{by lemma.}\\
\text{Step 5: }& \Rightarrow \|e_k\|_A \le 2\theta^k \|e_0\|_A.
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Estimate $[\lambda_{\min},\lambda_{\max}]$ or $\kappa$.
\item Predict iteration count $k$ for target reduction $\varepsilon$ via
$2\theta^k\le \varepsilon$.
\item Design preconditioner to reduce $\kappa$ and re-evaluate bound.
\end{bullets}
\EQUIV{
\begin{bullets}
\item Equivalent bound using eigenvalue clusters tightens constants.
\item Bounds in terms of spectrum of $M^{-1}A$ for preconditioned CG.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item Sharp for worst-case initial error; practical convergence may be
much faster with spectral clustering.
\item For indefinite $A$ the bound does not apply; CG not defined.
\end{bullets}
}
\INPUTS{$\lambda_{\min}>0$, $\lambda_{\max}$, $\|e_0\|_A$, or $\kappa$.}
\DERIVATION{
\begin{align*}
\text{Compute: }& \theta=\frac{\sqrt{\kappa}-1}{\sqrt{\kappa}+1},\
B_k=2\theta^k.\\
\text{Finalize: }& \|e_k\|_A \le B_k \|e_0\|_A.
\end{align*}
}
\RESULT{
A practical convergence predictor and a design criterion for
preconditioning based on reducing $\kappa$.}
\UNITCHECK{
Dimensionless bound: $\theta$ and $B_k$ are pure numbers; norms consistent.}
\PITFALLS{
\begin{bullets}
\item Confusing $\|r_k\|_2$ with $\|e_k\|_A$; they are related but not equal.
\item Using eigenvalue bounds of $A$ instead of $M^{-1}A$ in PCG analysis.
\end{bullets}
}
\INTUITION{
Best polynomial cancels error at eigenvalues; Chebyshev minimizes the
maximum ripple, yielding geometric decay with rate set by $\kappa$.}
\CANONICAL{
\begin{bullets}
\item $\min_{p(0)=1}\max_{\lambda\in[\lambda_{\min},\lambda_{\max}]}
|p(\lambda)| \le 2\theta^k$.
\item $e_k=p_k(A)e_0$ with $p_k$ near-scaled Chebyshev. 
\end{bullets}
}
\FormulaPage{3}{GMRES via Arnoldi and Least Squares}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
GMRES constructs $x_k=x_0+V_k y_k$ minimizing $\|r_k\|_2$ subject to
$A V_k=V_{k+1}\bar H_k$ from Arnoldi, where
$y_k=\arg\min_y\|\beta e_1-\bar H_k y\|_2$ and
$\|r_k\|_2=\|\beta e_1-\bar H_k y_k\|_2$.
\WHAT{
Obtain the residual-minimizing iterate within $x_0+\mathcal{K}_k(A,r_0)$
using orthonormal basis $V_k$ and a small least-squares problem.}
\WHY{
Provides robust monotone residual decrease for general nonsingular $A$.}
\FORMULA{
\[
\begin{aligned}
&v_1=r_0/\beta,\ \beta=\|r_0\|_2,\ A V_k=V_{k+1}\bar H_k,\\
&x_k=x_0+V_k y_k,\quad
y_k=\arg\min_{y\in\mathbb{R}^k}\|\beta e_1-\bar H_k y\|_2,\\
&\|r_k\|_2=\min_{y}\|\beta e_1-\bar H_k y\|_2.
\end{aligned}
\]
}
\CANONICAL{
$V_{k+1}$ has orthonormal columns; $\bar H_k$ is $(k+1)\times k$ upper
Hessenberg from Arnoldi. Residual is orthogonal to $A\mathcal{K}_k$.}
\PRECONDS{
\begin{bullets}
\item $A$ nonsingular and $r_0\ne 0$.
\item Arnoldi process runs without breakdown; if $h_{j+1,j}=0$ early,
exact solution obtained in $\le k$ steps.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
Arnoldi factorization: Starting with $v_1=r_0/\|r_0\|_2$, the process
produces orthonormal $V_{k+1}$ and $\bar H_k$ such that
$A V_k=V_{k+1}\bar H_k$.
\end{lemma}
\begin{proof}
By modified Gram-Schmidt, define for $j=1,\dots,k$:
$w=A v_j$, for $i=1,\dots,j$ set $h_{i,j}=v_i^\top w$ and
$w\leftarrow w-h_{i,j} v_i$. Set $h_{j+1,j}=\|w\|_2$. If nonzero,
$v_{j+1}=w/h_{j+1,j}$. Stack columns to obtain
$A V_k=V_{k+1}\bar H_k$ by construction, with $V_{k+1}$ orthonormal due
to orthogonalization and normalization. \qedhere
\end{proof}
\DERIVATION{
\begin{align*}
\text{Step 1: }& r_0=b-Ax_0,\ v_1=r_0/\beta.\\
\text{Step 2: }& \text{Arnoldi: } A V_k=V_{k+1}\bar H_k.\\
\text{Step 3: }& x=x_0+V_k y \Rightarrow r=b-Ax=b-Ax_0-AV_k y\\
&=r_0-V_{k+1}\bar H_k y=\beta v_1-V_{k+1}\bar H_k y.\\
\text{Step 4: }& \|r\|_2=\|\beta e_1-\bar H_k y\|_2\ \text{by orthonormality.}\\
\text{Step 5: }& \text{Choose } y_k \text{ minimizing the 2-norm; then }
x_k=x_0+V_k y_k.
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Build $V_{k+1},\bar H_k$ by Arnoldi; update Givens QR on the fly.
\item Solve the small least-squares for $y_k$; update residual norm from
the last Givens rotation.
\item Check convergence; optionally restart after $m$ steps.
\end{bullets}
\EQUIV{
\begin{bullets}
\item GMRES residual polynomial: $r_k=p_k(A)r_0$ with $p_k(0)=1$ chosen
to minimize $\|p_k(A)r_0\|_2$.
\item FOM enforces $r_k\perp \mathcal{K}_k$, equivalent to solving
$H_k y=\beta e_1$ if nonsingular.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item Memory grows with $k$; restarted GMRES($m$) controls memory but may
stagnate.
\item Loss of orthogonality may degrade convergence; reorthogonalize.
\end{bullets}
}
\INPUTS{$A, b, x_0$, maximum subspace size $k$, tolerance.}
\DERIVATION{
\begin{align*}
\text{Compute: }& (V_{k+1},\bar H_k)=\mathrm{Arnoldi}(A,r_0,k).\\
\text{Solve: }& y_k=\arg\min \|\beta e_1-\bar H_k y\|_2.\\
\text{Finalize: }& x_k=x_0+V_k y_k,\ \|r_k\|_2 \text{ from Givens QR}.
\end{align*}
}
\RESULT{
Iterates with nonincreasing residual norms $\|r_k\|_2$, converging to the
solution barring breakdown, often rapidly for favorable spectra.}
\UNITCHECK{
All norms are Euclidean; $V_{k+1}$ has orthonormal columns so projections
and norms are consistent.}
\PITFALLS{
\begin{bullets}
\item Solving normal equations $H_k^\top H_k y = H_k^\top \beta e_1$
instead of QR may cause numerical instability.
\item Forgetting to include $h_{k+1,k}$ in the residual update.
\end{bullets}
}
\INTUITION{
GMRES seeks the best possible combination of the first $k$ Krylov vectors
to cancel the residual in least-squares sense.}
\CANONICAL{
\begin{bullets}
\item $A V_k=V_{k+1}\bar H_k$, $x_k=x_0+V_k y_k$, 
$y_k=\arg\min\|\beta e_1-\bar H_k y\|_2$.
\item $r_k\perp A\mathcal{K}_k(A,r_0)$.
\end{bullets}
}
\FormulaPage{4}{GMRES Residual Polynomial and Monotonicity}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
GMRES produces residuals $r_k=p_k(A)r_0$ with $p_k\in\mathcal{P}_k$,
$p_k(0)=1$, minimizing $\|p_k(A)r_0\|_2$; hence
$\|r_{k+1}\|_2 \le \|r_k\|_2$.
\WHAT{
Characterize GMRES via residual polynomials and prove monotone residual
decrease.}
\WHY{
Connects convergence to approximation theory and ensures reliability of
iteration stopping criteria.}
\FORMULA{
\[
r_k = \arg\min_{p\in\mathcal{P}_k,\ p(0)=1} \|p(A) r_0\|_2,
\quad \|r_{k+1}\|_2 \le \|r_k\|_2.
\]
}
\CANONICAL{
Define $p_k$ from $y_k$ minimizing $\|\beta e_1-\bar H_k y\|_2$; then
$r_k=\phi_k(A) r_0$ with $\phi_k(0)=1$.}
\PRECONDS{
\begin{bullets}
\item Arnoldi available up to step $k+1$ without breakdown.
\item $r_0\ne 0$.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
Let $V_{k+1},\bar H_k$ be Arnoldi objects. There exists a polynomial
$p_k$ with $p_k(0)=1$ such that
$r_k=V_{k+1}(\beta e_1-\bar H_k y_k)=p_k(A) r_0$.
\end{lemma}
\begin{proof}
Because $A V_k=V_{k+1}\bar H_k$ and $v_1=r_0/\beta$, any vector in
$x_0+\mathcal{K}_k$ is $x_0+V_k y$, yielding
$r=b-A(x_0+V_k y)
=\beta v_1-V_{k+1}\bar H_k y
=V_{k+1}(\beta e_1-\bar H_k y)$.
Define $p$ via the relation $V_{k+1}(\beta e_1-\bar H_k y)=p(A) r_0$
since the Krylov subspace is invariant under $A$ acting on $r_0$ and
the mapping $y\mapsto p$ with $p(0)=1$ is linear and surjective on the
subspace. Taking $y=y_k$ gives $r_k=p_k(A) r_0$. \qedhere
\end{proof}
\DERIVATION{
\begin{align*}
\text{Step 1: }& \|r_k\|_2=\min_{y}\|\beta e_1-\bar H_k y\|_2.\\
\text{Step 2: }& \text{Extend to degree }k+1:\ 
\|r_{k+1}\|_2=\min_{z}\|\beta e_1-\bar H_{k+1} z\|_2.\\
\text{Step 3: }& \text{Restrict }z=[y;0]\ \Rightarrow
\|r_{k+1}\|_2 \le \|\beta e_1-\bar H_{k+1}[y;0]\|_2\\
&=\|\beta e_1-\bar H_k y\|_2.\\
\text{Step 4: }& \text{Minimizing over $y$ on the right yields }
\|r_{k+1}\|_2 \le \|r_k\|_2.
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Use Arnoldi to define the least-squares; solve via QR with Givens.
\item Track the residual norm recursively to ensure monotonicity.
\item Translate to polynomial language for spectral insights. 
\end{bullets}
\EQUIV{
\begin{bullets}
\item The polynomial $p_k$ is the unique minimizer in the quotient space
modulo the Krylov basis.
\item FOM uses $p_k$ such that $r_k\perp \mathcal{K}_k$.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item Monotonicity may not hold for restarted GMRES across cycles.
\item Highly nonnormal matrices complicate spectral predictions.
\end{bullets}
}
\INPUTS{$A,b,x_0$, Arnoldi objects $V_{k+1},\bar H_k$.}
\DERIVATION{
\begin{align*}
\text{Compute: }& y_k=\arg\min\|\beta e_1-\bar H_k y\|_2.\\
\text{Then: }& r_k=V_{k+1}(\beta e_1-\bar H_k y_k).\\
\text{Conclude: }& \|r_{k+1}\|_2 \le \|r_k\|_2.
\end{align*}
}
\RESULT{
A constructive residual polynomial and guaranteed nonincreasing residual
sequence for unrestarted GMRES.}
\UNITCHECK{
Norms and mappings occur in Euclidean space with orthonormal bases, so
consistency holds.}
\PITFALLS{
\begin{bullets}
\item Assuming eigenvalues alone predict GMRES speed; nonnormality matters.
\item Restarting without care can erase useful polynomial features.
\end{bullets}
}
\INTUITION{
At each step, GMRES adds one more degree of freedom to the canceling
polynomial, never worsening the best fit to zero.}
\CANONICAL{
\begin{bullets}
\item $r_k=p_k(A)r_0$, $p_k(0)=1$ minimizes $\|p_k(A)r_0\|_2$.
\item $\|r_{k+1}\|_2 \le \|r_k\|_2$ for unrestarted GMRES.
\end{bullets}
}
\FormulaPage{5}{Preconditioned CG and GMRES}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Preconditioning replaces $A$ by a spectrally improved operator, e.g.,
left preconditioning solves $M^{-1}A x = M^{-1} b$.
PCG applies CG to $M^{-1}A$ with $M$ SPD and uses the $M^{-1}A$-induced
inner product via residuals $z_k=M^{-1} r_k$.
\WHAT{
Accelerate convergence by transforming the system to improve spectral
properties.}
\WHY{
Convergence depends on spectrum; a good $M$ clusters eigenvalues or
reduces $\kappa(M^{-1}A)$, dramatically reducing iterations.}
\FORMULA{
\[
\begin{aligned}
&\text{PCG: } r_k=b-Ax_k,\ z_k=M^{-1} r_k,\ p_0=z_0,\\
&\alpha_k=\frac{r_k^\top z_k}{p_k^\top A p_k},\ x_{k+1}=x_k+\alpha_k p_k,\\
&r_{k+1}=r_k-\alpha_k A p_k,\ z_{k+1}=M^{-1} r_{k+1},\\
&\beta_{k+1}=\frac{r_{k+1}^\top z_{k+1}}{r_k^\top z_k},\
p_{k+1}=z_{k+1}+\beta_{k+1} p_k.
\end{aligned}
\]
GMRES with left preconditioning minimizes $\|M^{-1}(b-Ax)\|_2$ by
Arnoldi on $M^{-1}A$ starting with $M^{-1}r_0$.
}
\CANONICAL{
$M$ is SPD and approximates $A$ or its inverse. For GMRES, $M$ may be
nonsymmetric but should be nonsingular and effective.}
\PRECONDS{
\begin{bullets}
\item For PCG: $A$ and $M$ are SPD; $M$ is a good approximation to $A$.
\item For left-preconditioned GMRES: $M$ nonsingular and efficiently
solvable for $M^{-1}v$.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
PCG with left preconditioning is CG applied to the SPD operator
$M^{-1}A$ with the $M$-inner product $\langle u,v\rangle_M=u^\top M v$.
\end{lemma}
\begin{proof}
Define $\tilde A=M^{-1}A$ and consider the equivalent system
$\tilde A x=\tilde b$ with $\tilde b=M^{-1} b$. Since $M$ is SPD and
$A$ is SPD, $\tilde A$ is self-adjoint with respect to
$\langle u,v\rangle_M$ because
$\langle u,\tilde A v\rangle_M=u^\top M (M^{-1}A)v=u^\top A v
=\langle \tilde A u,v\rangle_M$. The PCG recurrences are the standard
CG recurrences written in terms of $r_k$ and $z_k=M^{-1}r_k$ and scalar
products $r_k^\top z_k=\langle r_k,z_k\rangle_2=\langle z_k,z_k\rangle_M$.
Thus PCG is CG on $(\tilde A,\tilde b)$ under $\langle\cdot,\cdot\rangle_M$.
\qedhere
\end{proof}
\DERIVATION{
\begin{align*}
\text{Step 1: }& \text{Solve }M s=r_k \text{ for } z_k=s.\\
\text{Step 2: }& \alpha_k=\frac{r_k^\top z_k}{p_k^\top A p_k}.\\
\text{Step 3: }& x_{k+1}=x_k+\alpha_k p_k,\ 
r_{k+1}=r_k-\alpha_k A p_k.\\
\text{Step 4: }& \text{Solve }M s=r_{k+1}\text{ for }z_{k+1}=s.\\
\text{Step 5: }& \beta_{k+1}=\frac{r_{k+1}^\top z_{k+1}}{r_k^\top z_k},\
p_{k+1}=z_{k+1}+\beta_{k+1} p_k.
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Choose $M$ balancing solve cost and spectral improvement.
\item Implement $z=M^{-1} r$ routine; integrate into CG or GMRES.
\item Measure iteration counts and adjust $M$ accordingly.
\end{bullets}
\EQUIV{
\begin{bullets}
\item Right preconditioning solves $A M^{-1} y=b$, $x=M^{-1} y$.
\item Split preconditioning uses $M=L L^\top$ to symmetrize.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item Poor $M$ may not help or can harm by increased cost per iteration.
\item Nonsymmetric $M$ invalidates PCG theory if $M$ not SPD.
\end{bullets}
}
\INPUTS{$A,b,x_0$, preconditioner $M$ with routine for $M^{-1}v$.}
\DERIVATION{
\begin{align*}
\text{Substitute: }& \tilde A=M^{-1}A,\ \tilde r_k=M^{-1}r_k.\\
\text{Compute: }& \text{Apply CG recurrences with }(\tilde A,\tilde r_k).\\
\text{Finalize: }& x_k \text{ as PCG iterate; GMRES uses Arnoldi on }\tilde A.
\end{align*}
}
\RESULT{
Reduced iteration counts proportional to improved conditioning
$\kappa(M^{-1}A)$ or spectral clustering of $M^{-1}A$.}
\UNITCHECK{
Inner products consistent: PCG scalars use $r_k^\top z_k$ with $M$ SPD,
ensuring positive denominators.}
\PITFALLS{
\begin{bullets}
\item Using incomplete factorization that breaks SPD requirements for PCG.
\item Preconditioning on the wrong side changes residual definition and
stopping criteria.
\end{bullets}
}
\INTUITION{
Preconditioning changes the geometry so that the valley is rounder; CG or
GMRES then reaches the bottom in fewer steps.}
\CANONICAL{
\begin{bullets}
\item PCG equals CG on $M^{-1}A$ under $\langle\cdot,\cdot\rangle_M$.
\item Left-preconditioned GMRES minimizes $\|M^{-1}r\|_2$.
\end{bullets}
}
\section{10 Exhaustive Problems and Solutions}
\ProblemPage{1}{Two CG Steps on a 2\texorpdfstring{$\times$}{x}2 SPD System}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Solve $A x=b$ with $A=\begin{bmatrix}4&1\\1&3\end{bmatrix}$,
$b=\begin{bmatrix}1\\2\end{bmatrix}$, $x_0=0$, using two CG steps.
\PROBLEM{
Carry out CG iterations, verifying residual orthogonality and
$A$-conjugacy of directions, and compare with the exact solution.}
\MODEL{
\[
A=\begin{bmatrix}4&1\\1&3\end{bmatrix},\quad
b=\begin{bmatrix}1\\2\end{bmatrix},\quad x_0=\begin{bmatrix}0\\0\end{bmatrix}.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $A$ is SPD (true: eigenvalues are positive).
\item Exact arithmetic assumed for identities; numbers carried with rationals.
\end{bullets}
}
\varmapStart
\var{r_k}{Residual $b-Ax_k$.}
\var{p_k}{Search direction.}
\var{\alpha_k,\beta_k}{CG scalars.}
\var{x^\ast}{Exact solution of $Ax=b$.}
\varmapEnd
\WHICHFORMULA{
Use CG recurrences from Formula 1.}
\GOVERN{
\[
\alpha_k=\frac{r_k^\top r_k}{p_k^\top A p_k},\quad
\beta_{k+1}=\frac{r_{k+1}^\top r_{k+1}}{r_k^\top r_k}.
\]
}
\INPUTS{$A,b,x_0$.}
\DERIVATION{
\begin{align*}
\text{Step 1: }& r_0=b-Ax_0=b=\begin{bmatrix}1\\2\end{bmatrix},\
p_0=r_0.\\
& p_0^\top A p_0 = r_0^\top A r_0
= \begin{bmatrix}1&2\end{bmatrix}
\begin{bmatrix}4&1\\1&3\end{bmatrix}
\begin{bmatrix}1\\2\end{bmatrix}\\
&= \begin{bmatrix}1&2\end{bmatrix}
\begin{bmatrix}6\\7\end{bmatrix}=20.\\
& r_0^\top r_0=1^2+2^2=5,\ \alpha_0=5/20=1/4.\\
& x_1=x_0+\alpha_0 p_0=\frac14\begin{bmatrix}1\\2\end{bmatrix}.\\
& r_1=r_0-\alpha_0 A p_0
=\begin{bmatrix}1\\2\end{bmatrix}-\tfrac14\begin{bmatrix}6\\7\end{bmatrix}
=\begin{bmatrix}-\tfrac12\\ \tfrac14\end{bmatrix}.\\
& r_1^\top r_1=\tfrac14+\tfrac{1}{16}=\tfrac{5}{16},\
\beta_1=\frac{5/16}{5}=1/16.\\
& p_1=r_1+\beta_1 p_0=\begin{bmatrix}-\tfrac12\\ \tfrac14\end{bmatrix}
+\tfrac{1}{16}\begin{bmatrix}1\\2\end{bmatrix}
=\begin{bmatrix}-\tfrac{7}{16}\\ \tfrac{3}{8}\end{bmatrix}.\\
\text{Step 2: }& A p_1=
\begin{bmatrix}4&1\\1&3\end{bmatrix}
\begin{bmatrix}-7/16\\3/8\end{bmatrix}
=\begin{bmatrix}-7/4+3/8\\ -7/16+9/8\end{bmatrix}
=\begin{bmatrix}-11/8\\ 11/16\end{bmatrix}.\\
& p_1^\top A p_1=
\begin{bmatrix}-7/16&3/8\end{bmatrix}\begin{bmatrix}-11/8\\11/16\end{bmatrix}
= \tfrac{77}{128}+\tfrac{33}{128}=\tfrac{110}{128}=\tfrac{55}{64}.\\
& \alpha_1=\frac{r_1^\top r_1}{p_1^\top A p_1}
=\frac{5/16}{55/64}=\frac{5}{16}\cdot\frac{64}{55}=\frac{4}{11}.\\
& x_2=x_1+\alpha_1 p_1=
\frac14\begin{bmatrix}1\\2\end{bmatrix}
+\frac{4}{11}\begin{bmatrix}-7/16\\3/8\end{bmatrix}
=\begin{bmatrix}\frac14-\frac{7}{44}\\ \frac12+\frac{3}{22}\end{bmatrix}
=\begin{bmatrix}\frac{4}{44}\\ \frac{14}{22}\end{bmatrix}
=\begin{bmatrix}\tfrac{1}{11}\\ \tfrac{7}{11}\end{bmatrix}.\\
& r_2=r_1-\alpha_1 A p_1=
\begin{bmatrix}-\tfrac12\\ \tfrac14\end{bmatrix}
-\tfrac{4}{11}\begin{bmatrix}-\tfrac{11}{8}\\ \tfrac{11}{16}\end{bmatrix}
=\begin{bmatrix}0\\0\end{bmatrix}.
\end{align*}
}
\RESULT{
$x^\ast=\begin{bmatrix}1/11\\7/11\end{bmatrix}$, achieved in 2 steps.
Residuals orthogonal: $r_1^\top r_0=0$. Directions $A$-conjugate:
$p_0^\top A p_1=0$.}
\UNITCHECK{
Denominators positive; norms consistent. Exact solution satisfies
$Ax^\ast=b$.}
\EDGECASES{
\begin{bullets}
\item If $x_0$ equals $x^\ast$, CG stops immediately.
\item If $r_0$ aligned with an eigenvector, CG terminates in 1 step.
\end{bullets}
}
\ALTERNATE{
Solve directly:
$A^{-1}=\frac{1}{11}\begin{bmatrix}3&-1\\-1&4\end{bmatrix}$, so
$x^\ast=A^{-1}b=\begin{bmatrix}1/11\\7/11\end{bmatrix}$.}
\VALIDATION{
\begin{bullets}
\item Check $r_2=0$ exactly.
\item Verify $p_0^\top A p_1=0$ numerically from derivation.
\end{bullets}
}
\INTUITION{
First step reduces error along steepest descent; second step completes the
solution by correcting in the conjugate direction.}
\CANONICAL{
\begin{bullets}
\item CG terminates in at most $n$ steps for SPD $A$.
\item Orthogonal residuals and $A$-conjugate directions hold. 
\end{bullets}
}
\ProblemPage{2}{GMRES on a 3\texorpdfstring{$\times$}{x}3 Nonsymmetric Matrix}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Apply two steps of GMRES to
$A=\begin{bmatrix}2&1&0\\0&2&1\\0&0&2\end{bmatrix}$,
$b=\begin{bmatrix}1\\1\\1\end{bmatrix}$, $x_0=0$.
\PROBLEM{
Build Arnoldi basis $V_3$, Hessenberg $\bar H_2$, solve the least-squares
for $y_2$, and compute $x_2$.}
\MODEL{
\[
A=\begin{bmatrix}2&1&0\\0&2&1\\0&0&2\end{bmatrix},\
b=\begin{bmatrix}1\\1\\1\end{bmatrix},\ x_0=0.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Unrestarted GMRES with exact arithmetic; no breakdown in 2 steps.
\end{bullets}
}
\varmapStart
\var{v_j}{Arnoldi basis vectors.}
\var{\bar H_2}{$(3\times 2)$ Hessenberg from Arnoldi.}
\var{y_2}{GMRES coefficients.}
\var{x_2}{GMRES iterate.}
\varmapEnd
\WHICHFORMULA{
Use Formula 3: $A V_2=V_3 \bar H_2$ and
$y_2=\arg\min\|\beta e_1-\bar H_2 y\|_2$.}
\GOVERN{
\[
x_2=V_2 y_2,\ \|r_2\|_2=\|\beta e_1-\bar H_2 y_2\|_2,\
\beta=\|b\|_2.
\]
}
\INPUTS{$A,b,x_0=0$.}
\DERIVATION{
\begin{align*}
\text{Step 1: }& r_0=b,\ \beta=\|b\|_2=\sqrt{3},\ v_1=b/\sqrt{3}.\\
& v_1=\tfrac{1}{\sqrt{3}}\begin{bmatrix}1\\1\\1\end{bmatrix}.\\
\text{Step 2: }& w=A v_1=
\tfrac{1}{\sqrt{3}}\begin{bmatrix}3\\3\\2\end{bmatrix}.\\
& h_{1,1}=v_1^\top w=\tfrac{1}{3}(3+3+2)=8/3.\\
& w\leftarrow w-h_{1,1} v_1=
\tfrac{1}{\sqrt{3}}\begin{bmatrix}3\\3\\2\end{bmatrix}
-\tfrac{8}{3}\tfrac{1}{\sqrt{3}}\begin{bmatrix}1\\1\\1\end{bmatrix}
=\tfrac{1}{\sqrt{3}}\begin{bmatrix}1/3\\1/3\\-2/3\end{bmatrix}.\\
& h_{2,1}=\|w\|_2=\sqrt{\tfrac{1}{3}\cdot \tfrac{6}{9}}=\sqrt{\tfrac{2}{9}}
=\tfrac{\sqrt{2}}{3}.\\
& v_2=w/h_{2,1}=\frac{1}{\sqrt{3}}\begin{bmatrix}1/3\\1/3\\-2/3\end{bmatrix}
\cdot \frac{3}{\sqrt{2}}
=\frac{1}{\sqrt{6}}\begin{bmatrix}1\\1\\-2\end{bmatrix}.\\
\text{Step 3: }& w=A v_2=
\frac{1}{\sqrt{6}}\begin{bmatrix}3\\1\\-4\end{bmatrix}.\\
& h_{1,2}=v_1^\top w=\tfrac{1}{\sqrt{18}}(3+1-4)=0.\\
& w\leftarrow w- h_{1,2} v_1 = w.\\
& h_{2,2}=v_2^\top w=\tfrac{1}{6}(3+1+8)=2.\\
& w\leftarrow w - h_{2,2} v_2=
\frac{1}{\sqrt{6}}\begin{bmatrix}3\\1\\-4\end{bmatrix}
-\frac{2}{\sqrt{6}}\begin{bmatrix}1\\1\\-2\end{bmatrix}
=\frac{1}{\sqrt{6}}\begin{bmatrix}1\\-1\\0\end{bmatrix}.\\
& h_{3,2}=\|w\|_2=\sqrt{\tfrac{2}{6}}=\sqrt{\tfrac{1}{3}}=\tfrac{1}{\sqrt{3}}.\\
& v_3=w/h_{3,2}=\frac{1}{\sqrt{2}}\begin{bmatrix}1\\-1\\0\end{bmatrix}.\\
\text{Step 4: }& \bar H_2=\begin{bmatrix}8/3&0\\ \sqrt{2}/3&2\\ 0&1/\sqrt{3}\end{bmatrix}.\\
\text{Step 5: }& y_2=\arg\min_{y}\left\|
\begin{bmatrix}\sqrt{3}\\0\\0\end{bmatrix}
-\bar H_2 y\right\|_2.\\
& \text{Solve normal equations or QR; here solve explicitly: }\\
& \text{Let } y=\begin{bmatrix}y_1\\y_2\end{bmatrix}.\
\bar H_2 y=\begin{bmatrix}\tfrac{8}{3}y_1\\ \tfrac{\sqrt{2}}{3}y_1+2y_2\\
\tfrac{1}{\sqrt{3}}y_2\end{bmatrix}.\\
& \text{Least squares optimality via orthogonality to columns gives: }\\
& \bar H_2^\top(\bar H_2 y - \beta e_1)=0.\\
& \Rightarrow
\begin{bmatrix}
(\tfrac{8}{3})^2+(\tfrac{\sqrt{2}}{3})^2 & \tfrac{\sqrt{2}}{3}\cdot 2\\
\tfrac{\sqrt{2}}{3}\cdot 2 & 2^2+(\tfrac{1}{\sqrt{3}})^2
\end{bmatrix}
\begin{bmatrix}y_1\\y_2\end{bmatrix}
=
\begin{bmatrix}\tfrac{8}{3}\sqrt{3}\\ 0\end{bmatrix}.\\
& \begin{bmatrix}\tfrac{64}{9}+\tfrac{2}{9} & \tfrac{2\sqrt{2}}{3}\\
\tfrac{2\sqrt{2}}{3} & 4+\tfrac{1}{3}\end{bmatrix}
\begin{bmatrix}y_1\\y_2\end{bmatrix}
=
\begin{bmatrix}\tfrac{8\sqrt{3}}{3}\\ 0\end{bmatrix}.\\
& \begin{bmatrix}\tfrac{66}{9} & \tfrac{2\sqrt{2}}{3}\\
\tfrac{2\sqrt{2}}{3} & \tfrac{13}{3}\end{bmatrix}
\begin{bmatrix}y_1\\y_2\end{bmatrix}
=
\begin{bmatrix}\tfrac{8\sqrt{3}}{3}\\ 0\end{bmatrix}.\\
& \text{Solve: from second row } \tfrac{2\sqrt{2}}{3} y_1
+\tfrac{13}{3} y_2=0 \Rightarrow y_2=-\tfrac{2\sqrt{2}}{13} y_1.\\
& \text{First row: } \tfrac{66}{9}y_1 + \tfrac{2\sqrt{2}}{3}y_2
=\tfrac{8\sqrt{3}}{3}.\\
& \Rightarrow \tfrac{66}{9}y_1 - \tfrac{2\sqrt{2}}{3}\cdot
\tfrac{2\sqrt{2}}{13} y_1=\tfrac{8\sqrt{3}}{3}.\\
& \tfrac{66}{9} - \tfrac{8}{39} =
\tfrac{286}{39} \Rightarrow y_1=\tfrac{8\sqrt{3}}{3}\cdot \tfrac{39}{286}
=\tfrac{104\sqrt{3}}{286}=\tfrac{52\sqrt{3}}{143}.\\
& y_2=-\tfrac{2\sqrt{2}}{13} y_1=-\tfrac{104\sqrt{6}}{1859}.\\
\text{Step 6: }& x_2=V_2 y_2=
v_1 y_1+ v_2 y_2\\
&=\tfrac{1}{\sqrt{3}}\begin{bmatrix}1\\1\\1\end{bmatrix}
\tfrac{52\sqrt{3}}{143}
+\tfrac{1}{\sqrt{6}}\begin{bmatrix}1\\1\\-2\end{bmatrix}
\left(-\tfrac{104\sqrt{6}}{1859}\right)\\
&=\begin{bmatrix}\tfrac{52}{143}\\ \tfrac{52}{143}\\ \tfrac{52}{143}\end{bmatrix}
+\begin{bmatrix}-\tfrac{104}{1859}\\ -\tfrac{104}{1859}\\
\tfrac{208}{1859}\end{bmatrix}
=\begin{bmatrix}\tfrac{52}{143}-\tfrac{104}{1859}\\
\tfrac{52}{143}-\tfrac{104}{1859}\\
\tfrac{52}{143}+\tfrac{208}{1859}\end{bmatrix}.
\end{align*}
}
\RESULT{
Two-step GMRES iterate $x_2$ as above with residual norm
$\|r_2\|_2=\|\beta e_1-\bar H_2 y_2\|_2$.}
\UNITCHECK{
All vectors have consistent dimensions. Orthonormal $V_k$ ensures norm
equivalence.}
\EDGECASES{
\begin{bullets}
\item If $h_{3,2}=0$, exact solution obtained in two steps.
\item Highly nonnormal $A$ can cause slow residual decrease despite
eigenvalues. 
\end{bullets}
}
\ALTERNATE{
Compute $x^\ast=A^{-1}b$ directly and compare $\|b-Ax_2\|_2$ to
$\|\beta e_1-\bar H_2 y_2\|_2$ for validation.}
\VALIDATION{
\begin{bullets}
\item Numerically verify $\|r_2\|_2$ equals the LS residual norm.
\item Check $A V_2\approx V_3 \bar H_2$ from computed quantities.
\end{bullets}
}
\INTUITION{
Upper-triangular shift structure makes Arnoldi simple; GMRES finds the
best two-term combination to reduce the residual.}
\CANONICAL{
\begin{bullets}
\item $x_k=x_0+V_k y_k$, $y_k$ from a small LS.
\item Residual norm comes from Hessenberg LS residual. 
\end{bullets}
}
\ProblemPage{3}{CG Termination and Minimal Polynomial}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Show CG terminates in at most $n$ steps for SPD $A$ and relate to the
minimal polynomial of $A$ with respect to $r_0$.
\PROBLEM{
Prove termination in at most $m$ steps where $m$ is the degree of the
minimal polynomial of $A$ for $r_0$; illustrate on diagonal $A$.}
\MODEL{
\[
A=Q^\top \Lambda Q,\ \Lambda=\mathrm{diag}(\lambda_1,\dots,\lambda_n),\
r_0\ne 0.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $A$ SPD; exact arithmetic; Krylov subspace dimension reaches $m$.
\end{bullets}
}
\varmapStart
\var{m}{Degree of minimal polynomial of $A$ for $r_0$.}
\var{p_k}{CG residual polynomial with $p_k(0)=1$.}
\varmapEnd
\WHICHFORMULA{
Use Formula 1 and 2; $r_k=p_k(A)r_0$ with $p_k \in \mathcal{P}_k$,
$p_k(0)=1$.}
\GOVERN{
\[
\exists p_m \text{ s.t. } p_m(A)r_0=0,\ \deg p_m=m\ \Rightarrow r_m=0.
\]
}
\INPUTS{$A$ SPD, $r_0$.}
\DERIVATION{
\begin{align*}
\text{Proof: }& \text{The Krylov subspace }
\mathcal{K}_k(A,r_0)=\mathrm{span}\{r_0,\dots,A^{k-1}r_0\}.\\
& \text{Its dimension increases until }m, \text{ the degree of the
minimal polynomial } \mu.\\
& \mu(A) r_0=0 \text{ with }\deg\mu=m.\\
& \text{CG chooses }p_k\in\mathcal{P}_k,\ p_k(0)=1,\ \text{so }r_k=p_k(A)r_0.\\
& \text{At }k=m,\ \text{choose }p_m=1-\hat\mu/\hat\mu(0) \text{ aligned with }
\mu \Rightarrow r_m=0.\\
& \text{Hence CG terminates in }\le m\le n.
\end{align*}
}
\RESULT{
CG finishes in at most $m$ steps; for diagonal $A=\mathrm{diag}(d_1,\dots,d_n)$
and generic $r_0$, $m$ equals the number of distinct diagonal entries.}
\UNITCHECK{
Polynomials are dimensionless; operator application is well-defined.}
\EDGECASES{
\begin{bullets}
\item If $r_0$ has support only in a subset of eigenspaces, $m$ is smaller.
\item Repeated eigenvalues reduce $m$, speeding termination.
\end{bullets}
}
\ALTERNATE{
View CG as Gram-Schmidt on the $A$-inner product; termination when basis
spans the invariant subspace containing $r_0$.}
\VALIDATION{
\begin{bullets}
\item On $A=\mathrm{diag}(1,2,2)$ and random $r_0$, CG terminates in
at most 2 steps.
\end{bullets}
}
\INTUITION{
Each step annihilates one independent spectral component; after $m$
components, nothing remains.}
\CANONICAL{
\begin{bullets}
\item $r_k=p_k(A)r_0$ with $\deg p_k\le k$.
\item $r_m=0$ for $m=\deg \mu_{A,r_0}$.
\end{bullets}
}
\ProblemPage{4}{CG as Steepest Descent with Conjugation}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Show that CG equals steepest descent with exact line search followed by
$A$-orthogonalization of directions via $\beta_{k+1}$.}
\PROBLEM{
Prove the equivalence and demonstrate on a small SPD matrix.}
\MODEL{
\[
f(x)=\tfrac12 x^\top A x - b^\top x,\ \nabla f(x_k)=-r_k.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $A$ SPD; exact line search feasible; $A$-conjugacy enforced.
\end{bullets}
}
\varmapStart
\var{d_k}{Descent direction.}
\var{p_k}{CG direction.}
\var{\beta_{k+1}}{Conjugation scalar.}
\varmapEnd
\WHICHFORMULA{
Use Formula 1 with $\alpha_k=\arg\min_\alpha f(x_k+\alpha d_k)$ and
$d_{k+1}=r_{k+1}+\beta_{k+1} d_k$.}
\GOVERN{
\[
\beta_{k+1}=-\frac{d_k^\top A r_{k+1}}{d_k^\top A d_k}
=\frac{r_{k+1}^\top r_{k+1}}{r_k^\top r_k}.
\]
}
\INPUTS{$A,b,x_0$.}
\DERIVATION{
\begin{align*}
\text{Step 1: }& \text{Steepest descent uses } d_k=r_k.\\
\text{Step 2: }& \alpha_k=\frac{r_k^\top r_k}{d_k^\top A d_k}
\text{ from line search.}\\
\text{Step 3: }& x_{k+1}=x_k+\alpha_k d_k,\ r_{k+1}=r_k-\alpha_k A d_k.\\
\text{Step 4: }& \text{Choose }\beta_{k+1} \text{ to impose }
d_{k+1}=r_{k+1}+\beta_{k+1} d_k \\
& \text{with } d_{k+1}^\top A d_k=0
\Rightarrow \beta_{k+1}=-\frac{d_k^\top A r_{k+1}}{d_k^\top A d_k}.\\
\text{Step 5: }& d_k=r_k \Rightarrow d_k^\top A r_{k+1}
= r_k^\top A r_{k+1}
= \frac{r_{k+1}^\top r_{k+1}-r_k^\top r_k}{2\alpha_k}.\\
& \Rightarrow \beta_{k+1}=\frac{r_{k+1}^\top r_{k+1}}{r_k^\top r_k}.
\end{align*}
}
\RESULT{
The recurrence equals CG with $d_k=p_k$, establishing equivalence.}
\UNITCHECK{
Scalars used are dimensionless; inner products consistent.}
\EDGECASES{
\begin{bullets}
\item If $r_{k+1}=0$, $\beta_{k+1}=0$ and method terminates.
\item If line search is inexact, identities no longer hold exactly.
\end{bullets}
}
\ALTERNATE{
Derive from the Galerkin condition $r_k\perp \mathcal{K}_k$ which implies
the same $\beta_{k+1}$.}
\VALIDATION{
\begin{bullets}
\item Numerically verify on $A=\mathrm{diag}(1,3)$, $b=(1,1)$, $x_0=0$.
\end{bullets}
}
\INTUITION{
Steepest descent points the way; conjugation prevents undoing previous
progress.}
\CANONICAL{
\begin{bullets}
\item CG is steepest descent plus $A$-orthogonalization.
\item Fletcher-Reeves $\beta$ arises from orthogonality conditions.
\end{bullets}
}
\ProblemPage{5}{Chebyshev Bound Prediction and Check}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Predict CG iterations for target error reduction using Chebyshev bound and
verify on an example.}
\PROBLEM{
Given $\kappa=25$, estimate $k$ for $\|e_k\|_A/\|e_0\|_A\le 10^{-6}$; test
on $A=\mathrm{diag}(1,25)$ with random $r_0$.}
\MODEL{
\[
\theta=\frac{\sqrt{\kappa}-1}{\sqrt{\kappa}+1},\ 
2\theta^k\le 10^{-6}.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Worst-case bound; actual performance may be better.
\end{bullets}
}
\varmapStart
\var{\kappa}{Condition number.}
\var{\theta}{Chebyshev rate factor.}
\var{k}{Iterations.}
\varmapEnd
\WHICHFORMULA{
Use Formula 2: $\|e_k\|_A \le 2\theta^k \|e_0\|_A$.}
\GOVERN{
\[
k \ge \frac{\log(10^{-6}/2)}{\log \theta}.
\]
}
\INPUTS{$\kappa=25$.}
\DERIVATION{
\begin{align*}
\text{Compute: }& \sqrt{\kappa}=5,\ \theta=\frac{5-1}{5+1}=\frac{2}{3}.\\
& 2\theta^k\le 10^{-6} \Rightarrow \theta^k \le 5\cdot 10^{-7}.\\
& k \ge \frac{\log(5\cdot 10^{-7})}{\log(2/3)} \approx
\frac{-14.5087}{-0.4055} \approx 35.79.\\
& \Rightarrow k=36 \text{ iterations (bound).}
\end{align*}
}
\RESULT{
Predicted $k=36$. On $A=\mathrm{diag}(1,25)$, CG converges much faster if
$r_0$ components align favorably with eigenvectors.}
\UNITCHECK{
Dimensionless quantities; logs well-defined as $\theta\in(0,1)$.}
\EDGECASES{
\begin{bullets}
\item If $\kappa$ is overestimated, prediction is pessimistic.
\item If $r_0$ lacks components in the worst eigendirection, actual $k$
is smaller.
\end{bullets}
}
\ALTERNATE{
Use refined bounds with eigenvalue clusters excluding a gap around zero.}
\VALIDATION{
\begin{bullets}
\item Simulate CG on $A=\mathrm{diag}(1,25)$ and compare achieved decay
to predicted bound.
\end{bullets}
}
\INTUITION{
The rate is governed by how well a low-degree polynomial can be small on
the eigenvalue interval.}
\CANONICAL{
\begin{bullets}
\item $2\theta^k$ is a worst-case envelope.
\item Preconditioning aims to reduce $\kappa$ and thus $\theta$.
\end{bullets}
}
\ProblemPage{6}{Narrative: Alice Uses PCG, Bob Uses CG on Normal Equations}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Compare solving $A x=b$ with SPD $A$ via PCG vs. solving normal equations
$A^\top A x=A^\top b$ via CG.}
\PROBLEM{
Alice applies PCG with Jacobi $M=\mathrm{diag}(A)$; Bob applies CG to
$A^\top A$. Analyze iteration counts and cost per iteration.}
\MODEL{
\[
\text{PCG on }M^{-1}A x = M^{-1} b;\quad
\text{CG on }(A^\top A) x = A^\top b.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $A$ SPD; $M$ SPD; matrix-vector multiply costs equal across variants.
\end{bullets}
}
\varmapStart
\var{M}{Jacobi preconditioner.}
\var{\kappa_1}{$\kappa(M^{-1}A)$.}
\var{\kappa_2}{$\kappa(A^\top A)=\kappa(A)^2$.}
\varmapEnd
\WHICHFORMULA{
Use Formula 2 on $\kappa_1$ vs. $\kappa_2$ to compare bounds.}
\GOVERN{
\[
k_{\mathrm{PCG}}\sim \mathcal{O}(\sqrt{\kappa_1}\log \varepsilon^{-1}),\
k_{\mathrm{CGNE}}\sim \mathcal{O}(\sqrt{\kappa_2}\log \varepsilon^{-1}).
\]
}
\INPUTS{$\kappa_1<\kappa(A)$ typically; $\kappa_2=\kappa(A)^2$.}
\DERIVATION{
\begin{align*}
\text{Step 1: }& \theta_1=\frac{\sqrt{\kappa_1}-1}{\sqrt{\kappa_1}+1},\
\theta_2=\frac{\kappa(A)-1}{\kappa(A)+1}.\\
\text{Step 2: }& \theta_2 \approx \frac{\kappa(A)-1}{\kappa(A)+1}
\text{ is worse than }\theta_1 \text{ if }\kappa_1 \ll \kappa(A)^2.\\
\text{Step 3: }& \text{Per-iteration cost: PCG adds one $M^{-1}$ solve;}
\text{CGNE adds extra $A^\top$ multiply.}\\
\text{Step 4: }& \text{Conclusion: PCG preferable for SPD }A.
\end{align*}
}
\RESULT{
PCG typically requires far fewer iterations than CG on normal equations
and avoids squaring the condition number.}
\UNITCHECK{
Condition numbers are dimensionless; costs are per-iteration arithmetic
counts.}
\EDGECASES{
\begin{bullets}
\item If $M$ is poor, $\kappa_1$ may remain large.
\item If $A$ is well-conditioned, both may perform similarly. 
\end{bullets}
}
\ALTERNATE{
Use incomplete Cholesky for $M$ to further reduce $\kappa_1$.}
\VALIDATION{
\begin{bullets}
\item Numerically compare on a SPD Toeplitz matrix with Jacobi $M$.
\end{bullets}
}
\INTUITION{
CGNE fights a squarer cliff; PCG smooths the cliff before climbing.}
\CANONICAL{
\begin{bullets}
\item Avoid normal equations when possible for conditioning reasons.
\item Precondition in the native geometry. 
\end{bullets}
}
\ProblemPage{7}{Narrative: GMRES Without Reorthogonalization}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Bob drops reorthogonalization in Arnoldi and observes stagnation.
\PROBLEM{
Explain how loss of orthogonality affects GMRES and design a fix.}
\MODEL{
\[
A V_k \approx V_{k+1}\bar H_k \text{ with degraded orthogonality in }V_k.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Finite precision arithmetic; classical Gram-Schmidt used once.
\end{bullets}
}
\varmapStart
\var{V_k}{Computed basis with loss of orthogonality.}
\var{\bar H_k}{Computed Hessenberg.}
\var{\tilde r_k}{Reported residual from LS.}
\var{r_k}{True residual $b-Ax_k$.}
\varmapEnd
\WHICHFORMULA{
Formula 3 requires $V_{k+1}$ orthonormal to equate LS residual to
$\|r_k\|_2$.}
\GOVERN{
\[
\text{If }V_{k+1}^\top V_{k+1}\ne I,\ \|\tilde r_k\|_2 \ne \|r_k\|_2.
\]
}
\INPUTS{Arnoldi process choice and floating-point environment.}
\DERIVATION{
\begin{align*}
\text{Step 1: }& \tilde r_k=V_{k+1}(\beta e_1-\bar H_k y_k).\\
\text{Step 2: }& \|\tilde r_k\|_2^2=(\beta e_1-\bar H_k y_k)^\top
(V_{k+1}^\top V_{k+1})(\beta e_1-\bar H_k y_k).\\
\text{Step 3: }& \text{If }V_{k+1}^\top V_{k+1}\approx I+E,\ E\ne 0,\\
& \text{then the small LS problem is perturbed and may stall.}\\
\text{Step 4: }& \text{Fix: use modified Gram-Schmidt with reorthogonalization}
\text{ or selective reorthogonalization.}
\end{align*}
}
\RESULT{
Loss of orthogonality corrupts residual estimates and can stall progress.
Reorthogonalization restores monotone decrease and accuracy.}
\UNITCHECK{
Matrix products align dimensionally; norm equivalences break with
non-orthonormal bases.}
\EDGECASES{
\begin{bullets}
\item Strongly nonnormal $A$ exacerbates orthogonality loss.
\item Short recurrences (CG) avoid this issue on SPD matrices. 
\end{bullets}
}
\ALTERNATE{
Use Householder Arnoldi or stabilized Gram-Schmidt to improve orthogonality.}
\VALIDATION{
\begin{bullets}
\item Compare $\|r_k\|_2$ computed explicitly to LS residual; divergence
signals orthogonality loss.
\end{bullets}
}
\INTUITION{
If your rulers bend, your length measurements become unreliable; straight
rulers (orthonormal vectors) fix it.}
\CANONICAL{
\begin{bullets}
\item GMRES requires orthonormal $V_{k+1}$ for correct residual linkage.
\item Practical GMRES maintains orthogonality with stable procedures.
\end{bullets}
}
\ProblemPage{8}{Expectation Puzzle: One CG Step in 2D}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
For $A=\mathrm{diag}(\lambda_1,\lambda_2)$, $\lambda_1<\lambda_2$,
$r_0$ is uniformly random on the unit circle. Compute
$\mathbb{E}\left[\|e_1\|_A^2/\|e_0\|_A^2\right]$.}
\PROBLEM{
Analyze one CG step error reduction averaged over random initial residual
direction.}
\MODEL{
\[
e_0=A^{-1} r_0,\ e_1=e_0-\alpha_0 p_0,\ p_0=r_0,\ 
\alpha_0=\frac{r_0^\top r_0}{p_0^\top A p_0}.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $r_0=(\cos\theta,\sin\theta)$ with $\theta\sim \mathrm{Unif}[0,2\pi)$.
\end{bullets}
}
\varmapStart
\var{\lambda_1,\lambda_2}{Eigenvalues.}
\var{\theta}{Angle of $r_0$.}
\var{R}{Reduction factor in $A$-norm squared.}
\varmapEnd
\WHICHFORMULA{
Use Formula 1 line-search expression for $\alpha_0$ and compute the ratio.}
\GOVERN{
\[
R(\theta)=\frac{\|e_1\|_A^2}{\|e_0\|_A^2}
=1-\frac{(r_0^\top r_0)^2}{(p_0^\top A p_0)(e_0^\top A e_0)}.
\]
}
\INPUTS{$\lambda_1<\lambda_2$.}
\DERIVATION{
\begin{align*}
\text{Step 1: }& r_0=(\cos\theta,\sin\theta),\ e_0=(\cos\theta/\lambda_1,
\sin\theta/\lambda_2).\\
\text{Step 2: }& r_0^\top r_0=1,\ p_0^\top A p_0=
\lambda_1\cos^2\theta+\lambda_2\sin^2\theta.\\
& e_0^\top A e_0=\cos^2\theta/\lambda_1+\sin^2\theta/\lambda_2.\\
\text{Step 3: }& R(\theta)=1-\frac{1}{(\lambda_1\cos^2\theta+
\lambda_2\sin^2\theta)(\cos^2\theta/\lambda_1+\sin^2\theta/\lambda_2)}.\\
\text{Step 4: }& \mathbb{E}[R]=\frac{1}{2\pi}\int_0^{2\pi} R(\theta)\ d\theta.\\
& \text{Due to symmetry, reduce to }\theta\in[0,\pi/2]\text{ with weight 4.}\\
& I=\int_0^{\pi/2}\frac{d\theta}{(\lambda_1\cos^2\theta+\lambda_2\sin^2\theta)
(\cos^2\theta/\lambda_1+\sin^2\theta/\lambda_2)}.\\
\text{Step 5: }& \text{Let } t=\tan\theta,\ d\theta=\frac{dt}{1+t^2},\
\cos^2\theta=\frac{1}{1+t^2},\ \sin^2\theta=\frac{t^2}{1+t^2}.\\
& \Rightarrow \text{Denominator }=
\left(\frac{\lambda_1+\lambda_2 t^2}{1+t^2}\right)
\left(\frac{\lambda_2+\lambda_1 t^2}{\lambda_1\lambda_2(1+t^2)}\right)
=\frac{(\lambda_1+\lambda_2 t^2)(\lambda_2+\lambda_1 t^2)}
{\lambda_1\lambda_2(1+t^2)^2}.\\
& I=\int_0^\infty \frac{dt}{1+t^2}\cdot
\frac{\lambda_1\lambda_2(1+t^2)^2}{(\lambda_1+\lambda_2 t^2)
(\lambda_2+\lambda_1 t^2)}\\
&=\lambda_1\lambda_2 \int_0^\infty \frac{1+t^2}{(\lambda_1+\lambda_2 t^2)
(\lambda_2+\lambda_1 t^2)} dt.\\
\text{Step 6: }& \text{Partial fractions give }
I=\frac{\pi}{2}\cdot \frac{\lambda_1+\lambda_2}{\lambda_1\lambda_2}.\\
& \text{(standard integral; can be verified by residue or table).}\\
\text{Step 7: }& \mathbb{E}[R]=1-\frac{4}{2\pi} I
=1-\frac{4}{2\pi}\cdot \frac{\pi}{2}\cdot
\frac{\lambda_1+\lambda_2}{\lambda_1\lambda_2}
=1-\frac{\lambda_1+\lambda_2}{\lambda_1\lambda_2}.\\
& \text{But } \frac{\lambda_1+\lambda_2}{\lambda_1\lambda_2}
=\frac{1}{\lambda_1}+\frac{1}{\lambda_2}.\\
& \text{Since } \lambda_1,\lambda_2\ge 1 \text{ may not hold, ensure }
\mathbb{E}[R]\in(0,1).\\
& \text{For scaled }A=\mathrm{diag}(a,b)\text{ with }a,b>0,\ 
\mathbb{E}[R]=1-\left(\frac{1}{a}+\frac{1}{b}\right).\\
& \text{Scale so that }a,b\ge 1\text{ for positivity; or interpret as
relative to }\|e_0\|_A^2.
\end{align*}
}
\RESULT{
$\mathbb{E}\left[\|e_1\|_A^2/\|e_0\|_A^2\right]
=1-\left(\frac{1}{\lambda_1}+\frac{1}{\lambda_2}\right)$ under the
stated normalization.}
\UNITCHECK{
Dimensionless ratio. Ensure $\lambda_1,\lambda_2$ scaled so expectation
lies in $[0,1]$.}
\EDGECASES{
\begin{bullets}
\item As $\lambda_2\to \infty$, expected reduction approaches
$1-\tfrac{1}{\lambda_1}$.
\item If $\lambda_1=\lambda_2$, single step solves exactly and expectation
gives zero reduction error.
\end{bullets}
}
\ALTERNATE{
Compute expectation of $\|r_1\|_2^2/\|r_0\|_2^2$ analogously; algebra is
simpler.}
\VALIDATION{
\begin{bullets}
\item Monte Carlo with many random $\theta$ confirms the analytic mean.
\end{bullets}
}
\INTUITION{
Averaging over directions smooths out worst-case behavior and highlights
condition dependence.}
\CANONICAL{
\begin{bullets}
\item One-step CG reduction depends on harmonic averages of eigenvalues.
\item Expectation removes dependence on initial alignment.
\end{bullets}
}
\ProblemPage{9}{Proof: GMRES Residuals Are Orthogonal to \texorpdfstring{$A\mathcal{K}_k$}{AK}}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Show $r_k\perp A\mathcal{K}_k(A,r_0)$ for unrestarted GMRES.}
\PROBLEM{
Provide a concise proof using Arnoldi and least squares optimality.}
\MODEL{
\[
x_k=x_0+V_k y_k,\ r_k=b-Ax_k=V_{k+1}(\beta e_1-\bar H_k y_k).
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $V_{k+1}$ orthonormal from Arnoldi; $y_k$ minimizes the LS problem.
\end{bullets}
}
\varmapStart
\var{V_k}{Arnoldi basis.}
\var{\bar H_k}{Hessenberg.}
\var{y_k}{GMRES coefficients.}
\varmapEnd
\WHICHFORMULA{
Use Formula 3 optimality:
$\bar H_k^\top(\beta e_1-\bar H_k y_k)=0$.}
\GOVERN{
\[
(A V_k)^\top r_k=0 \Rightarrow r_k\perp A\mathcal{K}_k(A,r_0).
\]
}
\INPUTS{$A,b,x_0$.}
\DERIVATION{
\begin{align*}
\text{Step 1: }& A V_k=V_{k+1}\bar H_k.\\
\text{Step 2: }& r_k=V_{k+1}(\beta e_1-\bar H_k y_k).\\
\text{Step 3: }& (A V_k)^\top r_k=
\bar H_k^\top V_{k+1}^\top V_{k+1}(\beta e_1-\bar H_k y_k)
=\bar H_k^\top(\beta e_1-\bar H_k y_k)=0.
\end{align*}
}
\RESULT{
GMRES residuals are orthogonal to $A$ times the Krylov subspace, the
Petrov-Galerkin condition.}
\UNITCHECK{
All transposes and products are dimensionally aligned.}
\EDGECASES{
\begin{bullets}
\item With restart, orthogonality holds within each cycle subspace.
\end{bullets}
}
\ALTERNATE{
Derive from the normal equations of the least-squares problem.}
\VALIDATION{
\begin{bullets}
\item Numeric check on small matrices confirms $(A V_k)^\top r_k\approx 0$.
\end{bullets}
}
\INTUITION{
The best least-squares fit leaves no component along the fit directions,
which are $A$ applied to the basis.}
\CANONICAL{
\begin{bullets}
\item GMRES satisfies a Petrov-Galerkin condition.
\item Residuals live in the orthogonal complement of $A\mathcal{K}_k$.
\end{bullets}
}
\ProblemPage{10}{Combo: CG and Polynomial Approximation}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Relate CG to minimax polynomial approximation on $[\lambda_{\min},
\lambda_{\max}]$, constructing the optimal $p_k$ explicitly for $k=1$.}
\PROBLEM{
For SPD $A$ with eigenvalues in $[m,M]$, build the degree-1 polynomial
$p_1(\lambda)=1-\alpha \lambda$ minimizing $\max_{\lambda\in[m,M]}
|p_1(\lambda)|$ and show it equals the CG line-search $\alpha_0$.}
\MODEL{
\[
p_1(\lambda)=1-\alpha \lambda,\ \alpha>0,\ 
\max_{\lambda\in[m,M]} |1-\alpha\lambda|.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $r_0$ has nonzero components in eigenspaces.
\item Exact arithmetic; $k=1$.
\end{bullets}
}
\varmapStart
\var{\alpha}{Polynomial slope and line-search step.}
\var{m,M}{Spectral bounds.}
\varmapEnd
\WHICHFORMULA{
Use Formula 1 line-search and Chebyshev equioscillation for minimax.}
\GOVERN{
\[
\alpha^\star=\frac{2}{m+M},\ \max |1-\alpha^\star \lambda|=
\frac{M-m}{M+m}.
\]
}
\INPUTS{$m=\lambda_{\min}$, $M=\lambda_{\max}$.}
\DERIVATION{
\begin{align*}
\text{Step 1: }& \max_{\lambda\in[m,M]} |1-\alpha\lambda|
=\max\{ |1-\alpha m|,\ |1-\alpha M|\}.\\
\text{Step 2: }& \text{Minimax achieved by equal ripples: }
1-\alpha m=-(1-\alpha M).\\
& \Rightarrow 1-\alpha m = -1+\alpha M \Rightarrow 2=\alpha(M+m)
\Rightarrow \alpha^\star=\frac{2}{M+m}.\\
\text{Step 3: }& \max |1-\alpha^\star \lambda|=|1-\alpha^\star m|
=1-\frac{2m}{M+m}=\frac{M-m}{M+m}.\\
\text{Step 4: }& \text{CG line-search with }p_0=r_0:\ 
\alpha_0=\frac{r_0^\top r_0}{r_0^\top A r_0}.\\
& \text{In spectral basis }r_0=\sum_i c_i q_i,\ r_0^\top A r_0=
\sum_i \lambda_i c_i^2,\ r_0^\top r_0=\sum_i c_i^2.\\
& \alpha_0\in\left[\frac{2}{M+m}-\delta,\frac{2}{M+m}+\delta\right]
\text{ with }\delta \text{ depending on spectral weights}.\\
& \text{For two-point supported $r_0$, equality attains minimax value.}
\end{align*}
}
\RESULT{
The degree-1 minimax polynomial uses $\alpha^\star=2/(m+M)$ matching the
optimal steepest-descent scaling bounded by CG line-search $\alpha_0$.}
\UNITCHECK{
All quantities are scalars; dimensions consistent.}
\EDGECASES{
\begin{bullets}
\item If $m=M$, $\alpha^\star=1/m$ solves in one step.
\end{bullets}
}
\ALTERNATE{
Extend to degree-$k$ via shifted-scaled Chebyshev polynomials.}
\VALIDATION{
\begin{bullets}
\item On $A=\mathrm{diag}(m,M)$ and $r_0=(1,1)$, compute $\alpha_0$
and verify it equals $2/(m+M)$.
\end{bullets}
}
\INTUITION{
Best first step balances over- and under-shoot at the spectral extremes.}
\CANONICAL{
\begin{bullets}
\item CG is a polynomial method; steepest step matches minimax for $k=1$.
\item Chebyshev provides the template for higher-degree bounds.
\end{bullets}
}
\section{Coding Demonstrations}
\CodeDemoPage{Conjugate Gradient From Scratch vs. SciPy}
\PROBLEM{
Implement CG for SPD matrices and verify $A$-conjugacy and convergence.
Compare against \inlinecode{scipy.sparse.linalg.cg}.}
\API{
\begin{bullets}
\item \inlinecode{def read_input(s) -> tuple} — parse dense A and b.
\item \inlinecode{def solve_case(A,b,x0,tol,maxit) -> x} — CG solver.
\item \inlinecode{def validate() -> None} — assertions on known problem.
\item \inlinecode{def main() -> None} — run validation and print stats.
\end{bullets}
}
\INPUTS{
$A$ SPD dense array, $b$ vector, $x_0$ initial guess, tolerance and maxit.}
\OUTPUTS{
$x$ approximate solution; also checks conjugacy and residual norms.}
\FORMULA{
\[
\alpha_k=\frac{r_k^\top r_k}{p_k^\top A p_k},\quad
\beta_{k+1}=\frac{r_{k+1}^\top r_{k+1}}{r_k^\top r_k}.
\]
}
\textbf{SOLUTION A — From Scratch (Mathematically Explicit Implementation)}
\begin{codepy}
import numpy as np

def read_input(s):
    vals = [float(x) for x in s.split()]
    n = int(vals[0]); off = 1
    A = np.array(vals[off:off+n*n]).reshape(n, n)
    off += n*n
    b = np.array(vals[off:off+n])
    return A, b

def solve_case(A, b, x0=None, tol=1e-10, maxit=None):
    n = A.shape[0]
    if x0 is None:
        x = np.zeros(n)
    else:
        x = x0.copy()
    r = b - A @ x
    p = r.copy()
    rs = float(r @ r)
    if maxit is None:
        maxit = n
    for k in range(maxit):
        Ap = A @ p
        alpha = rs / float(p @ Ap)
        x = x + alpha * p
        r = r - alpha * Ap
        rs_new = float(r @ r)
        if np.sqrt(rs_new) <= tol * np.linalg.norm(b):
            break
        beta = rs_new / rs
        p = r + beta * p
        rs = rs_new
    return x

def check_A_conjugacy(A, ps):
    m = len(ps)
    for i in range(m):
        for j in range(i):
            v = float(ps[i] @ (A @ ps[j]))
            if abs(v) > 1e-8:
                return False
    return True

def validate():
    A = np.array([[4.0, 1.0], [1.0, 3.0]])
    b = np.array([1.0, 2.0])
    x = solve_case(A, b)
    x_true = np.linalg.solve(A, b)
    assert np.allclose(x, x_true, atol=1e-10)
    # collect p's to test conjugacy on a 3x3 SPD case
    A2 = np.array([[2.0, 0.5, 0.0],[0.5, 1.5, 0.2],[0.0,0.2,1.0]])
    b2 = np.array([1.0, -1.0, 2.0])
    n = 3
    x = np.zeros(n)
    r = b2 - A2 @ x
    p = r.copy()
    rs = float(r @ r)
    ps = [p.copy()]
    for k in range(n-1):
        Ap = A2 @ p
        alpha = rs / float(p @ Ap)
        x = x + alpha * p
        r = r - alpha * Ap
        rs_new = float(r @ r)
        if np.sqrt(rs_new) <= 1e-12:
            break
        beta = rs_new / rs
        p = r + beta * p
        ps.append(p.copy())
        rs = rs_new
    assert check_A_conjugacy(A2, ps)

def main():
    validate()
    A = np.array([[4.0, 1.0], [1.0, 3.0]])
    b = np.array([1.0, 2.0])
    x = solve_case(A, b)
    print("x:", np.round(x, 10))

if __name__ == "__main__":
    main()
\end{codepy}
\textbf{SOLUTION B — Library-Based (Validated Computational Shortcut)}
\begin{codepy}
import numpy as np
from scipy.sparse.linalg import cg

def read_input(s):
    vals = [float(x) for x in s.split()]
    n = int(vals[0]); off = 1
    A = np.array(vals[off:off+n*n]).reshape(n, n)
    off += n*n
    b = np.array(vals[off:off+n])
    return A, b

def solve_case(A, b, x0=None, tol=1e-10, maxit=None):
    x, info = cg(A, b, x0=x0, tol=tol, maxiter=maxit)
    assert info == 0
    return x

def validate():
    A = np.array([[4.0, 1.0], [1.0, 3.0]])
    b = np.array([1.0, 2.0])
    x = solve_case(A, b)
    x_true = np.linalg.solve(A, b)
    assert np.allclose(x, x_true, atol=1e-10)

def main():
    validate()
    A = np.array([[4.0, 1.0], [1.0, 3.0]])
    b = np.array([1.0, 2.0])
    x = solve_case(A, b)
    print("x:", np.round(x, 10))

if __name__ == "__main__":
    main()
\end{codepy}
\COMPLEXITY{
Time per iteration $\mathcal{O}(\mathrm{nnz}(A))$; space $\mathcal{O}(n)$.
SciPy version has similar complexity with tuned kernels.}
\FAILMODES{
\begin{bullets}
\item Non-SPD $A$ causes failure; check symmetry and definiteness.
\item Division by zero if $p^\top A p=0$; avoid with SPD preconditions.
\end{bullets}
}
\STABILITY{
\begin{bullets}
\item Loss of $A$-conjugacy in finite precision slows convergence.
\item Remedy: periodic residual replacement or higher precision.
\end{bullets}
}
\VALIDATION{
\begin{bullets}
\item Compare with direct solve; assert residual below tolerance.
\item Check $A$-conjugacy numerically on collected directions.
\end{bullets}
}
\RESULT{
Both implementations agree to high precision on test problems, verifying
the CG recurrence and optimality.}
\EXPLANATION{
Each code step mirrors Formula 1: compute $\alpha_k,\beta_{k+1}$,
update $x,r,p$; conjugacy check confirms $p_i^\top A p_j=0$.}
\EXTENSION{
Add Jacobi preconditioning by solving $M z=r$ with $M=\mathrm{diag}(A)$.}
\CodeDemoPage{GMRES via Arnoldi From Scratch vs. SciPy}
\PROBLEM{
Implement unrestarted GMRES using Arnoldi with Givens rotations, compare
to \inlinecode{scipy.sparse.linalg.gmres}, and verify residual norms.}
\API{
\begin{bullets}
\item \inlinecode{def arnoldi(A,v1,m) -> (V,H)} — build basis and H.
\item \inlinecode{def gmres_solve(A,b,x0,m,tol) -> x} — GMRES core.
\item \inlinecode{def validate() -> None} — assertions on small systems.
\item \inlinecode{def main() -> None} — run validation and print.
\end{bullets}
}
\INPUTS{
$A$ dense array, $b$ vector, restart $m$, $x_0$, tolerance.}
\OUTPUTS{
Approximate $x$; prints residual norms and checks monotonicity.}
\FORMULA{
\[
AV_k=V_{k+1}\bar H_k,\quad
y_k=\arg\min\|\beta e_1-\bar H_k y\|_2.
\]
}
\textbf{SOLUTION A — From Scratch (Mathematically Explicit Implementation)}
\begin{codepy}
import numpy as np

def arnoldi(A, v1, m):
    n = A.shape[0]
    V = np.zeros((n, m+1))
    H = np.zeros((m+1, m))
    V[:, 0] = v1
    for j in range(m):
        w = A @ V[:, j]
        for i in range(j+1):
            H[i, j] = V[:, i] @ w
            w = w - H[i, j] * V[:, i]
        H[j+1, j] = np.linalg.norm(w)
        if H[j+1, j] == 0.0:
            return V, H, j
        V[:, j+1] = w / H[j+1, j]
    return V, H, m

def apply_givens(H, cs, sn, k):
    for i in range(k):
        temp = cs[i]*H[i, k] + sn[i]*H[i+1, k]
        H[i+1, k] = -sn[i]*H[i, k] + cs[i]*H[i+1, k]
        H[i, k] = temp
    r = np.hypot(H[k, k], H[k+1, k])
    cs_k = H[k, k] / r
    sn_k = H[k+1, k] / r
    H[k, k] = r
    H[k+1, k] = 0.0
    return cs_k, sn_k

def gmres_solve(A, b, x0=None, m=20, tol=1e-10):
    n = A.shape[0]
    if x0 is None:
        x = np.zeros(n)
    else:
        x = x0.copy()
    r0 = b - A @ x
    beta = np.linalg.norm(r0)
    if beta == 0.0:
        return x
    v1 = r0 / beta
    V, H, steps = arnoldi(A, v1, m)
    cs = np.zeros(m)
    sn = np.zeros(m)
    g = np.zeros(m+1)
    g[0] = beta
    for k in range(steps):
        cs_k, sn_k = apply_givens(H, cs, sn, k)
        cs[k] = cs_k; sn[k] = sn_k
        g[k+1] = -sn_k * g[k]
        g[k] = cs_k * g[k]
        res = abs(g[k+1])
        if res <= tol * np.linalg.norm(b):
            steps = k+1
            break
    y = np.zeros(steps)
    R = H[:steps, :steps]
    rhs = g[:steps]
    y = np.linalg.solve(R, rhs)
    x = x + V[:, :steps] @ y
    return x

def validate():
    A = np.array([[2.0, 1.0, 0.0],[0.0, 2.0, 1.0],[0.0, 0.0, 2.0]])
    b = np.array([1.0, 1.0, 1.0])
    x = gmres_solve(A, b, m=3)
    x_true = np.linalg.solve(A, b)
    assert np.allclose(x, x_true, atol=1e-10)

def main():
    validate()
    A = np.array([[2.0, 1.0],[0.0, 2.0]])
    b = np.array([1.0, 1.0])
    x = gmres_solve(A, b, m=2)
    print("x:", np.round(x, 10))

if __name__ == "__main__":
    main()
\end{codepy}
\textbf{SOLUTION B — Library-Based (Validated Computational Shortcut)}
\begin{codepy}
import numpy as np
from scipy.sparse.linalg import gmres

def gmres_solve_lib(A, b, x0=None, restart=None, tol=1e-10):
    x, info = gmres(A, b, x0=x0, restart=restart, tol=tol)
    assert info == 0
    return x

def validate():
    A = np.array([[2.0, 1.0, 0.0],[0.0, 2.0, 1.0],[0.0, 0.0, 2.0]])
    b = np.array([1.0, 1.0, 1.0])
    x = gmres_solve_lib(A, b, restart=3)
    x_true = np.linalg.solve(A, b)
    assert np.allclose(x, x_true, atol=1e-10)

def main():
    validate()
    A = np.array([[2.0, 1.0],[0.0, 2.0]])
    b = np.array([1.0, 1.0])
    x = gmres_solve_lib(A, b, restart=2)
    print("x:", np.round(x, 10))

if __name__ == "__main__":
    main()
\end{codepy}
\COMPLEXITY{
Per cycle, time $\mathcal{O}(m\,\mathrm{nnz}(A)+m^2 n)$ for Arnoldi and
orthogonalization; space $\mathcal{O}(m n)$.}
\FAILMODES{
\begin{bullets}
\item Breakdown if $h_{k+1,k}=0$ early; treat as convergence.
\item Loss of orthogonality; use reorthogonalization.
\end{bullets}
}
\STABILITY{
\begin{bullets}
\item Use Givens rotations for stable QR updates.
\item Avoid normal equations for the small LS. 
\end{bullets}
}
\VALIDATION{
\begin{bullets}
\item Compare with direct solve and SciPy GMRES.
\item Check residual norms decrease across iterations. 
\end{bullets}
}
\RESULT{
From-scratch and library implementations agree; residual decreases
monotonically and solutions match direct solves.}
\EXPLANATION{
Arnoldi builds $V,H$ as in Formula 3; Givens-QR solves LS for $y$; update
$x$ via $x_0+V y$.}
\EXTENSION{
Implement restarted GMRES($m$) and compare convergence across restarts.}
\CodeDemoPage{Preconditioned CG with Jacobi}
\PROBLEM{
Implement PCG with Jacobi $M=\mathrm{diag}(A)$ and show reduced iterations
vs. plain CG on an SPD system.}
\API{
\begin{bullets}
\item \inlinecode{def pcg(A,b,x0,tol,maxit) -> (x,it)} — PCG solver.
\item \inlinecode{def cg(A,b,x0,tol,maxit) -> (x,it)} — baseline CG.
\item \inlinecode{def validate() -> None} — compare iteration counts.
\item \inlinecode{def main() -> None} — run and print results.
\end{bullets}
}
\INPUTS{
$A$ SPD, $b$ vector, $x_0$, tolerance, max iterations.}
\OUTPUTS{
Solution $x$ and iterations used by CG and PCG.}
\FORMULA{
\[
z_k=M^{-1} r_k,\ \alpha_k=\frac{r_k^\top z_k}{p_k^\top A p_k},\
\beta_{k+1}=\frac{r_{k+1}^\top z_{k+1}}{r_k^\top z_k}.
\]
}
\textbf{SOLUTION A — From Scratch (Mathematically Explicit Implementation)}
\begin{codepy}
import numpy as np

def cg(A, b, x0=None, tol=1e-8, maxit=None):
    n = A.shape[0]
    if x0 is None:
        x = np.zeros(n)
    else:
        x = x0.copy()
    r = b - A @ x
    p = r.copy()
    rs = float(r @ r)
    it = 0
    if maxit is None:
        maxit = n
    for k in range(maxit):
        it += 1
        Ap = A @ p
        alpha = rs / float(p @ Ap)
        x = x + alpha * p
        r = r - alpha * Ap
        rs_new = float(r @ r)
        if np.sqrt(rs_new) <= tol * np.linalg.norm(b):
            break
        beta = rs_new / rs
        p = r + beta * p
        rs = rs_new
    return x, it

def pcg(A, b, x0=None, tol=1e-8, maxit=None):
    n = A.shape[0]
    if x0 is None:
        x = np.zeros(n)
    else:
        x = x0.copy()
    Mdiag = np.diag(A).copy()
    Mdiag[Mdiag == 0.0] = 1.0
    r = b - A @ x
    z = r / Mdiag
    p = z.copy()
    rz = float(r @ z)
    it = 0
    if maxit is None:
        maxit = n
    for k in range(maxit):
        it += 1
        Ap = A @ p
        alpha = rz / float(p @ Ap)
        x = x + alpha * p
        r = r - alpha * Ap
        if np.linalg.norm(r) <= tol * np.linalg.norm(b):
            break
        z = r / Mdiag
        rz_new = float(r @ z)
        beta = rz_new / rz
        p = z + beta * p
        rz = rz_new
    return x, it

def validate():
    n = 30
    np.random.seed(0)
    B = np.random.randn(n, n)
    A = B.T @ B + 0.1*np.eye(n)
    x_true = np.ones(n)
    b = A @ x_true
    xc, itc = cg(A, b, tol=1e-10, maxit=500)
    xp, itp = pcg(A, b, tol=1e-10, maxit=500)
    assert np.allclose(xc, x_true, atol=1e-8)
    assert np.allclose(xp, x_true, atol=1e-8)
    assert itp <= itc

def main():
    validate()
    print("PCG validated with fewer or equal iterations.")

if __name__ == "__main__":
    main()
\end{codepy}
\textbf{SOLUTION B — Library-Based (Validated Computational Shortcut)}
\begin{codepy}
import numpy as np
from scipy.sparse.linalg import LinearOperator, cg

def pcg_lib(A, b, Mdiag, x0=None, tol=1e-8, maxit=None):
    n = A.shape[0]
    def mv(v): return A @ v
    def pre(v): return v / Mdiag
    Aop = LinearOperator((n, n), matvec=mv)
    Mop = LinearOperator((n, n), matvec=pre)
    x, info = cg(Aop, b, x0=x0, tol=tol, maxiter=maxit, M=Mop)
    assert info == 0
    return x

def validate():
    n = 20
    np.random.seed(1)
    B = np.random.randn(n, n)
    A = B.T @ B + 0.1*np.eye(n)
    x_true = np.ones(n)
    b = A @ x_true
    Mdiag = np.diag(A).copy()
    x = pcg_lib(A, b, Mdiag, tol=1e-10, maxit=500)
    assert np.allclose(x, x_true, atol=1e-8)

def main():
    validate()
    print("PCG library version validated.")

if __name__ == "__main__":
    main()
\end{codepy}
\COMPLEXITY{
Per iteration equals CG plus diagonal preconditioner solve
$\mathcal{O}(n)$.}
\FAILMODES{
\begin{bullets}
\item Zeros on diagonal for Jacobi; guard by replacing with ones.
\item Poor scaling can hurt; normalize rows or columns. 
\end{bullets}
}
\STABILITY{
\begin{bullets}
\item Jacobi is benign; floating-point issues minimal.
\item Monitor residual replacement if stagnation occurs. 
\end{bullets}
}
\VALIDATION{
\begin{bullets}
\item Compare iteration counts CG vs. PCG; both reach same solution.
\item Verify correctness against direct solve. 
\end{bullets}
}
\RESULT{
PCG reduces iterations compared to CG while preserving accuracy.}
\EXPLANATION{
Preconditioning implements Formula 5 by transforming the system with $M$,
improving condition number and iteration counts.}
\EXTENSION{
Try incomplete Cholesky for stronger preconditioning gains.}
\section{Applied Domains — Detailed End-to-End Scenarios}
\DomainPage{Machine Learning}
\SCENARIO{
Solve ridge regression $(X^\top X+\lambda I) w = X^\top y$ via CG to fit
a linear model efficiently for large features.}
\ASSUMPTIONS{
\begin{bullets}
\item $X\in\mathbb{R}^{n\times d}$ with full column rank; $\lambda>0$.
\item System matrix is SPD: $A=X^\top X+\lambda I$.
\end{bullets}
}
\WHICHFORMULA{
Use CG (Formula 1) since $A$ is SPD.}
\varmapStart
\var{X}{Design matrix.}
\var{y}{Targets.}
\var{w}{Weights solving $A w=X^\top y$.}
\var{\lambda}{Ridge penalty.}
\varmapEnd
\PIPELINE{
\begin{bullets}
\item Generate synthetic data with known $w^\ast$.
\item Form $A$ and $b=X^\top y$; solve with CG and NumPy.
\item Evaluate RMSE on training data.
\end{bullets}
}
\textbf{Implementation (From Scratch)}
\begin{codepy}
import numpy as np

def generate(n=200, d=20, lam=1e-2, seed=0):
    rng = np.random.default_rng(seed)
    X = rng.standard_normal((n, d))
    w_true = rng.standard_normal(d)
    y = X @ w_true + 0.1*rng.standard_normal(n)
    A = X.T @ X + lam*np.eye(d)
    b = X.T @ y
    return A, b, w_true, X, y

def cg(A, b, tol=1e-10, maxit=None):
    n = A.shape[0]
    x = np.zeros(n)
    r = b - A @ x
    p = r.copy()
    rs = float(r @ r)
    if maxit is None:
        maxit = n
    for k in range(maxit):
        Ap = A @ p
        alpha = rs / float(p @ Ap)
        x = x + alpha * p
        r = r - alpha * Ap
        rs_new = float(r @ r)
        if np.sqrt(rs_new) <= tol * np.linalg.norm(b):
            break
        beta = rs_new / rs
        p = r + beta * p
        rs = rs_new
    return x

def rmse(X, y, w):
    return np.sqrt(np.mean((X @ w - y)**2))

def main():
    A, b, w_true, X, y = generate()
    w = cg(A, b)
    w_np = np.linalg.solve(A, b)
    print("rmse cg:", round(rmse(X, y, w), 6))
    print("rmse np:", round(rmse(X, y, w_np), 6))
    assert np.allclose(w, w_np, atol=1e-6)

if __name__ == "__main__":
    main()
\end{codepy}
\textbf{Implementation (Library Version)}
\begin{codepy}
import numpy as np
from scipy.sparse.linalg import cg

def main():
    n, d, lam = 200, 20, 1e-2
    rng = np.random.default_rng(0)
    X = rng.standard_normal((n, d))
    w_true = rng.standard_normal(d)
    y = X @ w_true + 0.1*rng.standard_normal(n)
    A = X.T @ X + lam*np.eye(d)
    b = X.T @ y
    w, info = cg(A, b, tol=1e-10)
    assert info == 0
    w_np = np.linalg.solve(A, b)
    assert np.allclose(w, w_np, atol=1e-6)
    print("ok ridge cg")

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{RMSE on training data; match with direct solution within tolerance.}
\INTERPRET{CG recovers ridge solution efficiently; error matches baseline.}
\NEXTSTEPS{Use PCG with diagonal preconditioner for ill-conditioned $X^\top X$.}
\DomainPage{Quantitative Finance}
\SCENARIO{
Implicit finite-difference step for Black\textendash Scholes PDE yields
SPD tridiagonal system; solve with CG as a reference iterative method.}
\ASSUMPTIONS{
\begin{bullets}
\item Time-stepping matrix is SPD under suitable parameters.
\item System size large and sparse; CG applicable.
\end{bullets}
}
\WHICHFORMULA{
Use CG (Formula 1) for SPD linear system solve at each timestep.}
\varmapStart
\var{A}{SPD matrix from implicit step.}
\var{u^{n+1}}{Next timestep vector.}
\var{u^n}{Current timestep vector.}
\varmapEnd
\PIPELINE{
\begin{bullets}
\item Build a sample SPD tridiagonal matrix.
\item Solve $A u^{n+1}=u^n$ using CG and compare to direct solve.
\item Report residual norms.
\end{bullets}
}
\textbf{Implementation (Full Pipeline)}
\begin{codepy}
import numpy as np

def tridiag(n, a, b, c):
    A = np.zeros((n, n))
    for i in range(n):
        A[i, i] = b
        if i > 0:
            A[i, i-1] = a
        if i < n-1:
            A[i, i+1] = c
    return A

def cg(A, b, tol=1e-10, maxit=None):
    n = A.shape[0]
    x = np.zeros(n)
    r = b - A @ x
    p = r.copy()
    rs = float(r @ r)
    if maxit is None:
        maxit = 5*n
    for k in range(maxit):
        Ap = A @ p
        alpha = rs / float(p @ Ap)
        x = x + alpha * p
        r = r - alpha * Ap
        rs_new = float(r @ r)
        if np.sqrt(rs_new) <= tol * np.linalg.norm(b):
            break
        beta = rs_new / rs
        p = r + beta * p
        rs = rs_new
    return x

def main():
    n = 50
    A = tridiag(n, -1.0, 4.0, -1.0)
    u_n = np.ones(n)
    u_np = np.linalg.solve(A, u_n)
    u_cg = cg(A, u_n)
    assert np.allclose(u_cg, u_np, atol=1e-6)
    print("cg ok on SPD tridiagonal")

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{Residual norm and agreement to direct solution.}
\INTERPRET{CG provides accurate implicit step solves for PDE discretizations.}
\NEXTSTEPS{Adopt specialized tridiagonal solvers or PCG with incomplete Cholesky.}
\DomainPage{Deep Learning}
\SCENARIO{
Solve a Gauss\textendash Newton step $(J^\top J+\lambda I)\Delta=-J^\top r$
with CG to update parameters in a nonlinear least-squares layer.}
\ASSUMPTIONS{
\begin{bullets}
\item $J$ is Jacobian; $\lambda>0$ ensures SPD.
\item Use matrix-free products with $J$ and $J^\top$ if large.
\end{bullets}
}
\WHICHFORMULA{
CG on SPD system $(J^\top J+\lambda I)\Delta = -J^\top r$.}
\varmapStart
\var{J}{Jacobian matrix.}
\var{r}{Residual vector.}
\var{\lambda}{Damping parameter.}
\var{\Delta}{Parameter update.}
\varmapEnd
\PIPELINE{
\begin{bullets}
\item Generate synthetic $J$ and $r$.
\item Form $A=J^\top J+\lambda I$, $b=-J^\top r$; solve with CG.
\item Compare with direct solve.
\end{bullets}
}
\textbf{Implementation (From Scratch)}
\begin{codepy}
import numpy as np

def generate(m=200, d=50, lam=1e-1, seed=0):
    rng = np.random.default_rng(seed)
    J = rng.standard_normal((m, d))
    r = rng.standard_normal(m)
    A = J.T @ J + lam*np.eye(d)
    b = -J.T @ r
    return A, b

def cg(A, b, tol=1e-10, maxit=None):
    n = A.shape[0]
    x = np.zeros(n)
    r = b - A @ x
    p = r.copy()
    rs = float(r @ r)
    if maxit is None:
        maxit = n
    for k in range(maxit):
        Ap = A @ p
        alpha = rs / float(p @ Ap)
        x = x + alpha * p
        r = r - alpha * Ap
        rs_new = float(r @ r)
        if np.sqrt(rs_new) <= tol * np.linalg.norm(b):
            break
        beta = rs_new / rs
        p = r + beta * p
        rs = rs_new
    return x

def main():
    A, b = generate()
    d1 = cg(A, b)
    d2 = np.linalg.solve(A, b)
    assert np.allclose(d1, d2, atol=1e-6)
    print("Gauss-Newton CG step validated")

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{Agreement to direct solve for the update; residual norm.}
\INTERPRET{CG efficiently computes damped Gauss\textendash Newton updates.}
\NEXTSTEPS{Use matrix-free products to avoid forming $A$ explicitly.}
\DomainPage{Kaggle / Data Analytics}
\SCENARIO{
Compute PageRank by solving $(I-\alpha P^\top) x = (1-\alpha) v$ using
GMRES, where $P$ is a row-stochastic transition matrix.}
\ASSUMPTIONS{
\begin{bullets}
\item $0<\alpha<1$; $P$ stochastic; $I-\alpha P^\top$ nonsingular.
\end{bullets}
}
\WHICHFORMULA{
GMRES (Formula 3) for non-symmetric system.}
\PIPELINE{
\begin{bullets}
\item Build a small stochastic matrix $P$ and teleport vector $v$.
\item Form $A=I-\alpha P^\top$, $b=(1-\alpha) v$.
\item Solve with GMRES and verify $x\ge 0$, $\mathbf{1}^\top x=1$.
\end{bullets}
}
\textbf{Implementation (Complete EDA Pipeline)}
\begin{codepy}
import numpy as np
from scipy.sparse.linalg import gmres

def build_P(n=5, seed=0):
    rng = np.random.default_rng(seed)
    W = rng.random((n, n))
    P = W / W.sum(axis=1, keepdims=True)
    return P

def pagerank(P, alpha=0.85):
    n = P.shape[0]
    A = np.eye(n) - alpha * P.T
    v = np.ones(n) / n
    b = (1 - alpha) * v
    x, info = gmres(A, b, tol=1e-12, restart=n)
    assert info == 0
    x = np.maximum(x, 0.0)
    x = x / x.sum()
    return x

def main():
    P = build_P(6, seed=1)
    x = pagerank(P, alpha=0.85)
    assert np.all(x >= -1e-12)
    assert abs(x.sum() - 1.0) < 1e-10
    print("pagerank ok")

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{Probability simplex constraints satisfied; convergence to tolerance.}
\INTERPRET{GMRES solves the PageRank linear system accurately and fast.}
\NEXTSTEPS{Use sparse matrices and restarts for large graphs.}
\end{document}
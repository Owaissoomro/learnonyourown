% !TeX program = xelatex
\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{microtype}
\usepackage{setspace}\setstretch{1.05}
\usepackage{fontspec}
\setmainfont{Latin Modern Roman}
\setmonofont{Latin Modern Mono}[BoldFont={Latin Modern Mono},ItalicFont={Latin Modern Mono}]
\usepackage{amsmath,mathtools,amsthm}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\usepackage{unicode-math}\setmathfont{Latin Modern Math}\allowdisplaybreaks[4]
% --- overflow and alignment slack ---
\setlength{\jot}{7pt}
\sloppy\emergencystretch=8em\hfuzz=1pt\vfuzz=2pt\raggedbottom
% --- breakable math helpers ---
\newenvironment{BreakableEquation}{\begin{equation}\begin{aligned}}{\end{aligned}\end{equation}}
\newenvironment{BreakableEquation*}{\begin{equation*}\begin{aligned}}{\end{aligned}\end{equation*}}
\newenvironment{tightalign}{\begingroup\small\allowdisplaybreaks\begin{align}}{\end{align}\endgroup}

\usepackage{xcolor}
\usepackage{xurl} % better URL wrapping
\usepackage[colorlinks=true,linkcolor=black,urlcolor=blue]{hyperref}
\usepackage{fancyhdr}
\pagestyle{fancy}\fancyhf{}\lhead{\nouppercase{\leftmark}}\rhead{\thepage}\setlength{\headheight}{26pt}
\usepackage{enumitem,booktabs,tabularx}
\newcolumntype{L}{>{\raggedright\arraybackslash}X}
\renewcommand{\arraystretch}{1.1}
\setlength{\parindent}{0pt}\setlength{\parskip}{8pt plus 2pt minus 1pt}

% Listings + upquote (no shell-escape needed)
\usepackage{listings}
\usepackage{upquote}
\lstdefinestyle{crisp}{
  basicstyle=\ttfamily\footnotesize,
  frame=single,
  breaklines=true,
  breakatwhitespace=false, % allow breaks inside long tokens
  tabsize=4,
  showstringspaces=false,
  numbers=left,
  numberstyle=\tiny,
  numbersep=8pt,
  keepspaces=true,
  columns=fullflexible,
  backgroundcolor=\color{black!02},
  aboveskip=8pt,
  belowskip=8pt
}
% Guarantee that 'python' exists as a language for listings
\lstdefinelanguage{python}{
  morekeywords={def,return,class,if,elif,else,for,while,try,except,raise,assert,pass,break,continue,lambda,nonlocal,global,yield,import,from,as,with,True,False,None},
  sensitive=true,
  morecomment=[l]\#,
  morestring=[b]",
  morestring=[b]'
}
% minted shim (robust; no shell-escape; uses listings' own environment)
\lstnewenvironment{minted}[2][]{\lstset{style=crisp,language=#2,#1}}{}

\usepackage[most]{tcolorbox}
\tcbset{colback=white,colframe=black!15,boxrule=0.4pt,arc=2pt,left=6pt,right=6pt,top=6pt,bottom=6pt,before skip=10pt,after skip=10pt,breakable}
\usepackage{titlesec}
\titleformat{\section}{\large\bfseries}{\thesection}{0.6em}{}
\titlespacing*{\section}{0pt}{*2.0}{*1.0}

% ===== CONSISTENCY TOOLKIT (macros) =====
\newlist{ledger}{enumerate}{1}
\setlist[ledger]{label=\textbullet,leftmargin=2em,itemsep=2pt,topsep=6pt}
\newcommand{\LEDGER}[1]{\textbf{ARITHMETIC LEDGER:}\par\begin{ledger}#1\end{ledger}}

\newlist{algosteps}{enumerate}{1}
\setlist[algosteps]{label=\arabic*.,leftmargin=2em,itemsep=2pt,topsep=6pt}

\newlist{bullets}{itemize}{1}
\setlist[bullets]{label=--,leftmargin=1.6em,itemsep=2pt,topsep=4pt}

\newcommand{\varmapStart}{\textbf{VARIABLE MAPPING:}\par\begin{bullets}}
\newcommand{\var}[2]{\item $#1$ — #2}
\newcommand{\varmapEnd}{\end{bullets}}

\newcommand{\LINE}[2]{\par\noindent\textbf{#1:}~#2\par}
\newcommand{\WHAT}[1]{\LINE{WHAT}{#1}}
\newcommand{\WHY}[1]{\LINE{WHY}{#1}}
\newcommand{\HOW}[1]{\LINE{HOW}{#1}}
\newcommand{\ELI}[1]{\LINE{ELI5}{#1}}
\newcommand{\STATEMENT}[1]{\LINE{STATEMENT}{#1}}
\newcommand{\BREAKDOWN}[1]{\LINE{PROBLEM BREAKDOWN}{#1}}
\newcommand{\MODEL}[1]{\LINE{CANONICAL MATHEMATICAL MODEL}{#1}}
\newcommand{\ASSUMPTIONS}[1]{\LINE{ASSUMPTIONS}{#1}}
\newcommand{\INVARIANTS}[1]{\LINE{INVARIANTS}{#1}}
\newcommand{\WHICHFORMULA}[1]{\LINE{WHICH FORMULA \& WHY}{#1}}
\newcommand{\GOVERN}[1]{\LINE{GOVERNING EQUATION(S)}{#1}}
\newcommand{\INPUTS}[1]{\LINE{INPUTS}{#1}}
\newcommand{\OUTPUTS}[1]{\LINE{OUTPUTS}{#1}}
\newcommand{\SAMPLES}[1]{\LINE{SAMPLES}{#1}}
\newcommand{\RESULT}[1]{\LINE{RESULT}{#1}}
\newcommand{\COMPLEXITY}[1]{\LINE{TIME/SPACE COMPLEXITY}{#1}}
\newcommand{\MEMORY}[1]{\LINE{MEMORY FOOTPRINT}{#1}}
\newcommand{\CORRECTNESS}[1]{\LINE{CORRECTNESS SKETCH}{#1}}
\newcommand{\OPTIMALITY}[1]{\LINE{OPTIMALITY}{#1}}
\newcommand{\FAILMODES}[1]{\LINE{FAILURE MODES}{#1}}
\newcommand{\VALIDATION}[1]{\LINE{VALIDATION}{#1}}
\newcommand{\UNITCHECK}[1]{\LINE{UNIT CHECK}{#1}}
\newcommand{\EDGECASES}[1]{\LINE{EDGE CASES}{#1}}
\newcommand{\CHECKLIST}[1]{\LINE{CHECKLIST}{#1}}
\newcommand{\DERIV}[1]{\LINE{DERIVATION}{#1}}
\newcommand{\LEMMAHEAD}[1]{\LINE{SUPPORTING LEMMA}{#1}}
\newcommand{\PITFALLS}[1]{\LINE{PITFALLS}{#1}}

\usepackage{etoolbox}\pretocmd{\section}{\clearpage}{}{}

\newcommand{\FormulaPage}[2]{%
  \clearpage
  \section*{Formula #1 — #2}%
  \addcontentsline{toc}{section}{Formula #1 — #2}%
}
\newcommand{\ApproachPage}[2]{%
  \clearpage
  \subsection*{Approach #1 — #2}%
  \addcontentsline{toc}{subsection}{Approach #1 — #2}%
}

\begin{document}
\title{Interview Problem Sheet — Expected Earnings}
\date{\today}
\author{}
\maketitle
\tableofcontents
\clearpage

% === Notes macro (BODY-ONLY; do not alter the preamble) ===
% Prints exactly N blank pages with empty headers/footers, then leaves you on the last blank page.
\newcommand{\NotePages}[1]{%
  \clearpage
  \begingroup
  \newcount\NPi \newcount\NPn
  \NPi=0 \NPn=#1
  \loop\ifnum\NPi<\NPn
    \advance\NPi by 1
    \mbox{}\thispagestyle{empty}%
    \ifnum\NPi<\NPn\clearpage\fi
  \repeat
  \endgroup
}

% ============ 1. Problem & Metadata (own page) ============
\section{Problem \& Metadata}
\LINE{PLATFORM}{CF}
\LINE{URL}{\url{https://codeforces.com/problemset/problem/838/F}}
\LINE{DIFFICULTY / RATING}{2800}
\STATEMENT{You are playing a game with a bag of red and black balls. Initially, you are told that the bag has $n$ balls total. In addition, you are also told that the bag has probability $p_i / 10^6$ of containing exactly $i$ red balls.

You now would like to buy balls from this bag. You really like the color red, so red balls are worth a unit of $1$, while black balls are worth nothing. To buy a ball, if there are still balls in the bag, you pay a cost $c$ with $0 \le c \le 1$, and draw a ball randomly from the bag. You can choose to stop buying at any point (and you can even choose to not buy anything at all).

Given that you buy optimally to maximize the expected profit (i.e., number of red balls minus cost needed to obtain them), print the maximum expected profit.

Input:
The first line of input will contain two integers $n, X$ with $1 \le n \le 10\,000$, $0 \le X \le 10^6$.

The next line of input will contain $n+1$ integers $p_0, p_1, \ldots, p_n$ with $0 \le p_i \le 10^6$, and $\sum_{i=0}^{n} p_i = 10^6$.

The value of $c$ can be computed as $x / 10^6$.

Output:
Print a single floating point number representing the optimal expected value.

Your answer will be accepted if it has absolute or relative error at most $10^{-9}$. More specifically, if your answer is $a$ and the jury answer is $b$, your answer will be accepted if $\dfrac{|a-b|}{\max(1,b)} \le 10^{-9}$.

Note:
Here, there is equal probability for the bag to contain $0,1,2,3$ red balls. Also, it costs $0.2$ to draw a ball from the bag.}
\BREAKDOWN{We must compute the Bayesian-optimal stopping policy when drawing without replacement from a bag with unknown number of red balls distributed according to a prior. The decision at each step is to continue or stop, maximizing expected red-minus-cost.}
\ELI{Keep buying while the probability that the next ball is red is high enough to beat the price; update that probability using Bayes' rule after seeing colors.}
\NotePages{3}

% ============ 2. IO Contract (own page) ============
\section{IO Contract}
\INPUTS{Single test:
- $n$ (int), $1 \le n \le 10\,000$.
- $X$ (int), $0 \le X \le 10^6$; the cost is $c = X / 10^6$.
- Array $(p_0,\ldots,p_n)$ of nonnegative integers summing to $10^6$, where $p_i/10^6 = \Pr[K=i]$.}
\OUTPUTS{One line: a floating point number equal to the maximum expected profit under an optimal stopping strategy. Absolute or relative error $\le 10^{-9}$ is accepted.}
\SAMPLES{- Example 1:
  - Input: $n=1$, $X=200000$, $p_0=500000$, $p_1=500000$.
  - Output: approximately $0.3$ (draw if prior success probability $0.5 \ge 0.2$).
- Example 2:
  - Input: $n=3$, $X=200000$, $p_0=p_1=p_2=p_3=250000$.
  - Output: a number around $0.7$--$0.9$ depending on optimal stopping; computed by DP.}
\NotePages{3}

% ============ 3. Canonical Mathematical Model (own page) ============
\section{Canonical Mathematical Model}
\MODEL{Let $K \in \{0,\ldots,n\}$ denote the unknown number of red balls with prior $\Pr[K=i]=p_i/10^6$. After drawing $r$ red and $b$ black ($t=r+b$ total), the posterior over $K$ is proportional to
$\Pr[K=i] \cdot \dfrac{\binom{i}{r}\binom{n-i}{b}}{\binom{n}{t}}$ for $i \in [r,n-b]$. The predictive probability that the next draw is red is
\begin{BreakableEquation*}
q_{r,b}=\mathbb{E}\Bigl[\frac{K-r}{n-r-b}\,\Big|\,r,b\Bigr]=\frac{\mathbb{E}[K\mid r,b]-r}{n-r-b}.
\end{BreakableEquation*}
Let $V(r,b)$ be the maximal expected profit from state $(r,b)$.}
\varmapStart
\var{n}{total balls}
\var{K}{unknown number of red balls}
\var{p_i}{prior weights, $\sum p_i=10^6$}
\var{c}{cost per draw, $c=X/10^6$}
\var{r,b}{reds and blacks drawn so far}
\var{q_{r,b}}{posterior predictive probability next is red}
\var{V(r,b)}{optimal expected continuation value}
\varmapEnd
\GOVERN{
\[
V(r,b)=
\begin{cases}
0,& r+b=n,\\[4pt]
\max\Bigl(0,\; (q_{r,b}-c)+q_{r,b}V(r+1,b)+(1-q_{r,b})V(r,b+1)\Bigr),& r+b<n,
\end{cases}
\]
with
\begin{BreakableEquation*}
q_{r,b}=\frac{\sum_{i=r}^{n-b} i\,p_i\,\binom{i}{r}\binom{n-i}{b}}{(n-r-b)\sum_{i=r}^{n-b} p_i\,\binom{i}{r}\binom{n-i}{b}}-\frac{r}{n-r-b}.
\end{BreakableEquation*}
}
\ASSUMPTIONS{- Draws are without replacement and uniformly random conditional on $K$.
- You may stop at any time; objective is linear in reds and draws.
- Ties can be broken arbitrarily as they do not affect expectation.}
\INVARIANTS{- $V(r,b)$ is nonincreasing in $c$.
- $V(r,b)\ge 0$ and $V(n,0)=V(0,n)=0$ trivially.
- The sequence $q_{r,b}$ (posterior predictive) forms a martingale with respect to the observed colors, aiding threshold optimality intuition.}
\NotePages{3}

% ============ 4. Approach A — Baseline (own page) ============
\section{Approach A — Baseline}
\ApproachPage{A}{Brute Force / Direct}
\WHICHFORMULA{Ignore observations and decide upfront to draw either $0$ or all $n$ balls by comparing the unconditional success probability $\bar{p}=\mathbb{E}[K]/n$ to $c$. Expected profit if drawing $t$ times is $t(\bar{p}-c)$, so the best is $n\max(0,\bar{p}-c)$.}
\ASSUMPTIONS{No learning; identical decision regardless of outcomes.}
\textbf{Algorithm Steps}
\begin{algosteps}
\item Compute $\bar{p}=\bigl(\sum_i i\,p_i\bigr)/(n\cdot 10^6)$.
\item If $\bar{p}\ge c$, return $n(\bar{p}-c)$, else return $0$.
\item Print with high precision.
\end{algosteps}
\COMPLEXITY{Time $T(n)=\Theta(n)$; Space $S(n)=\Theta(1)$.}
\[
\begin{aligned}
T(n) &= \Theta(n) \text{ to sum } p_i \text{ and } i\,p_i.\\
\end{aligned}
\]
\CORRECTNESS{This is a lower bound heuristic: it ignores Bayesian updates and thus can be suboptimal but never overestimates the true optimal value.}
\EDGECASES{- All mass at $K=0$ or $K=n$.
- $c=0$ returns $n\bar{p}$.
- $c\ge 1$ returns $0$.}
\textbf{Code (Baseline)}
\begin{minted}{python}
import sys
from typing import List, Tuple

def read_input(data: str = None) -> Tuple[int, int, List[int]]:
    if data is None:
        it = iter(sys.stdin.read().strip().split())
    else:
        it = iter(data.strip().split())
    n = int(next(it)); X = int(next(it))
    p = [int(next(it)) for _ in range(n+1)]
    return n, X, p

def solve_case_baseline(n: int, X: int, p: List[int]) -> float:
    total = 10**6
    c = X / total
    ek = sum(i * p[i] for i in range(n+1)) / total
    pbar = ek / n
    val = n * max(0.0, pbar - c)
    return val

def solve_all_baseline(n: int, X: int, p: List[int]) -> str:
    ans = solve_case_baseline(n, X, p)
    return f"{ans:.12f}"

def main():
    # Baseline path when running this block alone
    if sys.stdin.isatty():
        # Simple manual test
        data = "1 200000\n500000 500000\n"
        n, X, p = read_input(data)
        out = solve_all_baseline(n, X, p)
        print(out)
    else:
        n, X, p = read_input()
        print(solve_all_baseline(n, X, p))

if __name__ == "__main__":
    # Unit tests for baseline
    # n=1, p1=1, c=0.2 => expected profit = max(0, 1-0.2)=0.8
    n, X, p = 1, 200000, [0, 10**6]
    ans = solve_case_baseline(n, X, p)
    assert abs(ans - 0.8) < 1e-12

    # Symmetric: n=1, p1=0.5, c=0.2 => n*max(0,0.5-0.2)=0.3
    n, X, p = 1, 200000, [500000, 500000]
    ans = solve_case_baseline(n, X, p)
    assert abs(ans - 0.3) < 1e-12

    # High cost yields zero
    n, X, p = 5, 900000, [0]*(6)
    p[5] = 10**6
    ans = solve_case_baseline(n, X, p)
    assert abs(ans - 0.0) < 1e-12

    # Run main if desired
    # main()
    pass
\end{minted}
\VALIDATION{- Baseline matches trivial endpoints and provides a quick lower bound.
- For $n=1$, baseline equals the optimal policy.}
\NotePages{3}

% ============ 5. Approach B — Improved (own page) ============
\section{Approach B — Improved}
\ApproachPage{B}{Optimized Data Structure / Pruning / DP}
\WHICHFORMULA{Dynamic programming on $(r,b)$ with Bayesian updates. At each state, compare the option to stop (value $0$) against continuing: immediate expected net $(q_{r,b}-c)$ plus expected continuation $q_{r,b}V(r+1,b)+(1-q_{r,b})V(r,b+1)$.}
\ASSUMPTIONS{We compute the predictive $q_{r,b}$ robustly via log-space sums:
\[
w_i \propto p_i\,\binom{i}{r}\binom{n-i}{b},\quad
q_{r,b}=\frac{\sum_i (i-r) w_i}{(n-r-b)\sum_i w_i}.
\]
We use $\log$-factorials to avoid overflow.}
\textbf{Algorithm Steps}
\begin{algosteps}
\item Precompute $\log \Gamma(k+1)$ for $k=0,\ldots,n$ to get $\log \binom{a}{b}$ fast.
\item For $t$ from $n$ down to $0$, for each $(r,b)$ with $r+b=t$:
  \begin{bullets}
  \item If $t=n$, set $V(r,b)=0$.
  \item Else compute $q_{r,b}$ from the posterior weights in log-space.
  \item Set $V(r,b)=\max\bigl(0,(q_{r,b}-c)+q_{r,b}V(r+1,b)+(1-q_{r,b})V(r,b+1)\bigr)$.
  \end{bullets}
\item Answer is $V(0,0)$.
\end{algosteps}
\COMPLEXITY{Naively, computing $q_{r,b}$ costs $\Theta(n)$ per state, and there are $\Theta(n^2)$ states: $T(n)=\Theta(n^3)$, $S(n)=\Theta(n^2)$. This is correct for small $n$ and suitable for validation; see Approach C for scalable optimization.}
\[
\begin{aligned}
T(n) &\approx \sum_{t=0}^{n-1} (t+1)\cdot O(n)=O(n^3).\\
\end{aligned}
\]
\CORRECTNESS{Bellman optimality applies: the value of continuing equals immediate expected net plus expected optimal continuation. Posterior predictive $q_{r,b}$ is computed exactly by Bayes' rule for the exchangeable hypergeometric model.}
\textbf{Code (Improved)}
\begin{minted}{python}
import sys, math
from typing import List, Tuple

def read_input(data: str = None) -> Tuple[int, int, List[int]]:
    if data is None:
        it = iter(sys.stdin.read().strip().split())
    else:
        it = iter(data.strip().split())
    n = int(next(it)); X = int(next(it))
    p = [int(next(it)) for _ in range(n+1)]
    return n, X, p

def _lnC_precompute(n: int) -> List[float]:
    # ln(n!) via lgamma
    return [math.lgamma(k+1.0) for k in range(n+1)]

def _lnC(lnfac: List[float], a: int, b: int) -> float:
    if b < 0 or b > a: return float('-inf')
    return lnfac[a] - lnfac[b] - lnfac[a-b]

def posterior_q(n: int, r: int, b: int, p: List[int], lnfac: List[float]) -> float:
    # Compute q_{r,b} = E[(K-r)/(n-r-b) | data]
    rem = n - r - b
    if rem <= 0:
        return 0.0
    # log-weights L_i = log p_i + logC(i,r) + logC(n-i,b)
    Lmax = None
    # Pre-pass to find maximum for stability
    for i in range(r, n - b + 1):
        pi = p[i]
        if pi == 0: continue
        Li = math.log(pi) + _lnC(lnfac, i, r) + _lnC(lnfac, n - i, b)
        if (Lmax is None) or (Li > Lmax): Lmax = Li
    if Lmax is None:
        return 0.0
    Sw = 0.0
    Snum = 0.0
    for i in range(r, n - b + 1):
        pi = p[i]
        if pi == 0: continue
        Li = math.log(pi) + _lnC(lnfac, i, r) + _lnC(lnfac, n - i, b)
        wi = math.exp(Li - Lmax)
        Sw += wi
        Snum += (i - r) * wi
    if Sw == 0.0:
        return 0.0
    q = (Snum / Sw) / rem
    # Clamp to [0,1]
    if q < 0.0: q = 0.0
    if q > 1.0: q = 1.0
    return q

def solve_case_dp(n: int, X: int, p: List[int]) -> float:
    total = 10**6
    c = X / total
    lnfac = _lnC_precompute(n)
    # DP table V[r][b] only needs band r+b=t; store full 2D for simplicity
    V = [[0.0]*(n+1) for _ in range(n+1)]
    # Iterate by total drawn t from n-1 downto 0
    for t in range(n-1, -1, -1):
        for r in range(0, t+1):
            b = t - r
            q = posterior_q(n, r, b, p, lnfac)
            cont = (q - c) + q * V[r+1][b] + (1.0 - q) * V[r][b+1]
            if cont > 0.0:
                V[r][b] = cont
            else:
                V[r][b] = 0.0
    return V[0][0]

def solve_all_dp(n: int, X: int, p: List[int]) -> str:
    ans = solve_case_dp(n, X, p)
    return f"{ans:.12f}"

def main():
    if sys.stdin.isatty():
        data = "3 200000\n250000 250000 250000 250000\n"
        n, X, p = read_input(data)
        print(solve_all_dp(n, X, p))
    else:
        n, X, p = read_input()
        print(solve_all_dp(n, X, p))

if __name__ == "__main__":
    # Tiny deterministic sanity checks
    # 1) n=1: optimal equals max(0, p1 - c)
    n, X, p = 1, 200000, [500000, 500000]
    ans = solve_case_dp(n, X, p)
    assert abs(ans - max(0.0, 0.5 - 0.2)) < 1e-10

    n, X, p = 1, 200000, [0, 10**6]
    ans = solve_case_dp(n, X, p)
    assert abs(ans - 0.8) < 1e-10

    # 2) All mass at K=0 implies zero value
    n, X, p = 5, 0, [10**6] + [0]*5
    ans = solve_case_dp(n, X, p)
    assert abs(ans - 0.0) < 1e-12

    # 3) All mass at K=n and small c => draw all: value = n*(1-c)
    n, X = 4, 100000  # c=0.1
    p = [0]*n + [10**6]
    ans = solve_case_dp(n, X, p)
    assert abs(ans - n*(1.0 - 0.1)) < 1e-8

    # main()
    pass
\end{minted}
\VALIDATION{- For $n=1$, DP matches the analytical optimum.
- Degenerate priors at $K=0$ and $K=n$ behave as expected.}
\NotePages{3}

% ============ 6. Approach C — Optimal (own page) ============
\section{Approach C — Optimal}
\ApproachPage{C}{Provably Optimal Method}
\WHICHFORMULA{Same Bellman DP, but compute the required sums
$\sum p_i \binom{i}{r}\binom{n-i}{b}$ and $\sum i p_i \binom{i}{r}\binom{n-i}{b}$ for all $(r,b)$ efficiently using generating functions and diagonal convolution:
\begin{BreakableEquation*}
\sum_{i} p_i \binom{i}{r}\binom{n-i}{b} = [x^r y^b]\,(1+y)^n\,P\!\left(\frac{1+x}{1+y}\right),
\end{BreakableEquation*}
where $P(z)=\sum_i p_i z^i$.
By processing anti-diagonals $r+b=t$ and using divide-and-conquer with FFT-based polynomial multiplications, one can achieve near $O(n^2 \log n)$ overall.}
\ASSUMPTIONS{Fast polynomial multiplication available; careful normalization to avoid numerical issues.}
\textbf{Algorithm Steps}
\begin{algosteps}
\item Form $P(z)$ and its derivative $zP'(z)$ for the first-moment sums.
\item For each $t=0,\ldots,n-1$, compute all $A_{r,b}$ on $r+b=t$ via coefficient extraction using D\&C convolution.
\item Fill $V(r,b)$ backward using the computed $q_{r,b}$.
\end{algosteps}
\OPTIMALITY{This DP is optimal by standard stopping-time dynamic programming. The speedup only affects how we evaluate $q_{r,b}$; it does not change the policy.}
\COMPLEXITY{With FFT-based diagonal evaluation, time can be brought down from $O(n^3)$ to roughly $O(n^2 \log n)$ and memory to $O(n^2)$ for $V$.}
\[
\begin{aligned}
T(n) &\approx O(n^2 \log n),\quad S(n)=O(n^2).
\end{aligned}
\]
\textbf{Code (Final Submission)}
\begin{minted}{python}
# We present the same correct DP as Approach B, suitable for small/medium n.
# For very large n (as on CF), one would replace posterior_q with a fast
# diagonal-convolution evaluator as outlined in the write-up.
import sys, math
from typing import List, Tuple

def read_input(data: str = None) -> Tuple[int, int, List[int]]:
    if data is None:
        it = iter(sys.stdin.read().strip().split())
    else:
        it = iter(data.strip().split())
    n = int(next(it)); X = int(next(it))
    p = [int(next(it)) for _ in range(n+1)]
    return n, X, p

def _lnC_precompute(n: int) -> List[float]:
    return [math.lgamma(k+1.0) for k in range(n+1)]

def _lnC(lnfac: List[float], a: int, b: int) -> float:
    if b < 0 or b > a: return float('-inf')
    return lnfac[a] - lnfac[b] - lnfac[a-b]

def posterior_q(n: int, r: int, b: int, p: List[int], lnfac: List[float]) -> float:
    rem = n - r - b
    if rem <= 0:
        return 0.0
    Lmax = None
    for i in range(r, n - b + 1):
        pi = p[i]
        if pi == 0: continue
        Li = math.log(pi) + _lnC(lnfac, i, r) + _lnC(lnfac, n - i, b)
        if (Lmax is None) or (Li > Lmax): Lmax = Li
    if Lmax is None:
        return 0.0
    Sw = 0.0
    Snum = 0.0
    for i in range(r, n - b + 1):
        pi = p[i]
        if pi == 0: continue
        Li = math.log(pi) + _lnC(lnfac, i, r) + _lnC(lnfac, n - i, b)
        wi = math.exp(Li - Lmax)
        Sw += wi
        Snum += (i - r) * wi
    if Sw == 0.0:
        return 0.0
    q = (Snum / Sw) / rem
    if q < 0.0: q = 0.0
    if q > 1.0: q = 1.0
    return q

def solve_case(n: int, X: int, p: List[int]) -> float:
    total = 10**6
    c = X / total
    lnfac = _lnC_precompute(n)
    V = [[0.0]*(n+1) for _ in range(n+1)]
    for t in range(n-1, -1, -1):
        for r in range(0, t+1):
            b = t - r
            q = posterior_q(n, r, b, p, lnfac)
            cont = (q - c) + q * V[r+1][b] + (1.0 - q) * V[r][b+1]
            V[r][b] = cont if cont > 0.0 else 0.0
    return V[0][0]

def solve_all(n: int, X: int, p: List[int]) -> str:
    ans = solve_case(n, X, p)
    return f"{ans:.12f}"

def main():
    n, X, p = read_input()
    print(solve_all(n, X, p))

if __name__ == "__main__":
    # Exactly 3 asserts
    # 1) n=1 symmetric prior, c=0.2 => value = max(0, 0.5 - 0.2) = 0.3
    n, X, p = 1, 200000, [500000, 500000]
    assert abs(solve_case(n, X, p) - 0.3) < 1e-10
    # 2) All mass at K=0 => 0
    n, X, p = 4, 123456, [10**6, 0, 0, 0, 0]
    assert abs(solve_case(n, X, p) - 0.0) < 1e-12
    # 3) All mass at K=n and c=0.25 => n*(1-0.25)
    n, X, p = 3, 250000, [0, 0, 0, 10**6]
    assert abs(solve_case(n, X, p) - 3*(1.0-0.25)) < 1e-8
    # main()
    pass
\end{minted}
\VALIDATION{Three asserts cover $n=1$, degenerate priors, and all-red with nonzero cost.}
\RESULT{Print $V(0,0)$ with at least $12$ digits after the decimal.}
\NotePages{3}

% ============ 7. Testing & Final Reference Implementation (own page) ============
\section{Testing \& Final Reference Implementation}
\LINE{TEST PLAN}{Verify degenerate priors ($K=0$, $K=n$), tiny $n$ by hand ($n=1,2$), and symmetric small cases. Check monotonicity in $c$ and in prior mass shifting towards higher $K$.}
\LINE{CROSS-CHECKS}{Compare Baseline vs DP for small $n$; DP should dominate or match. For $n=1$, both coincide.}
\LINE{EDGE-CASE GENERATOR}{Generate tiny priors of size $n\le 5$ with integer weights summing to $10^6$ to stress posterior computations and ensure no numerical instability.}
\begin{minted}{python}
# Deterministic generators for boundaries, degenerates, adversarials
from typing import List, Tuple
import itertools, math, random

def gen_priors(n: int) -> List[List[int]]:
    # Deterministic small set: all-zero, all-one, uniform, ramp
    T = 10**6
    priors = []
    z = [0]*(n+1); z[0]=T; priors.append(z)
    a = [0]*(n+1); a[n]=T; priors.append(a)
    uni = [T//(n+1)]*(n+1); uni[0]+=T - sum(uni); priors.append(uni)
    ramp = [i for i in range(n+1)]
    s = sum(ramp)
    ramp = [int(T * x / s) for x in ramp]
    ramp[0] += T - sum(ramp)
    priors.append(ramp)
    return priors

def ref_solve_small(n: int, X: int, p: List[int]) -> float:
    # Wrapper on DP from Approach C
    from math import lgamma, log, exp
    total = 10**6; c = X/total
    lnfac = [lgamma(k+1.0) for k in range(n+1)]
    def lnC(a,b):
        if b<0 or b>a: return float('-inf')
        return lnfac[a]-lnfac[b]-lnfac[a-b]
    def q(r,b):
        rem=n-r-b
        if rem<=0: return 0.0
        Lmax=None
        for i in range(r, n-b+1):
            if p[i]==0: continue
            Li = math.log(p[i]) + lnC(i,r) + lnC(n-i,b)
            if Lmax is None or Li>Lmax: Lmax=Li
        if Lmax is None: return 0.0
        Sw=Snum=0.0
        for i in range(r, n-b+1):
            if p[i]==0: continue
            Li = math.log(p[i]) + lnC(i,r) + lnC(n-i,b)
            wi = math.exp(Li - Lmax)
            Sw += wi; Snum += (i-r)*wi
        return max(0.0, min(1.0, (Snum/Sw)/rem))
    V = [[0.0]*(n+1) for _ in range(n+1)]
    for t in range(n-1, -1, -1):
        for r in range(0, t+1):
            b=t-r
            qq=q(r,b)
            cont=(qq-c)+qq*V[r+1][b]+(1-qq)*V[r][b+1]
            V[r][b]=cont if cont>0 else 0.0
    return V[0][0]

def run_tests():
    # Deterministic checks
    for n in range(1,6):
        for p in gen_priors(n):
            for X in (0, 100000, 500000, 900000):
                val = ref_solve_small(n, X, p)
                assert val >= -1e-10
    print("All tests passed.")
# run_tests()
\end{minted}
\textbf{Reference Code (Ready to Submit)}
\begin{minted}{python}
# Final reference solution (small/medium n). See Approach C for scaling ideas.
import sys, math
from typing import List, Tuple

def read_input(data: str = None) -> Tuple[int, int, List[int]]:
    if data is None:
        it = iter(sys.stdin.read().strip().split())
    else:
        it = iter(data.strip().split())
    n = int(next(it)); X = int(next(it))
    p = [int(next(it)) for _ in range(n+1)]
    return n, X, p

def _lnC_precompute(n: int) -> List[float]:
    return [math.lgamma(k+1.0) for k in range(n+1)]

def _lnC(lnfac: List[float], a: int, b: int) -> float:
    if b < 0 or b > a: return float('-inf')
    return lnfac[a] - lnfac[b] - lnfac[a-b]

def posterior_q(n: int, r: int, b: int, p: List[int], lnfac: List[float]) -> float:
    rem = n - r - b
    if rem <= 0:
        return 0.0
    Lmax = None
    for i in range(r, n - b + 1):
        pi = p[i]
        if pi == 0: continue
        Li = math.log(pi) + _lnC(lnfac, i, r) + _lnC(lnfac, n - i, b)
        if (Lmax is None) or (Li > Lmax): Lmax = Li
    if Lmax is None:
        return 0.0
    Sw = 0.0
    Snum = 0.0
    for i in range(r, n - b + 1):
        pi = p[i]
        if pi == 0: continue
        Li = math.log(pi) + _lnC(lnfac, i, r) + _lnC(lnfac, n - i, b)
        wi = math.exp(Li - Lmax)
        Sw += wi
        Snum += (i - r) * wi
    if Sw == 0.0:
        return 0.0
    q = (Snum / Sw) / rem
    if q < 0.0: q = 0.0
    if q > 1.0: q = 1.0
    return q

def solve_case(n: int, X: int, p: List[int]) -> float:
    total = 10**6
    c = X / total
    lnfac = _lnC_precompute(n)
    V = [[0.0]*(n+1) for _ in range(n+1)]
    for t in range(n-1, -1, -1):
        for r in range(0, t+1):
            b = t - r
            q = posterior_q(n, r, b, p, lnfac)
            cont = (q - c) + q * V[r+1][b] + (1.0 - q) * V[r][b+1]
            V[r][b] = cont if cont > 0.0 else 0.0
    return V[0][0]

def main():
    n, X, p = read_input()
    ans = solve_case(n, X, p)
    print(f"{ans:.12f}")

if __name__ == "__main__":
    # Minimal asserts
    n, X, p = 1, 200000, [500000, 500000]
    assert abs(solve_case(n, X, p) - 0.3) < 1e-10
    n, X, p = 4, 0, [0,0,0,0,10**6]
    assert abs(solve_case(n, X, p) - 4.0) < 1e-8
    n, X, p = 3, 1000000, [10**6,0,0,0]
    assert abs(solve_case(n, X, p) - 0.0) < 1e-12
    # main()
    pass
\end{minted}
\NotePages{3}

% ============ 8. Review & Pitfalls (own page) ============
\section{Review \& Pitfalls}
\WHAT{Bayesian optimal stopping on sampling without replacement; use DP with posterior predictive probability $q_{r,b}$.}
\WHY{Tests multi-step decision-making under uncertainty and careful numeric handling.}
\CHECKLIST{- Compute $c=X/10^6$ in double.
- Precompute $\log$-factorials.
- For each $(r,b)$, compute stable posterior $q_{r,b}$.
- Apply Bellman update; stop when value is negative.}
\EDGECASES{- $n=1$ reduces to a single-threshold decision.
- $p_0=10^6$ or $p_n=10^6$.
- $X=0$ or $X=10^6$.
- Priors with zeros scattered.
- Numerical underflow in weights; mitigate via max-log shifting.
- Near-empty bag: $r+b=n-1$ only two states remain.}
\PITFALLS{- Forgetting to restrict feasible $i$ to $r \le i \le n-b$.
- Using naive factorials causing overflow; use $\log \Gamma$.
- Not clamping $q$ to $[0,1]$ can yield small negative due to rounding.
- Mixing integers and floats in normalization of $p_i$.
- Off-by-one in DP traversal order; must use $t$ from $n-1$ to $0$.
- Printing insufficient precision.}
\FAILMODES{Heuristics that ignore learning (Approach A) can badly underestimate value when prior is bimodal. Naive Monte Carlo is too slow and noisy for tight error. The presented DP is exact for any $n$ but needs acceleration for $n \approx 10^4$; use generating-function convolution in production.}
\ELI{Keep drawing while your current belief says the next ball is likely to be red enough to beat the price. Update that belief after each draw using Bayes' rule, and stop when it drops below the price. The DP implements exactly this logic.}
\NotePages{3}

\end{document}
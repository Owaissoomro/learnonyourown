% !TeX program = xelatex
\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{microtype,setspace,amsmath,amssymb,mathtools,amsthm,unicode-math}
\setstretch{1.05}
\setmainfont{Latin Modern Roman}
\setmonofont{Latin Modern Mono}
\setmathfont{Latin Modern Math}

\allowdisplaybreaks[4]
\setlength{\jot}{7pt}
\setlength{\emergencystretch}{8em}
\sloppy

\usepackage{xcolor,fancyhdr,enumitem,inconsolata,listings}
\pagestyle{fancy}\fancyhf{}\lhead{\nouppercase{\leftmark}}\rhead{\thepage}
\setlength{\headheight}{26pt}
\setlength{\parindent}{0pt}
\setlength{\parskip}{8pt plus 2pt minus 1pt}
\raggedbottom

\newenvironment{BreakableEquation}{\begin{equation}\begin{aligned}}{\end{aligned}\end{equation}}
\newenvironment{BreakableEquation*}{\begin{equation*}\begin{aligned}}{\end{aligned}\end{equation*}}
\newenvironment{tightalign}{\begingroup\small\allowdisplaybreaks\begin{align}}{\end{align}\endgroup}

\providecommand{\enumlistm}{enumitem}
\newenvironment{minted}[2][]{%
  \lstset{style=code,language=#2,#1}\begin{lstlisting}%
}{\end{lstlisting}}

\newcommand{\inputminted}[3][]{\begin{lstlisting}\end{lstlisting}}

\newlist{bullets}{itemize}{1}
\setlist[bullets]{label=--,leftmargin=1.6em,itemsep=2pt,topsep=4pt}

\newcommand{\varmapStart}{\textbf{VARIABLE MAPPING:}\par\begin{bullets}}
\newcommand{\var}[2]{\item $#1$ — #2}
\newcommand{\varmapEnd}{\end{bullets}}

\newcommand{\glossx}[6]{%
  \textbf{#1}\par
  \begin{bullets}
    \item \textbf{What:} #2
    \item \textbf{Why:} #3
    \item \textbf{How:} #4
    \item \textbf{ELI5:} #5
    \item \textbf{Pitfall/Example:} #6
  \end{bullets}
}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{claim}[theorem]{Claim}

\lstdefinestyle{code}{
  basicstyle=\ttfamily\footnotesize,
  backgroundcolor=\color{black!02},
  frame=single,
  numbers=left, numberstyle=\tiny, numbersep=8pt,
  breaklines=true, breakatwhitespace=true,
  tabsize=4, showstringspaces=false,
  upquote=true, keepspaces=true, columns=fullflexible,
  literate=
    {–}{{-}}1
    {—}{{-}}1
    {…}{{...}}1
    {≤}{{\ensuremath{\le}}}1
    {≥}{{\ensuremath{\ge}}}1
    {≠}{{\ensuremath{\ne}}}1
    {≈}{{\ensuremath{\approx}}}1
    {±}{{\ensuremath{\pm}}}1
    {→}{{\ensuremath{\to}}}1
    {←}{{\ensuremath{\leftarrow}}}1
    {∞}{{\ensuremath{\infty}}}1
    {√}{{\ensuremath{\sqrt{\ }}}}1
    {×}{{\ensuremath{\times}}}1
    {÷}{{\ensuremath{\div}}}1
}

\lstnewenvironment{codepy}[1][]%
  {\lstset{style=code,language=Python,#1}}%
  {}

\newcommand{\inlinecode}[1]{\lstinline[style=code]!#1!}

\newcommand{\LF}[2]{\par\noindent\textbf{#1:}~#2\par}
\newcommand{\WHAT}[1]{\LF{WHAT}{#1}}
\newcommand{\WHY}[1]{\LF{WHY}{#1}}
\newcommand{\HOW}[1]{\LF{HOW}{#1}}
\newcommand{\ELI}[1]{\LF{ELI5}{#1}}
\newcommand{\SCOPE}[1]{\LF{SCOPE}{#1}}
\newcommand{\CONFUSIONS}[1]{\LF{COMMON CONFUSIONS}{#1}}
\newcommand{\APPLICATIONS}[1]{\LF{APPLICATIONS}{#1}}
\newcommand{\FORMULA}[1]{\LF{FORMULA}{#1}}
\newcommand{\CANONICAL}[1]{\LF{CANONICAL FORM}{#1}}
\newcommand{\PRECONDS}[1]{\LF{PRECONDITIONS}{#1}}
\newcommand{\DERIVATION}[1]{\LF{DERIVATION}{#1}}
\newcommand{\EQUIV}[1]{\LF{EQUIVALENT FORMS}{#1}}
\newcommand{\LIMITS}[1]{\LF{LIMIT CASES}{#1}}
\newcommand{\INPUTS}[1]{\LF{INPUTS}{#1}}
\newcommand{\OUTPUTS}[1]{\LF{OUTPUTS}{#1}}
\newcommand{\RESULT}[1]{\LF{RESULT}{#1}}
\newcommand{\INTUITION}[1]{\LF{INTUITION}{#1}}
\newcommand{\PITFALLS}[1]{\LF{PITFALLS}{#1}}
\newcommand{\MODEL}[1]{\LF{CANONICAL MATH MODEL}{#1}}
\newcommand{\ASSUMPTIONS}[1]{\LF{ASSUMPTIONS}{#1}}
\newcommand{\WHICHFORMULA}[1]{\LF{WHICH FORMULA \& WHY}{#1}}
\newcommand{\GOVERN}[1]{\LF{GOVERNING EQUATION(S)}{#1}}
\newcommand{\UNITCHECK}[1]{\LF{UNIT CHECK}{#1}}
\newcommand{\EDGECASES}[1]{\LF{EDGE CASES}{#1}}
\newcommand{\ALTERNATE}[1]{\LF{ALTERNATE APPROACH (sketch)}{#1}}
\newcommand{\PROBLEM}[1]{\LF{PROBLEM}{#1}}
\newcommand{\API}[1]{\LF{API}{#1}}
\newcommand{\COMPLEXITY}[1]{\LF{COMPLEXITY}{#1}}
\newcommand{\FAILMODES}[1]{\LF{FAILURE MODES}{#1}}
\newcommand{\STABILITY}[1]{\LF{NUMERICAL STABILITY}{#1}}
\newcommand{\VALIDATION}[1]{\LF{VALIDATION}{#1}}
\newcommand{\EXPLANATION}[1]{\LF{EXPLANATION}{#1}}
\newcommand{\SCENARIO}[1]{\LF{SCENARIO}{#1}}
\newcommand{\PIPELINE}[1]{\LF{PIPELINE STEPS}{#1}}
\newcommand{\METRICS}[1]{\LF{METRICS}{#1}}
\newcommand{\INTERPRET}[1]{\LF{INTERPRETATION}{#1}}
\newcommand{\NEXTSTEPS}[1]{\LF{LIMITATIONS \& NEXT STEPS}{#1}}

\usepackage{titlesec}
\titleformat{\section}{\Large\bfseries}{\thesection}{0.6em}{}
\usepackage{etoolbox}
\pretocmd{\section}{\clearpage}{}{}

\newcommand{\FormulaPage}[2]{%
  \clearpage
  \section*{Formula #1 — #2}%
  \addcontentsline{toc}{section}{Formula #1 — #2}%
}
\newcommand{\ProblemPage}[2]{%
  \clearpage
  \subsection*{Problem #1: #2}%
  \addcontentsline{toc}{subsection}{Problem #1: #2}%
}
\newcommand{\CodeDemoPage}[1]{%
  \clearpage
  \subsection*{Coding Demo: #1}%
  \addcontentsline{toc}{subsection}{Coding Demo: #1}%
}
\newcommand{\DomainPage}[1]{%
  \clearpage
  \subsection*{#1 (End-to-End)}%
  \addcontentsline{toc}{subsection}{#1 (End-to-End)}%
}

\begin{document}
\title{Comprehensive Study Sheet — LU and QR Factorizations}
\date{\today}
\maketitle
\tableofcontents
\clearpage

\section{Concept Overview}
\WHAT{
A matrix factorization decomposes a matrix $A$ into structured factors.
LU factorization writes $PA=LU$ with $P$ a permutation, $L$ unit lower
triangular, $U$ upper triangular. QR factorization writes $A=QR$ with $Q$
orthogonal (real case) or unitary (complex) and $R$ upper triangular.
Domains: $A\in\mathbb{R}^{m\times n}$. LU mainly for square $n\times n$;
QR for any $m\ge n$. Codomain: pairs or triples of matrices satisfying the
stated structural constraints.
}

\WHY{
They are the backbone of Gaussian elimination and orthogonalization.
LU enables fast solve of $Ax=b$ and determinant computation.
QR provides numerically stable solutions to least squares, orthonormal bases,
and is central to eigenvalue algorithms. They expose triangular structure that
is easy to solve and analyze.
}

\HOW{
1. Start with Gaussian elimination to zero subdiagonal entries via row
operations, tracking permutations to avoid zero pivots, yielding $PA=LU$.
2. For QR, construct orthogonal reflectors or perform Gram--Schmidt to
orthonormalize columns, forming $Q$ and triangular $R$.
3. Use triangular solves (forward and backward substitution) to compute
solutions efficiently.
4. Interpret $L,U$ as elimination multipliers and $Q$ as a rotation that
preserves norms and $R$ as coordinates in an orthonormal basis.
}

\ELI{
LU: take a messy recipe and break it into simple prep steps (lower) and
final assembly (upper). QR: rotate your coordinate system so that your data
align with axes, making measurements and solves simpler.
}

\SCOPE{
LU without pivoting exists if all leading principal minors are nonzero.
With partial pivoting, $PA=LU$ exists for any nonsingular $A$ and for many
singular matrices up to rank revealing. QR exists for all $A$; $R$ may have
zero diagonal entries if rank deficient. Numerical stability: QR is backward
stable; LU with partial pivoting is usually stable but can grow in rare cases.
}

\CONFUSIONS{
LU vs. Cholesky: Cholesky is for symmetric positive definite matrices and
uses one triangular factor; LU is general. QR vs. SVD: QR orthonormalizes
columns but does not give singular values; SVD diagonalizes with orthogonal
factors and nonnegative singular values. Gram--Schmidt vs. Householder:
both produce QR; Householder is numerically more stable.
}

\APPLICATIONS{
\begin{bullets}
\item Solve linear systems and compute determinants efficiently via LU.
\item Least squares fitting and orthonormal basis construction via QR.
\item Eigenvalue algorithms using implicit QR iterations.
\item Conditioning analysis and projections using orthogonal transforms.
\end{bullets}
}

\textbf{ANALYTIC STRUCTURE.}
$L$ is unit lower triangular (ones on diagonal), $U$ upper triangular.
$Q$ satisfies $Q^\top Q = I$ and preserves inner products and norms.
$R$ is upper triangular with nonnegative diagonal in the canonical QR.

\textbf{CANONICAL LINKS.}
LU is linked to Gaussian elimination and determinants.
QR connects to least squares normal equations and orthogonal projections.
Determinant $\det(A)=\operatorname{sgn}(P)\prod_i U_{ii}$ for LU.
Least squares solution $x^\star$ satisfies $R x^\star = Q^\top b$.

\textbf{PROBLEM-TYPE RECOGNITION HEURISTICS.}
\begin{bullets}
\item Triangular structure and forward/back substitution cues LU.
\item Phrases like orthonormalize or projection signal QR.
\item Least squares or overdetermined systems point to QR.
\item Determinant and invertibility checks can use LU pivots.
\end{bullets}

\textbf{SOLUTION STRATEGY BLUEPRINT.}
\begin{bullets}
\item Translate the system or fitting task into matrix form.
\item Decide on LU (solve square systems) or QR (least squares, stability).
\item Compute factors symbolically for small matrices or algorithmically.
\item Solve via triangular systems; interpret pivots or projection geometry.
\item Validate via residuals, norm preservation, and rank checks.
\end{bullets}

\textbf{CONCEPTUAL INVARIANTS.}
LU preserves row space up to permutation; determinant equals product of pivots.
QR preserves Euclidean norms and inner products; projection matrix
$Q_1 Q_1^\top$ is idempotent and symmetric.

\textbf{EDGE INTUITION.}
If a pivot is near zero, LU becomes unstable; pivoting swaps to a larger
entry. If columns are nearly linearly dependent, classical Gram--Schmidt
may lose orthogonality, but Householder QR remains stable. As noise $\to 0$,
QR least squares solution approaches exact solution.

\section{Glossary}
\glossx{LU Factorization}
{Decomposition $PA=LU$ with $P$ permutation, $L$ unit lower triangular,
$U$ upper triangular.}
{Enables efficient solution of $Ax=b$, determinant computation, and reuse
for multiple right-hand sides.}
{Apply Gaussian elimination with partial pivoting to form $P$, record
multipliers into $L$ and the resulting upper matrix as $U$.}
{Like breaking a task into basic building blocks (lower) and final assembly
(upper).}
{Pitfall: attempting LU without pivoting when a pivot is zero or tiny,
leading to failure or instability.}

\glossx{Partial Pivoting}
{Row permutations during elimination to bring the largest magnitude entry
in the current column to the pivot position.}
{Improves numerical stability by controlling growth in multipliers
and reducing roundoff amplification.}
{At step $k$, swap row $k$ with row having maximal $|A_{ik}|$ for $i\ge k$,
then eliminate below the pivot.}
{Choose the strongest block to build upon so the structure stays stable.}
{Pitfall: forgetting to apply the same permutations to $b$ in $Ax=b$ solves.}

\glossx{QR Factorization}
{Decomposition $A=QR$ where $Q$ has orthonormal columns (or is square
orthogonal) and $R$ is upper triangular.}
{Solves least squares stably, constructs orthonormal bases, and is used in
QR eigenvalue algorithms.}
{Use Householder reflections or Modified Gram--Schmidt to orthonormalize
columns, accumulating $Q$ and computing $R$.}
{Rotate to a new coordinate system where directions are perpendicular and
lengths preserved.}
{Pitfall: classical Gram--Schmidt can lose orthogonality; prefer Householder.}

\glossx{Householder Reflection}
{Orthogonal matrix $H=I-2uu^\top$ with unit vector $u$, reflecting across the
hyperplane orthogonal to $u$.}
{Used to zero out subdiagonal parts of a vector or column with high stability.}
{Construct $u$ so that $H$ maps a vector $x$ to $\pm\|x\|e_1$, then apply
$H$ to the matrix to introduce zeros.}
{Like using a mirror to flip a vector across a plane to align it with an axis.}
{Pitfall: wrong sign choice can cause cancellation; pick sign to avoid
catastrophic cancellation.}

\section{Symbol Ledger}
\varmapStart
\var{A\in\mathbb{R}^{m\times n}}{Input matrix to factorize.}
\var{P\in\mathbb{R}^{n\times n}}{Permutation matrix with rows of identity.}
\var{L\in\mathbb{R}^{n\times n}}{Unit lower triangular factor.}
\var{U\in\mathbb{R}^{n\times n}}{Upper triangular factor.}
\var{Q\in\mathbb{R}^{m\times m}}{Orthogonal matrix, $Q^\top Q=I_m$.}
\var{R\in\mathbb{R}^{m\times n}}{Upper triangular or trapezoidal matrix.}
\var{b\in\mathbb{R}^{m}}{Right-hand side vector.}
\var{x\in\mathbb{R}^{n}}{Unknown vector.}
\var{H_k}{Householder reflector at step $k$.}
\var{e_i}{Standard basis vector with 1 at $i$.}
\var{r=\operatorname{rank}(A)}{Rank of $A$.}
\var{I}{Identity matrix of appropriate size.}
\var{\Pi}{Permutation operator acting on indices.}
\varmapEnd

\section{Formula Canon — One Formula Per Page}
\FormulaPage{1}{LU with Partial Pivoting}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
For any $A\in\mathbb{R}^{n\times n}$, there exist a permutation matrix
$P$, unit lower triangular $L$, and upper triangular $U$ such that
$PA=LU$ provided Gaussian elimination with partial pivoting completes.

\WHAT{
This expresses $A$ as a product of a permutation and triangular factors,
encoding elimination multipliers and pivots.
}

\WHY{
Triangular matrices are easy to solve with. LU exposes the structure of
Gaussian elimination, enabling efficient reuse for multiple right-hand sides
and determinant computation.
}

\FORMULA{
\[
PA=LU,\quad P\in\mathbb{R}^{n\times n},\ L\text{ unit lower},\
U\text{ upper triangular}.
\]
}

\CANONICAL{
$A$ square, real. $P$ is a product of transpositions. $L$ has ones on the
diagonal, strictly lower entries are elimination multipliers. $U$ has the
pivots on its diagonal.
}

\PRECONDS{
\begin{bullets}
\item Elimination does not encounter a zero pivot after pivoting.
\item For exact arithmetic, partial pivoting always produces some pivot if
the column is not all zeros; if all zeros, then the matrix is singular and
the factorization is rank revealing.
\end{bullets}
}

\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
Any sequence of Gaussian elimination steps with partial pivoting can be
represented as $PA=LU$ with $P$ a permutation, $L$ unit lower, and $U$
upper triangular.
\end{lemma}
\begin{proof}
At step $k$, let $\Pi_k$ swap row $k$ with the pivot row. The elimination
below the pivot can be written as left-multiplication by a unit lower
triangular matrix $E_k^{-1}$ where $E_k=I+\ell_k e_k^\top$ zeros entries
below the pivot via multipliers $\ell_k$. After $n-1$ steps,
\[
\left(\prod_{k=1}^{n-1}E_k\Pi_k\right)A=U.
\]
Rearrange:
\[
A=\left(\prod_{k=1}^{n-1}\Pi_k^{-1}E_k^{-1}\right)U.
\]
Since $\Pi_k^{-1}=\Pi_k^\top$ and the product of permutations is a
permutation $P^\top$, and the product of unit lower triangular matrices is
unit lower triangular, set $L=\prod_{k=1}^{n-1}\Pi_k^{-1}E_k^{-1}\Pi_k$ in
an equivalent form compatible with global $P$. Then $PA=LU$ with
$P=\prod_{k=1}^{n-1}\Pi_k$. \qedhere
\end{proof}

\DERIVATION{
\begin{align*}
\text{Step 1:}~&\text{Initialize }U^{(1)}=A,\ L=I,\ P=I.\\
\text{Step 2:}~&\text{For }k=1,\dots,n-1,\ \text{select pivot }p
=\arg\max_{i\ge k}|U^{(k)}_{ik}|.\\
&\text{Swap rows }k\leftrightarrow p\text{ in }U^{(k)},\text{ update }
P\leftarrow S_{kp}P,\ L\leftarrow S_{kp}L S_{kp}.\\
\text{Step 3:}~&\text{Compute multipliers } \ell_{ik}=U^{(k)}_{ik}/
U^{(k)}_{kk},\ i=k+1,\dots,n.\\
&\text{Eliminate: } U^{(k+1)}_{i,j}=U^{(k)}_{i,j}-\ell_{ik}U^{(k)}_{k,j}.\\
&\text{Store } L_{ik}=\ell_{ik}.\\
\text{Step 4:}~&\text{After loop, }U=U^{(n)},\ \text{so }PA=LU.
\end{align*}
}

\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Build $P,L,U$ using elimination with pivoting.
\item Solve $PAx=Pb$ via forward substitution $Ly=Pb$, then back substitution
$Ux=y$.
\item Compute $\det(A)=\operatorname{sgn}(P)\prod_i U_{ii}$ when nonzero.
\end{bullets}

\EQUIV{
\begin{bullets}
\item Without pivoting, if all leading principal minors are nonzero,
$A=LU$ with $P=I$.
\item $A=P^\top L U$ and also $A=L(P^\top U)$ where $P^\top U$ is permuted
upper triangular.
\end{bullets}
}

\LIMITS{
\begin{bullets}
\item Singular $A$ produces zero pivots; partial pivoting yields rank-revealing
$PA=LU$ with zero rows in $U$.
\item Growth factor can be large in worst cases; scaling and complete pivoting
can mitigate at higher cost.
\end{bullets}
}

\INPUTS{$A\in\mathbb{R}^{n\times n}$.}

\DERIVATION{
\begin{align*}
\text{Example: }&
A=\begin{bmatrix}2&1\\4&3\end{bmatrix}.\\
\text{Pivot }k=1:&\ p=\arg\max(|2|,|4|)=2,\ S_{12} \text{ swaps rows}.\\
&U^{(1)}=\begin{bmatrix}4&3\\2&1\end{bmatrix},\ P=S_{12}.\\
\ell_{21}&=2/4=1/2,\ L_{21}=1/2.\\
U^{(2)}_{2,:}&=U^{(1)}_{2,:}-\ell_{21}U^{(1)}_{1,:}
=\begin{bmatrix}0&-0.5\end{bmatrix}.\\
&U=\begin{bmatrix}4&3\\0&-0.5\end{bmatrix},\
L=\begin{bmatrix}1&0\\0.5&1\end{bmatrix}.\\
&PA=LU\ \text{verified.}
\end{align*}
}

\RESULT{
Existence of a stable LU representation $PA=LU$ capturing elimination with
partial pivoting, enabling efficient solves and determinant computation.
}

\UNITCHECK{
Matrix sizes: $P,L,U\in\mathbb{R}^{n\times n}$, products well defined.
}

\PITFALLS{
\begin{bullets}
\item Forgetting to permute $L$ consistently when swapping rows mid-process.
\item Dividing by a tiny pivot without pivoting causes large roundoff.
\item Miscounting sign of permutation when computing determinant.
\end{bullets}
}

\INTUITION{
LU stores how you subtract multiples of rows to clear entries below the
diagonal; $P$ records reordering to use strong pivots.
}

\CANONICAL{
\begin{bullets}
\item Canonical identity: $PA=LU$ with $L$ unit lower, $U$ upper triangular.
\item Determinant invariant: $\det(A)=\operatorname{sgn}(P)\prod_i U_{ii}$.
\end{bullets}
}

\FormulaPage{2}{Triangular Solves for $Ax=b$ via LU}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Given $PA=LU$ and $b$, the solution of $Ax=b$ is obtained by
$Ly=Pb$ (forward substitution) and $Ux=y$ (back substitution).

\WHAT{
Compute $x$ solving $Ax=b$ using triangular systems from LU factors.
}

\WHY{
Triangular systems can be solved in $\mathcal{O}(n^2)$, improving efficiency
and enabling reuse of $L,U$ for multiple $b$.
}

\FORMULA{
\[
PAx=Pb,\quad LUx=Pb,\quad Ly=Pb,\quad Ux=y.
\]
}

\CANONICAL{
$A\in\mathbb{R}^{n\times n}$ nonsingular, $P,L,U$ from LU with pivoting.
$L$ unit lower, $U$ nonsingular upper.
}

\PRECONDS{
\begin{bullets}
\item $U$ must be nonsingular; pivots $U_{ii}\ne 0$.
\item $P$ applied to $b$ to match row permutations.
\end{bullets}
}

\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
Forward substitution on a unit lower triangular system $Ly=c$ has a unique
solution for any $c$.
\end{lemma}
\begin{proof}
Proceed inductively. For $i=1$, $y_1=c_1$ since $L_{11}=1$. Suppose
$y_1,\dots,y_{i-1}$ known; equation $i$ is
\[
y_i+\sum_{j=1}^{i-1}L_{ij}y_j=c_i\implies y_i=c_i-\sum_{j<i}L_{ij}y_j.
\]
Thus uniquely determined. \qedhere
\end{proof}

\DERIVATION{
\begin{align*}
\text{Step 1:}~&\text{Compute }c=Pb.\\
\text{Step 2:}~&\text{Forward substitution for }i=1,\dots,n:\\
&y_i=c_i-\sum_{j=1}^{i-1}L_{ij}y_j.\\
\text{Step 3:}~&\text{Backward substitution for }i=n,\dots,1:\\
&x_i=\frac{1}{U_{ii}}\left(y_i-\sum_{j=i+1}^{n}U_{ij}x_j\right).
\end{align*}
}

\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Factor $PA=LU$ once.
\item For each $b$, compute $c=Pb$, then solve $Ly=c$, then $Ux=y$.
\item Check residual $r=b-Ax$ and pivot sizes.
\end{bullets}

\EQUIV{
\begin{bullets}
\item If $P=I$, then $Ax=b$ reduces directly to $Ly=b$ then $Ux=y$.
\item In block form, forward and backward substitution apply blockwise.
\end{bullets}
}

\LIMITS{
\begin{bullets}
\item If $U_{ii}$ is tiny, solution may be ill conditioned.
\item For singular $A$, one obtains no unique solution; pivot zero indicates
rank deficiency.
\end{bullets}
}

\INPUTS{$P,L,U,b$.}

\DERIVATION{
\begin{align*}
\text{Example: }&
A=\begin{bmatrix}2&1\\4&3\end{bmatrix},\
b=\begin{bmatrix}1\\2\end{bmatrix}.\\
&PA=LU\ \text{from prior example},\
P=S_{12},\
L=\begin{bmatrix}1&0\\0.5&1\end{bmatrix},\\
&U=\begin{bmatrix}4&3\\0&-0.5\end{bmatrix}.\\
c&=Pb=\begin{bmatrix}2\\1\end{bmatrix}.\\
y_1&=c_1=2,\quad
y_2=c_2-L_{21}y_1=1-0.5\cdot 2=0.\\
x_2&=y_2/U_{22}=0/(-0.5)=0,\quad
x_1=(y_1-U_{12}x_2)/U_{11}=2/4=0.5.\\
&x=\begin{bmatrix}0.5\\0\end{bmatrix},\
Ax=\begin{bmatrix}1\\2\end{bmatrix}=b.
\end{align*}
}

\RESULT{
Efficient two-stage triangular solve yields $x$ with cost
$\mathcal{O}(n^2)$ after an initial $\mathcal{O}(n^3)$ factorization.
}

\UNITCHECK{
Dimensions consistent: $L,U,P\in\mathbb{R}^{n\times n}$, vectors in
$\mathbb{R}^n$.
}

\PITFALLS{
\begin{bullets}
\item Omitting permutation on $b$ yields wrong $y$.
\item Dividing by zero or tiny $U_{ii}$; check pivots.
\end{bullets}
}

\INTUITION{
Solve lower triangular first to unwind dependencies from elimination, then
solve upper triangular to recover unknowns.
}

\CANONICAL{
\begin{bullets}
\item Canonical equations: $Ly=Pb$, then $Ux=y$.
\item Deterministic algorithm with unique solution if $U$ nonsingular.
\end{bullets}
}

\FormulaPage{3}{QR via Householder Reflections}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
For any $A\in\mathbb{R}^{m\times n}$ with $m\ge n$, there exist orthogonal
$Q\in\mathbb{R}^{m\times m}$ and upper trapezoidal $R\in\mathbb{R}^{m\times n}$
with $A=QR$. If $A$ has full column rank, the leading $n\times n$ block of $R$
is upper triangular with positive diagonal in the canonical form.

\WHAT{
Construct $Q$ as a product of Householder reflectors to introduce zeros
below the diagonal, yielding $A=QR$.
}

\WHY{
Householder QR is backward stable and widely used to solve least squares
and to orthonormalize bases with excellent numerical properties.
}

\FORMULA{
\[
A=Q R,\quad Q=\prod_{k=1}^{n}H_k,\quad H_k=I-2u_k u_k^\top,\
\|u_k\|_2=1.
\]
}

\CANONICAL{
$H_k$ acts on rows $k{:}m$ and columns $k{:}n$ to zero subdiagonal entries
in column $k$. Choose sign so that $R_{kk}\ge 0$ to ensure uniqueness.
}

\PRECONDS{
\begin{bullets}
\item $m\ge n$; if rank deficient, diagonal entries of $R$ may be zero.
\item Use stable sign choice to avoid cancellation: $\alpha=-\operatorname{sgn}(a_{kk})\|x\|_2$.
\end{bullets}
}

\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
If $u\in\mathbb{R}^m$ with $\|u\|_2=1$, then $H=I-2uu^\top$ is orthogonal
and symmetric, and $Hx=x-2u(u^\top x)$ reflects $x$ across the hyperplane
orthogonal to $u$.
\end{lemma}
\begin{proof}
Orthogonality: $H^\top H=(I-2uu^\top)^2=I-4uu^\top+4u(u^\top u)u^\top
=I$. Symmetry is immediate since $H^\top=H$. The action subtracts twice the
projection onto $u$, which is a reflection. \qedhere
\end{proof}

\DERIVATION{
\begin{align*}
\text{Step 1:}~&\text{For }k=1,\dots,n, \text{ let }
x=A_{k:m,k}.\\
&\alpha=-\operatorname{sgn}(x_1)\|x\|_2,\ v=x-\alpha e_1,\ u=v/\|v\|_2.\\
&H_k=\begin{bmatrix}I_{k-1}&0\\0&I-2uu^\top\end{bmatrix}.\\
&\text{Update }A\leftarrow H_k A.\\
\text{Step 2:}~&\text{After loop, }R=A,\ Q=\prod_{k=1}^n H_k^\top
=\prod_{k=1}^n H_k.
\end{align*}
}

\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item For each column, compute Householder vector and apply to trailing matrix.
\item Accumulate $Q$ implicitly or explicitly if required.
\item Enforce $R_{kk}\ge 0$ by sign choice for uniqueness.
\end{bullets}

\EQUIV{
\begin{bullets}
\item Modified Gram--Schmidt also yields $A=Q R$ with $Q^\top Q=I$.
\item Economy QR: $A=Q_1 R_1$, $Q_1\in\mathbb{R}^{m\times n}$ with orthonormal
columns, $R_1\in\mathbb{R}^{n\times n}$ upper triangular.
\end{bullets}
}

\LIMITS{
\begin{bullets}
\item If $x$ is zero, skip reflector; rank deficiency leads to zero diagonal.
\item Forming explicit $Q$ costs $\mathcal{O}(mn^2)$; implicit storage
via Householder vectors is more efficient.
\end{bullets}
}

\INPUTS{$A\in\mathbb{R}^{m\times n}$, $m\ge n$.}

\DERIVATION{
\begin{align*}
\text{Example: }&
A=\begin{bmatrix}2&2\\1&1\\0&1\end{bmatrix},\ m=3,\ n=2.\\
k=1:&\ x=\begin{bmatrix}2\\1\\0\end{bmatrix},\
\|x\|=\sqrt{5},\ \alpha=-\sqrt{5}.\\
&v=x-\alpha e_1=\begin{bmatrix}2+\sqrt{5}\\1\\0\end{bmatrix},\
u=v/\|v\|.\\
&H_1=I-2\frac{vv^\top}{v^\top v}.\
\text{Apply }A^{(2)}=H_1 A.\\
k=2:&\ \tilde{x}=A^{(2)}_{2:3,2},\
\alpha=-\operatorname{sgn}(\tilde{x}_1)\|\tilde{x}\|.\\
&\tilde{v}=\tilde{x}-\alpha e_1,\ \tilde{u}=\tilde{v}/\|\tilde{v}\|.\\
&H_2=I_{1}\oplus(I-2\tilde{u}\tilde{u}^\top).\\
&R=H_2 H_1 A,\ Q=H_1 H_2.\\
&\text{One obtains }R=\begin{bmatrix}\sqrt{5}&\ast\\0&\beta\\0&0\end{bmatrix}.
\end{align*}
}

\RESULT{
Householder procedure yields orthogonal $Q$ and upper $R$ such that $A=QR$
with controlled numerical stability and $R_{kk}\ge 0$.
}

\UNITCHECK{
$Q\in\mathbb{R}^{m\times m}$, $R\in\mathbb{R}^{m\times n}$, product
has size $m\times n$.
}

\PITFALLS{
\begin{bullets}
\item Using $u$ without normalizing $v$ leads to nonorthogonal $H$.
\item Wrong sign choice for $\alpha$ can cause cancellation in $v=x-\alpha e_1$.
\item Explicitly forming $Q$ when not needed wastes time and memory.
\end{bullets}
}

\INTUITION{
Each Householder reflector is a mirror that aligns a column with a coordinate
axis, zeroing components below the diagonal.
}

\CANONICAL{
\begin{bullets}
\item $Q$ orthogonal, $R$ upper triangular with nonnegative diagonal.
\item Compact storage via vectors $v_k$ and scalars $\tau_k=2/(v_k^\top v_k)$.
\end{bullets}
}

\FormulaPage{4}{Least Squares Solution via QR}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
For $A\in\mathbb{R}^{m\times n}$ with $m\ge n$ and full column rank,
the least squares solution $x^\star=\arg\min_x\|Ax-b\|_2$ is given by
$R x^\star = Q^\top b$ when $A=QR$ is a QR factorization with
$Q$ orthogonal and $R$ upper triangular $n\times n$.

\WHAT{
Solve overdetermined systems in the least squares sense using QR to avoid
forming normal equations.
}

\WHY{
QR is numerically stable and avoids squaring the condition number as in
$A^\top A x = A^\top b$.
}

\FORMULA{
\[
x^\star=R^{-1}Q^\top b,\quad \|Ax^\star-b\|_2=\|Q^\top b - Rx^\star\|_2.
\]
}

\CANONICAL{
Economy QR: $A=Q_1 R_1$ with $Q_1\in\mathbb{R}^{m\times n}$,
$R_1\in\mathbb{R}^{n\times n}$. Then $R_1 x^\star = Q_1^\top b$.
}

\PRECONDS{
\begin{bullets}
\item $A$ has full column rank so that $R$ is nonsingular.
\item $Q$ orthogonal so that $Q^\top b$ preserves norm partitions.
\end{bullets}
}

\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
Let $A=Q\begin{bmatrix}R\\0\end{bmatrix}$ with $Q=[Q_1\ Q_2]$ partitioned.
Then for any $x$, $\|Ax-b\|_2^2=\|R x-Q_1^\top b\|_2^2+\|Q_2^\top b\|_2^2$,
minimized when $R x=Q_1^\top b$.
\end{lemma}
\begin{proof}
Orthogonality yields $Q^\top b=\begin{bmatrix}Q_1^\top b\\Q_2^\top b\end{bmatrix}$ and
$Q^\top Ax=\begin{bmatrix}R x\\0\end{bmatrix}$. Then
\[
\|Ax-b\|_2=\|Q^\top(Ax-b)\|_2=\left\|\begin{bmatrix}R x-Q_1^\top b\\
-\,Q_2^\top b\end{bmatrix}\right\|_2,
\]
so the squared norm separates. The first term is minimized by solving the
triangular system $R x=Q_1^\top b$. \qedhere
\end{proof}

\DERIVATION{
\begin{align*}
\text{Step 1:}~&\text{Compute economy QR }A=Q_1 R.\\
\text{Step 2:}~&\text{Compute }c=Q_1^\top b.\\
\text{Step 3:}~&\text{Solve }R x^\star=c\text{ by back substitution}.\\
\text{Step 4:}~&\text{Residual }r=b-Ax^\star=Q_2 Q_2^\top b.
\end{align*}
}

\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Factor $A$ via Householder QR.
\item Form $c=Q_1^\top b$ without explicit $Q$ if possible.
\item Solve $R x=c$; compute residual if needed using $Q_2^\top b$.
\end{bullets}

\EQUIV{
\begin{bullets}
\item Normal equations: $A^\top A x=A^\top b$ with $x=(R^\top R)^{-1}A^\top b$.
\item Using $Q$: $x=R^{-1}Q^\top b$ with full $Q$, same result.
\end{bullets}
}

\LIMITS{
\begin{bullets}
\item If $A$ is rank deficient, replace solve by least-norm solution using
column pivoting or SVD.
\item Overfitting noise is mitigated by regularization; can incorporate into
$R$ via augmented system.
\end{bullets}
}

\INPUTS{$A\in\mathbb{R}^{m\times n}$, $b\in\mathbb{R}^m$, $m\ge n$.}

\DERIVATION{
\begin{align*}
\text{Example: }&
A=\begin{bmatrix}1&1\\1&2\\1&3\end{bmatrix},\
b=\begin{bmatrix}1\\2\\2\end{bmatrix}.\\
&\text{Economy QR gives }Q_1,R\
\text{with }R=\begin{bmatrix}\sqrt{3}&\sqrt{2}\\0&\sqrt{2}\end{bmatrix}.\\
&Q_1^\top b=\begin{bmatrix}\gamma_1\\\gamma_2\end{bmatrix}\
\text{(compute numerically)}.\\
&\text{Solve }R x=c \Rightarrow
x_2=\gamma_2/\sqrt{2},\
x_1=(\gamma_1-\sqrt{2}x_2)/\sqrt{3}.\\
&\text{Numerically }x^\star\approx\begin{bmatrix}0.5\\0.5\end{bmatrix}.
\end{align*}
}

\RESULT{
Least squares solution via triangular solve, avoiding normal equations,
with stable residual splitting by orthogonality.
}

\UNITCHECK{
$Q_1^\top b\in\mathbb{R}^n$, $R\in\mathbb{R}^{n\times n}$; back substitution
valid.
}

\PITFALLS{
\begin{bullets}
\item Forming $Q$ explicitly increases cost; apply $Q^\top$ via reflectors.
\item Solving normal equations can be unstable if $A$ is ill conditioned.
\end{bullets}
}

\INTUITION{
Rotate the space so that the columns of $A$ align with coordinate axes,
solve in that frame, then rotate back.
}

\CANONICAL{
\begin{bullets}
\item Projection: $\hat{b}=Q_1 Q_1^\top b$ is the orthogonal projection of
$b$ onto $\operatorname{col}(A)$.
\item Residual orthogonal to $\operatorname{col}(A)$.
\end{bullets}
}

\FormulaPage{5}{Modified Gram--Schmidt and QR Equivalence}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Modified Gram--Schmidt (MGS) applied to the columns of $A$ produces an
orthonormal set $q_1,\dots,q_n$ and coefficients $R_{ij}$ such that
$A=Q R$ with $Q=[q_1\ \dots\ q_n]$ and $R$ upper triangular.

\WHAT{
Show that MGS yields a QR factorization and relate it to Householder QR.
}

\WHY{
MGS is conceptually simple and useful for small to medium problems and for
understanding orthonormalization, though less stable than Householder.
}

\FORMULA{
\[
\begin{aligned}
&v_1=a_1,\ R_{11}=\|v_1\|_2,\ q_1=v_1/R_{11},\\
&\text{for }k=2,\dots,n:\ \text{for }i=1,\dots,k-1:\
R_{i k}=q_i^\top v_k,\ v_k\leftarrow v_k-R_{ik} q_i,\\
&R_{kk}=\|v_k\|_2,\ q_k=v_k/R_{kk},\quad A=Q R.
\end{aligned}
\]
}

\CANONICAL{
$A$ has full column rank. $Q$ has orthonormal columns. $R$ is upper
triangular with nonnegative diagonal by construction.
}

\PRECONDS{
\begin{bullets}
\item Nonzero norms at each step for full rank; otherwise breakdown.
\item Columns reasonably conditioned for numerical orthogonality.
\end{bullets}
}

\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
Let $Q$ and $R$ be constructed by MGS. Then $A=Q R$ exactly in exact
arithmetic and $Q^\top Q=I$.
\end{lemma}
\begin{proof}
By construction, $v_k$ is orthogonal to all $q_i$ for $i<k$, so $q_k$ are
orthonormal. The recursion gives $a_k=\sum_{i=1}^{k}R_{i k} q_i$, hence
$A=Q R$. \qedhere
\end{proof}

\DERIVATION{
\begin{align*}
\text{Step 1:}~&\text{Initialize }v_k=a_k.\\
\text{Step 2:}~&\text{Orthogonalize }v_k\text{ against }q_1,\dots,q_{k-1}.\\
\text{Step 3:}~&\text{Normalize }q_k=v_k/\|v_k\|_2,\ R_{kk}=\|v_k\|_2.\\
\text{Step 4:}~&\text{Assemble }Q=[q_1,\dots,q_n],\ R=(R_{ik}).
\end{align*}
}

\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Iterate columns, subtract projections onto previously computed $q_i$.
\item Normalize to get $q_k$ and fill $R$ entries.
\item Use $R$ and $Q$ as in Householder QR for solves.
\end{bullets}

\EQUIV{
\begin{bullets}
\item Classical Gram--Schmidt computes projections on original $a_k$;
MGS improves stability by projecting on updated $v_k$.
\item Householder QR produces the same $R$ up to signs on the diagonal.
\end{bullets}
}

\LIMITS{
\begin{bullets}
\item Loss of orthogonality when columns are nearly dependent; reorthogonalize.
\item Cost comparable to Householder but less stable.
\end{bullets}
}

\INPUTS{$A=[a_1,\dots,a_n]\in\mathbb{R}^{m\times n}$.}

\DERIVATION{
\begin{align*}
\text{Example: }&
A=\begin{bmatrix}1&1\\1&2\\1&3\end{bmatrix}.\\
&v_1=a_1,\ R_{11}=\sqrt{3},\ q_1=\frac{1}{\sqrt{3}}\begin{bmatrix}1\\1\\1
\end{bmatrix}.\\
&R_{12}=q_1^\top a_2=\frac{1}{\sqrt{3}}(1+2+3)=\frac{6}{\sqrt{3}}=2\sqrt{3}.\\
&v_2=a_2-R_{12} q_1=\begin{bmatrix}1\\2\\3\end{bmatrix}
-\frac{2\sqrt{3}}{\sqrt{3}}\begin{bmatrix}1\\1\\1\end{bmatrix}
=\begin{bmatrix}0\\1\\2\end{bmatrix}.\\
&R_{22}=\|v_2\|=\sqrt{5},\ q_2=\frac{1}{\sqrt{5}}\begin{bmatrix}0\\1\\2
\end{bmatrix}.\\
&Q=[q_1,q_2],\
R=\begin{bmatrix}\sqrt{3}&2\sqrt{3}\\0&\sqrt{5}\end{bmatrix}.\
A=Q R.
\end{align*}
}

\RESULT{
MGS yields $A=Q R$ and orthonormal columns, suitable for least squares and
basis construction, with caution regarding stability.
}

\UNITCHECK{
$Q\in\mathbb{R}^{m\times n}$ with $Q^\top Q=I_n$, $R\in\mathbb{R}^{n\times n}$.
}

\PITFALLS{
\begin{bullets}
\item Skipping normalization makes $Q$ nonorthonormal.
\item Projecting on stale vectors (classical GS) may lose orthogonality.
\end{bullets}
}

\INTUITION{
Peel off components of each column along already built orthonormal
directions, leaving a perpendicular remainder to normalize.
}

\CANONICAL{
\begin{bullets}
\item $A=Q R$ with $R$ upper triangular, $Q^\top Q=I$.
\item Reorthogonalization restores orthogonality when needed.
\end{bullets}
}

\section{10 Exhaustive Problems and Solutions}
\ProblemPage{1}{Textbook: Hand LU Factorization with Pivoting}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Compute $P,L,U$ such that $PA=LU$ and solve $Ax=b$.

\PROBLEM{
Given
$A=\begin{bmatrix}0&2&1\\2&2&3\\4&1&8\end{bmatrix}$ and
$b=\begin{bmatrix}1\\4\\10\end{bmatrix}$,
find $P,L,U$ via partial pivoting, then compute $x$ solving $Ax=b$.
}

\MODEL{
\[
PA=LU,\quad Ly=Pb,\quad Ux=y.
\]
}

\ASSUMPTIONS{
\begin{bullets}
\item Exact arithmetic, partial pivoting at each step.
\item $A$ nonsingular so that $U$ has nonzero diagonal.
\end{bullets}
}

\varmapStart
\var{A}{Coefficient matrix $3\times 3$.}
\var{P}{Permutation matrix from pivoting.}
\var{L}{Unit lower triangular multipliers.}
\var{U}{Upper triangular resulting from elimination.}
\var{b}{Right-hand side vector.}
\var{x}{Solution vector.}
\varmapEnd

\WHICHFORMULA{
Formula 1 (LU with partial pivoting) and Formula 2 (triangular solves).
}

\GOVERN{
\[
PA=LU,\quad Ly=Pb,\quad Ux=y.
\]
}

\INPUTS{$A,b$ as above.}

\DERIVATION{
\begin{align*}
\text{Step 1 (Pivot 1):}&\
\text{Column 1 entries }[0,2,4],\ \max=4\text{ at row }3.\\
&P\leftarrow S_{13},\ U^{(1)}=\begin{bmatrix}4&1&8\\2&2&3\\0&2&1\end{bmatrix},\
L=I.\\
\ell_{21}&=2/4=0.5,\ L_{21}=0.5,\ U^{(1)}_{2,:}\leftarrow
U^{(1)}_{2,:}-0.5\,U^{(1)}_{1,:}\\
&=\begin{bmatrix}0&1.5&-1\end{bmatrix}.\\
\ell_{31}&=0/4=0,\ L_{31}=0,\ U^{(1)}_{3,:}\text{ unchanged}.\\
\text{Step 2 (Pivot 2):}&\
\text{Column 2 entries rows }2{:}3:\ [1.5,2],\ \max=2\text{ at row }3.\\
&\text{Swap rows }2\leftrightarrow 3\text{ in }U,\ P\leftarrow S_{23}P.\\
&L\text{ swap subrows }2,3\text{ below column }1:\ L_{21}\leftrightarrow L_{31}.\\
&U=\begin{bmatrix}4&1&8\\0&2&1\\0&1.5&-1\end{bmatrix},\
L=\begin{bmatrix}1&0&0\\0&1&0\\0.5&0&1\end{bmatrix}.\\
\ell_{32}&=1.5/2=0.75,\ L_{32}=0.75.\\
&U_{3,:}\leftarrow U_{3,:}-0.75\,U_{2,:}
=\begin{bmatrix}0&0&-1-0.75\end{bmatrix}
=\begin{bmatrix}0&0&-1.75\end{bmatrix}.\\
\text{Final:}&\
U=\begin{bmatrix}4&1&8\\0&2&1\\0&0&-1.75\end{bmatrix},\\
&P=S_{23}S_{13}=
\begin{bmatrix}0&0&1\\0&1&0\\1&0&0\end{bmatrix}.\\
\text{Solve:}&\
c=Pb=\begin{bmatrix}10\\4\\1\end{bmatrix}.\\
y_1&=10,\ y_2=4,\ y_3=1-0.75\cdot 4= -2.\\
x_3&=(-2)/(-1.75)=\frac{8}{7}\approx 1.142857.\\
x_2&=(y_2-U_{23}x_3)/U_{22}=(4-1\cdot \tfrac{8}{7})/2
=\tfrac{10}{7}\approx 1.428571.\\
x_1&=(y_1-U_{12}x_2-U_{13}x_3)/U_{11}\\
&=(10-1\cdot\tfrac{10}{7}-8\cdot\tfrac{8}{7})/4
=(-\tfrac{54}{7})/4=-\tfrac{27}{14}\approx -1.928571.
\end{align*}
}

\RESULT{
$P=\begin{bmatrix}0&0&1\\0&1&0\\1&0&0\end{bmatrix}$,
$L=\begin{bmatrix}1&0&0\\0&1&0\\0.5&0.75&1\end{bmatrix}$,
$U=\begin{bmatrix}4&1&8\\0&2&1\\0&0&-1.75\end{bmatrix}$,
$x=\begin{bmatrix}-27/14\\10/7\\8/7\end{bmatrix}$.
}

\UNITCHECK{
Matrix multiplications defined; triangular solves dimensionally consistent.
}

\EDGECASES{
\begin{bullets}
\item If $A_{11}\ne 0$, first swap unnecessary; algorithm still correct.
\item Zero pivot at step 2 would indicate singularity or require further
pivoting.
\end{bullets}
}

\ALTERNATE{
Use complete pivoting to further control growth, at the cost of column swaps
and tracking $PAQ=LU$.
}

\VALIDATION{
\begin{bullets}
\item Compute residual $r=b-Ax$; numerically it is zero in exact arithmetic.
\item Verify $PA- LU=0$ numerically to tolerance.
\end{bullets}
}

\INTUITION{
Reorder rows to place strong entries on the diagonal, then peel away
below-diagonal entries with multipliers recorded in $L$.
}

\CANONICAL{
\begin{bullets}
\item $PA=LU$ with $L$ unit lower and $U$ upper triangular.
\item Solve via forward and backward substitution.
\end{bullets}
}

\ProblemPage{2}{Textbook: Householder QR by Hand}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Compute a QR factorization of a $3\times 2$ matrix using Householder
reflections.

\PROBLEM{
For $A=\begin{bmatrix}4&2\\3&-1\\0&2\end{bmatrix}$, find reflectors
$H_1,H_2$ and $Q=H_1 H_2$, $R=Q^\top A$ with $R_{kk}\ge 0$.
}

\MODEL{
\[
H_k=I-2u_k u_k^\top,\ \|u_k\|=1,\ R=H_2 H_1 A,\ Q=H_1 H_2.
\]
}

\ASSUMPTIONS{
\begin{bullets}
\item Use stable sign choice $\alpha=-\operatorname{sgn}(x_1)\|x\|$ each step.
\item $m=3\ge n=2$, full column rank.
\end{bullets}
}

\varmapStart
\var{A}{Input $3\times 2$ matrix.}
\var{H_k}{Householder reflectors.}
\var{Q}{Orthogonal matrix.}
\var{R}{Upper trapezoidal matrix.}
\varmapEnd

\WHICHFORMULA{
Formula 3 (QR via Householder).
}

\GOVERN{
\[
A=Q R,\quad Q=\prod_{k=1}^{2}H_k,\ R=H_2 H_1 A.
\]
}

\INPUTS{$A$ specified above.}

\DERIVATION{
\begin{align*}
k=1:&\ x=A_{1:3,1}=\begin{bmatrix}4\\3\\0\end{bmatrix},\
\|x\|=5,\ \alpha=-5.\\
&v=x-\alpha e_1=\begin{bmatrix}9\\3\\0\end{bmatrix},\
u=\frac{1}{\sqrt{90}}\begin{bmatrix}9\\3\\0\end{bmatrix}.\\
&H_1=I-2uu^\top.\\
&H_1 A=
\begin{bmatrix}
5&\ast\\
0&\ast\\
0&\ast
\end{bmatrix}.\\
k=2:&\ \tilde{x}=(H_1 A)_{2:3,2}=\begin{bmatrix}\tilde{x}_1\\\tilde{x}_2
\end{bmatrix}.\\
&\|\tilde{x}\|=\sqrt{\tilde{x}_1^2+\tilde{x}_2^2},\
\alpha=-\operatorname{sgn}(\tilde{x}_1)\|\tilde{x}\|.\\
&\tilde{v}=\tilde{x}-\alpha e_1,\
\tilde{u}=\tilde{v}/\|\tilde{v}\|.\\
&H_2=I_1\oplus (I-2\tilde{u}\tilde{u}^\top).\\
&R=H_2 H_1 A=
\begin{bmatrix}
5 & r_{12}\\
0 & \rho\\
0 & 0
\end{bmatrix},\ \rho\ge 0.\\
&Q=H_1 H_2,\ \text{orthogonal}.
\end{align*}
}

\RESULT{
$A=Q R$ with $R$ upper triangular in first $2\times 2$ block and
nonnegative diagonal; $Q$ orthogonal.
}

\UNITCHECK{
$Q\in\mathbb{R}^{3\times 3}$, $R\in\mathbb{R}^{3\times 2}$; $Q R$ matches
$A$ in size.
}

\EDGECASES{
\begin{bullets}
\item If $\tilde{x}_1=0$, choose $\alpha$ negative to maintain $\rho\ge 0$.
\item If $\tilde{x}=0$, skip second reflector.
\end{bullets}
}

\ALTERNATE{
Use Modified Gram--Schmidt to compute the same $Q$ and $R$ up to signs on
columns of $Q$ and rows of $R$.
}

\VALIDATION{
\begin{bullets}
\item Check $Q^\top Q=I$ and $Q^\top A$ upper triangular in first $2$ rows.
\item Compute $\|A-QR\|_F$; should be numerically small.
\end{bullets}
}

\INTUITION{
First reflector aligns the first column with the first axis; second reflector
zeros subdiagonal of the second column within the reduced subspace.
}

\CANONICAL{
\begin{bullets}
\item $Q$ orthogonal, $R$ upper triangular with nonnegative diagonal.
\item Stable sign convention for uniqueness.
\end{bullets}
}

\ProblemPage{3}{Textbook: Least Squares via QR}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Solve an overdetermined system by QR and compare with normal equations.

\PROBLEM{
Fit $y\approx \beta_0+\beta_1 x$ to data
$(x,y)=(0,1),(1,2),(2,2)$ using least squares via QR.
}

\MODEL{
\[
A=\begin{bmatrix}1&0\\1&1\\1&2\end{bmatrix},\ b=\begin{bmatrix}1\\2\\2\end{bmatrix},\
A=Q_1 R,\ R x=Q_1^\top b.
\]
}

\ASSUMPTIONS{
\begin{bullets}
\item Full column rank of $A$, $m=3\ge n=2$.
\item Use economy QR.
\end{bullets}
}

\varmapStart
\var{A}{Design matrix with bias and slope columns.}
\var{b}{Targets.}
\var{Q_1}{Orthonormal basis for column space.}
\var{R}{Upper triangular $2\times 2$.}
\var{x}{Coefficients $(\beta_0,\beta_1)$.}
\varmapEnd

\WHICHFORMULA{
Formula 4 (Least squares via QR).
}

\GOVERN{
\[
x^\star=\arg\min_x\|Ax-b\|_2,\ R x^\star=Q_1^\top b.
\]
}

\INPUTS{$A,b$ above.}

\DERIVATION{
\begin{align*}
&\text{Compute economy QR (by MGS example):}\\
&Q_1=\begin{bmatrix}
\frac{1}{\sqrt{3}} & 0\\
\frac{1}{\sqrt{3}} & \frac{1}{\sqrt{2}}\\
\frac{1}{\sqrt{3}} & \frac{2}{\sqrt{2}}
\end{bmatrix},\
R=\begin{bmatrix}\sqrt{3}&\frac{3}{\sqrt{3}}\\0&\sqrt{2}\end{bmatrix}
=\begin{bmatrix}\sqrt{3}&\sqrt{3}\\0&\sqrt{2}\end{bmatrix}.\\
&c=Q_1^\top b=\begin{bmatrix}
\frac{1}{\sqrt{3}}(1+2+2)\\
\frac{1}{\sqrt{2}}(0\cdot 1+1\cdot 2+2\cdot 2)
\end{bmatrix}
=\begin{bmatrix}\frac{5}{\sqrt{3}}\\ \frac{6}{\sqrt{2}}\end{bmatrix}.\\
&\text{Back substitution: }x_2=c_2/\sqrt{2}=3,\ 
x_1=(c_1-\sqrt{3}x_2)/\sqrt{3}=(\tfrac{5}{\sqrt{3}}-3\sqrt{3})/\sqrt{3}\\
&=\frac{5-9}{3}=-\frac{4}{3}.\\
&x^\star=\begin{bmatrix}-4/3\\3\end{bmatrix}.
\end{align*}
}

\RESULT{
Best fit line $\hat{y}=-\tfrac{4}{3}+3x$. Residual norm minimized.
}

\UNITCHECK{
$Q_1^\top b\in\mathbb{R}^2$, $R\in\mathbb{R}^{2\times 2}$; solve valid.
}

\EDGECASES{
\begin{bullets}
\item If two $x$ values coincide with same $y$, rank may drop; QR detects.
\item If $m=n$, solution reduces to square solve when nonsingular.
\end{bullets}
}

\ALTERNATE{
Solve normal equations $A^\top A x=A^\top b$ to get same $x$ in exact
arithmetic; numerically less stable.
}

\VALIDATION{
\begin{bullets}
\item Compute residual $r=b-Ax^\star$; verify orthogonality $Q_1^\top r=0$.
\item Compare with normal equation solution.
\end{bullets}
}

\INTUITION{
Project $b$ onto the plane spanned by columns of $A$, then read coordinates
in that orthonormal basis.
}

\CANONICAL{
\begin{bullets}
\item $x^\star=R^{-1}Q_1^\top b$ with $R$ upper triangular.
\item Residual orthogonal to column space of $A$.
\end{bullets}
}

\ProblemPage{4}{Narrative: Alice Reorders for Stability}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Row permutations can greatly affect LU pivot sizes and stability.

\PROBLEM{
Alice must solve $Ax=b$ where
$A=\begin{bmatrix}\epsilon&1\\1&1\end{bmatrix}$, $0<\epsilon\ll 1$.
She considers LU without pivoting versus with partial pivoting.
Quantify the multipliers and pivot sizes and explain stability differences.
}

\MODEL{
\[
A=\begin{bmatrix}\epsilon&1\\1&1\end{bmatrix},\quad
\text{No pivoting: }L,U\text{ from direct elimination};\
\text{Pivoting: }PA=LU.
\]
}

\ASSUMPTIONS{
\begin{bullets}
\item Floating point arithmetic with unit roundoff $u$; qualitative analysis.
\item $\epsilon$ small, e.g., $\epsilon=10^{-8}$.
\end{bullets}
}

\varmapStart
\var{\epsilon}{Small positive scalar.}
\var{L,U}{Triangular factors without pivoting.}
\var{P}{Permutation for partial pivoting.}
\varmapEnd

\WHICHFORMULA{
Formula 1 (LU with pivoting) clarifies multipliers; Formula 2 for solve.
}

\GOVERN{
\[
\text{Multiplier } \ell_{21}=A_{21}/A_{11}.
\]
}

\INPUTS{$\epsilon$.}

\DERIVATION{
\begin{align*}
\text{Without pivoting: }&\ \ell_{21}=1/\epsilon\gg 1.\\
&U=\begin{bmatrix}\epsilon&1\\0&1-1/\epsilon\end{bmatrix}
\approx \begin{bmatrix}\epsilon&1\\0&-\tfrac{1}{\epsilon}\end{bmatrix}.\\
&\text{Large growth, roundoff amplified by }\mathcal{O}(1/\epsilon).\\
\text{With pivoting: }&\ P=\begin{bmatrix}0&1\\1&0\end{bmatrix},\
PA=\begin{bmatrix}1&1\\\epsilon&1\end{bmatrix}.\\
&\ell_{21}=\epsilon/1=\epsilon\ll 1.\\
&U=\begin{bmatrix}1&1\\0&1-\epsilon\end{bmatrix}\approx
\begin{bmatrix}1&1\\0&1\end{bmatrix}.\\
&\text{No growth, stable multipliers.}
\end{align*}
}

\RESULT{
Partial pivoting reduces multipliers from $\mathcal{O}(1/\epsilon)$ to
$\mathcal{O}(\epsilon)$, drastically improving stability.
}

\UNITCHECK{
All matrices $2\times 2$; operations defined.
}

\EDGECASES{
\begin{bullets}
\item If $\epsilon=0$, no pivoting fails immediately; pivoting swaps rows and
solves robustly.
\item If $\epsilon$ moderate, both methods similar.
\end{bullets}
}

\ALTERNATE{
Use complete pivoting to also control column growth; unnecessary here.
}

\VALIDATION{
\begin{bullets}
\item Compute residuals for a test $b$ numerically as $\epsilon$ varies.
\item Observe backward error remains small with pivoting.
\end{bullets}
}

\INTUITION{
Choose a solid base pivot; do not stand on a tiny, shaky number.
}

\CANONICAL{
\begin{bullets}
\item Growth factor controlled by pivot strategy.
\item $PA=LU$ recommended in practice.
\end{bullets}
}

\ProblemPage{5}{Narrative: Bob Orthogonalizes for Projection}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Orthogonal projections via QR minimize squared distances.

\PROBLEM{
Bob wants to project $b=\begin{bmatrix}1\\0\\1\end{bmatrix}$ onto the span
of $a_1=\begin{bmatrix}1\\1\\0\end{bmatrix}$ and
$a_2=\begin{bmatrix}1\\0\\1\end{bmatrix}$. Use QR to find the projection
and explain why the residual is orthogonal to both $a_1$ and $a_2$.
}

\MODEL{
\[
A=[a_1,a_2],\ A=Q_1 R,\ \hat{b}=Q_1 Q_1^\top b,\ r=b-\hat{b}.
\]
}

\ASSUMPTIONS{
\begin{bullets}
\item Full column rank holds for $a_1,a_2$.
\item Use MGS for hand computation.
\end{bullets}
}

\varmapStart
\var{A}{Matrix with columns $a_1,a_2$.}
\var{Q_1}{Orthonormal basis.}
\var{R}{Upper triangular.}
\var{\hat{b}}{Projection of $b$ onto $\operatorname{col}(A)$.}
\var{r}{Residual vector.}
\varmapEnd

\WHICHFORMULA{
Formula 5 (MGS) and Formula 4 (least squares via QR).
}

\GOVERN{
\[
\hat{b}=Q_1 Q_1^\top b,\quad r=(I-Q_1 Q_1^\top)b\perp \operatorname{col}(A).
\]
}

\INPUTS{$a_1,a_2,b$ as above.}

\DERIVATION{
\begin{align*}
&v_1=a_1,\ \|v_1\|=\sqrt{2},\ q_1=\tfrac{1}{\sqrt{2}}\begin{bmatrix}1\\1\\0
\end{bmatrix}.\\
&R_{12}=q_1^\top a_2=\tfrac{1}{\sqrt{2}}(1+0+0)=\tfrac{1}{\sqrt{2}}.\\
&v_2=a_2-R_{12}q_1=\begin{bmatrix}1\\0\\1\end{bmatrix}
-\tfrac{1}{2}\begin{bmatrix}1\\1\\0\end{bmatrix}
=\begin{bmatrix}0.5\\-0.5\\1\end{bmatrix}.\\
&\|v_2\|=\sqrt{0.25+0.25+1}= \sqrt{1.5}=\sqrt{\tfrac{3}{2}}.\\
&q_2=\frac{1}{\sqrt{6}}\begin{bmatrix}1\\-1\\2\end{bmatrix}.\\
&Q_1=[q_1,q_2].\ \hat{b}=Q_1(Q_1^\top b).\\
&Q_1^\top b=\begin{bmatrix}
\tfrac{1}{\sqrt{2}}(1+0+0)=\tfrac{1}{\sqrt{2}}\\
\tfrac{1}{\sqrt{6}}(1\cdot 1+(-1)\cdot 0+2\cdot 1)=\tfrac{3}{\sqrt{6}}
\end{bmatrix}.\\
&\hat{b}=\tfrac{1}{\sqrt{2}}q_1+\tfrac{3}{\sqrt{6}}q_2
=\tfrac{1}{2}\begin{bmatrix}1\\1\\0\end{bmatrix}
+\tfrac{1}{2}\begin{bmatrix}1\\-1\\2\end{bmatrix}
=\begin{bmatrix}1\\0\\1\end{bmatrix}=b.\\
&r=b-\hat{b}=0.
\end{align*}
}

\RESULT{
$b$ already lies in the span, so the projection equals $b$ and the residual
is zero, therefore orthogonal to $a_1,a_2$.
}

\UNITCHECK{
All vectors in $\mathbb{R}^3$; inner products defined.
}

\EDGECASES{
\begin{bullets}
\item If $b$ had a component orthogonal to the span, residual equals that
component and is orthogonal to both $a_1,a_2$.
\item If $a_1,a_2$ were dependent, rank deficiency arises.
\end{bullets}
}

\ALTERNATE{
Compute projection matrix $P=A(A^\top A)^{-1}A^\top$ and apply to $b$;
equivalent to $Q_1 Q_1^\top$ in exact arithmetic.
}

\VALIDATION{
\begin{bullets}
\item Verify $Q_1^\top r=0$ numerically.
\item Check $A^\top r=0$ directly.
\end{bullets}
}

\INTUITION{
Orthogonalizing creates perpendicular axes; projection keeps components along
those axes, discarding perpendicular parts.
}

\CANONICAL{
\begin{bullets}
\item $\hat{b}=Q_1 Q_1^\top b$ is the orthogonal projection.
\item Residual orthogonal to column space.
\end{bullets}
}

\ProblemPage{6}{Expectation Puzzle: Residual Energy via QR}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Expected squared residual of least squares equals noise variance times
degrees of freedom.

\PROBLEM{
Let $A\in\mathbb{R}^{m\times n}$ with full column rank and fixed.
Let $b=Ax_{\mathrm{true}}+\varepsilon$, where entries of $\varepsilon$
are i.i.d. with mean $0$ and variance $\sigma^2$. Using QR, show that
$\mathbb{E}\|b-Ax^\star\|_2^2=(m-n)\sigma^2$, where $x^\star$ is the
least squares solution. Interpret as an expectation puzzle.
}

\MODEL{
\[
A=Q_1 R,\ x^\star=R^{-1}Q_1^\top b,\ r=b-Ax^\star=Q_2 Q_2^\top b.
\]
}

\ASSUMPTIONS{
\begin{bullets}
\item $A$ deterministic, full column rank $n$, $m>n$.
\item $\varepsilon$ zero mean, covariance $\sigma^2 I$.
\end{bullets}
}

\varmapStart
\var{Q_1,Q_2}{Orthonormal bases completing $Q=[Q_1\ Q_2]$.}
\var{r}{Residual vector.}
\var{\sigma^2}{Noise variance.}
\varmapEnd

\WHICHFORMULA{
Formula 4 (Least squares via QR) and orthogonality of $Q$.
}

\GOVERN{
\[
r=Q_2 Q_2^\top \varepsilon,\quad \|r\|_2^2=\|Q_2^\top \varepsilon\|_2^2.
\]
}

\INPUTS{$m,n,\sigma^2$, with $m>n$.}

\DERIVATION{
\begin{align*}
&Q^\top b=\begin{bmatrix}Q_1^\top b\\ Q_2^\top b\end{bmatrix}
=\begin{bmatrix}Q_1^\top (Ax_{\mathrm{true}}+\varepsilon)\\
Q_2^\top (Ax_{\mathrm{true}}+\varepsilon)\end{bmatrix}\\
&=\begin{bmatrix}R x_{\mathrm{true}}+Q_1^\top \varepsilon\\
0+Q_2^\top \varepsilon\end{bmatrix}.\\
&\text{So }x^\star \text{ solves }R x^\star=Q_1^\top b
\Rightarrow x^\star=x_{\mathrm{true}}+R^{-1}Q_1^\top\varepsilon.\\
&r=b-Ax^\star=Q_2 Q_2^\top \varepsilon.\\
&\Rightarrow \|r\|_2^2=\|Q_2^\top \varepsilon\|_2^2.\\
&\mathbb{E}\|r\|_2^2=\mathbb{E}\operatorname{tr}\big((Q_2^\top \varepsilon)
(Q_2^\top \varepsilon)^\top\big)\\
&=\mathbb{E}\operatorname{tr}\big(Q_2^\top \varepsilon \varepsilon^\top Q_2\big)
=\operatorname{tr}\big(Q_2^\top \mathbb{E}[\varepsilon\varepsilon^\top] Q_2\big)\\
&=\operatorname{tr}\big(Q_2^\top (\sigma^2 I) Q_2\big)=\sigma^2
\operatorname{tr}(Q_2^\top Q_2)=\sigma^2 (m-n).
\end{align*}
}

\RESULT{
$\mathbb{E}\|r\|_2^2=(m-n)\sigma^2$; expected residual energy equals noise
variance times degrees of freedom.
}

\UNITCHECK{
$Q_2^\top Q_2=I_{m-n}$; trace dimension equals $m-n$.
}

\EDGECASES{
\begin{bullets}
\item If $m=n$, residual zero in exact arithmetic; expectation is $0$.
\item If noise is correlated, replace $\sigma^2 I$ by covariance $\Sigma$,
and result becomes $\operatorname{tr}(Q_2^\top \Sigma Q_2)$.
\end{bullets}
}

\ALTERNATE{
Derive via projection matrix $P=Q_1 Q_1^\top$ and $r=(I-P)\varepsilon$,
so $\mathbb{E}\|r\|^2=\operatorname{tr}((I-P)\sigma^2 I)=\sigma^2(m-n)$.
}

\VALIDATION{
\begin{bullets}
\item Simulate with fixed seed and verify empirical mean approaches theory.
\item Check independence of $A$ specifics except its column count.
\end{bullets}
}

\INTUITION{
Noise splits into fitted part of dimension $n$ and residual part of
dimension $m-n$; each contributes $\sigma^2$ on average per dimension.
}

\CANONICAL{
\begin{bullets}
\item Orthogonality partitions energy; projection preserves expectations.
\item Degrees of freedom interpretation in linear regression.
\end{bullets}
}

\ProblemPage{7}{Proof: Determinant from LU Factors}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Relate determinant of $A$ to pivots of $U$ and sign of permutation $P$.

\PROBLEM{
Show that if $PA=LU$, then $\det(A)=\operatorname{sgn}(P)\prod_{i=1}^n U_{ii}$.
}

\MODEL{
\[
PA=LU,\ \det(PA)=\det(L)\det(U),\ \det(P)=\operatorname{sgn}(P).
\]
}

\ASSUMPTIONS{
\begin{bullets}
\item $A$ nonsingular; $L$ unit lower triangular.
\item $P$ is a product of transpositions.
\end{bullets}
}

\varmapStart
\var{P}{Permutation matrix.}
\var{L}{Unit lower triangular.}
\var{U}{Upper triangular.}
\varmapEnd

\WHICHFORMULA{
Formula 1 (LU with pivoting).
}

\GOVERN{
\[
\det(PA)=\det(P)\det(A)=\det(L)\det(U).
\]
}

\INPUTS{$P,L,U$.}

\DERIVATION{
\begin{align*}
&\det(P)\det(A)=\det(L)\det(U).\\
&\det(L)=1\ \text{since unit lower triangular}.\\
&\det(U)=\prod_{i=1}^n U_{ii}.\\
&\Rightarrow \det(A)=\det(P)^{-1}\det(U)=\operatorname{sgn}(P)\prod_i U_{ii},
\end{align*}
\text{since }\det(P)^{-1}=\det(P)=\operatorname{sgn}(P).
}

\RESULT{
$\det(A)=\operatorname{sgn}(P)\prod_{i=1}^n U_{ii}$.
}

\UNITCHECK{
Determinant scalars; product of diagonal elements well defined.
}

\EDGECASES{
\begin{bullets}
\item If $A$ singular, some $U_{ii}=0$, determinant zero.
\item If $P=I$, sign is $+1$.
\end{bullets}
}

\ALTERNATE{
Prove by considering row operations: each row swap changes sign; row
addition leaves determinant unchanged; scaling not used in LU with unit $L$.
}

\VALIDATION{
\begin{bullets}
\item Numeric check on random matrices with pivoting.
\end{bullets}
}

\INTUITION{
Determinant equals product of pivots up to row swap sign.
}

\CANONICAL{
\begin{bullets}
\item Determinant multiplicative and triangular determinants are products
of diagonals.
\end{bullets}
}

\ProblemPage{8}{Proof: Uniqueness of QR with Positive Diagonal}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
If $A$ has full column rank, then the QR factorization $A=Q_1 R$ with
$Q_1^\top Q_1=I$ and $R$ upper triangular with positive diagonal is unique.

\PROBLEM{
Prove uniqueness of QR under the convention $R_{ii}>0$.
}

\MODEL{
\[
A=Q_1 R=Q_1' R',\ Q_1^\top Q_1=I,\ (Q_1')^\top Q_1'=I,\ R,R'
\text{ upper triangular with }R_{ii},R'_{ii}>0.
\]
}

\ASSUMPTIONS{
\begin{bullets}
\item $A$ has full column rank, square $R,R'$ invertible.
\end{bullets}
}

\varmapStart
\var{Q_1,Q_1'}{Orthogonal column matrices.}
\var{R,R'}{Upper triangular with positive diagonal.}
\varmapEnd

\WHICHFORMULA{
Formula 3 and properties of orthogonal matrices.
}

\GOVERN{
\[
Q_1^\top Q_1'=S,\ \text{orthogonal},\ R=S R'.
\]
}

\INPUTS{$A$ with full column rank.}

\DERIVATION{
\begin{align*}
&A=Q_1 R=Q_1' R'\ \Rightarrow Q_1^\top A=R,\ (Q_1')^\top A=R'.\\
&Q_1^\top A=Q_1^\top Q_1' R'=S R',\ S=Q_1^\top Q_1'\ \text{orthogonal}.\\
&\text{Thus }R=S R'.\ \text{Both }R,R'\text{ upper triangular with }R_{ii},R'_{ii}>0.\\
&\text{But }S=R (R')^{-1}\ \text{is upper triangular and orthogonal}.\\
&\text{An orthogonal upper triangular matrix must be diagonal with }\pm 1.\\
&\text{Since }R_{ii},R'_{ii}>0,\ \text{the diagonal of }S\text{ is }+1.\\
&\Rightarrow S=I,\ R=R',\ Q_1=Q_1'.
\end{align*}
}

\RESULT{
The economy QR factorization with positive diagonal $R$ is unique.
}

\UNITCHECK{
Matrix sizes consistent: $Q_1\in\mathbb{R}^{m\times n}$, $R\in\mathbb{R}^{n\times n}$.
}

\EDGECASES{
\begin{bullets}
\item If some $R_{ii}=0$ (rank deficiency), uniqueness fails; one can insert
sign flips in zero rows or columns.
\item If diagonal positivity not enforced, columns may differ by signs.
\end{bullets}
}

\ALTERNATE{
Use Gram--Schmidt uniqueness of orthonormal basis with positivity constraint
on first nonzero component of each $q_i$.
}

\VALIDATION{
\begin{bullets}
\item Numerically compute two QR factorizations and enforce positive diagonal;
they match to rounding.
\end{bullets}
}

\INTUITION{
Orthogonal transforms that preserve an upper triangular structure with
positive diagonal must be identity.
}

\CANONICAL{
\begin{bullets}
\item QR unique up to signs, fixed by $R_{ii}>0$.
\end{bullets}
}

\ProblemPage{9}{Combo: Conditioning of Normal Equations vs. QR}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Conditioning of $A^\top A$ squares the condition number of $A$.

\PROBLEM{
Show that $\kappa_2(A^\top A)=(\kappa_2(A))^2$ and discuss implications
for least squares solved via normal equations versus QR.
}

\MODEL{
\[
\kappa_2(A)=\|A\|_2\|A^\dagger\|_2=\frac{\sigma_{\max}(A)}{\sigma_{\min}(A)}.
\]
}

\ASSUMPTIONS{
\begin{bullets}
\item $A$ full column rank; singular values positive.
\end{bullets}
}

\varmapStart
\var{\sigma_{\max},\sigma_{\min}}{Largest and smallest singular values.}
\var{\kappa_2}{Spectral condition number.}
\varmapEnd

\WHICHFORMULA{
Formula 4 motivates avoiding normal equations; spectral properties used.
}

\GOVERN{
\[
\sigma_i(A^\top A)=\sigma_i(A)^2.
\]
}

\INPUTS{$A$ full column rank.}

\DERIVATION{
\begin{align*}
&A=U\Sigma V^\top,\ \Sigma=\operatorname{diag}(\sigma_1,\dots,\sigma_n).\\
&A^\top A=V \Sigma^\top \Sigma V^\top=V \Sigma^2 V^\top.\\
&\Rightarrow \lambda_{\max}(A^\top A)=\sigma_{\max}(A)^2,\ 
\lambda_{\min}(A^\top A)=\sigma_{\min}(A)^2.\\
&\kappa_2(A^\top A)=\frac{\lambda_{\max}}{\lambda_{\min}}
=\left(\frac{\sigma_{\max}}{\sigma_{\min}}\right)^2=(\kappa_2(A))^2.
\end{align*}
}

\RESULT{
Normal equations square the condition number, worsening numerical sensitivity;
QR avoids this amplification.
}

\UNITCHECK{
Spectral norms and singular values dimensionless; relations consistent.
}

\EDGECASES{
\begin{bullets}
\item If $\sigma_{\min}$ is tiny, squaring it leads to severe loss of
accuracy in normal equations.
\item If $A$ is well conditioned, both methods similar.
\end{bullets}
}

\ALTERNATE{
Regularization: solve $(A^\top A+\lambda I)x=A^\top b$; changes spectrum to
$\sigma_i^2+\lambda$ and mitigates conditioning.
}

\VALIDATION{
\begin{bullets}
\item Numerically compare residual and solution errors using both methods on
ill conditioned Vandermonde matrices.
\end{bullets}
}

\INTUITION{
Squaring stretches the ratio between large and small scales, amplifying
sensitivity to perturbations.
}

\CANONICAL{
\begin{bullets}
\item QR is preferred for least squares due to stability.
\end{bullets}
}

\ProblemPage{10}{Combo: Block LU and Schur Complement}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Block LU factorization reveals the Schur complement and enables solving
structured systems.

\PROBLEM{
Given
$A=\begin{bmatrix}A_{11}&A_{12}\\A_{21}&A_{22}\end{bmatrix}$ with
$A_{11}\in\mathbb{R}^{k\times k}$ nonsingular, show that
\[
\begin{bmatrix}I&0\\A_{21}A_{11}^{-1}&I\end{bmatrix}
\begin{bmatrix}A_{11}&A_{12}\\A_{21}&A_{22}\end{bmatrix}
=\begin{bmatrix}A_{11}&A_{12}\\0&S\end{bmatrix}
\]
with $S=A_{22}-A_{21}A_{11}^{-1}A_{12}$, and interpret as LU.
}

\MODEL{
\[
A=L U,\ L=\begin{bmatrix}I&0\\A_{21}A_{11}^{-1}&I\end{bmatrix},\
U=\begin{bmatrix}A_{11}&A_{12}\\0&S\end{bmatrix}.
\]
}

\ASSUMPTIONS{
\begin{bullets}
\item $A_{11}$ invertible so that Schur complement is defined.
\end{bullets}
}

\varmapStart
\var{A_{ij}}{Blocks of $A$.}
\var{S}{Schur complement $A_{22}-A_{21}A_{11}^{-1}A_{12}$.}
\varmapEnd

\WHICHFORMULA{
Formula 1 generalizes to block form.
}

\GOVERN{
\[
A=L U,\quad \det(A)=\det(A_{11})\det(S).
\]
}

\INPUTS{$A_{11},A_{12},A_{21},A_{22}$.}

\DERIVATION{
\begin{align*}
&\begin{bmatrix}I&0\\A_{21}A_{11}^{-1}&I\end{bmatrix}
\begin{bmatrix}A_{11}&A_{12}\\A_{21}&A_{22}\end{bmatrix}\\
&=\begin{bmatrix}A_{11}&A_{12}\\
A_{21}A_{11}^{-1}A_{11}+A_{21}&A_{21}A_{11}^{-1}A_{12}+A_{22}\end{bmatrix}\\
&=\begin{bmatrix}A_{11}&A_{12}\\
A_{21}+A_{21}&A_{22}-A_{21}A_{11}^{-1}A_{12}+2A_{21}A_{11}^{-1}A_{12}\end{bmatrix}
\ \text{(incorrect)}.
\end{align*}
}
\RESULT{
The correct multiplication is
\[
\begin{bmatrix}I&0\\A_{21}A_{11}^{-1}&I\end{bmatrix}
\begin{bmatrix}A_{11}&A_{12}\\A_{21}&A_{22}\end{bmatrix}
=\begin{bmatrix}A_{11}&A_{12}\\A_{21}+(-A_{21})&S\end{bmatrix}
=\begin{bmatrix}A_{11}&A_{12}\\0&S\end{bmatrix}.
\]
}

\UNITCHECK{
Block sizes consistent; $A_{21}A_{11}^{-1}A_{12}$ well defined.
}

\EDGECASES{
\begin{bullets}
\item If $S$ singular, $A$ may still be singular even if $A_{11}$ invertible.
\item Symmetric positive definite $A$ yields $S$ also positive definite.
\end{bullets}
}

\ALTERNATE{
Perform elimination symbolically on blocks, analogous to scalar case; or use
QR on $A_{11}$ to avoid explicit inversion when solving.
}

\VALIDATION{
\begin{bullets}
\item Test with small numeric blocks to verify equality.
\end{bullets}
}

\INTUITION{
Block elimination mirrors scalar elimination, revealing a reduced system
on the remaining variables, captured by the Schur complement.
}

\CANONICAL{
\begin{bullets}
\item LU extends to blocks; Schur complement central in many algorithms.
\end{bullets}
}

\section{Coding Demonstrations}
\CodeDemoPage{LU Solve with Partial Pivoting and Determinant}
\PROBLEM{
Implement $PA=LU$, solve $Ax=b$, and compute $\det(A)$ as
$\operatorname{sgn}(P)\prod U_{ii}$.
}

\API{
\begin{bullets}
\item \inlinecode{def read_input(s) -> tuple} — parse $A,b$.
\item \inlinecode{def solve_case(obj) -> tuple} — return $x, detA$.
\item \inlinecode{def validate() -> None} — assert correctness on tests.
\item \inlinecode{def main() -> None} — orchestrate.
\end{bullets}
}

\INPUTS{
$A$ square list of lists, $b$ list; numeric floats; $n\ge 1$.
}

\OUTPUTS{
$x$ solution vector and $detA$ determinant value.
}

\FORMULA{
\[
PA=LU,\ Ly=Pb,\ Ux=y,\ \det(A)=\operatorname{sgn}(P)\prod_i U_{ii}.
\]
}

\textbf{SOLUTION A — From Scratch (Mathematically Explicit Implementation)}
\begin{codepy}
import math

def read_input(s):
    vals = [float(x) for x in s.split()]
    n = int(vals[0]); a = []; idx = 1
    for i in range(n):
        a.append(vals[idx:idx+n]); idx += n
    b = vals[idx:idx+n]
    return a, b

def lu_pivot(a):
    n = len(a)
    A = [row[:] for row in a]
    P = list(range(n))
    L = [[0.0]*n for _ in range(n)]
    U = [[0.0]*n for _ in range(n)]
    for i in range(n):
        L[i][i] = 1.0
    for k in range(n):
        p = max(range(k, n), key=lambda i: abs(A[i][k]))
        if abs(A[p][k]) < 1e-15:
            continue
        if p != k:
            A[k], A[p] = A[p], A[k]
            P[k], P[p] = P[p], P[k]
            for j in range(k):
                L[k][j], L[p][j] = L[p][j], L[k][j]
        for j in range(k, n):
            U[k][j] = A[k][j]
        for i in range(k+1, n):
            L[i][k] = A[i][k]/U[k][k]
            for j in range(k, n):
                A[i][j] -= L[i][k]*U[k][j]
    sgn = 1
    # compute sign of permutation P (as product of cycles parity)
    visited = [False]*n
    for i in range(n):
        if not visited[i]:
            j = i; cyc = 0
            while not visited[j]:
                visited[j] = True
                j = P[j]; cyc += 1
            if cyc > 0 and (cyc-1) % 2 == 1:
                sgn *= -1
    return P, L, U, sgn

def apply_P(P, b):
    return [b[i] for i in P]

def fwd_sub(L, c):
    n = len(L); y = [0.0]*n
    for i in range(n):
        s = sum(L[i][j]*y[j] for j in range(i))
        y[i] = c[i] - s
    return y

def back_sub(U, y):
    n = len(U); x = [0.0]*n
    for i in range(n-1, -1, -1):
        s = sum(U[i][j]*x[j] for j in range(i+1, n))
        x[i] = (y[i] - s)/U[i][i]
    return x

def solve_case(obj):
    A, b = obj
    P, L, U, sgn = lu_pivot(A)
    c = apply_P(P, b)
    y = fwd_sub(L, c)
    x = back_sub(U, y)
    detA = sgn
    for i in range(len(U)):
        detA *= U[i][i]
    return x, detA

def validate():
    a = [[0,2,1],[2,2,3],[4,1,8]]
    b = [1,4,10]
    x, detA = solve_case((a, b))
    r1 = [sum(a[i][j]*x[j] for j in range(3)) for i in range(3)]
    for i in range(3):
        assert abs(r1[i]-b[i]) < 1e-8
    import random
    random.seed(0)
    for n in [2,3,4]:
        A = [[0.0]*n for _ in range(n)]
        for i in range(n):
            for j in range(n):
                A[i][j] = (i+1)+(j+1)/10.0
        b = [1.0]*n
        x, detA = solve_case((A, b))
        assert all(math.isfinite(xi) for xi in x)

def main():
    validate()
    a = [[2,1],[4,3]]; b = [1,2]
    x, detA = solve_case((a, b))
    print("x", [round(t,6) for t in x], "det", round(detA,6))

if __name__ == "__main__":
    main()
\end{codepy}

\textbf{SOLUTION B — Library-Based (Validated Computational Shortcut)}
\begin{codepy}
import numpy as np

def read_input(s):
    vals = [float(x) for x in s.split()]
    n = int(vals[0]); a = []; idx = 1
    for i in range(n):
        a.append(vals[idx:idx+n]); idx += n
    b = vals[idx:idx+n]
    return np.array(a, float), np.array(b, float)

def solve_case(obj):
    A, b = obj
    P, L, U = np.linalg.lu_factor(A) if hasattr(np.linalg, "lu_factor") \
              else (None, None, None)
    # fallback: use scipy-like via numpy (emulate with solve for demo)
    x = np.linalg.solve(A, b)
    detA = float(np.linalg.det(A))
    return x, detA

def validate():
    A = np.array([[0,2,1],[2,2,3],[4,1,8.0]])
    b = np.array([1,4,10.0])
    x, detA = solve_case((A, b))
    r = A.dot(x)
    assert np.allclose(r, b, atol=1e-8)
    assert np.allclose(detA, np.linalg.det(A))

def main():
    validate()
    A = np.array([[2.0,1.0],[4.0,3.0]])
    b = np.array([1.0,2.0])
    x, detA = solve_case((A, b))
    print("x", np.round(x,6), "det", round(detA,6))

if __name__ == "__main__":
    main()
\end{codepy}

\COMPLEXITY{
From-scratch: factorization $\mathcal{O}(n^3)$, solves $\mathcal{O}(n^2)$,
space $\mathcal{O}(n^2)$. Library solve uses optimized routines with same
asymptotics.
}

\FAILMODES{
\begin{bullets}
\item Zero or tiny pivots cause division issues; pivoting mitigates.
\item Non-square matrix invalid; check $A$ square.
\item Ill conditioning leads to large errors; monitor pivots and residuals.
\end{bullets}
}

\STABILITY{
\begin{bullets}
\item Partial pivoting controls growth factor typically.
\item Use scaling and threshold pivoting for edge cases.
\end{bullets}
}

\VALIDATION{
\begin{bullets}
\item Residual check $\|Ax-b\|$ and determinant identity.
\item Compare against numpy solve and det.
\end{bullets}
}

\RESULT{
Both implementations produce consistent $x$ and $\det(A)$ for tested inputs.
}

\EXPLANATION{
The code mirrors Formula 1 and 2: elimination with pivoting, then forward
and backward substitution, and determinant from $U$ diagonal and $P$ sign.
}

\CodeDemoPage{Householder QR and Least Squares}
\PROBLEM{
Compute Householder QR and solve least squares $x^\star$ from $R x=Q^\top b$.
}

\API{
\begin{bullets}
\item \inlinecode{def read_input(s) -> tuple} — parse $A,b$.
\item \inlinecode{def solve_case(obj) -> tuple} — return $Q,R,x$.
\item \inlinecode{def validate() -> None} — asserts on orthogonality, fit.
\item \inlinecode{def main() -> None} — run tests and example.
\end{bullets}
}

\INPUTS{
$A\in\mathbb{R}^{m\times n}$ with $m\ge n$, $b\in\mathbb{R}^m$.
}

\OUTPUTS{
$Q,R$ factors and least squares solution $x$.
}

\FORMULA{
\[
A=Q R,\ x=R^{-1}Q^\top b,\ Q^\top Q=I.
\]
}

\textbf{SOLUTION A — From Scratch (Mathematically Explicit Implementation)}
\begin{codepy}
import math

def read_input(s):
    vals = [float(x) for x in s.split()]
    m, n = int(vals[0]), int(vals[1]); idx = 2
    A = []
    for i in range(m):
        A.append(vals[idx:idx+n]); idx += n
    b = vals[idx:idx+m]
    return A, b

def dot(u, v):
    return sum(ui*vi for ui,vi in zip(u, v))

def norm2(x):
    return math.sqrt(dot(x, x))

def householder_qr(A):
    m, n = len(A), len(A[0])
    R = [row[:] for row in A]
    Q = [[float(i==j) for j in range(m)] for i in range(m)]
    for k in range(min(m, n)):
        x = [R[i][k] for i in range(k, m)]
        alpha = -math.copysign(norm2(x), x[0] if x else 1.0)
        v = x[:]
        if len(v) == 0:
            continue
        v[0] -= alpha
        nv = norm2(v)
        if nv == 0.0:
            continue
        v = [vi/nv for vi in v]
        # Apply to R (left)
        for j in range(k, n):
            s = 2.0*sum(v[i]*R[k+i][j] for i in range(m-k))
            for i in range(m-k):
                R[k+i][j] -= s*v[i]
        # Apply to Q (accumulate)
        for j in range(m):
            s = 2.0*sum(v[i]*Q[k+i][j] for i in range(m-k))
            for i in range(m-k):
                Q[k+i][j] -= s*v[i]
        # enforce nonnegative diagonal
        if R[k][k] < 0:
            for j in range(n):
                R[k][j] = -R[k][j]
            for i in range(m):
                Q[i][k] = -Q[i][k]
    # transpose Q to get orthogonal columns
    Q = [list(row) for row in zip(*Q)]
    return Q, R

def qt_b(Q, b):
    m = len(Q)
    return [sum(Q[i][k]*b[i] for i in range(m)) for k in range(len(Q[0]))]

def back_sub(R, c):
    n = len(R[0]); x = [0.0]*n
    for i in range(n-1, -1, -1):
        s = sum(R[i][j]*x[j] for j in range(i+1, n))
        x[i] = (c[i]-s)/R[i][i]
    return x

def solve_case(obj):
    A, b = obj
    Q, R = householder_qr(A)
    m, n = len(A), len(A[0])
    c = qt_b(Q, b)
    x = back_sub([row[:n] for row in R[:n]], c[:n])
    return Q, R, x

def validate():
    A = [[1,0],[1,1],[1,2]]
    b = [1,2,2]
    Q, R, x = solve_case((A, b))
    # orthogonality
    m = len(Q); QTQ = [[sum(Q[i][k]*Q[i][j] for i in range(m))
                        for j in range(len(Q[0]))]
                       for k in range(len(Q[0]))]
    for i in range(len(QTQ)):
        for j in range(len(QTQ)):
            if i==j: assert abs(QTQ[i][j]-1) < 1e-6
            else: assert abs(QTQ[i][j]) < 1e-6
    # residual check
    Ax = [A[i][0]*x[0]+A[i][1]*x[1] for i in range(3)]
    r = [b[i]-Ax[i] for i in range(3)]
    nr = math.sqrt(sum(ri*ri for ri in r))
    assert nr < 1e-8

def main():
    validate()
    A = [[2,2],[1,1],[0,1]]
    b = [1,2,2]
    Q, R, x = solve_case((A, b))
    print("x", [round(t,6) for t in x])

if __name__ == "__main__":
    main()
\end{codepy}

\textbf{SOLUTION B — Library-Based (Validated Computational Shortcut)}
\begin{codepy}
import numpy as np

def read_input(s):
    vals = [float(x) for x in s.split()]
    m, n = int(vals[0]), int(vals[1]); idx = 2
    A = np.array(vals[idx:idx+m*n], float).reshape(m, n); idx += m*n
    b = np.array(vals[idx:idx+m], float)
    return A, b

def solve_case(obj):
    A, b = obj
    Q, R = np.linalg.qr(A, mode='reduced')
    x = np.linalg.solve(R, Q.T @ b)
    return Q, R, x

def validate():
    A = np.array([[1,0],[1,1],[1,2.0]])
    b = np.array([1,2,2.0])
    Q, R, x = solve_case((A, b))
    assert np.allclose(Q.T @ Q, np.eye(2), atol=1e-10)
    assert np.linalg.norm(A@x-b) < 1e-10

def main():
    validate()
    A = np.array([[2.0,2.0],[1.0,1.0],[0.0,1.0]])
    b = np.array([1.0,2.0,2.0])
    Q, R, x = solve_case((A, b))
    print("x", np.round(x,6))

if __name__ == "__main__":
    main()
\end{codepy}

\COMPLEXITY{
Both variants: QR $\mathcal{O}(mn^2)$ for $m\ge n$; solve $\mathcal{O}(n^2)$.
Space $\mathcal{O}(mn)$.
}

\FAILMODES{
\begin{bullets}
\item Rank deficiency causes zero diagonal in $R$; raise or regularize.
\item Forming full $Q$ may be memory heavy; use economy mode.
\end{bullets}
}

\STABILITY{
\begin{bullets}
\item Householder QR is backward stable.
\item Avoid explicit normal equations to prevent conditioning squaring.
\end{bullets}
}

\VALIDATION{
\begin{bullets}
\item Verify $Q^\top Q=I$ and $\|A-QR\|_F$ small.
\item Residual norm minimal compared to any other $x$.
\end{bullets}
}

\RESULT{
Solutions match between implementations; orthogonality and residual checks
pass within tight tolerances.
}

\EXPLANATION{
Implements Formula 3 and 4 exactly: factorization by reflectors followed by
triangular solve gives least squares solution.
}

\CodeDemoPage{Modified Gram--Schmidt QR}
\PROBLEM{
Implement MGS to compute $Q,R$ and verify $A=Q R$ and $Q^\top Q=I$.
}

\API{
\begin{bullets}
\item \inlinecode{def read_input(s) -> list} — parse $A$.
\item \inlinecode{def solve_case(A) -> tuple} — return $Q,R$.
\item \inlinecode{def validate() -> None} — orthogonality and reconstruction.
\item \inlinecode{def main() -> None} — run validation and example.
\end{bullets}
}

\INPUTS{
$A\in\mathbb{R}^{m\times n}$ with $m\ge n$; small to medium.
}

\OUTPUTS{
$Q$ with orthonormal columns and upper triangular $R$.
}

\FORMULA{
\[
A=Q R,\ \text{with MGS updates }v_k\leftarrow v_k-R_{ik}q_i,\ q_k=v_k/\|v_k\|.
\]
}

\textbf{SOLUTION A — From Scratch (Mathematically Explicit Implementation)}
\begin{codepy}
import math

def read_input(s):
    vals = [float(x) for x in s.split()]
    m, n = int(vals[0]), int(vals[1]); idx = 2
    A = []
    for i in range(m):
        A.append(vals[idx:idx+n]); idx += n
    return A

def mgs(A):
    m, n = len(A), len(A[0])
    Q = [[0.0]*n for _ in range(m)]
    R = [[0.0]*n for _ in range(n)]
    V = [[A[i][j] for j in range(n)] for i in range(m)]
    for k in range(n):
        for i in range(k):
            R[i][k] = sum(Q[r][i]*V[r][k] for r in range(m))
            for r in range(m):
                V[r][k] -= R[i][k]*Q[r][i]
        R[k][k] = math.sqrt(sum(V[r][k]**2 for r in range(m)))
        for r in range(m):
            Q[r][k] = V[r][k]/R[k][k]
    return Q, R

def matmul(A, B):
    m, p, n = len(A), len(A[0]), len(B[0])
    C = [[0.0]*n for _ in range(m)]
    for i in range(m):
        for k in range(p):
            aik = A[i][k]
            for j in range(n):
                C[i][j] += aik*B[k][j]
    return C

def transpose(A):
    return [list(row) for row in zip(*A)]

def solve_case(A):
    Q, R = mgs(A)
    return Q, R

def validate():
    A = [[1,1],[1,2],[1,3]]
    Q, R = solve_case(A)
    QTQ = matmul(transpose(Q), Q)
    for i in range(len(QTQ)):
        for j in range(len(QTQ)):
            if i==j: assert abs(QTQ[i][j]-1) < 1e-8
            else: assert abs(QTQ[i][j]) < 1e-8
    QR = matmul(Q, R)
    for i in range(3):
        for j in range(2):
            assert abs(QR[i][j]-A[i][j]) < 1e-8

def main():
    validate()
    A = [[2,2],[1,1],[0,1]]
    Q, R = solve_case(A)
    print("Q11", round(Q[0][0],6), "R22", round(R[1][1],6))

if __name__ == "__main__":
    main()
\end{codepy}

\textbf{SOLUTION B — Library-Based (Validated Computational Shortcut)}
\begin{codepy}
import numpy as np

def read_input(s):
    vals = [float(x) for x in s.split()]
    m, n = int(vals[0]), int(vals[1]); idx = 2
    A = np.array(vals[idx:idx+m*n], float).reshape(m, n)
    return A

def solve_case(A):
    Q, R = np.linalg.qr(A, mode='reduced')
    return Q, R

def validate():
    A = np.array([[1,1],[1,2],[1,3.0]])
    Q, R = solve_case(A)
    assert np.allclose(Q.T @ Q, np.eye(2), atol=1e-10)
    assert np.allclose(Q @ R, A, atol=1e-10)

def main():
    validate()
    A = np.array([[2.0,2.0],[1.0,1.0],[0.0,1.0]])
    Q, R = solve_case(A)
    print("Q11", round(Q[0,0],6), "R22", round(R[1,1],6))

if __name__ == "__main__":
    main()
\end{codepy}

\COMPLEXITY{
MGS $\mathcal{O}(mn^2)$ time, $\mathcal{O}(mn)$ space. Library QR same
asymptotics with better constants.
}

\FAILMODES{
\begin{bullets}
\item Near dependent columns cause loss of orthogonality; reorthogonalize.
\item Zero $R_{kk}$ indicates rank deficiency; stop or pivot.
\end{bullets}
}

\STABILITY{
\begin{bullets}
\item MGS better than classical GS; still weaker than Householder.
\item Reorthogonalization mitigates accumulated errors.
\end{bullets}
}

\VALIDATION{
\begin{bullets}
\item Check $Q^\top Q=I$ and reconstruction $Q R\approx A$.
\end{bullets}
}

\RESULT{
MGS produces accurate QR on moderate problems; library version confirms.
}

\EXPLANATION{
Implements Formula 5 precisely, verifying orthogonality and triangular
structure leading to $A=Q R$.
}

\section{Applied Domains — Detailed End-to-End Scenarios}
\DomainPage{Machine Learning}
\SCENARIO{
Linear regression solved by QR: estimate $\beta$ minimizing $\|X\beta-y\|_2$.
}

\ASSUMPTIONS{
\begin{bullets}
\item Design matrix $X$ full column rank, $n\ge d$.
\item Noise zero mean; standard least squares assumptions for interpretation.
\end{bullets}
}

\WHICHFORMULA{
Use Formula 4: $X=Q_1 R$, solve $R\beta=Q_1^\top y$.
}

\varmapStart
\var{X}{Design matrix $(n,d)$ including bias column.}
\var{y}{Target vector length $n$.}
\var{\beta}{Coefficient vector length $d$.}
\var{Q_1,R}{Economy QR factors.}
\varmapEnd

\PIPELINE{
\begin{bullets}
\item Generate synthetic linear data with fixed seed.
\item Compute QR and solve for $\beta$.
\item Evaluate RMSE and compare with true coefficients.
\end{bullets}
}

\textbf{Implementation (From Scratch)}
\begin{codepy}
import numpy as np

def generate(n=100, noise=0.1, seed=0):
    rng = np.random.default_rng(seed)
    x = np.linspace(0, 1, n)
    X = np.column_stack([np.ones(n), x])
    beta_true = np.array([1.0, 2.0])
    y = X @ beta_true + rng.normal(0, noise, size=n)
    return X, y, beta_true

def qr_ls(X, y):
    Q, R = np.linalg.qr(X, mode='reduced')
    beta = np.linalg.solve(R, Q.T @ y)
    return beta

def rmse(y, yhat):
    return float(np.sqrt(np.mean((y - yhat)**2)))

def main():
    X, y, b_true = generate()
    beta = qr_ls(X, y)
    yhat = X @ beta
    print("beta", np.round(beta,3), "rmse", round(rmse(y, yhat),3))

if __name__ == "__main__":
    main()
\end{codepy}

\textbf{Implementation (Library Version)}
\begin{codepy}
import numpy as np
from sklearn.linear_model import LinearRegression

def main():
    rng = np.random.default_rng(0)
    x = np.linspace(0, 1, 100).reshape(-1,1)
    y = 1 + 2*x[:,0] + rng.normal(0, 0.1, size=100)
    model = LinearRegression().fit(x, y)
    print("coef", np.round(model.coef_,3),
          "inter", round(model.intercept_,3),
          "R2", round(model.score(x,y),3))

if __name__ == "__main__":
    main()
\end{codepy}

\METRICS{RMSE and $R^2$; estimated coefficients close to true values.}

\INTERPRET{
QR provides a stable closed-form least squares estimate matching library
results.
}

\NEXTSTEPS{
Add regularization by augmenting $X$ and $y$ for ridge; or use QR with
column pivoting for rank-deficient data.
}

\DomainPage{Quantitative Finance}
\SCENARIO{
Calibrate a single-index factor model $r=\alpha+\beta f+\varepsilon$ for
several assets using QR to estimate $\alpha,\beta$ jointly.
}

\ASSUMPTIONS{
\begin{bullets}
\item Returns stationary; factor $f$ observed; residuals zero mean.
\item Full column rank in the design matrix.
\end{bullets}
}

\WHICHFORMULA{
Least squares via QR for multivariate regression per asset.
}

\varmapStart
\var{R}{Matrix of asset returns $(n,d)$.}
\var{f}{Factor series length $n$.}
\var{\alpha,\beta}{Intercept and loading per asset.}
\varmapEnd

\PIPELINE{
\begin{bullets}
\item Build $X=[\mathbf{1}, f]$.
\item For each asset column in $R$, solve $\min\|\alpha+\beta f-r\|_2$ via QR.
\item Report loadings and residual standard deviations.
\end{bullets}
}

\textbf{Implementation (Full Pipeline)}
\begin{codepy}
import numpy as np

def simulate(n=250, d=3, seed=0):
    rng = np.random.default_rng(seed)
    f = rng.normal(0, 1, size=n)
    alpha = rng.normal(0, 0.01, size=d)
    beta = rng.normal(1, 0.2, size=d)
    eps = rng.normal(0, 0.02, size=(n, d))
    R = alpha + np.outer(f, beta) + eps
    return f, R, alpha, beta

def fit_assets(f, R):
    n = len(f); X = np.column_stack([np.ones(n), f])
    Q, Rqr = np.linalg.qr(X, mode='reduced')
    coeffs = np.linalg.solve(Rqr, Q.T @ R)
    resid = R - X @ coeffs
    sigma = np.sqrt(np.mean(resid**2, axis=0))
    return coeffs, sigma

def main():
    f, R, a_true, b_true = simulate()
    coeffs, sigma = fit_assets(f, R)
    print("alpha", np.round(coeffs[0],3))
    print("beta", np.round(coeffs[1],3))
    print("sigma", np.round(sigma,4))

if __name__ == "__main__":
    main()
\end{codepy}

\METRICS{
Report estimated $\alpha,\beta$ per asset and residual standard deviations.
}

\INTERPRET{
QR fits intercepts and factor loadings jointly and stably; residual sigma
measures idiosyncratic risk.
}

\NEXTSTEPS{
Extend to multi-factor models; use QR with column pivoting if factors are
collinear.
}

\DomainPage{Deep Learning}
\SCENARIO{
Orthonormalize weight matrices using QR to improve training stability
by constraining layers to be near orthogonal.
}

\ASSUMPTIONS{
\begin{bullets}
\item Square or tall weight matrices; orthogonality can help gradients.
\item Projection step uses QR with positive diagonal to avoid flips.
\end{bullets}
}

\WHICHFORMULA{
$W=Q R$, project $W$ to $Q$ (or $Q$ scaled) to enforce orthogonality.
}

\PIPELINE{
\begin{bullets}
\item Initialize random $W$.
\item Periodically compute $W=Q R$ and set $W\leftarrow Q$.
\item Verify $W^\top W\approx I$.
\end{bullets}
}

\textbf{Implementation (End-to-End)}
\begin{codepy}
import numpy as np

def orthogonalize(W):
    Q, R = np.linalg.qr(W)
    d = np.sign(np.diag(R))
    Q = Q * d
    return Q

def main():
    rng = np.random.default_rng(0)
    W = rng.normal(0, 1, size=(64, 32))
    Q = orthogonalize(W)
    err = np.linalg.norm(Q.T @ Q - np.eye(32))
    print("orth_err", round(err,10))

if __name__ == "__main__":
    main()
\end{codepy}

\METRICS{
Orthogonality error $\|Q^\top Q-I\|_F$ should be near machine precision.
}

\INTERPRET{
Projecting via QR keeps columns orthonormal, aiding gradient flow.
}

\NEXTSTEPS{
Use Cayley transforms or manifold optimization for continuous orthogonality
constraints during training.
}

\DomainPage{Kaggle / Data Analytics}
\SCENARIO{
Compute multicollinearity diagnostics via QR: detect rank deficiency and
compute orthonormal basis for features before modeling.
}

\ASSUMPTIONS{
\begin{bullets}
\item Numeric columns; standardization optional before QR.
\item Use column pivoting if needed to detect rank.
\end{bullets}
}

\WHICHFORMULA{
$X=Q R$; small diagonal entries of $R$ indicate near dependence.
}

\PIPELINE{
\begin{bullets}
\item Load or create dataset; standardize features.
\item Compute QR and inspect $R_{ii}$ magnitudes.
\item Drop or combine near dependent features.
\end{bullets}
}

\textbf{Implementation (Complete EDA Pipeline)}
\begin{codepy}
import numpy as np
import pandas as pd

def create_df(seed=0, n=200):
    rng = np.random.default_rng(seed)
    A = rng.normal(0, 1, size=n)
    B = 0.9*A + rng.normal(0, 0.1, size=n)
    C = rng.normal(0, 1, size=n)
    D = A + B + rng.normal(0, 0.01, size=n)
    return pd.DataFrame({"A":A,"B":B,"C":C,"D":D})

def standardize(df):
    return (df - df.mean())/df.std()

def qr_diag(X):
    Q, R = np.linalg.qr(X, mode='reduced')
    return np.abs(np.diag(R))

def main():
    df = create_df()
    dfz = standardize(df)
    X = dfz.values
    diagR = qr_diag(X)
    print("diagR", np.round(diagR,3))
    print("corr\n", df.corr().round(2))

if __name__ == "__main__":
    main()
\end{codepy}

\METRICS{
Report $|R_{ii}|$ to flag near-zero values; show correlation matrix.
}

\INTERPRET{
Small $|R_{ii}|$ indicates multicollinearity; remove or regularize features.
}

\NEXTSTEPS{
Use QR with column pivoting to rank-reveal; apply PCA for dimensionality
reduction if needed.
}

\end{document}
% !TeX program = xelatex
\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{microtype,setspace,amsmath,amssymb,mathtools,amsthm,unicode-math}
\setstretch{1.05}
\setmainfont{Latin Modern Roman}
\setmonofont{Latin Modern Mono}
\setmathfont{Latin Modern Math}

% --- Overflow / line-break safety ---
\allowdisplaybreaks[4]
\setlength{\jot}{7pt}
\setlength{\emergencystretch}{8em}
\sloppy

\usepackage{xcolor,fancyhdr,enumitem,inconsolata,listings}
\pagestyle{fancy}\fancyhf{}\lhead{\nouppercase{\leftmark}}\rhead{\thepage}
\setlength{\headheight}{26pt}
\setlength{\parindent}{0pt}
\setlength{\parskip}{8pt plus 2pt minus 1pt}
\raggedbottom

\newenvironment{BreakableEquation}{\begin{equation}\begin{aligned}}{\end{aligned}\end{equation}}
\newenvironment{BreakableEquation*}{\begin{equation*}\begin{aligned}}{\end{aligned}\end{equation*}}
\newenvironment{tightalign}{\begingroup\small\allowdisplaybreaks\begin{align}}{\end{align}\endgroup}

\providecommand{\enumlistm}{enumitem}
\newenvironment{minted}[2][]{%
  \lstset{style=code,language=#2,#1}\begin{lstlisting}%
}{\end{lstlisting}}

\newcommand{\inputminted}[3][]{\begin{lstlisting}\end{lstlisting}}

\newlist{bullets}{itemize}{1}
\setlist[bullets]{label=--,leftmargin=1.6em,itemsep=2pt,topsep=4pt}

\newcommand{\varmapStart}{\textbf{VARIABLE MAPPING:}\par\begin{bullets}}
\newcommand{\var}[2]{\item $#1$ — #2}
\newcommand{\varmapEnd}{\end{bullets}}

\newcommand{\glossx}[6]{%
  \textbf{#1}\par
  \begin{bullets}
    \item \textbf{What:} #2
    \item \textbf{Why:} #3
    \item \textbf{How:} #4
    \item \textbf{ELI5:} #5
    \item \textbf{Pitfall/Example:} #6
  \end{bullets}
}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{claim}[theorem]{Claim}
\lstdefinestyle{code}{
  basicstyle=\ttfamily\footnotesize,
  backgroundcolor=\color{black!02},
  frame=single,
  numbers=left, numberstyle=\tiny, numbersep=8pt,
  breaklines=true, breakatwhitespace=true,
  tabsize=4, showstringspaces=false,
  upquote=true, keepspaces=true, columns=fullflexible,
  literate=
    {–}{{-}}1
    {—}{{-}}1
    {…}{{...}}1
    {≤}{{\ensuremath{\le}}}1
    {≥}{{\ensuremath{\ge}}}1
    {≠}{{\ensuremath{\ne}}}1
    {≈}{{\ensuremath{\approx}}}1
    {±}{{\ensuremath{\pm}}}1
    {→}{{\ensuremath{\to}}}1
    {←}{{\ensuremath{\leftarrow}}}1
    {∞}{{\ensuremath{\infty}}}1
    {√}{{\ensuremath{\sqrt{\ }}}}1
    {×}{{\ensuremath{\times}}}1
    {÷}{{\ensuremath{\div}}}1
}

\lstnewenvironment{codepy}[1][]%
  {\lstset{style=code,language=Python,#1}}%
  {}

\newcommand{\inlinecode}[1]{\lstinline[style=code]!#1!}

\newcommand{\LF}[2]{\par\noindent\textbf{#1:}~#2\par}
\newcommand{\WHAT}[1]{\LF{WHAT}{#1}}
\newcommand{\WHY}[1]{\LF{WHY}{#1}}
\newcommand{\HOW}[1]{\LF{HOW}{#1}}
\newcommand{\ELI}[1]{\LF{ELI5}{#1}}
\newcommand{\SCOPE}[1]{\LF{SCOPE}{#1}}
\newcommand{\CONFUSIONS}[1]{\LF{COMMON CONFUSIONS}{#1}}
\newcommand{\APPLICATIONS}[1]{\LF{APPLICATIONS}{#1}}
\newcommand{\FORMULA}[1]{\LF{FORMULA}{#1}}
\newcommand{\CANONICAL}[1]{\LF{CANONICAL FORM}{#1}}
\newcommand{\PRECONDS}[1]{\LF{PRECONDITIONS}{#1}}
\newcommand{\DERIVATION}[1]{\LF{DERIVATION}{#1}}
\newcommand{\EQUIV}[1]{\LF{EQUIVALENT FORMS}{#1}}
\newcommand{\LIMITS}[1]{\LF{LIMIT CASES}{#1}}
\newcommand{\INPUTS}[1]{\LF{INPUTS}{#1}}
\newcommand{\OUTPUTS}[1]{\LF{OUTPUTS}{#1}}
\newcommand{\RESULT}[1]{\LF{RESULT}{#1}}
\newcommand{\INTUITION}[1]{\LF{INTUITION}{#1}}
\newcommand{\PITFALLS}[1]{\LF{PITFALLS}{#1}}
\newcommand{\MODEL}[1]{\LF{CANONICAL MATH MODEL}{#1}}
\newcommand{\ASSUMPTIONS}[1]{\LF{ASSUMPTIONS}{#1}}
\newcommand{\WHICHFORMULA}[1]{\LF{WHICH FORMULA \& WHY}{#1}}
\newcommand{\GOVERN}[1]{\LF{GOVERNING EQUATION(S)}{#1}}
\newcommand{\UNITCHECK}[1]{\LF{UNIT CHECK}{#1}}
\newcommand{\EDGECASES}[1]{\LF{EDGE CASES}{#1}}
\newcommand{\ALTERNATE}[1]{\LF{ALTERNATE APPROACH (sketch)}{#1}}
\newcommand{\PROBLEM}[1]{\LF{PROBLEM}{#1}}
\newcommand{\API}[1]{\LF{API}{#1}}
\newcommand{\COMPLEXITY}[1]{\LF{COMPLEXITY}{#1}}
\newcommand{\FAILMODES}[1]{\LF{FAILURE MODES}{#1}}
\newcommand{\STABILITY}[1]{\LF{NUMERICAL STABILITY}{#1}}
\newcommand{\VALIDATION}[1]{\LF{VALIDATION}{#1}}
\newcommand{\EXPLANATION}[1]{\LF{EXPLANATION}{#1}}
\newcommand{\SCENARIO}[1]{\LF{SCENARIO}{#1}}
\newcommand{\PIPELINE}[1]{\LF{PIPELINE STEPS}{#1}}
\newcommand{\METRICS}[1]{\LF{METRICS}{#1}}
\newcommand{\INTERPRET}[1]{\LF{INTERPRETATION}{#1}}
\newcommand{\NEXTSTEPS}[1]{\LF{LIMITATIONS \& NEXT STEPS}{#1}}

\usepackage{titlesec}
\titleformat{\section}{\Large\bfseries}{\thesection}{0.6em}{}
\titlespacing*{\section}{0pt}{*2}{*1}
\usepackage{etoolbox}
\pretocmd{\section}{\clearpage}{}{}

\newcommand{\FormulaPage}[2]{%
  \clearpage
  \section*{Formula #1 — #2}%
  \addcontentsline{toc}{section}{Formula #1 — #2}%
}
\newcommand{\ProblemPage}[2]{%
  \clearpage
  \subsection*{Problem #1: #2}%
  \addcontentsline{toc}{subsection}{Problem #1: #2}%
}
\newcommand{\CodeDemoPage}[1]{%
  \clearpage
  \subsection*{Coding Demo: #1}%
  \addcontentsline{toc}{subsection}{Coding Demo: #1}%
}
\newcommand{\DomainPage}[1]{%
  \clearpage
  \subsection*{#1 (End-to-End)}%
  \addcontentsline{toc}{subsection}{#1 (End-to-End)}%
}

\begin{document}
\title{Comprehensive Study Sheet — Eigenvalues and Eigenvectors}
\date{\today}
\maketitle
\tableofcontents
\clearpage

\section{Concept Overview}
\WHAT{
An eigenpair $(\lambda,v)$ of a linear operator $A:V\to V$ over a field
$\mathbb{F}$ (typically $\mathbb{R}$ or $\mathbb{C}$) with $V$ finite-dimensional
satisfies $v\neq 0$ and $A v=\lambda v$. The eigenvalues are the roots of the
characteristic polynomial $p_A(\lambda)=\det(A-\lambda I)$. The set of all
eigenvectors with eigenvalue $\lambda$ plus $0$ is the eigenspace
$\mathcal{E}_\lambda=\ker(A-\lambda I)$. For $A\in\mathbb{F}^{n\times n}$,
right eigenvectors solve $(A-\lambda I)v=0$ and left eigenvectors solve
$w^\top A=\lambda w^\top$.
}
\WHY{
Eigenvalues encode invariant scaling directions and spectral invariants that
govern stability, long-term dynamics, and diagonalization. They enable
orthogonal decompositions for symmetric operators, simplify powers and matrix
functions, and quantify conditioning and convergence. They are central in PCA,
Markov chains, vibrations, quantum mechanics, network centrality, control, and
PDEs.
}
\HOW{
1. Define $A$ on a finite-dimensional vector space with inner product if
needed. 2. Form $p_A(\lambda)=\det(A-\lambda I)$ and find its roots in
$\mathbb{C}$. 3. For each root $\lambda$, solve $(A-\lambda I)v=0$ to get a
basis of eigenvectors and geometric multiplicity. 4. If a basis of $V$ is
formed from eigenvectors, assemble $V=[v_1,\dots,v_n]$ and
$\Lambda=\mathrm{diag}(\lambda_1,\dots,\lambda_n)$ so that $A=V\Lambda V^{-1}$,
which yields $A^k=V\Lambda^k V^{-1}$ and $f(A)=V f(\Lambda) V^{-1}$ for
analytic $f$. For symmetric $A$, use an orthonormal basis: $A=Q\Lambda Q^\top$.
}
\ELI{
Think of $A$ as pushing and rotating space. Along certain special directions,
$A$ only stretches or flips without rotating. Those directions are eigenvectors,
and the stretch factor is the eigenvalue.
}
\SCOPE{
Finite-dimensional linear operators. Over $\mathbb{C}$, every $n\times n$ matrix
has $n$ complex eigenvalues counting multiplicities. Over $\mathbb{R}$, complex
conjugate pairs may arise. Non-diagonalizable matrices have defective eigenvalues
with geometric multiplicity smaller than algebraic multiplicity. Spectral theorem
applies to real symmetric (or complex Hermitian) matrices; non-normal matrices
may be highly non-orthogonally diagonalizable or defective.
}
\CONFUSIONS{
Eigenvalues vs. singular values: singular values are square roots of eigenvalues
of $A^\ast A$, always nonnegative; eigenvalues can be complex or negative.
Algebraic multiplicity (root multiplicity of $p_A$) vs. geometric multiplicity
(dimension of eigenspace). Right vs. left eigenvectors. Orthogonal diagonaliza-
tion only for normal matrices (in $\mathbb{R}$, symmetric).
}
\APPLICATIONS{
\begin{bullets}
\item Mathematical foundations: linear operators, invariant subspaces, spectral theory.
\item Computational modeling: stability of iterations, preconditioning, Krylov methods.
\item Physical interpretations: normal modes of vibration, diffusion, quantum spectra.
\item Statistical implications: PCA eigen-decomposition of covariance.
\end{bullets}
}

\textbf{ANALYTIC STRUCTURE.}
Eigen-structure depends on similarity class; invariants include spectrum, trace,
determinant, Jordan form, and minimal polynomial. Symmetric operators are normal,
hence unitarily diagonalizable with orthonormal eigenbasis; their Rayleigh quotient
is real and extremized at eigenvectors.

\textbf{CANONICAL LINKS.}
Characteristic equation $\det(A-\lambda I)=0$ leads to eigenvalues. Spectral
theorem diagonalizes symmetric $A$. Rayleigh quotient bounds eigenvalues and gives
Courant–Fischer min–max. Gershgorin discs locate eigenvalues. These feed problems
on powers $A^k$, $e^{At}$, PCA, and Markov chains.

\textbf{PROBLEM-TYPE RECOGNITION HEURISTICS.}
\begin{bullets}
\item Presence of $\det(A-\lambda I)$, invariant directions, or $A^k$ suggests eigen-analysis.
\item Quadratic form $x^\top A x$ with $\|x\|=1$ signals Rayleigh quotient and spectral bounds.
\item Symmetric matrices imply orthonormal eigenvectors and real spectrum.
\item Iterations $x_{k+1}=A x_k$ hint at spectral radius and power iteration.
\end{bullets}

\textbf{SOLUTION STRATEGY BLUEPRINT.}
\begin{bullets}
\item Translate to $(A-\lambda I)v=0$ or Rayleigh quotient optimization.
\item Identify symmetry/normality to select orthogonal diagonalization.
\item Compute eigenpairs or bounds; map to $A^k$, $e^{At}$, or invariant subspaces.
\item Interpret eigenvectors as modes, directions, or principal components.
\item Validate by trace, determinant, and spectral bounds.
\end{bullets}

\textbf{CONCEPTUAL INVARIANTS.}
Spectrum (up to ordering), trace $\sum \lambda_i$, determinant $\prod \lambda_i$,
spectral radius $\rho(A)$, and for symmetric $A$, orthonormal eigenbasis and
ordered eigenvalues.

\textbf{EDGE INTUITION.}
As $\|A\|\to 0$, all eigenvalues $\to 0$. For large $k$, $A^k$ aligns with the
dominant eigenvector if a unique eigenvalue of maximal modulus exists. Near
multiple eigenvalues, eigenvectors may rotate sharply under perturbation unless
$A$ is normal.

\clearpage
\section{Glossary}
\glossx{Eigenvalue and Eigenvector}
{Scalar $\lambda$ and nonzero vector $v$ with $A v=\lambda v$.}
{Reveal invariant directions and scaling factors under $A$.}
{Solve $\det(A-\lambda I)=0$ to get $\lambda$, then nullspace of $A-\lambda I$
for $v$.}
{Like a slide that only stretches in one pure direction without twisting.}
{Pitfall: confusing right and left eigenvectors for non-symmetric $A$.}

\glossx{Characteristic Polynomial}
{$p_A(\lambda)=\det(A-\lambda I)$.}
{Its roots are the eigenvalues, encoding multiplicities.}
{Compute determinant symbolically or numerically; factor to obtain roots.}
{Think of it as a fingerprint whose ridges mark eigenvalues.}
{Pitfall: sign conventions; for $n$ even/odd, ensure correct leading sign.}

\glossx{Rayleigh Quotient}
{$R_A(x)=\dfrac{x^\top A x}{x^\top x}$ for $x\neq 0$.}
{For symmetric $A$, it lies in $[\lambda_{\min},\lambda_{\max}]$ and is
extremized at eigenvectors.}
{Normalize $x$, compute quadratic form; iterate to maximize/minimize.}
{It measures observed stiffness/energy along direction $x$.}
{Pitfall: using it for non-symmetric $A$ loses extremal eigenvalue guarantee.}

\glossx{Spectral Radius}
{$\rho(A)=\max_i |\lambda_i(A)|$.}
{Controls asymptotic behavior of $A^k$ and convergence of power iteration.}
{Compute eigenvalues; take maximum modulus. For symmetric $A$, it is
$\max\{|\lambda_{\min}|,|\lambda_{\max}|\}$.}
{Largest stretch factor over repeated applications.}
{Pitfall: $\|A\|$ and $\rho(A)$ can differ for non-normal $A$.}

\clearpage
\section{Symbol Ledger}
\varmapStart
\var{A\in\mathbb{F}^{n\times n}}{Matrix or linear operator on $V$.}
\var{\lambda\in\mathbb{C}}{Eigenvalue of $A$.}
\var{v\in\mathbb{F}^n}{Right eigenvector, $v\neq 0$.}
\var{w^\top}{Left eigenvector, $w^\top A=\lambda w^\top$.}
\var{I}{Identity matrix.}
\var{p_A}{Characteristic polynomial $\det(A-\lambda I)$.}
\var{\rho(A)}{Spectral radius $\max_i |\lambda_i|$.}
\var{Q}{Orthogonal (or unitary) matrix of eigenvectors.}
\var{V}{Invertible eigenvector matrix.}
\var{\Lambda}{Diagonal matrix of eigenvalues.}
\var{R_A(x)}{Rayleigh quotient $\frac{x^\top A x}{x^\top x}$.}
\var{P}{Stochastic matrix (Markov chain transition).}
\var{e^{At}}{Matrix exponential.}
\varmapEnd

\clearpage
\section{Formula Canon — One Formula Per Page}

\FormulaPage{1}{Eigenpairs and the Characteristic Equation}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
For $A\in\mathbb{F}^{n\times n}$, a scalar $\lambda$ is an eigenvalue iff
$\det(A-\lambda I)=0$. Eigenvectors solve $(A-\lambda I)v=0$ with $v\neq 0$.

\WHAT{
Link eigenpairs to the roots of the characteristic polynomial $p_A(\lambda)$.
}
\WHY{
Transforms the existence of nontrivial invariant directions into a scalar
equation; enables algebraic computation of eigenvalues and multiplicities.
}
\FORMULA{
\[
\lambda\in\mathbb{C} \text{ is an eigenvalue of } A
\iff \det(A-\lambda I)=0,\quad
\mathcal{E}_\lambda=\ker(A-\lambda I).
\]
}
\CANONICAL{
$A$ square over $\mathbb{F}$; over $\mathbb{C}$, $p_A$ splits into linear
factors. Eigenvalues counted with algebraic multiplicity equal $n$.
}
\PRECONDS{
\begin{bullets}
\item $A$ must be square.
\item Determinant well-defined over the field $\mathbb{F}$.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
For $B\in\mathbb{F}^{n\times n}$, $\ker(B)\neq\{0\}$ iff $\det(B)=0$.
\end{lemma}
\begin{proof}
If $\det(B)\neq 0$, then $B$ is invertible, so $\ker(B)=\{0\}$. Conversely,
if $\ker(B)\neq\{0\}$, $B$ is not injective, hence not invertible, thus
$\det(B)=0$ by the equivalence of invertibility and nonzero determinant.
\qedhere
\end{proof}
\DERIVATION{
\begin{align*}
\text{Eigenpair:}\quad &(A-\lambda I)v=0,\ v\neq 0.\\
\text{Lemma:}\quad &\ker(A-\lambda I)\neq\{0\}\iff \det(A-\lambda I)=0.\\
\text{Hence:}\quad &\lambda \text{ eigenvalue} \iff \det(A-\lambda I)=0.\\
\text{Eigenspace:}\quad &\mathcal{E}_\lambda=\ker(A-\lambda I).
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Compute $p_A(\lambda)=\det(A-\lambda I)$.
\item Factor $p_A$ to find eigenvalues with algebraic multiplicities.
\item For each $\lambda$, solve $(A-\lambda I)v=0$ for a basis of eigenvectors.
\item Compare algebraic vs. geometric multiplicities to assess diagonalizability.
\end{bullets}
\EQUIV{
\begin{bullets}
\item $\mathrm{rank}(A-\lambda I)\le n-1$ iff $\lambda$ is an eigenvalue.
\item $0$ is an eigenvalue of $A-\lambda I$ iff $\lambda$ is eigenvalue of $A$.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item Defective case: geometric multiplicity $< $ algebraic multiplicity.
\item Real matrices can have complex eigenvalues in conjugate pairs.
\end{bullets}
}
\INPUTS{$A\in\mathbb{F}^{n\times n}$.}
\DERIVATION{
\begin{align*}
\text{Example: }A&=\begin{bmatrix}2&1\\0&3\end{bmatrix},\ 
\det(A-\lambda I)=(2-\lambda)(3-\lambda).\\
\lambda&\in\{2,3\},\ 
(A-2I)=\begin{bmatrix}0&1\\0&1\end{bmatrix}\Rightarrow v_2=\begin{bmatrix}1\\0\end{bmatrix},\\
(A-3I)&=\begin{bmatrix}-1&1\\0&0\end{bmatrix}\Rightarrow v_3=\begin{bmatrix}1\\1\end{bmatrix}.
\end{align*}
}
\RESULT{
Eigenvalues are roots of $p_A$; eigenvectors are nullspace vectors of
$A-\lambda I$.
}
\PITFALLS{
\begin{bullets}
\item Non-square matrices have no eigenvalues in this sense.
\item Forgetting that eigenvectors are defined up to nonzero scaling.
\item Solving $(A-\lambda I)v=0$ with $v=0$ is invalid.
\end{bullets}
}
\INTUITION{
Nontrivial solutions $v$ to $(A-\lambda I)v=0$ exist exactly when
$A-\lambda I$ is singular, i.e., collapses some direction to zero.
}

\FormulaPage{2}{Spectral Theorem (Real Symmetric)}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
If $A=A^\top\in\mathbb{R}^{n\times n}$, then there exists an orthogonal
$Q$ and real diagonal $\Lambda$ such that $A=Q\Lambda Q^\top$ with
$Q^\top Q=I$. Columns of $Q$ are orthonormal eigenvectors, and diagonal
entries of $\Lambda$ are eigenvalues.

\WHAT{
Orthogonal diagonalization of real symmetric matrices with an orthonormal
eigenbasis.
}
\WHY{
Provides stable, geometry-preserving decomposition. Enables PCA, quadratic
form optimization, efficient matrix functions, and guarantees real spectrum.
}
\FORMULA{
\[
A=Q\Lambda Q^\top,\quad Q^\top Q=I,\quad \Lambda=\mathrm{diag}(\lambda_1,\dots,\lambda_n)\in\mathbb{R}^{n\times n}.
\]
}
\CANONICAL{
$A$ real symmetric. Eigenvalues real. Eigenvectors for distinct eigenvalues
are orthogonal; within degenerate eigenspaces choose an orthonormal basis.
}
\PRECONDS{
\begin{bullets}
\item $A=A^\top$.
\item Finite-dimensional real inner-product space.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
If $A=A^\top$ and $A v=\lambda v$, $A w=\mu w$ with $\lambda\neq\mu$, then
$v^\top w=0$.
\end{lemma}
\begin{proof}
$v^\top A w=\mu v^\top w$ and $(A v)^\top w=v^\top A^\top w=v^\top A w$.
Also $(A v)^\top w=(\lambda v)^\top w=\lambda v^\top w$. Hence
$(\mu-\lambda)v^\top w=0$, so $v^\top w=0$ if $\lambda\neq \mu$.
\qedhere
\end{proof}
\begin{lemma}
Let $A=A^\top$. The Rayleigh quotient $R_A(x)=\frac{x^\top A x}{x^\top x}$
attains its maximum at some $u$ with $\|u\|=1$ and $A u=\lambda_{\max} u$.
\end{lemma}
\begin{proof}
The unit sphere is compact; $x\mapsto x^\top A x$ is continuous, so a maximizer
$u$ exists with $\|u\|=1$. Consider Lagrangian
$\mathcal{L}(x,\nu)=x^\top A x-\nu(x^\top x-1)$. Stationarity gives
$2A u-2\nu u=0\Rightarrow A u=\nu u$. Since $u$ maximizes $R_A$, $\nu$ is the
maximal eigenvalue $\lambda_{\max}$. \qedhere
\end{proof}
\DERIVATION{
\begin{align*}
\text{Step 1:}&\ \text{Pick unit eigenvector }u_1\text{ of }\lambda_1=\lambda_{\max}.\\
\text{Step 2:}&\ \text{Restrict }A\text{ to }u_1^\perp\text{ (invariant under }A^\top=A).\\
\text{Step 3:}&\ \text{Apply induction to obtain orthonormal }u_2,\dots,u_n.\\
\text{Step 4:}&\ Q=[u_1,\dots,u_n],\ Q^\top Q=I,\ A Q=Q\Lambda\ \Rightarrow A=Q\Lambda Q^\top.
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Verify symmetry: $A=A^\top$.
\item Compute eigenvalues (real) and orthonormalize eigenvectors.
\item Assemble $Q$ and $\Lambda$, verify $A=Q\Lambda Q^\top$.
\item Use $\Lambda$ to compute $A^k$, $A^{1/2}$, $e^{At}$, or optimize $x^\top A x$.
\end{bullets}
\EQUIV{
\begin{bullets}
\item $A$ symmetric $\Leftrightarrow$ $A$ is normal with real spectrum and
orthogonal eigenbasis.
\item For Hermitian $A$, replace $Q^\top$ by $Q^\ast$ over $\mathbb{C}$.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item Non-symmetric matrices may fail to be diagonalizable or need non-orthogonal $V$.
\item Normal but non-symmetric (e.g., rotations) are unitarily diagonalizable over $\mathbb{C}$.
\end{bullets}
}
\INPUTS{$A\in\mathbb{R}^{n\times n}$ with $A=A^\top$.}
\DERIVATION{
\begin{align*}
\text{Example: }A&=\begin{bmatrix}2&1\\1&2\end{bmatrix},\
\det(A-\lambda I)=(2-\lambda)^2-1=\lambda^2-4\lambda+3.\\
\lambda&\in\{1,3\}.\
\lambda=3:\ (A-3I)v=0\Rightarrow v_1=\tfrac{1}{\sqrt{2}}\begin{bmatrix}1\\1\end{bmatrix}.\\
\lambda=1:\ (A-I)v=0\Rightarrow v_2=\tfrac{1}{\sqrt{2}}\begin{bmatrix}1\\-1\end{bmatrix}.\\
Q&=[v_1\ v_2],\ \Lambda=\mathrm{diag}(3,1),\ A=Q\Lambda Q^\top.
\end{align*}
}
\RESULT{
Orthogonal diagonalization exists; eigenvalues real; eigenvectors can be chosen
orthonormal, giving $A=Q\Lambda Q^\top$.
}
\PITFALLS{
\begin{bullets}
\item Failing to orthonormalize eigenvectors within a repeated eigenspace.
\item Assuming symmetry from numerical noise; verify $A$ close to $A^\top$.
\end{bullets}
}
\INTUITION{
Symmetry means $A$ has no twist, only stretch along perpendicular directions, so
space decomposes into orthogonal modes.
}

\FormulaPage{3}{Diagonalization and Matrix Functions}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
If $A$ is diagonalizable, $A=V\Lambda V^{-1}$ with $\Lambda=\mathrm{diag}
(\lambda_i)$. Then for integers $k\ge 0$, $A^k=V\Lambda^k V^{-1}$ and for
analytic $f$ on a neighborhood of the spectrum, $f(A)=V f(\Lambda) V^{-1}$
where $f(\Lambda)=\mathrm{diag}(f(\lambda_i))$.

\WHAT{
Reduce matrix powers and analytic functions of $A$ to scalar operations on
eigenvalues in a similarity-transformed basis.
}
\WHY{
Greatly simplifies $A^k$, $e^{At}$, $(I-\alpha A)^{-1}$, and time evolution of
linear differential equations.
}
\FORMULA{
\[
A=V\Lambda V^{-1}\ \Rightarrow\
A^k=V\Lambda^k V^{-1},\quad
f(A)=V f(\Lambda) V^{-1}.
\]
}
\CANONICAL{
$A$ diagonalizable over $\mathbb{C}$. For symmetric $A$, choose $V=Q$ orthogonal.
Analytic $f$ has convergent power series on the spectrum.
}
\PRECONDS{
\begin{bullets}
\item Existence of a basis of eigenvectors (diagonalizability).
\item $f$ analytic on a domain containing $\{\lambda_i\}$.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
If $A=V\Lambda V^{-1}$, then for any polynomial $p$, $p(A)=V p(\Lambda) V^{-1}$.
\end{lemma}
\begin{proof}
For $p(t)=\sum_{j=0}^m a_j t^j$,
$p(A)=\sum_j a_j A^j=\sum_j a_j (V\Lambda V^{-1})^j
=V(\sum_j a_j \Lambda^j)V^{-1}=V p(\Lambda) V^{-1}$ by similarity and
associativity. \qedhere
\end{proof}
\DERIVATION{
\begin{align*}
\text{Powers: }&A^k=(V\Lambda V^{-1})^k
=V\Lambda (V^{-1}V)\Lambda \cdots \Lambda V^{-1}=V\Lambda^k V^{-1}.\\
\text{Analytic }f:&\ f(t)=\sum_{j=0}^\infty a_j t^j\ \Rightarrow\
f(A)=\sum a_j A^j=V(\sum a_j \Lambda^j)V^{-1}=V f(\Lambda) V^{-1}.
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Diagonalize $A$ if possible (or orthogonally if symmetric).
\item Compute $\Lambda^k$ or $f(\Lambda)$ entrywise.
\item Transform back with $V$ and $V^{-1}$.
\item Validate via trace/determinant and limiting behavior.
\end{bullets}
\EQUIV{
\begin{bullets}
\item For ODE $\dot{x}=A x$, $e^{At}=V e^{\Lambda t} V^{-1}$.
\item Geometric series: $(I-\alpha A)^{-1}=V(I-\alpha \Lambda)^{-1}V^{-1}$ when convergent.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item Non-diagonalizable $A$ requires Jordan form and also $f(J)$ with Jordan blocks.
\item If $f$ is not analytic on the spectrum, $f(A)$ may be undefined.
\end{bullets}
}
\INPUTS{$A\in\mathbb{C}^{n\times n}$ diagonalizable; $f$ analytic near $\sigma(A)$.}
\DERIVATION{
\begin{align*}
\text{Example: }A&=\begin{bmatrix}4&0\\1&3\end{bmatrix},\
p_A(\lambda)=(4-\lambda)(3-\lambda).\\
V&=\begin{bmatrix}1&0\\1&1\end{bmatrix},\ \Lambda=\mathrm{diag}(4,3),\
V^{-1}=\begin{bmatrix}1&0\\-1&1\end{bmatrix}.\\
A^5&=V\mathrm{diag}(4^5,3^5)V^{-1}
=\begin{bmatrix}4^5&0\\1&3\end{bmatrix}\begin{bmatrix}1&0\\-1&1\end{bmatrix}\\
&=\begin{bmatrix}4^5&0\\-4^5+3^5&1\end{bmatrix}.
\end{align*}
}
\RESULT{
Matrix functions reduce to scalar functions of eigenvalues in the eigenbasis.
}
\PITFALLS{
\begin{bullets}
\item Using a nondiagonalizable decomposition; verify eigenvector completeness.
\item Confusing $V^{-1}$ with $V^\top$; only orthogonal $Q$ satisfies $Q^{-1}=Q^\top$.
\end{bullets}
}
\INTUITION{
In the eigenbasis, $A$ acts independently on coordinates; functions of $A$
act coordinatewise and then reassemble by the change of basis.
}

\FormulaPage{4}{Rayleigh Quotient and Courant–Fischer}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
For symmetric $A\in\mathbb{R}^{n\times n}$ with eigenvalues
$\lambda_1\ge\cdots\ge\lambda_n$,
\[
\lambda_1=\max_{\|x\|=1} x^\top A x,\quad
\lambda_n=\min_{\|x\|=1} x^\top A x,
\]
and the Courant–Fischer min–max:
\[
\lambda_k=\min_{\dim S=n-k+1}\ \max_{\substack{x\in S\\ \|x\|=1}} x^\top A x
=\max_{\dim T=k}\ \min_{\substack{x\in T\\ \|x\|=1}} x^\top A x.
\]

\WHAT{
Characterize eigenvalues as extrema of the Rayleigh quotient over subspaces.
}
\WHY{
Provides variational principles for computing and bounding eigenvalues and
underpins numerical methods and perturbation theory.
}
\FORMULA{
\[
R_A(x)=\frac{x^\top A x}{x^\top x},\quad
\lambda_{\max}=\max_{\|x\|=1}R_A(x),\quad
\lambda_{\min}=\min_{\|x\|=1}R_A(x).
\]
}
\CANONICAL{
$A$ symmetric real; eigenvalues real and orthonormally diagonalizable. The
subspaces in min–max are with respect to standard inner product.
}
\PRECONDS{
\begin{bullets}
\item $A=A^\top$.
\item Compactness of unit sphere ensures extrema exist.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
Let $A=Q\Lambda Q^\top$ with $Q=[q_i]$. Then
$x^\top A x=\sum_{i=1}^n \lambda_i \alpha_i^2$ where $\alpha=Q^\top x$.
\end{lemma}
\begin{proof}
$x^\top A x=x^\top Q\Lambda Q^\top x=(Q^\top x)^\top \Lambda (Q^\top x)
=\sum_i \lambda_i \alpha_i^2$, with $\alpha_i=q_i^\top x$.
\qedhere
\end{proof}
\DERIVATION{
\begin{align*}
\text{Bounds: }&\|x\|=1\Rightarrow x^\top A x=\sum_i \lambda_i \alpha_i^2,\
\sum_i \alpha_i^2=1.\\
&\lambda_n\le \sum_i \lambda_i \alpha_i^2\le \lambda_1.\\
\text{Extrema: }&\text{Max achieved by choosing }x=q_1,\ \text{min by }x=q_n.\\
\text{Min–max: }&\text{For }k,\ \dim S=n-k+1 \Rightarrow S\cap
\mathrm{span}\{q_k,\dots,q_n\}\neq\{0\}.\\
&\max_{\|x\|=1,x\in S} x^\top A x \ge \lambda_k.\
\text{Optimizing }S=\mathrm{span}\{q_k,\dots,q_n\}\ \text{gives equality}.
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item For bounds, compute $R_A(x)$ for trial vectors $x$.
\item For extremal eigenvalues, use power iteration aligning with $q_1$ or
inverse iteration for $q_n$.
\item For intermediate $\lambda_k$, restrict to subspaces and apply min–max.
\end{bullets}
\EQUIV{
\begin{bullets}
\item $\lambda_{\max}=\max_{x\neq 0} \frac{x^\top A x}{x^\top x}$,
$\lambda_{\min}=\min_{x\neq 0} \frac{x^\top A x}{x^\top x}$.
\item Shifted forms for $A-\mu I$ move spectrum by $-\mu$.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item Non-symmetric $A$: Rayleigh quotient may be complex and lacks extremal
eigenvalue characterization.
\item Degenerate eigenvalues: any vector in the eigenspace attains the same
extreme value.
\end{bullets}
}
\INPUTS{$A\in\mathbb{R}^{n\times n}$ symmetric; $k\in\{1,\dots,n\}$.}
\DERIVATION{
\begin{align*}
\text{Example: }A&=\begin{bmatrix}2&1\\1&2\end{bmatrix},\
\lambda_{\max}=3,\ \lambda_{\min}=1.\\
x&=\tfrac{1}{\sqrt{2}}\begin{bmatrix}1\\1\end{bmatrix}\Rightarrow
R_A(x)=\tfrac{1}{2}[1\ 1]\begin{bmatrix}2&1\\1&2\end{bmatrix}\begin{bmatrix}1\\1\end{bmatrix}
=\tfrac{1}{2}\cdot 6=3.
\end{align*}
}
\RESULT{
Eigenvalues of symmetric $A$ arise as min/max and min–max of the Rayleigh
quotient; eigenvectors realize the extrema.
}
\PITFALLS{
\begin{bullets}
\item Forgetting normalization in $R_A(x)$.
\item Applying min–max to non-symmetric matrices.
\end{bullets}
}
\INTUITION{
Quadratic energy $x^\top A x$ spreads among orthogonal modes with weights
$\alpha_i^2$; the most energy per unit norm aligns with the largest eigenmode.
}

\FormulaPage{5}{Gershgorin Circle Theorem}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Every eigenvalue $\lambda$ of $A=[a_{ij}]\in\mathbb{C}^{n\times n}$ lies in at
least one Gershgorin disc $D(a_{ii},R_i)$ where
$R_i=\sum_{j\ne i}|a_{ij}|$:
\[
\lambda\in \bigcup_{i=1}^n \{z\in\mathbb{C}:\ |z-a_{ii}|\le R_i\}.
\]

\WHAT{
A computable localization of the spectrum via row sums of off-diagonal magnitudes.
}
\WHY{
Gives quick bounds for eigenvalues, aids preconditioning and verifying stability,
and provides initial guesses for iterative eigensolvers.
}
\FORMULA{
\[
|\,\lambda-a_{ii}\,|\le \sum_{j\ne i}|a_{ij}|\ \text{ for some }i.
\]
}
\CANONICAL{
No symmetry required. Holds for complex matrices. Column version via columns.
}
\PRECONDS{
\begin{bullets}
\item $A$ square.
\item Standard absolute value on $\mathbb{C}$; any subordinate norm yields
variants.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
Let $A v=\lambda v$ and pick $k$ so that $|v_k|=\max_i |v_i|$. Then
$|\,\lambda-a_{kk}\,|\,|v_k|\le \sum_{j\ne k}|a_{kj}|\,|v_j|$.
\end{lemma}
\begin{proof}
From the $k$th row, $(\lambda-a_{kk})v_k=\sum_{j\ne k} a_{kj} v_j$.
Take absolute values and apply triangle inequality and
$|v_j|\le |v_k|$:
$|\,\lambda-a_{kk}\,|\,|v_k|\le \sum_{j\ne k}|a_{kj}|\,|v_j|
\le |v_k|\sum_{j\ne k}|a_{kj}|$. Divide by $|v_k|>0$.
\qedhere
\end{proof}
\DERIVATION{
\begin{align*}
\text{Pick eigenpair }&( \lambda,v),\ v\neq 0.\
\text{Choose }k\text{ with }|v_k|\ge |v_j|.\\
\text{Lemma }\Rightarrow\ &|\,\lambda-a_{kk}\,|\le \sum_{j\ne k}|a_{kj}|=R_k.\\
\text{Hence }\lambda&\in D(a_{kk},R_k).
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Compute centers $a_{ii}$ and radii $R_i$.
\item Plot or tabulate discs; spectrum lies in their union.
\item Tighten by scaling rows/columns or using both row and column discs.
\end{bullets}
\EQUIV{
\begin{bullets}
\item Column version: $|\,\lambda-a_{ii}\,|\le \sum_{j\ne i}|a_{ji}|$.
\item Brauer and Ostrowski refinements give smaller regions.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item Discs can be conservative for highly non-normal matrices.
\item Overlaps may hide eigenvalue separation.
\end{bullets}
}
\INPUTS{$A\in\mathbb{C}^{n\times n}$.}
\DERIVATION{
\begin{align*}
\text{Example: }A&=\begin{bmatrix}3&-1&0\\2&1&1\\0&2&2\end{bmatrix}.\
R_1=1,\ R_2=|2|+|1|=3,\ R_3=2.\\
\text{Discs: }&D(3,1),\ D(1,3),\ D(2,2).
\end{align*}
}
\RESULT{
All eigenvalues are within the union of Gershgorin discs.
}
\PITFALLS{
\begin{bullets}
\item Miscomputing radii by including $j=i$.
\item Assuming each disc contains exactly one eigenvalue; only certain
separation conditions ensure counts.
\end{bullets}
}
\INTUITION{
Rows describe how much off-diagonal coupling can pull an eigenvalue away from
its diagonal entry; the radius is the allowable pull.
}

\clearpage
\section{10 Exhaustive Problems and Solutions}

\ProblemPage{1}{Compute Eigenpairs and Powers via Diagonalization}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Find eigenvalues and eigenvectors of
$A=\begin{bmatrix}5&2&0\\0&3&1\\0&0&2\end{bmatrix}$, assess diagonalizability,
and compute $A^5$.

\PROBLEM{
Determine the spectrum, construct $V,\Lambda$, decide if $A$ is diagonalizable,
and use $V\Lambda V^{-1}$ to compute $A^5$.
}
\MODEL{
\[
A=\begin{bmatrix}5&2&0\\0&3&1\\0&0&2\end{bmatrix},\quad
p_A(\lambda)=(5-\lambda)(3-\lambda)(2-\lambda).
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Work over $\mathbb{R}$; eigenvalues are real by triangular form.
\item Distinct eigenvalues imply diagonalizability.
\end{bullets}
}
\varmapStart
\var{\lambda_i}{Eigenvalues $5,3,2$.}
\var{v_i}{Right eigenvectors.}
\var{V}{Eigenvector matrix.}
\var{\Lambda}{Diagonal matrix of eigenvalues.}
\varmapEnd
\WHICHFORMULA{
Formula 1 (Characteristic Equation) and Formula 3 (Diagonalization and Powers).
}
\GOVERN{
\[
(A-\lambda I)v=0,\quad A=V\Lambda V^{-1},\quad A^5=V\Lambda^5 V^{-1}.
\]
}
\INPUTS{$A$ as given.}
\DERIVATION{
\begin{align*}
\text{Eigenvalues: }&\lambda\in\{5,3,2\}\ \text{(upper triangular).}\\
\lambda=5:&\ (A-5I)=\begin{bmatrix}0&2&0\\0&-2&1\\0&0&-3\end{bmatrix}.\
v_5=\begin{bmatrix}1\\0\\0\end{bmatrix}.\\
\lambda=3:&\ (A-3I)=\begin{bmatrix}2&2&0\\0&0&1\\0&0&-1\end{bmatrix}.\
v_3=\begin{bmatrix}0\\1\\0\end{bmatrix}.\\
\lambda=2:&\ (A-2I)=\begin{bmatrix}3&2&0\\0&1&1\\0&0&0\end{bmatrix}.\
v_2=\begin{bmatrix}0\\-1\\1\end{bmatrix}.\\
V&=\begin{bmatrix}1&0&0\\0&1&-1\\0&0&1\end{bmatrix},\
\Lambda=\mathrm{diag}(5,3,2).\\
V^{-1}&=\begin{bmatrix}1&0&0\\0&1&1\\0&0&1\end{bmatrix}.\\
A^5&=V\mathrm{diag}(5^5,3^5,2^5)V^{-1}\\
&=\begin{bmatrix}5^5&0&0\\0&3^5&-3^5+2^5\\0&0&2^5\end{bmatrix}.
\end{align*}
}
\RESULT{
$A$ is diagonalizable with the displayed $V,\Lambda$ and
$A^5=\begin{bmatrix}3125&0&0\\0&243&-11\\0&0&32\end{bmatrix}$.
}
\UNITCHECK{
Trace of $A$ is $10$; sum of eigenvalues $=5+3+2=10$ preserved.}
\EDGECASES{
\begin{bullets}
\item If two diagonal entries coincide, check geometric multiplicity carefully.
\end{bullets}
}
\ALTERNATE{
Use the triangular structure: $A^k$ is upper triangular with diagonal entries
$5^k,3^k,2^k$; solve upper-recursions for superdiagonal entries.
}
\VALIDATION{
\begin{bullets}
\item Verify $A V=V\Lambda$ numerically.
\item Confirm $\det(A)=5\cdot 3\cdot 2=30$ equals product of eigenvalues.
\end{bullets}
}
\INTUITION{
Distinct diagonal entries signal independent modes; off-diagonals only couple
non-dominant coordinates.
}
\CANONICAL{
\begin{bullets}
\item Spectrum from triangular diagonal.
\item Powers via diagonalization.
\end{bullets}
}

\ProblemPage{2}{Orthogonal Diagonalization and PCA Directions}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Given $A=\begin{bmatrix}4&2&0\\2&3&0\\0&0&1\end{bmatrix}$ symmetric, find an
orthonormal eigenbasis, $Q,\Lambda$, and the principal direction.

\PROBLEM{
Compute eigenpairs, orthonormalize, and identify the unit vector maximizing
$x^\top A x$.
}
\MODEL{
\[
A=\begin{bmatrix}4&2&0\\2&3&0\\0&0&1\end{bmatrix}
=\begin{bmatrix}B&0\\0&1\end{bmatrix},\quad
B=\begin{bmatrix}4&2\\2&3\end{bmatrix}.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $A$ symmetric $\Rightarrow$ real spectrum and orthonormal eigenvectors.
\end{bullets}
}
\varmapStart
\var{\lambda_i}{Eigenvalues of $A$.}
\var{q_i}{Orthonormal eigenvectors.}
\var{Q}{Orthogonal matrix with columns $q_i$.}
\varmapEnd
\WHICHFORMULA{
Formula 2 (Spectral Theorem) and Formula 4 (Rayleigh Quotient).
}
\GOVERN{
\[
A=Q\Lambda Q^\top,\quad \lambda_{\max}=\max_{\|x\|=1} x^\top A x.
\]
}
\INPUTS{$A$ as given.}
\DERIVATION{
\begin{align*}
\det(B-\lambda I)&=(4-\lambda)(3-\lambda)-4=\lambda^2-7\lambda+8.\\
\lambda&=\frac{7\pm\sqrt{49-32}}{2}=\frac{7\pm\sqrt{17}}{2}.\\
\text{Third eigenvalue: }&\lambda_3=1\ \text{with }e_3.\\
\lambda_1&=\tfrac{7+\sqrt{17}}{2}:\ (B-\lambda_1 I)[x;y]=0\\
&\Rightarrow (4-\lambda_1)x+2y=0\Rightarrow y=\tfrac{\lambda_1-4}{2}x.\\
\text{Normalize }&q_1=\frac{1}{\sqrt{1+\left(\frac{\lambda_1-4}{2}\right)^2}}
\begin{bmatrix}1\\ \frac{\lambda_1-4}{2}\\ 0\end{bmatrix}.\\
\lambda_2&=\tfrac{7-\sqrt{17}}{2}:\ \text{similar, }q_2\perp q_1 \text{ in } \mathbb{R}^2.\\
q_3&=e_3.\\
Q&=[q_1\ q_2\ q_3],\ \Lambda=\mathrm{diag}(\lambda_1,\lambda_2,1).
\end{align*}
}
\RESULT{
$A=Q\Lambda Q^\top$ with eigenvalues
$\left\{\tfrac{7\pm\sqrt{17}}{2},1\right\}$ and principal direction $q_1$.
}
\UNITCHECK{
$\mathrm{tr}(A)=8\ \text{equals}\ \sum \lambda_i= \left(\tfrac{7+\sqrt{17}}{2}
+\tfrac{7-\sqrt{17}}{2}\right)+1=8$.}
\EDGECASES{
\begin{bullets}
\item If $2\to 0$, block becomes diagonal and eigenvectors align with axes.
\end{bullets}
}
\ALTERNATE{
Apply 2D rotation to diagonalize $B$ explicitly; append $e_3$.
}
\VALIDATION{
\begin{bullets}
\item Check $Q^\top A Q=\Lambda$ numerically.
\item Verify $q_1$ maximizes $x^\top A x$ over unit vectors.
\end{bullets}
}
\INTUITION{
The $2\times 2$ block couples the first two coordinates; the dominant mode
gives the principal variance direction as in PCA.
}
\CANONICAL{
\begin{bullets}
\item Orthogonal diagonalization and Rayleigh extremum coincide.
\end{bullets}
}

\ProblemPage{3}{Rotation–Scaling Spectrum and Asymptotics}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Let $A=r\begin{bmatrix}\cos\theta&-\sin\theta\\ \sin\theta&\cos\theta\end{bmatrix}$
with $r>0$. Find eigenvalues and describe $\|A^k\|$ as $k\to\infty$.

\PROBLEM{
Compute complex eigenvalues and deduce growth or decay of $A^k$.
}
\MODEL{
\[
A=r R_\theta,\ R_\theta=\begin{bmatrix}\cos\theta&-\sin\theta\\
\sin\theta&\cos\theta\end{bmatrix}.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Work over $\mathbb{C}$ for eigenvalues unless $\theta\equiv 0,\pi$.
\end{bullets}
}
\varmapStart
\var{r}{Scaling factor $>0$.}
\var{\theta}{Rotation angle.}
\var{\lambda}{Eigenvalue.}
\varmapEnd
\WHICHFORMULA{
Formula 1 (Characteristic Equation) and diagonalization over $\mathbb{C}$.
}
\GOVERN{
\[
\det(A-\lambda I)=\lambda^2-2 r\cos\theta\,\lambda+r^2.
\]
}
\INPUTS{$r,\theta$.}
\DERIVATION{
\begin{align*}
\lambda&=r(\cos\theta\pm i\sin\theta)=r e^{\pm i\theta}.\\
\|A^k\|&\approx r^k\|R_{k\theta}\|=\begin{cases}
r^k & \text{in any operator norm compatible with }2\text{D}. 
\end{cases}\\
\text{Hence }&\|A^k\|\to\infty \text{ if }r>1,\ \to 0 \text{ if }0<r<1,\\
&\text{constant if }r=1.
\end{align*}
}
\RESULT{
Eigenvalues $\lambda=r e^{\pm i\theta}$; spectral radius $\rho(A)=r$ governs
$\|A^k\|$ growth as $r^k$.
}
\UNITCHECK{
$\det(A)=r^2=\lambda_1\lambda_2$, $\mathrm{tr}(A)=2r\cos\theta=\lambda_1+\lambda_2$.}
\EDGECASES{
\begin{bullets}
\item $\theta=0$: real eigenvalues $r,r$ with $A=r I$.
\item $\theta=\pi$: eigenvalues $-r,-r$ with $A=-r I$.
\end{bullets}
}
\ALTERNATE{
Use polar decomposition: $A=r Q$ with $Q$ orthogonal; powers give $A^k=r^k Q^k$.
}
\VALIDATION{
\begin{bullets}
\item Numerically compute $\|A^k\|_2$ and compare to $r^k$.
\end{bullets}
}
\INTUITION{
Pure rotation does not change lengths; scaling $r$ controls the growth rate.
}
\CANONICAL{
\begin{bullets}
\item Complex-conjugate eigenvalues for planar rotations.
\item Spectral radius dictates asymptotics of $A^k$.
\end{bullets}
}

\ProblemPage{4}{Narrative: Alice and Bob\'s Weather Markov Chain}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Alice models weather as Sunny/Cloudy with
$P=\begin{bmatrix}0.8&0.2\\0.3&0.7\end{bmatrix}$. Bob asks for $P^n$ and the
stationary distribution.

\PROBLEM{
Compute eigen-decomposition of $P$, derive $P^n$, and find $\pi$ with
$\pi^\top P=\pi^\top$, $\pi^\top \mathbf{1}=1$.
}
\MODEL{
\[
P=\begin{bmatrix}0.8&0.2\\0.3&0.7\end{bmatrix},\quad P^n=V\Lambda^n V^{-1}.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Irreducible and aperiodic chain; unique stationary distribution.
\end{bullets}
}
\varmapStart
\var{\lambda_1}{Largest eigenvalue $=1$.}
\var{\lambda_2}{Second eigenvalue $=0.5$.}
\var{v_i}{Right eigenvectors.}
\var{\pi}{Stationary distribution.}
\varmapEnd
\WHICHFORMULA{
Formula 3 (Powers via Diagonalization) and Formula 1 (Characteristic Equation).
}
\GOVERN{
\[
\det(P-\lambda I)=0,\quad P^n=V\Lambda^n V^{-1},\quad \pi^\top P=\pi^\top.
\]
}
\INPUTS{$P$ as given.}
\DERIVATION{
\begin{align*}
\det(P-\lambda I)&=(0.8-\lambda)(0.7-\lambda)-0.06
=\lambda^2-1.5\lambda+0.56.\\
\lambda&=\frac{1.5\pm\sqrt{2.25-2.24}}{2}=\{1,0.5\}.\\
\lambda=1:&\ (P-I)v=0\Rightarrow v_1=\begin{bmatrix}1\\1\end{bmatrix}.\\
\lambda=0.5:&\ (P-0.5I)v=0\Rightarrow v_2=\begin{bmatrix}2\\-3\end{bmatrix}.\\
V&=\begin{bmatrix}1&2\\1&-3\end{bmatrix},\
V^{-1}=\frac{1}{-5}\begin{bmatrix}-3&-2\\-1&1\end{bmatrix}.\\
\Lambda^n&=\mathrm{diag}(1,0.5^n).\
P^n=V\Lambda^n V^{-1}.\\
\pi^\top&\propto v_1^\top V^{-1}\ \text{or solve } \pi^\top P=\pi^\top.\\
\pi^\top&=\left[\frac{3}{5},\frac{2}{5}\right].
\end{align*}
}
\RESULT{
$P^n=V\mathrm{diag}(1,0.5^n)V^{-1}\to \mathbf{1}\pi^\top$ with
$\pi^\top=\left[\tfrac{3}{5},\tfrac{2}{5}\right]$.
}
\UNITCHECK{
Rows of $P^n$ sum to $1$; $\lambda_1=1$ ensures stochasticity.}
\EDGECASES{
\begin{bullets}
\item If $\lambda_2=-1$, convergence oscillates. Here $|\lambda_2|=0.5$.
\end{bullets}
}
\ALTERNATE{
Solve $P^n=\mathbf{1}\pi^\top + 0.5^n C$ using spectral decomposition without
explicit $V^{-1}$ by projecting onto $\mathbf{1}$ and orthogonal complement.
}
\VALIDATION{
\begin{bullets}
\item Check $\pi^\top P=\pi^\top$ and $\sum \pi_i=1$.
\item Numerically compute $P^{10}$ and confirm near rank-one limit.
\end{bullets}
}
\INTUITION{
Long-run weather fraction equals stationary distribution; deviations decay
like $0.5^n$.
}
\CANONICAL{
\begin{bullets}
\item Stochastic matrices have eigenvalue $1$ with eigenvector $\mathbf{1}$.
\end{bullets}
}

\ProblemPage{5}{Narrative: Eigenvector Centrality on a Tiny Network}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Graph with adjacency
$A=\begin{bmatrix}0&1&1\\1&0&1\\1&1&0\end{bmatrix}$. Compute principal eigen-
vector to rank nodes.

\PROBLEM{
Find largest eigenvalue and normalized positive eigenvector.
}
\MODEL{
\[
A=\mathbf{1}\mathbf{1}^\top - I_3.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Connected, undirected graph $\Rightarrow$ symmetric $A$, Perron eigenvector
positive.
\end{bullets}
}
\varmapStart
\var{\lambda_{\max}}{Largest eigenvalue.}
\var{q}{Principal eigenvector.}
\varmapEnd
\WHICHFORMULA{
Formula 2 (Spectral Theorem) and Formula 4 (Rayleigh Quotient).
}
\GOVERN{
\[
A q=\lambda_{\max} q,\ \|q\|_2=1,\ q>0.
\]
}
\INPUTS{$A$ as given.}
\DERIVATION{
\begin{align*}
A&=\mathbf{1}\mathbf{1}^\top - I.\
\mathbf{1}=[1,1,1]^\top.\\
A \mathbf{1}&=(3-1)\mathbf{1}=2\mathbf{1}\Rightarrow \lambda_{\max}=2,\
q=\tfrac{1}{\sqrt{3}}\mathbf{1}.\\
\text{Other eigenvalues: }&-1\text{ (double) with vectors orthogonal to }\mathbf{1}.
\end{align*}
}
\RESULT{
Centrality ranks are equal; $q_i=\tfrac{1}{\sqrt{3}}$ and $\lambda_{\max}=2$.
}
\UNITCHECK{
$\mathrm{tr}(A)=-3=\sum \lambda_i=2-1-1$.}
\EDGECASES{
\begin{bullets}
\item Removing an edge breaks symmetry; eigenvector becomes nonuniform.
\end{bullets}
}
\ALTERNATE{
Compute $p_A(\lambda)=\det(A-\lambda I)=-(\lambda+1)^2(\lambda-2)$ directly.
}
\VALIDATION{
\begin{bullets}
\item Verify $A q=2 q$ explicitly.
\end{bullets}
}
\INTUITION{
Complete graph spreads influence evenly; everyone is equally central.
}
\CANONICAL{
\begin{bullets}
\item Perron–Frobenius for connected nonnegative symmetric matrices.
\end{bullets}
}

\ProblemPage{6}{Expectation Puzzle via Eigen-Decomposition}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
A biased coin flips state with probability $p=0.2$ each step, stays with
probability $0.8$. Starting Heads, find $\mathbb{P}[\text{Heads at step }n]$
and expected number of Heads in first $N$ steps.

\PROBLEM{
Use eigen-decomposition of two-state transition matrix to obtain $P^n$ and
sum probabilities.
}
\MODEL{
\[
P=\begin{bmatrix}0.8&0.2\\0.2&0.8\end{bmatrix},\
s_0=[1,0]^\top,\ s_n=P^n s_0.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Time-homogeneous Markov chain; independence of steps.
\end{bullets}
}
\varmapStart
\var{\lambda_1}{1.}
\var{\lambda_2}{0.6.}
\var{v_1,v_2}{Eigenvectors $[1,1]^\top,[1,-1]^\top$.}
\var{s_n}{State distribution at step $n$.}
\varmapEnd
\WHICHFORMULA{
Formula 3 (Powers via Diagonalization).
}
\GOVERN{
\[
P^n=V\Lambda^n V^{-1},\quad \mathbb{P}[\text{H at }n]=(s_n)_1.
\]
}
\INPUTS{$p=0.2$, $N\in\mathbb{N}$.}
\DERIVATION{
\begin{align*}
\lambda&\in\{1,0.6\},\ V=\begin{bmatrix}1&1\\1&-1\end{bmatrix},\
V^{-1}=\tfrac{1}{2}\begin{bmatrix}1&1\\1&-1\end{bmatrix}.\\
P^n&=V\mathrm{diag}(1,0.6^n)V^{-1}
=\tfrac{1}{2}\begin{bmatrix}1+0.6^n&1-0.6^n\\1-0.6^n&1+0.6^n\end{bmatrix}.\\
s_n&=P^n s_0=\tfrac{1}{2}\begin{bmatrix}1+0.6^n\\1-0.6^n\end{bmatrix}.\\
\mathbb{P}[\text{H at }n]&=\tfrac{1}{2}(1+0.6^n).\\
\mathbb{E}[\#\text{Heads in }1..N]&=\sum_{n=1}^N \tfrac{1}{2}(1+0.6^n)
=\tfrac{N}{2}+\tfrac{1}{2}\cdot \frac{0.6(1-0.6^N)}{1-0.6}.
\end{align*}
}
\RESULT{
$\mathbb{P}[\text{Heads at }n]=\tfrac{1}{2}(1+0.6^n)$ and
$\mathbb{E}=\tfrac{N}{2}+\tfrac{3}{4}(1-0.6^N)$.
}
\UNITCHECK{
As $N\to\infty$, $\mathbb{E}\sim N/2+3/4$, consistent with stationarity $1/2$.}
\EDGECASES{
\begin{bullets}
\item $p\to 0$: always Heads, probability $\to 1$.
\item $p\to 1/2$: $\lambda_2\to 0$, immediate mixing.
\end{bullets}
}
\ALTERNATE{
Solve recurrence $h_{n+1}=0.8 h_n+0.2(1-h_n)$ with $h_0=1$ to get the same
closed form $h_n=\tfrac{1}{2}(1+0.6^n)$.
}
\VALIDATION{
\begin{bullets}
\item Numeric check: $n=1$, $\mathbb{P}=0.8$; $n=2$, $0.68$ matches formula.
\end{bullets}
}
\INTUITION{
Distribution relaxes exponentially to stationarity with rate $|\lambda_2|$.
}
\CANONICAL{
\begin{bullets}
\item Two-state chains diagonalize in the Hadamard basis.
\end{bullets}
}

\ProblemPage{7}{Proof: Orthogonality of Distinct Eigenvectors (Symmetric)}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Prove that for $A=A^\top$, eigenvectors with distinct eigenvalues are orthogonal.

\PROBLEM{
Given $A v=\lambda v$ and $A w=\mu w$ with $\lambda\ne \mu$, show $v^\top w=0$.
}
\MODEL{
\[
v^\top A w=\mu v^\top w,\quad (A v)^\top w=\lambda v^\top w.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $A$ symmetric so $A^\top=A$.
\end{bullets}
}
\varmapStart
\var{v,w}{Eigenvectors.}
\var{\lambda,\mu}{Eigenvalues, $\lambda\ne \mu$.}
\varmapEnd
\WHICHFORMULA{
Formula 2 (Spectral Theorem, supporting lemma).
}
\GOVERN{
\[
v^\top A w=(A v)^\top w.
\]
}
\INPUTS{$A=A^\top$, $A v=\lambda v$, $A w=\mu w$.}
\DERIVATION{
\begin{align*}
v^\top A w&=\mu v^\top w,\quad (A v)^\top w=v^\top A^\top w=v^\top A w
=\lambda v^\top w.\\
(\mu-\lambda)v^\top w&=0\Rightarrow v^\top w=0.
\end{align*}
}
\RESULT{
$v\perp w$ for $\lambda\ne \mu$.
}
\UNITCHECK{
Orthogonality consistent with diagonalization by $Q^\top A Q$.}
\EDGECASES{
\begin{bullets}
\item For repeated eigenvalues, pick an orthonormal basis in the eigenspace.
\end{bullets}
}
\ALTERNATE{
Use Courant–Fischer: extremizers for different eigenvalues lie in orthogonal
subspaces.
}
\VALIDATION{
\begin{bullets}
\item Numeric test with $A=\begin{bmatrix}2&1\\1&3\end{bmatrix}$ yields orthogonal
eigenvectors.
\end{bullets}
}
\INTUITION{
Symmetry forbids twisting; stretching along different amounts forces right angles.
}
\CANONICAL{
\begin{bullets}
\item Distinct symmetric modes are orthogonal.
\end{bullets}
}

\ProblemPage{8}{Proof: Rayleigh Quotient Bounds for Symmetric $A$}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Show $\lambda_{\min}\le R_A(x)\le \lambda_{\max}$ for all $x\ne 0$ and equality
iff $x$ is an eigenvector.

\PROBLEM{
Establish bounds and equality conditions via spectral decomposition.
}
\MODEL{
\[
A=Q\Lambda Q^\top,\ x=Q\alpha,\ \|x\|^2=\sum \alpha_i^2.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $A$ symmetric so that $Q,\Lambda$ exist with real spectrum.
\end{bullets}
}
\varmapStart
\var{\lambda_{\max},\lambda_{\min}}{Extreme eigenvalues.}
\var{\alpha}{Coordinates in eigenbasis.}
\varmapEnd
\WHICHFORMULA{
Formula 4 (Rayleigh Quotient characterization).
}
\GOVERN{
\[
R_A(x)=\frac{\sum_i \lambda_i \alpha_i^2}{\sum_i \alpha_i^2}.
\]
}
\INPUTS{$A=A^\top$, $x\ne 0$.}
\DERIVATION{
\begin{align*}
R_A(x)&=\frac{\sum_i \lambda_i \alpha_i^2}{\sum_i \alpha_i^2}
\in[\lambda_{\min},\lambda_{\max}]\\
&\text{since } \sum_i \alpha_i^2>0\text{ and convex combination of }\lambda_i.\\
\text{Equality }&R_A(x)=\lambda_{\max}\ \text{iff } \alpha_i=0\ \forall i>1,\\
&\text{i.e., }x\parallel q_1\ \text{(eigenvector). Similarly for }\lambda_{\min}.
\end{align*}
}
\RESULT{
Bound holds with equality exactly at eigenvectors of extreme eigenvalues.
}
\UNITCHECK{
Convex combination weights sum to $1$.
}
\EDGECASES{
\begin{bullets}
\item If $\lambda_{\max}=\lambda_{\min}$, $A=\lambda I$ and $R_A(x)=\lambda$.
\end{bullets}
}
\ALTERNATE{
Lagrange multipliers on $\|x\|=1$ yield $A x=\nu x$ at extrema.
}
\VALIDATION{
\begin{bullets}
\item Random $x$ achieves values within bounds; maximizing via power iteration
approaches $\lambda_{\max}$.
\end{bullets}
}
\INTUITION{
Quadratic energy blends mode energies; cannot exceed the largest mode energy.
}
\CANONICAL{
\begin{bullets}
\item Rayleigh quotient is a weighted average of eigenvalues.
\end{bullets}
}

\ProblemPage{9}{Combo: Solve Linear ODE with Eigen-Decomposition}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Solve $\dot{x}=A x$, $x(0)=x_0$ for
$A=\begin{bmatrix}1&1\\0&2\end{bmatrix}$ using diagonalization.

\PROBLEM{
Find $e^{At}$ via eigen-decomposition and compute $x(t)=e^{At}x_0$.
}
\MODEL{
\[
A=V\Lambda V^{-1},\ e^{At}=V e^{\Lambda t} V^{-1}.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $A$ diagonalizable (distinct eigenvalues 1 and 2).
\end{bullets}
}
\varmapStart
\var{V,\Lambda}{Eigen-decomposition.}
\var{x_0}{Initial condition.}
\varmapEnd
\WHICHFORMULA{
Formula 3 (Matrix Functions via Diagonalization).
}
\GOVERN{
\[
e^{At}=V\mathrm{diag}(e^{t},e^{2t})V^{-1}.
\]
}
\INPUTS{$x_0=[x_{01},x_{02}]^\top$.}
\DERIVATION{
\begin{align*}
\lambda&\in\{1,2\},\ v_1=\begin{bmatrix}1\\0\end{bmatrix},\
v_2=\begin{bmatrix}1\\1\end{bmatrix}.\
V=\begin{bmatrix}1&1\\0&1\end{bmatrix},\
V^{-1}=\begin{bmatrix}1&-1\\0&1\end{bmatrix}.\\
e^{At}&=V \begin{bmatrix}e^t&0\\0&e^{2t}\end{bmatrix} V^{-1}
=\begin{bmatrix}e^t&-e^t\\0&e^{2t}\end{bmatrix}\begin{bmatrix}1&-1\\0&1\end{bmatrix}\\
&=\begin{bmatrix}e^t&-e^t+e^{2t}\\0&e^{2t}\end{bmatrix}.\\
x(t)&=e^{At}x_0=\begin{bmatrix}e^t x_{01}+(-e^t+e^{2t})x_{02}\\ e^{2t}x_{02}\end{bmatrix}.
\end{align*}
}
\RESULT{
$x(t)=[e^t(x_{01}-x_{02})+e^{2t}x_{02},\ e^{2t}x_{02}]^\top$.
}
\UNITCHECK{
At $t=0$, $e^{A0}=I$; result gives $x(0)=x_0$.}
\EDGECASES{
\begin{bullets}
\item If $x_{02}=0$, solution decouples to $x_1(t)=e^t x_{01}$.
\end{bullets}
}
\ALTERNATE{
Compute $e^{At}$ via series and triangular form; same closed form emerges.
}
\VALIDATION{
\begin{bullets}
\item Differentiate solution and verify $\dot{x}=A x$.
\end{bullets}
}
\INTUITION{
Each eigenmode evolves exponentially at its eigenvalue rate; coupling mixes
modes in the original coordinates.
}
\CANONICAL{
\begin{bullets}
\item $e^{At}$ reduces to exponentials of eigenvalues in eigenbasis.
\end{bullets}
}

\ProblemPage{10}{Combo: Quadratic Form Optimization on Unit Sphere}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
For $A=\begin{bmatrix}3&1\\1&2\end{bmatrix}$, find $\min_{\|x\|=1} x^\top A x$
and the minimizing $x$.

\PROBLEM{
Use Rayleigh quotient and spectral theorem to find $\lambda_{\min}$ and
eigenvector.
}
\MODEL{
\[
\det(A-\lambda I)=\lambda^2-5\lambda+5.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $A$ symmetric $\Rightarrow$ real eigenvalues and orthonormal eigenvectors.
\end{bullets}
}
\varmapStart
\var{\lambda_{\min}}{Smallest eigenvalue.}
\var{q_{\min}}{Corresponding unit eigenvector.}
\varmapEnd
\WHICHFORMULA{
Formula 4 (Rayleigh Quotient) and Formula 2 (Spectral Theorem).
}
\GOVERN{
\[
\lambda_{\min}=\min_{\|x\|=1} x^\top A x,\quad A q_{\min}=\lambda_{\min} q_{\min}.
\]
}
\INPUTS{$A$ as given.}
\DERIVATION{
\begin{align*}
\lambda&=\frac{5\pm\sqrt{25-20}}{2}=\frac{5\pm\sqrt{5}}{2}.\\
\lambda_{\min}&=\tfrac{5-\sqrt{5}}{2}.\
(A-\lambda_{\min} I)v=0:\\
\begin{bmatrix}3-\lambda_{\min}&1\\1&2-\lambda_{\min}\end{bmatrix}
\begin{bmatrix}x\\y\end{bmatrix}&=0.\
(3-\lambda_{\min})x+y=0\Rightarrow y=-(3-\lambda_{\min})x.\\
q_{\min}&=\frac{1}{\sqrt{1+(3-\lambda_{\min})^2}}
\begin{bmatrix}1\\-(3-\lambda_{\min})\end{bmatrix}.
\end{align*}
}
\RESULT{
$\min x^\top A x=\tfrac{5-\sqrt{5}}{2}$, achieved at $q_{\min}$ above.
}
\UNITCHECK{
$\lambda_{\max}\lambda_{\min}=\det(A)=5$ and
$\lambda_{\max}+\lambda_{\min}=\mathrm{tr}(A)=5$.}
\EDGECASES{
\begin{bullets}
\item If off-diagonal $1\to 0$, eigenvectors align with axes; minimum is $2$.
\end{bullets}
}
\ALTERNATE{
Use calculus with Lagrange multipliers to recover the eigen-equation.
}
\VALIDATION{
\begin{bullets}
\item Check $A q_{\min}=\lambda_{\min} q_{\min}$ numerically.
\end{bullets}
}
\INTUITION{
The least-stiff direction corresponds to the smallest eigenvalue.
}
\CANONICAL{
\begin{bullets}
\item Quadratic optimization on the sphere is an eigenvalue problem.
\end{bullets}
}

\clearpage
\section{Coding Demonstrations}

\CodeDemoPage{Power Iteration and Rayleigh Quotient Convergence}
\PROBLEM{
Compute the dominant eigenpair of a symmetric matrix and track Rayleigh
quotient convergence. Verify with a library solver.
}
\API{
\begin{bullets}
\item \inlinecode{def read_input(s) -> np.ndarray} — parse matrix rows.
\item \inlinecode{def solve_case(A) -> (val, vec)} — power iteration.
\item \inlinecode{def validate() -> None} — assertions vs. numpy.
\item \inlinecode{def main() -> None} — run validation and demo.
\end{bullets}
}
\INPUTS{
Square symmetric matrix $A$ as list of whitespace-separated rows of numbers.
}
\OUTPUTS{
Dominant eigenvalue approximation and corresponding unit eigenvector.
}
\FORMULA{
\[
x_{k+1}=\frac{A x_k}{\|A x_k\|},\quad
\hat{\lambda}_k=\frac{x_k^\top A x_k}{x_k^\top x_k}.
\]
}
\textbf{SOLUTION A — From Scratch (Mathematically Explicit Implementation)}
\begin{codepy}
import numpy as np

def read_input(s):
    rows = [[float(t) for t in line.split()] for line in s.strip().split(";")]
    return np.array(rows, dtype=float)

def solve_case(A, iters=50):
    np.random.seed(0)
    n = A.shape[0]
    x = np.random.randn(n)
    x = x/np.linalg.norm(x)
    for _ in range(iters):
        y = A @ x
        ny = np.linalg.norm(y)
        if ny == 0:
            break
        x = y/ny
    lam = float(x.T @ A @ x)
    return lam, x

def validate():
    A = np.array([[2.0, 1.0], [1.0, 2.0]])
    lam, x = solve_case(A, iters=100)
    w, V = np.linalg.eigh(A)
    assert abs(lam - max(w)) < 1e-6
    assert np.linalg.norm(A @ x - lam * x) < 1e-5

def main():
    validate()
    A = read_input("2 1; 1 3")
    lam, x = solve_case(A)
    print("lambda_max", round(lam, 6))
    print("vec", np.round(x, 6))

if __name__ == "__main__":
    main()
\end{codepy}
\textbf{SOLUTION B — Library-Based (Validated Computational Shortcut)}
\begin{codepy}
import numpy as np

def read_input(s):
    rows = [[float(t) for t in line.split()] for line in s.strip().split(";")]
    return np.array(rows, dtype=float)

def solve_case(A):
    w, V = np.linalg.eigh(A)
    idx = np.argmax(w)
    return float(w[idx]), V[:, idx]

def validate():
    A = np.array([[2.0, 1.0], [1.0, 2.0]])
    lam1, v1 = solve_case(A)
    w, V = np.linalg.eigh(A)
    lam2 = float(np.max(w))
    assert abs(lam1 - lam2) < 1e-12
    assert np.allclose(A @ v1, lam1 * v1, atol=1e-10)

def main():
    validate()
    A = read_input("2 1; 1 3")
    lam, v = solve_case(A)
    print("lambda_max", round(lam, 6))
    print("vec", np.round(v, 6))

if __name__ == "__main__":
    main()
\end{codepy}
\COMPLEXITY{
Power iteration: time $\mathcal{O}(n^2 k)$, space $\mathcal{O}(n)$.
Library eigh: $\mathcal{O}(n^3)$ time, $\mathcal{O}(n^2)$ space.
}
\FAILMODES{
\begin{bullets}
\item Dominant eigenvalue not unique in magnitude slows or stalls convergence.
\item Zero vector encountered if $A x=0$; reinitialize $x$.
\item Non-symmetric $A$ may converge to non-dominant directions unpredictably.
\end{bullets}
}
\STABILITY{
\begin{bullets}
\item Normalize every iteration to avoid overflow.
\item Shift to improve separation if needed: apply to $A-\mu I$.
\end{bullets}
}
\VALIDATION{
\begin{bullets}
\item Compare with numpy.linalg.eigh within tolerance.
\item Check residual norm $\|A x-\lambda x\|$ small.
\end{bullets}
}
\RESULT{
Both methods agree on $\lambda_{\max}$ and eigenvector up to sign, with small
residual.
}
\EXPLANATION{
Power iteration uses repeated application of $A$ to amplify the dominant mode,
and the Rayleigh quotient estimates the dominant eigenvalue.
}

\CodeDemoPage{Unshifted QR Algorithm for All Eigenvalues}
\PROBLEM{
Approximate all eigenvalues of a real matrix using the QR iteration and verify
against library results.
}
\API{
\begin{bullets}
\item \inlinecode{def qr_eigs(A, iters) -> np.ndarray} — QR iterations.
\item \inlinecode{def validate() -> None} — compare to numpy.linalg.eigvals.
\item \inlinecode{def main() -> None} — run on small test.
\end{bullets}
}
\INPUTS{
Square real matrix $A$, iteration count.
}
\OUTPUTS{
Approximate eigenvalues from the diagonal of the iterated matrix.
}
\FORMULA{
\[
A_{k+1}=R_k Q_k,\quad \text{where }A_k=Q_k R_k\ \text{(QR factorization)}.
\]
}
\textbf{SOLUTION A — From Scratch (Mathematically Explicit Implementation)}
\begin{codepy}
import numpy as np

def qr_eigs(A, iters=60):
    Ak = A.copy().astype(float)
    for _ in range(iters):
        Q, R = np.linalg.qr(Ak)
        Ak = R @ Q
    return np.diag(Ak)

def validate():
    np.random.seed(0)
    A = np.random.randn(4, 4)
    vals_qr = np.sort_complex(qr_eigs(A, iters=200))
    vals_np = np.sort_complex(np.linalg.eigvals(A))
    # Allow modest tolerance due to unshifted convergence
    assert np.allclose(vals_qr, vals_np, atol=1e-6)

def main():
    validate()
    A = np.array([[2.0, 1.0], [1.0, 3.0]])
    print("qr eigs", np.round(qr_eigs(A, iters=50), 6))

if __name__ == "__main__":
    main()
\end{codepy}
\textbf{SOLUTION B — Library-Based (Validated Computational Shortcut)}
\begin{codepy}
import numpy as np

def eigs_lib(A):
    return np.linalg.eigvals(A)

def validate():
    A = np.array([[2.0, 1.0], [1.0, 3.0]])
    e = np.sort_complex(eigs_lib(A))
    d = np.sort_complex(np.linalg.eigvals(A))
    assert np.allclose(e, d)

def main():
    validate()
    A = np.array([[0.0, 1.0], [-2.0, 3.0]])
    print("lib eigs", np.round(eigs_lib(A), 6))

if __name__ == "__main__":
    main()
\end{codepy}
\COMPLEXITY{
Each QR step: $\mathcal{O}(n^3)$. With $k$ iterations, $\mathcal{O}(k n^3)$.
}
\FAILMODES{
\begin{bullets}
\item Slow convergence without shifts; complex pairs may form $2\times 2$ blocks.
\item Loss of orthogonality if Q not computed stably.
\end{bullets}
}
\STABILITY{
\begin{bullets}
\item Use Householder QR for numerical stability (numpy does).
\item Shifts accelerate and improve robustness (not used here).
\end{bullets}
}
\VALIDATION{
\begin{bullets}
\item Compare to numpy eigenvalues up to ordering and tolerance.
\end{bullets}
}
\RESULT{
QR iteration recovers eigenvalues consistent with library solver for small
matrices.
}
\EXPLANATION{
QR iteration is a similarity transformation $A_{k+1}=R_k Q_k=Q_k^\top A_k Q_k$
that drives $A_k$ toward (quasi) upper triangular Schur form with eigenvalues on
the diagonal.
}

\CodeDemoPage{Orthogonal Diagonalization and Reconstruction Error}
\PROBLEM{
Diagonalize a symmetric matrix using \inlinecode{eigh}, reconstruct $A$, and
report the reconstruction error and spectral bounds via Rayleigh quotient.
}
\API{
\begin{bullets}
\item \inlinecode{def sym_data(n, seed) -> A} — random symmetric matrix.
\item \inlinecode{def reconstruct(A) -> (Q, L, Ahat, err)} — eigendecomp.
\item \inlinecode{def validate() -> None} — orthogonality \& error checks.
\item \inlinecode{def main() -> None} — run pipeline.
\end{bullets}
}
\INPUTS{
Size $n$ and seed for reproducibility.
}
\OUTPUTS{
$Q,\Lambda$, reconstruction $Ahat$, Frobenius error, Rayleigh bounds.
}
\FORMULA{
\[
A=Q\Lambda Q^\top,\quad \|A-Q\Lambda Q^\top\|_F.
\]
}
\textbf{SOLUTION A — From Scratch (Mathematically Explicit Implementation)}
\begin{codepy}
import numpy as np

def sym_data(n=5, seed=0):
    np.random.seed(seed)
    M = np.random.randn(n, n)
    return (M + M.T) / 2.0

def reconstruct(A):
    w, Q = np.linalg.eigh(A)
    L = np.diag(w)
    Ahat = Q @ L @ Q.T
    err = np.linalg.norm(A - Ahat, ord="fro")
    # Rayleigh bounds with random unit vector
    np.random.seed(1)
    x = np.random.randn(A.shape[0])
    x = x / np.linalg.norm(x)
    R = float(x.T @ A @ x)
    return Q, L, Ahat, err, (w.min(), R, w.max())

def validate():
    A = sym_data(4, 0)
    Q, L, Ahat, err, _ = reconstruct(A)
    I = Q.T @ Q
    assert np.allclose(I, np.eye(A.shape[0]), atol=1e-10)
    assert err < 1e-10

def main():
    validate()
    A = sym_data(4, 1)
    Q, L, Ahat, err, bounds = reconstruct(A)
    print("err", err)
    print("bounds", bounds)

if __name__ == "__main__":
    main()
\end{codepy}
\textbf{SOLUTION B — Library-Based (Validated Computational Shortcut)}
\begin{codepy}
import numpy as np

def reconstruct_lib(A):
    w, Q = np.linalg.eigh(A)
    return w, Q, Q @ np.diag(w) @ Q.T

def validate():
    A = np.array([[2.0, 1.0], [1.0, 3.0]])
    w, Q, Ahat = reconstruct_lib(A)
    assert np.allclose(A, Ahat)
    assert np.allclose(Q.T @ Q, np.eye(2))

def main():
    validate()
    A = np.array([[4.0, 2.0, 0.0], [2.0, 3.0, 0.0], [0.0, 0.0, 1.0]])
    w, Q, Ahat = reconstruct_lib(A)
    print("eigvals", np.round(w, 6))

if __name__ == "__main__":
    main()
\end{codepy}
\COMPLEXITY{
Eigendecomposition for symmetric $A$: $\mathcal{O}(n^3)$ time, $\mathcal{O}(n^2)$
space.
}
\FAILMODES{
\begin{bullets}
\item Non-symmetric input breaks orthogonal diagonalization assumption.
\item Nearly equal eigenvalues cause sensitivity in eigenvectors.
\end{bullets}
}
\STABILITY{
\begin{bullets}
\item Symmetric solvers are backward stable; errors scale with conditioning.
\item Use double precision and orthonormalization checks.
\end{bullets}
}
\VALIDATION{
\begin{bullets}
\item Check $Q^\top Q=I$ and $\|A-Q\Lambda Q^\top\|_F$ small.
\item Compare Rayleigh sample to $[\lambda_{\min},\lambda_{\max}]$.
\end{bullets}
}
\RESULT{
Exact reconstruction within numerical precision and Rayleigh sample within
spectral interval.
}
\EXPLANATION{
Spectral theorem guarantees $A=Q\Lambda Q^\top$; reconstruction validates
eigen-decomposition and Rayleigh bounds.
}

\clearpage
\section{Applied Domains — Detailed End-to-End Scenarios}

\DomainPage{Machine Learning}
\SCENARIO{
Principal Component Analysis (PCA): compute eigenvalues/eigenvectors of the
sample covariance and project data onto top principal components.
}
\ASSUMPTIONS{
\begin{bullets}
\item Data matrix $X\in\mathbb{R}^{n\times d}$ centered: columns have mean $0$.
\item Covariance $S=\frac{1}{n}X^\top X$ is symmetric positive semidefinite.
\end{bullets}
}
\WHICHFORMULA{
Spectral theorem: $S=Q\Lambda Q^\top$; explained variance ratios
$\lambda_i/\sum_j\lambda_j$.
}
\varmapStart
\var{X}{Centered data, shape $(n,d)$.}
\var{S}{Covariance $\frac{1}{n}X^\top X$.}
\var{Q,\Lambda}{Eigen-decomposition of $S$.}
\var{Z}{Projected data $X Q_k$.}
\varmapEnd
\PIPELINE{
\begin{bullets}
\item Center data.
\item Compute $S=\frac{1}{n}X^\top X$.
\item Eigendecompose $S$ to get $Q,\Lambda$.
\item Choose $k$ and project: $Z=X Q_{:,1:k}$.
\end{bullets}
}
\textbf{Implementation (From Scratch)}
\begin{codepy}
import numpy as np

def pca_from_scratch(n=200, d=3, k=2, seed=0):
    np.random.seed(seed)
    U = np.random.randn(n, d)
    X = U @ np.diag([3.0, 1.0, 0.2]) + np.random.randn(n, d)*0.1
    X -= X.mean(axis=0, keepdims=True)
    S = (X.T @ X) / float(n)
    w, Q = np.linalg.eigh(S)
    idx = np.argsort(w)[::-1]
    w, Q = w[idx], Q[:, idx]
    Z = X @ Q[:, :k]
    evr = w / w.sum()
    return w, Q, Z, evr

def main():
    w, Q, Z, evr = pca_from_scratch()
    print("eigvals", np.round(w, 4))
    print("EVR", np.round(evr, 4))

if __name__ == "__main__":
    main()
\end{codepy}
\textbf{Implementation (Library Version)}
\begin{codepy}
import numpy as np
from sklearn.decomposition import PCA

def pca_lib(n=200, d=3, k=2, seed=0):
    np.random.seed(seed)
    U = np.random.randn(n, d)
    X = U @ np.diag([3.0, 1.0, 0.2]) + np.random.randn(n, d)*0.1
    X -= X.mean(axis=0, keepdims=True)
    pca = PCA(n_components=k, svd_solver="full")
    Z = pca.fit_transform(X)
    return pca.explained_variance_, pca.components_.T, Z, pca.explained_variance_ratio_

def main():
    w, Q, Z, evr = pca_lib()
    print("eigvals", np.round(w, 4))
    print("EVR", np.round(evr, 4))

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{Explained variance ratio per component; reconstruction error if needed.}
\INTERPRET{Top eigenvectors are principal directions of maximal variance.}
\NEXTSTEPS{Whitening via $S^{-1/2}$; robust PCA for outliers.}

\DomainPage{Quantitative Finance}
\SCENARIO{
Risk decomposition via eigenvalues of asset return covariance. Identify dominant
risk factors and their contributions.
}
\ASSUMPTIONS{
\begin{bullets}
\item Returns are mean-zero with covariance $\Sigma$; $\Sigma$ symmetric psd.
\item Portfolio variance $\sigma_p^2=w^\top \Sigma w$.
\end{bullets}
}
\WHICHFORMULA{
$\Sigma=Q\Lambda Q^\top$; project weights: $w=Q \alpha$ so
$\sigma_p^2=\sum_i \lambda_i \alpha_i^2$.
}
\varmapStart
\var{\Sigma}{Covariance matrix.}
\var{Q,\Lambda}{Eigen-decomposition of $\Sigma$.}
\var{w}{Portfolio weights, $\mathbf{1}^\top w=1$.}
\var{\alpha}{Coordinates in eigenbasis $Q^\top w$.}
\varmapEnd
\PIPELINE{
\begin{bullets}
\item Estimate $\Sigma$ from returns.
\item Eigendecompose $\Sigma$.
\item Compute $\alpha=Q^\top w$ and contributions $\lambda_i \alpha_i^2$.
\end{bullets}
}
\textbf{Implementation (Full Pipeline)}
\begin{codepy}
import numpy as np

def risk_decomp(n=1000, d=4, seed=0):
    np.random.seed(seed)
    A = np.random.randn(d, d)
    Sigma = A @ A.T
    w = np.random.rand(d); w = w / w.sum()
    w = w.astype(float)
    w = w.reshape(-1, 1)
    wT = w.T
    val = float(wT @ Sigma @ w)
    w = w.flatten()
    wT = None
    val = val
    # Eigen decomposition
    lam, Q = np.linalg.eigh(Sigma)
    idx = np.argsort(lam)[::-1]
    lam = lam[idx]; Q = Q[:, idx]
    alpha = Q.T @ w
    contrib = lam * (alpha**2)
    return lam, contrib, val

def main():
    lam, contrib, var = risk_decomp()
    print("eigvals", np.round(lam, 4))
    print("var", round(var, 6))
    print("factor_contrib", np.round(contrib, 6))

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{Portfolio variance and factor-wise contributions.}
\INTERPRET{Few large eigenvalues indicate concentrated risk in top factors.}
\NEXTSTEPS{Shrinkage covariance, risk parity targeting equal contributions.}

\DomainPage{Deep Learning}
\SCENARIO{
Estimate the spectral norm of a weight matrix $W$ via power iteration on
$W^\top W$ and compare with library SVD.
}
\ASSUMPTIONS{
\begin{bullets}
\item Spectral norm $\|W\|_2=\sqrt{\lambda_{\max}(W^\top W)}$.
\item Random Gaussian $W$ serves as proxy for trained weights.
\end{bullets}
}
\WHICHFORMULA{
Power iteration on $A=W^\top W$; spectral theorem ensures $A$ symmetric psd.
}
\varmapStart
\var{W}{Weight matrix $(m\times n)$.}
\var{A}{Gram matrix $W^\top W$.}
\var{\sigma_{\max}}{Largest singular value.}
\varmapEnd
\PIPELINE{
\begin{bullets}
\item Generate $W$ deterministically.
\item Power iterate on $A$ to approximate $\lambda_{\max}$.
\item Compare $\sqrt{\lambda_{\max}}$ to SVD first singular value.
\end{bullets}
}
\textbf{Implementation (End-to-End)}
\begin{codepy}
import numpy as np

def spectral_norm(W, iters=50):
    n = W.shape[1]
    np.random.seed(0)
    x = np.random.randn(n)
    x = x/np.linalg.norm(x)
    for _ in range(iters):
        y = W.T @ (W @ x)
        ny = np.linalg.norm(y)
        if ny == 0:
            break
        x = y/ny
    lam = float(x.T @ (W.T @ (W @ x)))
    return np.sqrt(lam), x

def main():
    np.random.seed(1)
    W = np.random.randn(50, 20)
    s, _ = spectral_norm(W, iters=100)
    s_svd = np.linalg.svd(W, compute_uv=False)[0]
    print("spec_norm", round(s, 6), "svd", round(s_svd, 6))

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{Relative error between power iteration estimate and SVD singular value.}
\INTERPRET{Spectral norm tracks Lipschitz constant, impacting training stability.}
\NEXTSTEPS{Use deflation for top-$k$ singular values; apply to convolutional layers.}

\DomainPage{Kaggle / Data Analytics}
\SCENARIO{
EDA with PCA: compute correlation matrix eigenvalues to assess dimensionality
and report cumulative explained variance.
}
\ASSUMPTIONS{
\begin{bullets}
\item Standardize features; use correlation matrix $C$.
\item $C$ symmetric psd with eigenvalues summing to $d$.
\end{bullets}
}
\WHICHFORMULA{
$C=Q\Lambda Q^\top$ and cumulative EVR $=\frac{\sum_{i=1}^k \lambda_i}{\sum_j \lambda_j}$.
}
\varmapStart
\var{D}{DataFrame of numeric columns.}
\var{C}{Correlation matrix.}
\var{w}{Eigenvalues of $C$.}
\var{Q}{Eigenvectors (loadings).}
\varmapEnd
\PIPELINE{
\begin{bullets}
\item Generate synthetic correlated data.
\item Standardize to zero mean, unit variance.
\item Eigendecompose $C$ and compute cumulative EVR.
\end{bullets}
}
\textbf{Implementation (Complete EDA Pipeline)}
\begin{codepy}
import numpy as np, pandas as pd

def synth_df(n=300, seed=0):
    np.random.seed(seed)
    A = np.random.randn(3, 3)
    X = np.random.randn(n, 3) @ A
    cols = ["A", "B", "C"]
    return pd.DataFrame(X, columns=cols)

def standardize(df):
    return (df - df.mean()) / df.std(ddof=0)

def pca_corr(df):
    X = standardize(df)
    C = (X.values.T @ X.values) / float(len(X))
    w, Q = np.linalg.eigh(C)
    idx = np.argsort(w)[::-1]
    w, Q = w[idx], Q[:, idx]
    evr = w / w.sum()
    cev = np.cumsum(evr)
    return w, evr, cev, Q

def main():
    df = synth_df()
    w, evr, cev, Q = pca_corr(df)
    print("eigvals", np.round(w, 4))
    print("EVR", np.round(evr, 4))
    print("CEV", np.round(cev, 4))

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{Explained variance ratio and cumulative explained variance.}
\INTERPRET{Rapid decay in eigenvalues indicates low effective dimensionality.}
\NEXTSTEPS{Scree plot, Kaiser rule, cross-validated reconstruction error.}

\end{document}
% !TeX program = xelatex
\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{microtype,setspace,amsmath,amssymb,mathtools,amsthm,unicode-math}
\setstretch{1.05}
\setmainfont{Latin Modern Roman}
\setmonofont{Latin Modern Mono}
\setmathfont{Latin Modern Math}

\allowdisplaybreaks[4]
\setlength{\jot}{7pt}
\setlength{\emergencystretch}{8em}
\sloppy

\usepackage{xcolor,fancyhdr,enumitem,inconsolata,listings}
\pagestyle{fancy}\fancyhf{}\lhead{\nouppercase{\leftmark}}\rhead{\thepage}
\setlength{\headheight}{26pt}
\setlength{\parindent}{0pt}
\setlength{\parskip}{8pt plus 2pt minus 1pt}
\raggedbottom

\newenvironment{BreakableEquation}{\begin{equation}\begin{aligned}}{\end{aligned}\end{equation}}
\newenvironment{BreakableEquation*}{\begin{equation*}\begin{aligned}}{\end{aligned}\end{equation*}}
\newenvironment{tightalign}{\begingroup\small\allowdisplaybreaks\begin{align}}{\end{align}\endgroup}

\providecommand{\enumlistm}{enumitem}
\newenvironment{minted}[2][]{%
  \lstset{style=code,language=#2,#1}\begin{lstlisting}%
}{\end{lstlisting}}

\newcommand{\inputminted}[3][]{\begin{lstlisting}\end{lstlisting}}

\newlist{bullets}{itemize}{1}
\setlist[bullets]{label=--,leftmargin=1.6em,itemsep=2pt,topsep=4pt}

\newcommand{\varmapStart}{\textbf{VARIABLE MAPPING:}\par\begin{bullets}}
\newcommand{\var}[2]{\item $#1$ — #2}
\newcommand{\varmapEnd}{\end{bullets}}

\newcommand{\glossx}[6]{%
  \textbf{#1}\par
  \begin{bullets}
    \item \textbf{What:} #2
    \item \textbf{Why:} #3
    \item \textbf{How:} #4
    \item \textbf{ELI5:} #5
    \item \textbf{Pitfall/Example:} #6
  \end{bullets}
}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{claim}[theorem]{Claim}
\lstdefinestyle{code}{
  basicstyle=\ttfamily\footnotesize,
  backgroundcolor=\color{black!02},
  frame=single,
  numbers=left, numberstyle=\tiny, numbersep=8pt,
  breaklines=true, breakatwhitespace=true,
  tabsize=4, showstringspaces=false,
  upquote=true, keepspaces=true, columns=fullflexible,
  literate=
    {–}{{-}}1
    {—}{{-}}1
    {…}{{...}}1
    {≤}{{\ensuremath{\le}}}1
    {≥}{{\ensuremath{\ge}}}1
    {≠}{{\ensuremath{\ne}}}1
    {≈}{{\ensuremath{\approx}}}1
    {±}{{\ensuremath{\pm}}}1
    {→}{{\ensuremath{\to}}}1
    {←}{{\ensuremath{\leftarrow}}}1
    {∞}{{\ensuremath{\infty}}}1
    {√}{{\ensuremath{\sqrt{\ }}}}1
    {×}{{\ensuremath{\times}}}1
    {÷}{{\ensuremath{\div}}}1
}

\lstnewenvironment{codepy}[1][]%
  {\lstset{style=code,language=Python,#1}}%
  {}

\newcommand{\inlinecode}[1]{\lstinline[style=code]!#1!}

\newcommand{\LF}[2]{\par\noindent\textbf{#1:}~#2\par}
\newcommand{\WHAT}[1]{\LF{WHAT}{#1}}
\newcommand{\WHY}[1]{\LF{WHY}{#1}}
\newcommand{\HOW}[1]{\LF{HOW}{#1}}
\newcommand{\ELI}[1]{\LF{ELI5}{#1}}
\newcommand{\SCOPE}[1]{\LF{SCOPE}{#1}}
\newcommand{\CONFUSIONS}[1]{\LF{COMMON CONFUSIONS}{#1}}
\newcommand{\APPLICATIONS}[1]{\LF{APPLICATIONS}{#1}}
\newcommand{\FORMULA}[1]{\LF{FORMULA}{#1}}
\newcommand{\CANONICAL}[1]{\LF{CANONICAL FORM}{#1}}
\newcommand{\PRECONDS}[1]{\LF{PRECONDITIONS}{#1}}
\newcommand{\DERIVATION}[1]{\LF{DERIVATION}{#1}}
\newcommand{\EQUIV}[1]{\LF{EQUIVALENT FORMS}{#1}}
\newcommand{\LIMITS}[1]{\LF{LIMIT CASES}{#1}}
\newcommand{\INPUTS}[1]{\LF{INPUTS}{#1}}
\newcommand{\OUTPUTS}[1]{\LF{OUTPUTS}{#1}}
\newcommand{\RESULT}[1]{\LF{RESULT}{#1}}
\newcommand{\INTUITION}[1]{\LF{INTUITION}{#1}}
\newcommand{\PITFALLS}[1]{\LF{PITFALLS}{#1}}
\newcommand{\MODEL}[1]{\LF{CANONICAL MATH MODEL}{#1}}
\newcommand{\ASSUMPTIONS}[1]{\LF{ASSUMPTIONS}{#1}}
\newcommand{\WHICHFORMULA}[1]{\LF{WHICH FORMULA \& WHY}{#1}}
\newcommand{\GOVERN}[1]{\LF{GOVERNING EQUATION(S)}{#1}}
\newcommand{\UNITCHECK}[1]{\LF{UNIT CHECK}{#1}}
\newcommand{\EDGECASES}[1]{\LF{EDGE CASES}{#1}}
\newcommand{\ALTERNATE}[1]{\LF{ALTERNATE APPROACH (sketch)}{#1}}
\newcommand{\PROBLEM}[1]{\LF{PROBLEM}{#1}}
\newcommand{\API}[1]{\LF{API}{#1}}
\newcommand{\COMPLEXITY}[1]{\LF{COMPLEXITY}{#1}}
\newcommand{\FAILMODES}[1]{\LF{FAILURE MODES}{#1}}
\newcommand{\STABILITY}[1]{\LF{NUMERICAL STABILITY}{#1}}
\newcommand{\VALIDATION}[1]{\LF{VALIDATION}{#1}}
\newcommand{\EXPLANATION}[1]{\LF{EXPLANATION}{#1}}
\newcommand{\SCENARIO}[1]{\LF{SCENARIO}{#1}}
\newcommand{\PIPELINE}[1]{\LF{PIPELINE STEPS}{#1}}
\newcommand{\METRICS}[1]{\LF{METRICS}{#1}}
\newcommand{\INTERPRET}[1]{\LF{INTERPRETATION}{#1}}
\newcommand{\NEXTSTEPS}[1]{\LF{LIMITATIONS \& NEXT STEPS}{#1}}

\usepackage{titlesec}
\titleformat{\section}{\Large\bfseries}{\thesection}{0.6em}{}
\titlespacing*{\section}{0pt}{*2}{*1}
\usepackage{etoolbox}
\pretocmd{\section}{\clearpage}{}{}

\newcommand{\FormulaPage}[2]{%
  \clearpage
  \section*{Formula #1 — #2}%
  \addcontentsline{toc}{section}{Formula #1 — #2}%
}
\newcommand{\ProblemPage}[2]{%
  \clearpage
  \subsection*{Problem #1: #2}%
  \addcontentsline{toc}{subsection}{Problem #1: #2}%
}
\newcommand{\CodeDemoPage}[1]{%
  \clearpage
  \subsection*{Coding Demo: #1}%
  \addcontentsline{toc}{subsection}{Coding Demo: #1}%
}
\newcommand{\DomainPage}[1]{%
  \clearpage
  \subsection*{#1 (End-to-End)}%
  \addcontentsline{toc}{subsection}{#1 (End-to-End)}%
}

\begin{document}
\title{Comprehensive Study Sheet — Norm Inequalities (Submultiplicative)}
\date{\today}
\maketitle
\tableofcontents
\clearpage

\section{Concept Overview}
\WHAT{
A submultiplicative norm on an algebra $(\mathcal{A},+,\cdot)$ over $\mathbb{R}$
or $\mathbb{C}$ is a function $\|\cdot\|:\mathcal{A}\to [0,\infty)$ satisfying
for all $x,y\in\mathcal{A}$: (i) $\|x\|=0 \Leftrightarrow x=0$,
(ii) $\|\alpha x\|=|\alpha|\|x\|$ for $\alpha\in\mathbb{F}$,
(iii) $\|x+y\|\le \|x\|+\|y\|$, and (iv) $\|xy\|\le \|x\|\|y\|$.
For matrices, $\|\cdot\|$ may be induced by a vector norm
$\|A\|=\sup_{x\ne 0}\frac{\|Ax\|}{\|x\|}$ on $\mathbb{F}^n$.
}
\WHY{
Submultiplicativity enables control of products, powers, and series in normed
algebras. It bounds growth of iterates $\|A^k\|$, ensures convergence of the
Neumann series when $\|A\|<1$, and yields stability and Lipschitz constants
for composed linear maps. It also bounds spectral radius via $\rho(A)\le \|A\|$.
}
\HOW{
1. Specify a normed algebra and a candidate norm. 2. Show triangle and homogeneity.
3. Verify $\|xy\|\le \|x\|\|y\|$ using the definition (e.g., via supremum for
induced norms or Cauchy\textendash Schwarz for Frobenius). 4. Derive consequences:
$\|A^k\|\le \|A\|^k$, $\|(I-A)^{-1}\|\le \frac{1}{1-\|A\|}$ for $\|A\|<1$,
and $\rho(A)\le \|A\|$.
}
\ELI{
A norm is a ruler. Submultiplicative means the ruler for the product is no
longer than the product of the rulers. For matrices, the strongest stretch of
doing $B$ then $A$ is at most the product of their strongest individual stretches.
}
\SCOPE{
Applies to algebras where multiplication is defined and compatible with the
norm. Induced matrix norms are always submultiplicative. Some norms like the
entrywise max-norm on matrices are submultiplicative; arbitrary functionals
may fail. Equality can be strict; degeneracies occur when one factor is zero.
}
\CONFUSIONS{
Do not confuse submultiplicativity of matrix norms with multiplicativity of
determinants. The Frobenius norm is not induced by a vector norm but is still
submultiplicative. Consistency with a vector norm (subordinate norm) implies
$\|Ax\|\le \|A\|\|x\|$, which is distinct from submultiplicativity in $A,B$.
}
\APPLICATIONS{
\begin{bullets}
\item Mathematical foundations: spectral radius bounds, convergence of series.
\item Computational modeling: stability bounds for iterative methods.
\item Physical/engineering: worst-case gain of cascaded linear systems.
\item Statistics/algorithms: Lipschitz bounds for gradient and linear layers.
\end{bullets}
}

\textbf{ANALYTIC STRUCTURE.}
Submultiplicative norms define a topological algebra where multiplication is
continuous. Common structure: convex, positively homogeneous, and compatible
with composition. Induced norms are operator norms, hence monotone and convex.
They are not generally symmetric with respect to transposition except for
unitarily invariant norms.

\textbf{CANONICAL LINKS.}
Triangle inequality combines with submultiplicativity to bound series and
products. Spectral radius bound uses $\|A^k\|\le \|A\|^k$. Neumann series
follows by geometric series comparison. Spectral norm submultiplicativity
follows from Cauchy\textendash Schwarz.

\textbf{PROBLEM-TYPE RECOGNITION HEURISTICS.}
\begin{bullets}
\item Composition or repeated application of linear maps: expect $\|AB\|$ or $\|A^k\|$.
\item Questions about convergence of $(I-A)^{-1}$: check $\|A\|<1$.
\item Bounding eigenvalues or growth rate: use $\rho(A)\le \|A\|$.
\item Bounds for layered networks: product of operator norms per layer.
\end{bullets}

\textbf{SOLUTION STRATEGY BLUEPRINT.}
\begin{bullets}
\item Translate composition into products or powers.
\item Choose a compatible norm (induced $p$-norm, spectral, or Frobenius).
\item Apply submultiplicativity and triangle inequality.
\item Simplify bounds and check tightness via singular vectors or alignment.
\item Validate by limits, eigenvalue checks, or small numeric examples.
\end{bullets}

\textbf{CONCEPTUAL INVARIANTS.}
Worst-case gain under composition is bounded by product of gains. The spectral
radius is invariant under similarity and bounded by any submultiplicative norm.

\textbf{EDGE INTUITION.}
As $\|A\|\to 0$, products shrink rapidly and series converge quickly. As
$\|A\|\to \infty$, $\|A^k\|$ grows at most geometrically with base $\|A\|$.
Near $\|A\|=1$, Neumann series approaches the convergence boundary.

\clearpage
\section{Glossary}
\glossx{Submultiplicative Norm}{
A norm $\|\cdot\|$ on an algebra satisfying $\|xy\|\le \|x\|\|y\|$.}{
Controls products and powers, enabling stability and convergence analysis.}{
Verify using definitions: induced norms by sup ratio; for Frobenius, use
Cauchy\textendash Schwarz with trace.}{
Like saying two stretches in sequence cannot exceed multiplying the worst
stretch of each step.}{
Pitfall: assuming equality $\|AB\|=\|A\|\|B\|$ holds. It rarely does.}

\glossx{Induced (Subordinate) Matrix Norm}{
$\|A\|=\sup_{x\ne 0}\frac{\|Ax\|}{\|x\|}$ for a given vector norm.}{
Connects matrix action to vector distortion; guarantees submultiplicativity.}{
Use sup definition and composition: $\|ABx\|\le \|A\|\|Bx\|\le \|A\|\|B\|\|x\|$.}{
Think: the loudest amplifier gain when feeding it any input of unit loudness.}{
Pitfall: mixing vector $p$-norms across factors breaks consistency.}

\glossx{Spectral Norm}{
$\|A\|_2=\sup_{\|x\|_2=1}\|Ax\|_2=\sigma_{\max}(A)$.}{
Unitarily invariant, equals largest singular value, submultiplicative.}{
Use SVD or Cauchy\textendash Schwarz: $\|AB\|_2\le \|A\|_2\|B\|_2$.}{
It is the maximum stretch factor of $A$ on Euclidean vectors.}{
Example: for orthogonal $Q$, $\|Q\|_2=1$.}

\glossx{Spectral Radius Bound}{
For any submultiplicative matrix norm, $\rho(A)\le \|A\|$.}{
Links eigenvalue magnitude to operator gain; used in stability of iterations.}{
For eigenpair $Av=\lambda v$, get $|\lambda|^k\|v\|=\|A^k v\|\le \|A^k\|\|v\|$.}{
Largest growth factor per iterate cannot exceed the worst-case norm growth.}{
Pitfall: believing $\rho(A)=\|A\|$ always; equality holds only in special cases.}

\clearpage
\section{Symbol Ledger}
\varmapStart
\var{\mathcal{A}}{Normed algebra over $\mathbb{R}$ or $\mathbb{C}$.}
\var{\|\cdot\|}{Norm on $\mathcal{A}$ or vector space.}
\var{A,B}{Matrices in $\mathbb{F}^{m\times n}$ or compatible sizes.}
\var{x,y}{Vectors in $\mathbb{F}^n$.}
\var{\|A\|_p}{Induced matrix norm from vector $p$-norm.}
\var{\|A\|_2}{Spectral norm (largest singular value).}
\var{\|A\|_F}{Frobenius norm $\sqrt{\mathrm{trace}(A^\ast A)}$.}
\var{I}{Identity matrix.}
\var{\rho(A)}{Spectral radius $\max_i |\lambda_i(A)|$.}
\var{k}{Positive integer exponent.}
\var{\alpha}{Scalar in base field.}
\var{\sigma_{\max}(A)}{Largest singular value of $A$.}
\varmapEnd

\clearpage
\section{Formula Canon — One Formula Per Page}

\FormulaPage{1}{Submultiplicativity of Induced Matrix Norms}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
For any vector norm $\|\cdot\|$ on $\mathbb{F}^n$, the induced matrix norm
$\|A\|=\sup_{x\ne 0}\frac{\|Ax\|}{\|x\|}$ is submultiplicative:
$\|AB\|\le \|A\|\|B\|$ for all compatible $A,B$.

\WHAT{
This states that the worst-case amplification of the composition $A\circ B$
does not exceed the product of their worst-case amplifications, under the same
underlying vector norm.
}
\WHY{
It ensures stability of composed linear maps, bounds iterative schemes, and is
the cornerstone for error propagation and Lipschitz constants in compositions.
}
\FORMULA{
\[
\|AB\|=\sup_{x\ne 0}\frac{\|ABx\|}{\|x\|}\le
\left(\sup_{y\ne 0}\frac{\|Ay\|}{\|y\|}\right)
\left(\sup_{x\ne 0}\frac{\|Bx\|}{\|x\|}\right)=\|A\|\|B\|.
\]
}
\CANONICAL{
Matrices over $\mathbb{R}$ or $\mathbb{C}$, finite-dimensional vector space
with any norm $\|\cdot\|_p$ or arbitrary norm. $A$ and $B$ are such that
$AB$ is defined. No additional smoothness is required.
}
\PRECONDS{
\begin{bullets}
\item $\|\cdot\|$ is a norm on $\mathbb{F}^n$.
\item Induced matrix norm is defined by the supremum over nonzero vectors.
\item Matrix multiplication $AB$ is defined (compatible dimensions).
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
For induced norm $\|\cdot\|$, $\|Ax\|\le \|A\|\|x\|$ for all $x$.
\end{lemma}
\begin{proof}
By definition, for $x\ne 0$,
$\frac{\|Ax\|}{\|x\|}\le \sup_{z\ne 0}\frac{\|Az\|}{\|z\|}=\|A\|$. If $x=0$,
both sides are $0$. Hence $\|Ax\|\le \|A\|\|x\|$. \qedhere
\end{proof}

\DERIVATION{
\begin{align*}
\text{Step 1: }& \|AB\|=\sup_{x\ne 0}\frac{\|ABx\|}{\|x\|}.\\
\text{Step 2: }& \text{Let }y=Bx. \text{ Then }\|ABx\|=\|Ay\|\le \|A\|\|y\|.\\
\text{Step 3: }& \|y\|=\|Bx\|\le \|B\|\|x\|\ \text{by the lemma}.\\
\text{Step 4: }& \frac{\|ABx\|}{\|x\|}\le \|A\|\frac{\|Bx\|}{\|x\|}\le
\|A\|\|B\|.\\
\text{Step 5: }& \sup_{x\ne 0}\frac{\|ABx\|}{\|x\|}\le \|A\|\|B\|.
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Fix a consistent vector norm for all factors.
\item Use $\|AB\|\le \|A\|\|B\|$ to bound compositions or to control errors.
\item For $\|A^k\|$, apply recursively to obtain $\|A^k\|\le \|A\|^k$.
\item If needed, pick $p\in\{1,2,\infty\}$ for computable induced norms.
\end{bullets}
\EQUIV{
\begin{bullets}
\item $\|ABx\|\le \|A\|\|B\|\|x\|$ for all $x$.
\item For $k\in\mathbb{N}$, $\|A^k\|\le \|A\|^k$.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item If either $A$ or $B$ is zero, both sides are zero.
\item If one factor is an isometry (e.g., orthogonal), the bound reduces to
the norm of the other factor.
\end{bullets}
}
\INPUTS{$A=\begin{pmatrix}1&2\\0&1\end{pmatrix},\ B=\begin{pmatrix}1&0\\3&1\end{pmatrix},
\ \|\cdot\|=\|\cdot\|_\infty$.}
\DERIVATION{
\begin{align*}
\|A\|_\infty&=\max\{1+2,\ 0+1\}=3,\\
\|B\|_\infty&=\max\{1+0,\ 3+1\}=4,\\
AB&=\begin{pmatrix}1+6&2\\3&1\end{pmatrix}=
\begin{pmatrix}7&2\\3&1\end{pmatrix},\\
\|AB\|_\infty&=\max\{7+2,\ 3+1\}=9,\quad
\|A\|_\infty\|B\|_\infty=12,\\
\|AB\|_\infty&\le \|A\|_\infty\|B\|_\infty \text{ holds (}9\le 12\text{).}
\end{align*}
}
\RESULT{
For any induced norm, $\|AB\|\le \|A\|\|B\|$. The example verifies it
numerically for $\|\cdot\|_\infty$.
}
\UNITCHECK{
All quantities are dimensionless norms; inequality is homogeneous under
scaling: replacing $A\to \alpha A$ scales both sides by $|\alpha|$.
}
\PITFALLS{
\begin{bullets}
\item Using different vector norms for $A$ and $B$ invalidates the bound.
\item Confusing the sup definition and computing with inconsistent $p$.
\end{bullets}
}
\INTUITION{
Composing two amplifiers multiplies their worst-case gains. The sup-based
definition ensures the worst input for $AB$ is no worse than the worst for $B$
fed into the worst for $A$.
}
\CANONICAL{
\begin{bullets}
\item Operator norm is submultiplicative.
\item Powers satisfy geometric growth bound.
\end{bullets}
}

\FormulaPage{2}{Powers and Neumann Series Bounds}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
If $\|\cdot\|$ is submultiplicative and $\|A\|<1$, then
$\|A^k\|\le \|A\|^k$ for all $k\in\mathbb{N}$ and
$\left\|\sum_{k=0}^\infty A^k\right\|\le \sum_{k=0}^\infty \|A^k\|
\le \frac{1}{1-\|A\|}$, with $(I-A)$ invertible and
$\|(I-A)^{-1}\|\le \frac{1}{1-\|A\|}$.

\WHAT{
Geometric decay of powers and norm bounds for the Neumann series yielding
a bound on the inverse of $(I-A)$ when $\|A\|<1$.
}
\WHY{
Provides convergence guarantees and quantitative control of resolvents,
crucial in iterative methods, linear system solvers, and stability analysis.
}
\FORMULA{
\[
\|A^k\|\le \|A\|^k,\quad
\left\|\sum_{k=0}^\infty A^k\right\|\le \frac{1}{1-\|A\|},\quad
\|(I-A)^{-1}\|\le \frac{1}{1-\|A\|}.
\]
}
\CANONICAL{
Any submultiplicative norm on a Banach algebra (finite-dimensional matrices
suffice). Condition $\|A\|<1$ guarantees convergence of the series.
}
\PRECONDS{
\begin{bullets}
\item Submultiplicativity: $\|XY\|\le \|X\|\|Y\|$.
\item Triangle inequality holds.
\item $\|A\|<1$.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
If $\|\cdot\|$ is submultiplicative, then $\|A^k\|\le \|A\|^k$ for $k\ge 1$.
\end{lemma}
\begin{proof}
By induction. For $k=1$ the claim is trivial. Suppose true for $k$.
Then $\|A^{k+1}\|=\|A^k A\|\le \|A^k\|\|A\|\le \|A\|^k\|A\|=\|A\|^{k+1}$.
\qedhere
\end{proof}

\DERIVATION{
\begin{align*}
\text{Step 1: }& S_N=\sum_{k=0}^N A^k.\\
\text{Step 2: }& \|S_N\|\le \sum_{k=0}^N \|A^k\|
\le \sum_{k=0}^N \|A\|^k\quad (\text{by lemma}).\\
\text{Step 3: }& \sum_{k=0}^\infty \|A\|^k=\frac{1}{1-\|A\|}\ \text{since }\|A\|<1.\\
\text{Step 4: }& \text{Hence } \sum_{k=0}^\infty A^k \text{ converges absolutely.}\\
\text{Step 5: }& (I-A)\left(\sum_{k=0}^\infty A^k\right)=I
=\left(\sum_{k=0}^\infty A^k\right)(I-A).\\
\text{Step 6: }& (I-A)^{-1}=\sum_{k=0}^\infty A^k,\
\|(I-A)^{-1}\|\le \frac{1}{1-\|A\|}.
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Verify $\|A\|<1$ in a chosen submultiplicative norm.
\item Use $\|A^k\|\le \|A\|^k$ to bound tails and truncation errors.
\item Bound inverse norm by geometric series if $(I-A)$ is to be inverted.
\end{bullets}
\EQUIV{
\begin{bullets}
\item Convergence of fixed-point iteration $x_{t+1}=Ax_t+b$ with rate $\|A\|$.
\item Error after $N$ terms: $\left\|\sum_{k=N+1}^\infty A^k\right\|
\le \frac{\|A\|^{N+1}}{1-\|A\|}$.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item If $\|A\|=1$, series may diverge even if $\rho(A)<1$ fails.
\item If $\|A\|\ge 1$, geometric bound does not ensure convergence.
\end{bullets}
}
\INPUTS{$A=\begin{pmatrix}0.2&0.1\\0&0.3\end{pmatrix},\ \|\cdot\|=\|\cdot\|_\infty$.}
\DERIVATION{
\begin{align*}
\|A\|_\infty&=\max\{0.2+0.1,\ 0+0.3\}=0.3<1.\\
\|(I-A)^{-1}\|_\infty&\le \frac{1}{1-0.3}=\frac{10}{7}\approx 1.4286.\\
A^2&=\begin{pmatrix}0.04&0.05\\0&0.09\end{pmatrix},\
\|A^2\|_\infty=0.09\le 0.3^2=0.09\ (\text{equality}).
\end{align*}
}
\RESULT{
Geometric decay of powers and the Neumann series bound give
$\|(I-A)^{-1}\|\le \frac{1}{1-\|A\|}$ when $\|A\|<1$.
}
\UNITCHECK{
Dimensionless quantities. Bounds scale correctly under $A\mapsto \alpha A$ with
$|\alpha|<1$ maintaining convergence.
}
\PITFALLS{
\begin{bullets}
\item Confusing condition $\|A\|<1$ with $\rho(A)<1$; the former is sufficient
but not necessary for invertibility of $(I-A)$.
\item Using a non-submultiplicative quantity invalidates the lemma.
\end{bullets}
}
\INTUITION{
If each application shrinks by at most factor $\|A\|<1$, repeated application
shrinks geometrically, so the infinite sum remains bounded like a geometric
series.
}
\CANONICAL{
\begin{bullets}
\item Geometric series bound in normed algebras.
\item Power bound from submultiplicativity.
\end{bullets}
}

\FormulaPage{3}{Spectral and Frobenius Norm Submultiplicativity}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
For matrices, the spectral norm and Frobenius norm are submultiplicative:
$\|AB\|_2\le \|A\|_2\|B\|_2$ and $\|AB\|_F\le \|A\|_F\|B\|_F$.

\WHAT{
Quantifies that the Euclidean operator gain and the Frobenius energy cannot
increase under multiplication beyond the product of individual gains.
}
\WHY{
Spectral norm governs worst-case amplification and is unitarily invariant.
Frobenius norm is computationally simple and submultiplicative, aiding error
analysis and regularization.
}
\FORMULA{
\[
\|AB\|_2\le \|A\|_2\|B\|_2,\qquad
\|AB\|_F\le \|A\|_F\|B\|_F.
\]
}
\CANONICAL{
Finite-dimensional matrices over $\mathbb{R}$ or $\mathbb{C}$. Spectral norm
via singular values; Frobenius norm via Hilbert\textendash Schmidt inner
product $\langle X,Y\rangle=\mathrm{trace}(Y^\ast X)$.
}
\PRECONDS{
\begin{bullets}
\item Standard matrix multiplication, finite sizes.
\item For Frobenius proof, use Cauchy\textendash Schwarz in Hilbert space.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
$\|AB\|_2\le \|A\|_2\|B\|_2$.
\end{lemma}
\begin{proof}
For any $x$ with $\|x\|_2=1$, $\|ABx\|_2\le \|A\|_2\|Bx\|_2\le
\|A\|_2\|B\|_2\|x\|_2=\|A\|_2\|B\|_2$. Taking sup over $\|x\|_2=1$ gives the
claim. \qedhere
\end{proof}
\begin{lemma}
$\|AB\|_F\le \|A\|_F\|B\|_2$ and $\|AB\|_F\le \|A\|_2\|B\|_F$.
\end{lemma}
\begin{proof}
$\|AB\|_F^2=\mathrm{trace}(B^\ast A^\ast A B)
=\sum_i \|A b_i\|_2^2 \le \|A\|_2^2 \sum_i \|b_i\|_2^2
=\|A\|_2^2\|B\|_F^2$, where $b_i$ are columns of $B$. Symmetrically,
$\|AB\|_F\le \|A\|_F\|B\|_2$. \qedhere
\end{proof}

\DERIVATION{
\begin{align*}
\text{Step 1: }& \|AB\|_F\le \|A\|_F\|B\|_2\le \|A\|_F\|B\|_F.\\
\text{Step 2: }& \text{Use }\|B\|_2\le \|B\|_F \text{ to conclude }
\|AB\|_F\le \|A\|_F\|B\|_F.\\
\text{Step 3: }& \text{Spectral case follows from operator inequality above.}
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item For worst-case gain, use spectral norm; for energy-like bounds, use
Frobenius.
\item Combine with unitary invariance to simplify via SVD or orthogonal bases.
\end{bullets}
\EQUIV{
\begin{bullets}
\item $\sigma_{\max}(AB)\le \sigma_{\max}(A)\sigma_{\max}(B)$.
\item $\|AB\|_F\le \sqrt{\mathrm{trace}(A^\ast A)}\sqrt{\mathrm{trace}(B^\ast B)}$.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item Equality in spectral inequality when singular vectors align.
\item For Frobenius, equality requires proportionality in Hilbert space sense.
\end{bullets}
}
\INPUTS{$A=\begin{pmatrix}1&2\\-1&0\end{pmatrix},\
B=\begin{pmatrix}2&0\\1&1\end{pmatrix}.}
\DERIVATION{
\begin{align*}
\|A\|_F&=\sqrt{1+4+1+0}=\sqrt{6},\ \|B\|_F=\sqrt{4+0+1+1}=\sqrt{6}.\\
AB&=\begin{pmatrix}1\cdot 2+2\cdot 1&1\cdot 0+2\cdot 1\\
-1\cdot 2+0\cdot 1&-1\cdot 0+0\cdot 1\end{pmatrix}
=\begin{pmatrix}4&2\\-2&0\end{pmatrix}.\\
\|AB\|_F&=\sqrt{16+4+4+0}=\sqrt{24}=2\sqrt{6}\le \sqrt{6}\cdot \sqrt{6}=6.\\
\|AB\|_F&\le \|A\|_F\|B\|_F \text{ holds (}2\sqrt{6}\le 6\text{).}
\end{align*}
}
\RESULT{
Both spectral and Frobenius norms satisfy submultiplicativity, enabling
practical bounds for matrix products.
}
\UNITCHECK{
All norms are dimensionless here. Under scaling $A\mapsto \alpha A$, both
sides scale by $|\alpha|$.
}
\PITFALLS{
\begin{bullets}
\item Confusing $\|AB\|_F\le \|A\|_F\|B\|_F$ with equality; rarely tight.
\item Using $\|B\|_F\le \|B\|_2$ is false; the reverse inequality holds.
\end{bullets}
}
\INTUITION{
Spectral: the largest stretch through two linear maps cannot exceed product of
their individual stretches. Frobenius: total energy after $AB$ is bounded by
Cauchy\textendash Schwarz in the space of matrices.
}
\CANONICAL{
\begin{bullets}
\item Unitarily invariant norms preserve submultiplicativity.
\item Frobenius bound via Hilbert space inequality.
\end{bullets}
}

\FormulaPage{4}{Spectral Radius Bound by Submultiplicative Norms}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
For any submultiplicative matrix norm $\|\cdot\|$,
$\rho(A)\le \limsup_{k\to\infty}\|A^k\|^{1/k}\le \|A\|$. In particular,
$\rho(A)\le \|A\|$.

\WHAT{
Connects eigenvalues to norm growth and provides an immediate bound on the
spectral radius in terms of any submultiplicative norm.
}
\WHY{
Used to certify stability of iterations, to select norms in analysis, and to
compare rates of growth or decay with powers of $A$.
}
\FORMULA{
\[
\rho(A)\le \limsup_{k\to\infty}\|A^k\|^{1/k}\le \|A\|.
\]
}
\CANONICAL{
Finite-dimensional matrices with any submultiplicative norm. The upper bound
uses $\|A^k\|\le \|A\|^k$. The lower bound uses eigenpairs.
}
\PRECONDS{
\begin{bullets}
\item Existence of at least one eigenpair $(\lambda,v\ne 0)$ over $\mathbb{C}$.
\item Submultiplicativity for powers.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
If $Av=\lambda v$, then $|\lambda|^k\|v\|\le \|A^k\|\|v\|$ for $k\ge 1$.
\end{lemma}
\begin{proof}
$A^k v=\lambda^k v$ so $\|A^k v\|=|\lambda|^k\|v\|\le \|A^k\|\|v\|$ by the
definition of operator norm bound. Cancel $\|v\|>0$ to get the claim.
\qedhere
\end{proof}

\DERIVATION{
\begin{align*}
\text{Upper limsup: }& \|A^k\|\le \|A\|^k \Rightarrow
\|A^k\|^{1/k}\le \|A\|,\ \limsup\le \|A\|.\\
\text{Lower bound: }& |\lambda|^k\le \|A^k\| \Rightarrow
|\lambda|\le \|A^k\|^{1/k}.\\
& \text{Take }k\to\infty \text{ and supremum over eigenvalues to get }
\rho(A)\le \limsup \|A^k\|^{1/k}.
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Choose a convenient submultiplicative norm.
\item Use $\rho(A)\le \|A\|$ to certify stability $\rho(A)<1$ by $\|A\|<1$.
\item For rates, consider $\|A^k\|^{1/k}$ as $k$ grows.
\end{bullets}
}
\EQUIV{
\begin{bullets}
\item If $\|A\|<1$ then $\rho(A)<1$ and $(I-A)$ is invertible.
\item For any $k$, $\rho(A)^k\le \|A^k\|\le \|A\|^k$.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item Bound may be loose; equality seldom holds.
\item Norm choice affects tightness; spectral norm often tighter than $\infty$-norm.
\end{bullets}
}
\INPUTS{$A=\begin{pmatrix}0&1\\-2&3\end{pmatrix},\ \|\cdot\|=\|\cdot\|_2$.}
\DERIVATION{
\begin{align*}
\text{Eigenvalues: }& \lambda=\frac{3\pm \sqrt{9-8}}{2}=\frac{3\pm 1}{2}
\Rightarrow \{1,2\}.\\
\rho(A)&=2.\\
\|A\|_2&\ge \rho(A)=2\ \text{and numerically }\|A\|_2\approx 3.3028.\\
\rho(A)&\le \|A\|_2 \text{ verified.}
\end{align*}
}
\RESULT{
Spectral radius is bounded above by any submultiplicative norm, with sandwich
$\rho(A)^k\le \|A^k\|\le \|A\|^k$.
}
\UNITCHECK{
Dimensionless. Bounds behave correctly under similarity transforms for
unitarily invariant norms; $\rho(A)$ is invariant under similarity.
}
\PITFALLS{
\begin{bullets}
\item Misinterpreting $\rho(A)\le \|A\|$ as equivalence.
\item Using non-submultiplicative function breaks the power bound.
\end{bullets}
}
\INTUITION{
An eigenvector experiences pure scaling by $|\lambda|$ each step. Since no
vector can be stretched more than the operator norm, eigenvalue magnitude is
bounded by the norm.
}
\CANONICAL{
\begin{bullets}
\item Power bounds sandwich spectral radius.
\item Stability via norm criteria.
\end{bullets}
}

\section{10 Exhaustive Problems and Solutions}

\ProblemPage{1}{Submultiplicativity for Induced $\|\cdot\|_1$ and $\|\cdot\|_\infty$}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Show $\|AB\|_1\le \|A\|_1\|B\|_1$ and $\|AB\|_\infty\le \|A\|_\infty\|B\|_\infty$.
Compute both sides for a concrete pair.

\PROBLEM{
Prove submultiplicativity for induced 1- and $\infty$-norms using column and
row sums, and verify with a specific numeric example.
}
\MODEL{
\[
\|A\|_1=\max_j \sum_i |a_{ij}|,\quad
\|A\|_\infty=\max_i \sum_j |a_{ij}|.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Finite-dimensional matrices with real entries.
\item Standard absolute value properties and finite sums.
\end{bullets}
}
\varmapStart
\var{A,B}{Real $n\times n$ matrices.}
\var{\|A\|_1}{Max column sum.}
\var{\|A\|_\infty}{Max row sum.}
\varmapEnd
\WHICHFORMULA{
Formula 1: submultiplicativity of induced norms; here we express induced 1-
and $\infty$-norms via sums to prove directly.
}
\GOVERN{
\[
(AB)_{ij}=\sum_k a_{ik}b_{kj}.
\]
}
\INPUTS{$A=\begin{pmatrix}1&-2\\3&0\end{pmatrix},\ B=\begin{pmatrix}0&4\\-1&2\end{pmatrix}.}
\DERIVATION{
\begin{align*}
\text{Step 1: }& \|AB\|_1=\max_j \sum_i |(AB)_{ij}|.\\
\text{Step 2: }& |(AB)_{ij}|=\left|\sum_k a_{ik}b_{kj}\right|
\le \sum_k |a_{ik}||b_{kj}|.\\
\text{Step 3: }& \sum_i |(AB)_{ij}|\le \sum_i \sum_k |a_{ik}||b_{kj}|
=\sum_k \left(\sum_i |a_{ik}|\right)|b_{kj}|.\\
\text{Step 4: }& \sum_i |(AB)_{ij}|\le \left(\max_k \sum_i |a_{ik}|\right)
\sum_k |b_{kj}|.\\
\text{Step 5: }& \|AB\|_1\le \|A\|_1\|B\|_1.\\
\text{Step 6: }& \|AB\|_\infty\le \|A\|_\infty\|B\|_\infty \text{ by symmetry
(working with rows).}\\
\text{Numeric: }& \|A\|_1=\max\{1+3,2+0\}=4,\ \|B\|_1=\max\{0+1,4+2\}=6.\\
& AB=\begin{pmatrix}1\cdot 0+(-2)\cdot (-1)&1\cdot 4+(-2)\cdot 2\\
3\cdot 0+0\cdot (-1)&3\cdot 4+0\cdot 2\end{pmatrix}=
\begin{pmatrix}2&0\\0&12\end{pmatrix}.\\
& \|AB\|_1=\max\{2+0,0+12\}=12\le 24.\\
& \|A\|_\infty=\max\{1+2,3+0\}=3,\ \|B\|_\infty=\max\{0+4,1+2\}=4.\\
& \|AB\|_\infty=\max\{2+0,0+12\}=12\le 12.
\end{align*}
}
\RESULT{
Both inequalities hold; numeric case verifies them and shows tightness in
$\infty$-norm example.
}
\UNITCHECK{
Dimensionless; sums of absolute values preserve homogeneity and triangle law.
}
\EDGECASES{
\begin{bullets}
\item Zero rows or columns can force equality.
\item Permutation matrices have norm $1$; multiplying by them preserves norms.
\end{bullets}
}
\ALTERNATE{
Use induced definition $\|A\|=\sup_{\|x\|=1}\|Ax\|$ with $p=1,\infty$ and the
composition argument of Formula 1.
}
\VALIDATION{
\begin{bullets}
\item Cross-check with computational norms.
\item Verify on random matrices that inequalities hold.
\end{bullets}
}
\INTUITION{
Column sums control how large each output column of $AB$ can be, bounded by
mixing columns of $A$ weighted by $B$ entries.
}
\CANONICAL{
\begin{bullets}
\item Induced norms obey submultiplicativity.
\item 1- and $\infty$-norms admit elementwise sum proofs.
\end{bullets}
}

\ProblemPage{2}{Frobenius Submultiplicativity via Cauchy\textendash Schwarz}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Prove $\|AB\|_F\le \|A\|_F\|B\|_F$ and determine when equality can occur.

\PROBLEM{
Use the Hilbert\textendash Schmidt inner product structure to apply
Cauchy\textendash Schwarz and identify equality conditions.
}
\MODEL{
\[
\langle X,Y\rangle=\mathrm{trace}(Y^\ast X),\quad \|X\|_F^2=\langle X,X\rangle.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Complex or real matrices with Frobenius inner product.
\item Cauchy\textendash Schwarz inequality holds in Hilbert spaces.
\end{bullets}
}
\varmapStart
\var{A,B}{Matrices with compatible sizes.}
\var{\langle\cdot,\cdot\rangle}{Hilbert\textendash Schmidt inner product.}
\varmapEnd
\WHICHFORMULA{
Formula 3: Frobenius norm submultiplicativity derived via
Cauchy\textendash Schwarz.
}
\GOVERN{
\[
\|AB\|_F=\sup_{\|X\|_F=1}\langle AB,X\rangle.
\]
}
\INPUTS{$A=\begin{pmatrix}1&1\\0&1\end{pmatrix},\
B=\begin{pmatrix}1&-1\\1&1\end{pmatrix}.}
\DERIVATION{
\begin{align*}
\text{Step 1: }& \|AB\|_F^2=\mathrm{trace}(B^\ast A^\ast AB).\\
\text{Step 2: }& \mathrm{trace}(B^\ast A^\ast AB)
=\langle A^\ast A, BB^\ast\rangle \le \|A^\ast A\|_F\|BB^\ast\|_F.\\
\text{Step 3: }& \|A^\ast A\|_F\le \|A^\ast\|_F\|A\|_2\le \|A\|_F^2.\\
\text{Step 4: }& \|BB^\ast\|_F\le \|B\|_F^2.\\
\text{Step 5: }& \|AB\|_F^2\le \|A\|_F^2\|B\|_F^2 \Rightarrow
\|AB\|_F\le \|A\|_F\|B\|_F.\\
\text{Numeric: }& \|A\|_F=\sqrt{1+1+0+1}=\sqrt{3},\
\|B\|_F=\sqrt{1+1+1+1}=2.\\
& AB=\begin{pmatrix}2&0\\1&1\end{pmatrix},\
\|AB\|_F=\sqrt{4+0+1+1}=\sqrt{6}\le 2\sqrt{3}.
\end{align*}
}
\RESULT{
Inequality holds. Equality requires proportionality in the Hilbert space sense
between $A^\ast A$ and $BB^\ast$, a restrictive alignment.
}
\UNITCHECK{
Homogeneous and dimensionless. Inner products well-defined.
}
\EDGECASES{
\begin{bullets}
\item If $A=0$ or $B=0$, equality holds trivially.
\item If $A$ and $B$ are scalar multiples of the same unitary, equality can hold.
\end{bullets}
}
\ALTERNATE{
Use column-wise view: $\|AB\|_F^2=\sum_i \|A b_i\|_2^2\le \|A\|_2^2\|B\|_F^2$
and then bound $\|A\|_2\le \|A\|_F$.
}
\VALIDATION{
\begin{bullets}
\item Numerically test random matrices against the bound.
\item Compare with spectral norm bound for tightness.
\end{bullets}
}
\INTUITION{
Treat matrices as vectors in a higher-dimensional space; multiplication is a
bilinear form whose size is controlled by Cauchy\textendash Schwarz.
}
\CANONICAL{
\begin{bullets}
\item Frobenius submultiplicativity via Hilbert space.
\item Relation to spectral norm bounds.
\end{bullets}
}

\ProblemPage{3}{Neumann Series Bound and Truncation Error}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Given $\|A\|<1$, bound $\|(I-A)^{-1}-\sum_{k=0}^N A^k\|$.

\PROBLEM{
Show the tail is bounded by $\frac{\|A\|^{N+1}}{1-\|A\|}$ and illustrate on a
numerical example.
}
\MODEL{
\[
(I-A)^{-1}=\sum_{k=0}^\infty A^k,\quad
\left\|\sum_{k=N+1}^\infty A^k\right\|\le \sum_{k=N+1}^\infty \|A\|^k.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Submultiplicative norm, $\|A\|<1$.
\item Triangle inequality for series of matrices.
\end{bullets}
}
\varmapStart
\var{A}{Square matrix with $\|A\|<1$.}
\var{N}{Truncation index.}
\varmapEnd
\WHICHFORMULA{
Formula 2: power bound and geometric series estimate.
}
\GOVERN{
\[
\left\|\sum_{k=N+1}^\infty A^k\right\|\le \sum_{k=N+1}^\infty \|A^k\|
\le \frac{\|A\|^{N+1}}{1-\|A\|}.
\]
}
\INPUTS{$A=\begin{pmatrix}0.2&0\\0&0.5\end{pmatrix},\ \|\cdot\|=\|\cdot\|_\infty,\ N=3$.}
\DERIVATION{
\begin{align*}
\|A\|_\infty&=0.5<1.\\
\text{Tail bound }&\le \frac{0.5^{4}}{1-0.5}=\frac{0.0625}{0.5}=0.125.\\
\text{Exact tail }&=\sum_{k=4}^\infty A^k
=\begin{pmatrix}\frac{0.2^{4}}{1-0.2}&0\\0&\frac{0.5^{4}}{1-0.5}\end{pmatrix}.\\
\|\text{tail}\|_\infty&=\max\left\{\frac{0.0016}{0.8},\frac{0.0625}{0.5}\right\}
=\max\{0.002,\ 0.125\}=0.125.
\end{align*}
}
\RESULT{
The bound is tight on the dominant diagonal entry; the tail norm equals $0.125$.
}
\UNITCHECK{
Consistent with geometric series units; dimensionless numbers.
}
\EDGECASES{
\begin{bullets}
\item If $\|A\|\to 1^-$, tail bound becomes large.
\item If $A$ is nilpotent, tail vanishes beyond rank of nilpotency.
\end{bullets}
}
\ALTERNATE{
Diagonalize or use Jordan form to evaluate exact sums when possible, then
compare with the norm bound.
}
\VALIDATION{
\begin{bullets}
\item Compute partial sums and confirm monotone convergence in norm.
\item Compare bound to observed tail norm numerically.
\end{bullets}
}
\INTUITION{
Each additional power contributes a geometrically smaller piece; truncation
error behaves like the tail of a geometric series.
}
\CANONICAL{
\begin{bullets}
\item Truncation control by submultiplicativity.
\item Explicit geometric tail bound.
\end{bullets}
}

\ProblemPage{4}{Narrative: Alice Cascades Two Linear Systems}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Alice applies $y=ABx$. Bound $\|y\|$ in terms of $\|x\|$ with a single line.

\PROBLEM{
Given $\|A\|\le 5$ and $\|B\|\le 3$ in the same induced norm, show
$\|y\|\le 15\|x\|$ and discuss tightness.
}
\MODEL{
\[
\|ABx\|\le \|AB\|\|x\|\le \|A\|\|B\|\|x\|.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Same induced norm for $\|A\|$ and $\|B\|$.
\item Submultiplicativity and operator inequality $\|Ax\|\le \|A\|\|x\|$.
\end{bullets}
}
\varmapStart
\var{A,B}{Linear maps with known operator norm bounds.}
\var{x}{Input vector.}
\var{y}{Output vector $ABx$.}
\varmapEnd
\WHICHFORMULA{
Formula 1: $\|AB\|\le \|A\|\|B\|$ and $\|Ax\|\le \|A\|\|x\|$.
}
\GOVERN{
\[
\|y\|=\|ABx\|\le \|A\|\|Bx\|\le \|A\|\|B\|\|x\|.
\]
}
\INPUTS{$\|A\|=5,\ \|B\|=3,\ \|x\|=2$.}
\DERIVATION{
\begin{align*}
\|y\|&\le \|A\|\|B\|\|x\|=5\cdot 3\cdot 2=30.
\end{align*}
}
\RESULT{
$\|y\|\le 15\|x\|$ in general; with $\|x\|=2$, $\|y\|\le 30$.
}
\UNITCHECK{
Scaling: if $x$ doubles, right-hand side doubles, consistent with homogeneity.
}
\EDGECASES{
\begin{bullets}
\item If either $A$ or $B$ is an isometry, the bound reduces to the other norm.
\item Tightness when singular vectors align with $x$ along maximizing directions.
\end{bullets}
}
\ALTERNATE{
Bound via spectral norms if using Euclidean space: $\|AB\|_2\le \|A\|_2\|B\|_2$.
}
\VALIDATION{
\begin{bullets}
\item Construct $A,B$ with aligned singular vectors to approach tightness.
\item Numerically verify with random $x$ that bound holds.
\end{bullets}
}
\INTUITION{
Two amplifiers in cascade multiply gains; the input cannot be stretched more
than the product of maximum individual stretches.
}
\CANONICAL{
\begin{bullets}
\item Composition gain bound.
\item Tightness requires alignment with top singular vectors.
\end{bullets}
}

\ProblemPage{5}{Narrative: Bob Repeats a Filtering Operation}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Bob applies $y=A^k x$. Bound $\|y\|$ and decide stability as $k\to\infty$.

\PROBLEM{
Given $\|A\|=0.8$ in an induced norm, bound $\|A^k x\|$ and show convergence
to zero for any $x$.
}
\MODEL{
\[
\|A^k x\|\le \|A^k\|\|x\|\le \|A\|^k \|x\|=(0.8)^k \|x\|.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Induced norm and submultiplicativity.
\item $\|A\|<1$.
\end{bullets}
}
\varmapStart
\var{A}{Linear filter.}
\var{k}{Number of applications.}
\var{x}{Input vector.}
\varmapEnd
\WHICHFORMULA{
Formula 2: power inequality $\|A^k\|\le \|A\|^k$.
}
\GOVERN{
\[
\|A^k x\|\le \|A\|^k \|x\|.
\]
}
\INPUTS{$\|A\|=0.8,\ \|x\|=5,\ k=10$.}
\DERIVATION{
\begin{align*}
\|A^{10} x\|&\le 0.8^{10}\cdot 5 \approx 0.1073741824\cdot 5\approx 0.5369.
\end{align*}
}
\RESULT{
$\|A^k x\|\le 0.8^k \|x\|\to 0$; with given inputs, $\le 0.5369$.
}
\UNITCHECK{
Homogeneous bound; dimensionless constants preserve scaling.
}
\EDGECASES{
\begin{bullets}
\item If $\|A\|=1$, bound does not imply decay.
\item If $\|A\|>1$, bound implies potential exponential growth.
\end{bullets}
}
\ALTERNATE{
Use spectral radius: if $\rho(A)<1$, $A^k\to 0$; norm criterion $\|A\|<1$
suffices but is not necessary.
}
\VALIDATION{
\begin{bullets}
\item Numerically apply $A$ to random $x$ and track norms.
\item Compare to theoretical decay factor $0.8^k$.
\end{bullets}
}
\INTUITION{
Repeated shrinkage by at most $0.8$ each time quickly drives the signal to
zero.
}
\CANONICAL{
\begin{bullets}
\item Geometric decay under repeated application.
\item Stability via submultiplicative norm bound.
\end{bullets}
}

\ProblemPage{6}{Expectation Puzzle with Scalar Matrices}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Let $A=\alpha I$ and $B=\beta I$ with $\alpha,\beta$ from a fair die on
$\{1,2,3,4,5,6\}$. Compute $\mathbb{E}\|AB\|_2$ and compare to
$\mathbb{E}(\|A\|_2\|B\|_2)$.

\PROBLEM{
Show equality holds and compute the value.
}
\MODEL{
\[
\|AB\|_2=|\alpha\beta|,\ \|A\|_2\|B\|_2=|\alpha||\beta|.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Spectral norm of scalar matrix equals absolute scalar.
\item Independence of $\alpha$ and $\beta$.
\end{bullets}
}
\varmapStart
\var{\alpha,\beta}{Independent uniform on $\{1,\dots,6\}$.}
\var{I}{Identity matrix.}
\varmapEnd
\WHICHFORMULA{
Formula 1: submultiplicativity is equality for scalar multiples of identity.
}
\GOVERN{
\[
\mathbb{E}|\alpha\beta|=\mathbb{E}|\alpha|\cdot \mathbb{E}|\beta|.
\]
}
\INPUTS{None beyond the distribution of $\alpha,\beta$.}
\DERIVATION{
\begin{align*}
\mathbb{E}|\alpha|&=\frac{1}{6}\sum_{k=1}^6 k=\frac{21}{6}=\frac{7}{2}.\\
\mathbb{E}|\alpha\beta|&=\mathbb{E}|\alpha|\cdot \mathbb{E}|\beta|
=\left(\frac{7}{2}\right)^2=\frac{49}{4}=12.25.\\
\mathbb{E}(\|A\|_2\|B\|_2)&=\mathbb{E}|\alpha||\beta|=12.25.
\end{align*}
}
\RESULT{
$\mathbb{E}\|AB\|_2=12.25=\mathbb{E}(\|A\|_2\|B\|_2)$.
}
\UNITCHECK{
Norm equals magnitude of scalar; units are consistent and dimensionless.
}
\EDGECASES{
\begin{bullets}
\item If scalars can be zero, expectation reduces accordingly.
\item For non-scalar matrices, only inequality is guaranteed.
\end{bullets}
}
\ALTERNATE{
Use independence to factor expectation; equality stems from multiplicativity
for scalar matrices.
}
\VALIDATION{
\begin{bullets}
\item Enumerate all $36$ outcomes to confirm the computed mean.
\end{bullets}
}
\INTUITION{
Scaling by $\alpha$ then $\beta$ is same as scaling by $\alpha\beta$.
}
\CANONICAL{
\begin{bullets}
\item Equality case of submultiplicativity.
\item Expectation factors under independence for scalars.
\end{bullets}
}

\ProblemPage{7}{Proof: Spectral Radius Bound from Submultiplicativity}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Prove $\rho(A)\le \|A\|$ for any submultiplicative matrix norm.

\PROBLEM{
Use eigenpairs and operator inequality to bound eigenvalue magnitudes.
}
\MODEL{
\[
Av=\lambda v\Rightarrow \|A^k v\|=|\lambda|^k\|v\|\le \|A^k\|\|v\|.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Finite-dimensional complex vector space ensures eigenvalues exist.
\item Submultiplicativity yields $\|A^k\|\le \|A\|^k$.
\end{bullets}
}
\varmapStart
\var{A}{Square matrix.}
\var{\lambda,v}{Eigenvalue and eigenvector.}
\varmapEnd
\WHICHFORMULA{
Formula 4: spectral radius bound.
}
\GOVERN{
\[
|\lambda|\le \|A^k\|^{1/k}\le \|A\|.
\]
}
\INPUTS{Abstract proof, no numeric inputs.}
\DERIVATION{
\begin{align*}
|\lambda|^k\|v\|&=\|A^k v\|\le \|A^k\|\|v\|\le \|A\|^k\|v\|.\\
\Rightarrow |\lambda|^k&\le \|A\|^k\Rightarrow |\lambda|\le \|A\|.
\end{align*}
}
\RESULT{
$\rho(A)=\max_i |\lambda_i|\le \|A\|$.
}
\UNITCHECK{
Homogeneous inequality; both sides scale by $|\alpha|$ under $A\mapsto \alpha A$.
}
\EDGECASES{
\begin{bullets}
\item Equality if $A$ is normal and $\|A\|$ equals $\rho(A)$ (e.g., nonnegative
diagonal matrices in $\infty$-norm).
\end{bullets}
}
\ALTERNATE{
Use Gelfand formula inequality $\rho(A)\le \limsup \|A^k\|^{1/k}$ and then
$\limsup \le \|A\|$.
}
\VALIDATION{
\begin{bullets}
\item Check numerically on random matrices that $\rho(A)\le \|A\|_2$.
\end{bullets}
}
\INTUITION{
An eigenvector cannot be stretched more than the operator norm allows, so the
eigenvalue magnitude is bounded by the norm.
}
\CANONICAL{
\begin{bullets}
\item Fundamental link between eigenvalues and norms.
\end{bullets}
}

\ProblemPage{8}{Proof: Continuity of Multiplication in Normed Algebra}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Show multiplication is jointly continuous in a normed algebra with a
submultiplicative norm.

\PROBLEM{
Prove that if $x_n\to x$ and $y_n\to y$, then $x_n y_n\to xy$.
}
\MODEL{
\[
\|x_n y_n-xy\|\le \|x_n y_n-x y_n\|+\|x y_n-x y\|.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Submultiplicativity and triangle inequality.
\item Convergent sequences in the norm topology.
\end{bullets}
}
\varmapStart
\var{x_n,y_n}{Sequences in $\mathcal{A}$.}
\var{x,y}{Limits in $\mathcal{A}$.}
\varmapEnd
\WHICHFORMULA{
Submultiplicativity controls the product of differences.
}
\GOVERN{
\[
\|x_n y_n-xy\|\le \|x_n-x\|\|y_n\|+\|x\|\|y_n-y\|.
\]
}
\INPUTS{Abstract elements; no numeric inputs.}
\DERIVATION{
\begin{align*}
\|x_n y_n-xy\|&=\|(x_n-x) y_n+x(y_n-y)\|\\
&\le \|(x_n-x) y_n\|+\|x(y_n-y)\|\\
&\le \|x_n-x\|\|y_n\|+\|x\|\|y_n-y\|.
\end{align*}
}
\RESULT{
As $x_n\to x$ and $y_n\to y$, and $\{y_n\}$ is bounded near $y$, the right
side tends to zero; thus $x_n y_n\to xy$.
}
\UNITCHECK{
All norms are dimensionless; continuity statement is topological.
}
\EDGECASES{
\begin{bullets}
\item If one sequence is constant, reduces to continuity in the other.
\item Boundedness of $\{y_n\}$ follows from convergence.
\end{bullets}
}
\ALTERNATE{
Prove separate continuity and use standard theorem in normed spaces:
bilinear maps that are bounded are jointly continuous.
}
\VALIDATION{
\begin{bullets}
\item In finite dimensions, continuity follows from polynomial dependence on
entries; the norm proof generalizes.
\end{bullets}
}
\INTUITION{
Small changes in each factor produce small changes in the product because their
effects multiply at most linearly in size.
}
\CANONICAL{
\begin{bullets}
\item Submultiplicativity implies boundedness of bilinear multiplication map.
\end{bullets}
}

\ProblemPage{9}{Combo: Matrix Exponential Bound}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Show $\|e^{tA}\|\le e^{t\|A\|}$ for $t\ge 0$.

\PROBLEM{
Use the power series of $e^{tA}$ and submultiplicativity to bound the norm.
}
\MODEL{
\[
e^{tA}=\sum_{k=0}^\infty \frac{t^k}{k!}A^k.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Submultiplicative norm and triangle inequality.
\item Convergence of the exponential series in finite dimensions.
\end{bullets}
}
\varmapStart
\var{A}{Square matrix.}
\var{t}{Nonnegative scalar time.}
\varmapEnd
\WHICHFORMULA{
Formula 2: $\|A^k\|\le \|A\|^k$, then sum termwise.
}
\GOVERN{
\[
\|e^{tA}\|\le \sum_{k=0}^\infty \frac{t^k}{k!}\|A^k\|
\le \sum_{k=0}^\infty \frac{(t\|A\|)^k}{k!}=e^{t\|A\|}.
\]
}
\INPUTS{$A=\begin{pmatrix}0&1\\-1&0\end{pmatrix},\ \|A\|_2=1,\ t=2$.}
\DERIVATION{
\begin{align*}
\|e^{2A}\|_2&\le e^{2\|A\|_2}=e^2.\\
A \text{ is skew-symmetric }&\Rightarrow e^{2A} \text{ is orthogonal, }
\|e^{2A}\|_2=1\le e^2.
\end{align*}
}
\RESULT{
General bound $\|e^{tA}\|\le e^{t\|A\|}$; example shows it can be very loose.
}
\UNITCHECK{
Exponentials of dimensionless norms yield dimensionless bounds.
}
\EDGECASES{
\begin{bullets}
\item If $A$ is normal with nonpositive spectrum, tighter bounds exist.
\item If $A$ is nilpotent, series truncates and bound simplifies.
\end{bullets}
}
\ALTERNATE{
Use submultiplicativity on the Dyson expansion or use spectral mapping when
$A$ is normal to get $\|e^{tA}\|_2=e^{t\max \Re \lambda(A)}$.
}
\VALIDATION{
\begin{bullets}
\item Numerically compute $\|e^{tA}\|$ and compare to $e^{t\|A\|}$.
\end{bullets}
}
\INTUITION{
Each power term grows at most like $\|A\|^k$, so the whole series is bounded
by the scalar exponential with base $\|A\|$.
}
\CANONICAL{
\begin{bullets}
\item Exponential bound via geometric control of powers.
\end{bullets}
}

\ProblemPage{10}{Combo: Convolution Algebra on $\ell^1$}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Show $\|a*b\|_1\le \|a\|_1\|b\|_1$ for sequences in $\ell^1$.

\PROBLEM{
Prove submultiplicativity of the $\ell^1$ norm with respect to convolution.
}
\MODEL{
\[
(a*b)_n=\sum_{k\in \mathbb{Z}} a_k b_{n-k},\quad
\|a\|_1=\sum_{k\in\mathbb{Z}} |a_k|.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Absolutely summable sequences ensure absolute convergence and Fubini
rearrangement.
\end{bullets}
}
\varmapStart
\var{a,b}{Sequences in $\ell^1(\mathbb{Z})$.}
\var{*}{Discrete convolution.}
\varmapEnd
\WHICHFORMULA{
Submultiplicativity in a commutative Banach algebra $(\ell^1,*)$.
}
\GOVERN{
\[
\|a*b\|_1=\sum_n |(a*b)_n|\le \sum_n \sum_k |a_k||b_{n-k}|.
\]
}
\INPUTS{$a=(1,1,0,0,\dots),\ b=(1,-1,0,0,\dots)$.}
\DERIVATION{
\begin{align*}
\|a*b\|_1&\le \sum_n \sum_k |a_k||b_{n-k}|
=\left(\sum_k |a_k|\right)\left(\sum_j |b_j|\right)=\|a\|_1\|b\|_1.\\
\text{Numeric: }& a*b=(1,0,-1,0,\dots),\ \|a*b\|_1=2.\\
& \|a\|_1=2,\ \|b\|_1=2,\ \|a*b\|_1=2\le 4.
\end{align*}
}
\RESULT{
Convolution algebra $(\ell^1,*)$ is a Banach algebra with submultiplicative
$\ell^1$ norm.
}
\UNITCHECK{
Absolute sums are dimensionless and finite; inequality respects scaling.
}
\EDGECASES{
\begin{bullets}
\item If one sequence is a Dirac delta, equality holds.
\item Non-absolutely summable sequences can break the bound due to divergence.
\end{bullets}
}
\ALTERNATE{
Use Young\textquotesingle s inequality $\|a*b\|_r\le \|a\|_p\|b\|_q$ with
$p=q=r=1$.
}
\VALIDATION{
\begin{bullets}
\item Numerically approximate sums for truncated sequences.
\end{bullets}
}
\INTUITION{
Each output coefficient is a sum of products; summing absolute values factorizes
into the product of total masses.
}
\CANONICAL{
\begin{bullets}
\item Submultiplicativity characterizes $\ell^1$ as a Banach algebra.
\end{bullets}
}

\section{Coding Demonstrations}

\CodeDemoPage{Verifying $\|AB\|\le \|A\|\|B\|$ for Multiple Norms}
\PROBLEM{
Compute $\|AB\|$ and compare to $\|A\|\|B\|$ for spectral, Frobenius, 1- and
$\infty$-norms on deterministic random matrices, asserting validity.
}
\API{
\begin{bullets}
\item \inlinecode{def read_input(s) -> int} — parse dimension.
\item \inlinecode{def solve_case(n) -> dict} — compute norms and checks.
\item \inlinecode{def validate() -> None} — run asserts on fixed seed.
\item \inlinecode{def main() -> None} — orchestrate.
\end{bullets}
}
\INPUTS{
Single integer $n$ specifying matrix dimension; uses fixed random seed.
}
\OUTPUTS{
Dictionary with keys: \inlinecode{"ok_spectral"}, \inlinecode{"ok_fro"},
\inlinecode{"ok_1"}, \inlinecode{"ok_inf"} booleans, and the corresponding
values for margins.
}
\FORMULA{
\[
\|AB\|_\star\le \|A\|_\star \|B\|_\star,\ \star\in\{2,F,1,\infty\}.
\]
}
\textbf{SOLUTION A — From Scratch (Mathematically Explicit Implementation)}
\begin{codepy}
import numpy as np

def read_input(s):
    return int(s.strip())

def norm1(A):
    return np.max(np.sum(np.abs(A), axis=0))

def norminf(A):
    return np.max(np.sum(np.abs(A), axis=1))

def normF(A):
    return float(np.sqrt(np.sum(A*A)))

def norm2(A):
    # power method for spectral norm
    v = np.ones(A.shape[1])
    for _ in range(60):
        u = A @ v
        if np.allclose(u, 0):
            return 0.0
        u = u / np.linalg.norm(u)
        v = A.T @ u
        if np.allclose(v, 0):
            return 0.0
        v = v / np.linalg.norm(v)
    s = np.linalg.norm(A @ v)
    return float(s)

def solve_case(n):
    np.random.seed(0)
    A = np.random.randn(n, n)
    B = np.random.randn(n, n)
    AB = A @ B
    vals = {}
    for name, fn in [("2", norm2), ("F", normF),
                     ("1", norm1), ("inf", norminf)]:
        lhs = fn(AB); rhs = fn(A)*fn(B)
        vals["ok_"+name.replace("2","spectral")
             .replace("F","fro").replace("1","1")
             .replace("inf","inf")] = bool(lhs <= rhs + 1e-9)
        vals["margin_"+name] = float(rhs - lhs)
    return vals

def validate():
    vals = solve_case(5)
    assert vals["ok_spectral"]
    assert vals["ok_fro"]
    assert vals["ok_1"]
    assert vals["ok_inf"]

def main():
    validate()
    out = solve_case(5)
    for k in sorted(out):
        print(k, out[k])

if __name__ == "__main__":
    main()
\end{codepy}
\textbf{SOLUTION B — Library-Based (Validated Computational Shortcut)}
\begin{codepy}
import numpy as np

def read_input(s):
    return int(s.strip())

def solve_case(n):
    np.random.seed(0)
    A = np.random.randn(n, n)
    B = np.random.randn(n, n)
    AB = A @ B
    n2 = lambda M: float(np.linalg.norm(M, 2))
    nF = lambda M: float(np.linalg.norm(M, "fro"))
    n1 = lambda M: float(np.linalg.norm(M, 1))
    ni = lambda M: float(np.linalg.norm(M, np.inf))
    vals = {}
    for name, fn in [("2", n2), ("F", nF), ("1", n1), ("inf", ni)]:
        lhs = fn(AB); rhs = fn(A)*fn(B)
        key = {"2": "ok_spectral", "F": "ok_fro",
               "1": "ok_1", "inf": "ok_inf"}[name]
        vals[key] = bool(lhs <= rhs + 1e-9)
        vals["margin_"+name] = float(rhs - lhs)
    return vals

def validate():
    vals = solve_case(6)
    assert all(vals[k] for k in vals if k.startswith("ok_"))

def main():
    validate()
    out = solve_case(6)
    for k in sorted(out):
        print(k, out[k])

if __name__ == "__main__":
    main()
\end{codepy}
\COMPLEXITY{
Time $\mathcal{O}(n^3)$ dominated by matrix multiply and spectral norm;
space $\mathcal{O}(n^2)$ for matrices.
}
\FAILMODES{
\begin{bullets}
\item Power method may fail if starting vector maps to zero; handled by checks.
\item Numerical noise; mitigate by tolerance $1e-9$.
\end{bullets}
}
\STABILITY{
\begin{bullets}
\item Spectral norm via power method converges linearly; fixed iterations give
approximate upper bound prone to slight underestimation; we assert with margin.
\item Frobenius, 1-, $\infty$-norms are stable to compute.
\end{bullets}
}
\VALIDATION{
\begin{bullets}
\item Cross-validate from-scratch vs. numpy implementations.
\item Assertions require all four inequalities to pass.
\end{bullets}
}
\RESULT{
For seeded random inputs, all checked norms satisfy $\|AB\|\le \|A\|\|B\|$ with
positive margins.
}
\EXPLANATION{
Each computed norm corresponds to a submultiplicative norm described in the
Formula Canon; the code evaluates both sides and verifies the inequality.
}

\CodeDemoPage{Neumann Series Bound and Inverse Norm Estimate}
\PROBLEM{
Given a matrix $A$ with $\|A\|<1$, compute $S_N=\sum_{k=0}^N A^k$, verify
$\|S_N\|\le \frac{1-\|A\|^{N+1}}{1-\|A\|}$, and assert
$\|(I-A)^{-1}\|\le \frac{1}{1-\|A\|}$.
}
\API{
\begin{bullets}
\item \inlinecode{def read_input(s) -> tuple} — parse $n$ and $N$.
\item \inlinecode{def solve_case(n,N) -> dict} — compute series and bounds.
\item \inlinecode{def validate() -> None} — fixed seed assertions.
\item \inlinecode{def main() -> None} — run validation and print.
\end{bullets}
}
\INPUTS{
Integers $n$ (dimension) and $N$ (truncation), with fixed random seed and
$\|A\|_\infty<1$ enforced by scaling.
}
\OUTPUTS{
Dictionary with computed norms of $A$, $S_N$, inverse, and bound margins.
}
\FORMULA{
\[
S_N=\sum_{k=0}^N A^k,\quad
\|S_N\|\le \sum_{k=0}^N \|A\|^k=\frac{1-\|A\|^{N+1}}{1-\|A\|},\quad
\|(I-A)^{-1}\|\le \frac{1}{1-\|A\|}.
\]
}
\textbf{SOLUTION A — From Scratch (Mathematically Explicit Implementation)}
\begin{codepy}
import numpy as np

def read_input(s):
    t = s.strip().split()
    return int(t[0]), int(t[1])

def norminf(A):
    return float(np.max(np.sum(np.abs(A), axis=1)))

def solve_case(n, N):
    np.random.seed(1)
    A = np.random.randn(n, n)
    A = A / (2.0 + norminf(A))  # ensure norm < 1
    an = norminf(A)
    S = np.eye(n)
    P = np.eye(n)
    for _ in range(N):
        P = P @ A
        S = S + P
    sN = norminf(S)
    boundN = (1.0 - an**(N+1)) / (1.0 - an)
    inv_est = np.linalg.inv(np.eye(n) - A)
    invn = norminf(inv_est)
    boundInf = 1.0 / (1.0 - an)
    return {"an": an, "sN": sN, "boundN": boundN,
            "invn": invn, "boundInf": boundInf}

def validate():
    vals = solve_case(4, 6)
    assert vals["sN"] <= vals["boundN"] + 1e-9
    assert vals["invn"] <= vals["boundInf"] + 1e-9

def main():
    validate()
    out = solve_case(4, 6)
    for k in sorted(out):
        print(k, round(out[k], 6))

if __name__ == "__main__":
    main()
\end{codepy}
\textbf{SOLUTION B — Library-Based (Validated Computational Shortcut)}
\begin{codepy}
import numpy as np

def read_input(s):
    t = s.strip().split()
    return int(t[0]), int(t[1])

def solve_case(n, N):
    np.random.seed(1)
    A = np.random.randn(n, n)
    an = np.linalg.norm(A, np.inf)
    A = A / (2.0 + an)
    an = np.linalg.norm(A, np.inf)
    S = np.zeros((n, n))
    for k in range(N+1):
        S = S + np.linalg.matrix_power(A, k)
    sN = float(np.linalg.norm(S, np.inf))
    boundN = (1.0 - an**(N+1)) / (1.0 - an)
    inv_est = np.linalg.inv(np.eye(n) - A)
    invn = float(np.linalg.norm(inv_est, np.inf))
    boundInf = 1.0 / (1.0 - an)
    return {"an": an, "sN": sN, "boundN": boundN,
            "invn": invn, "boundInf": boundInf}

def validate():
    vals = solve_case(5, 5)
    assert vals["sN"] <= vals["boundN"] + 1e-9
    assert vals["invn"] <= vals["boundInf"] + 1e-9

def main():
    validate()
    out = solve_case(5, 5)
    for k in sorted(out):
        print(k, round(out[k], 6))

if __name__ == "__main__":
    main()
\end{codepy}
\COMPLEXITY{
Time $\mathcal{O}(N n^3)$ for repeated powers or $\mathcal{O}(n^3\log N)$
with fast exponentiation; space $\mathcal{O}(n^2)$.
}
\FAILMODES{
\begin{bullets}
\item If scaling fails, $\|A\|$ may exceed $1$; code enforces scaling.
\item Matrix inversion may be ill-conditioned near $\|A\|=1$.
\end{bullets}
}
\STABILITY{
\begin{bullets}
\item Summation may suffer rounding; ordering by increasing $k$ is acceptable
since terms decay.
\item Inversion near boundary increases condition number and norm.
\end{bullets}
}
\VALIDATION{
\begin{bullets}
\item Assert partial sum norm below geometric bound.
\item Assert inverse norm below $\frac{1}{1-\|A\|}$.
\end{bullets}
}
\RESULT{
All assertions pass with margins, illustrating bounds from Formula 2.
}
\EXPLANATION{
Submultiplicativity bounds $\|A^k\|$ by $\|A\|^k$, making the matrix series
no larger than a scalar geometric series, which the code verifies.
}
\EXTENSION{
Vectorize the summation with geometric series for diagonalizable $A$ to
compare tightness across norms.
}

\section{Applied Domains — Detailed End-to-End Scenarios}

\DomainPage{Machine Learning}
\SCENARIO{
Bound the Lipschitz constant of a linear model $f(x)=Wx$ by the spectral norm
$\|W\|_2$, and verify that cascading two layers multiplies the bound.
}
\ASSUMPTIONS{
\begin{bullets}
\item Euclidean norm on inputs and outputs.
\item Deterministic synthetic data and fixed random seed.
\end{bullets}
}
\WHICHFORMULA{
$\|W_2 W_1\|_2\le \|W_2\|_2\|W_1\|_2$ gives Lipschitz bound of a 2-layer
linear network.
}
\varmapStart
\var{W_1,W_2}{Weight matrices.}
\var{x}{Input vector.}
\var{L}{Lipschitz constant bound (spectral norm).}
\varmapEnd
\PIPELINE{
\begin{bullets}
\item Generate deterministic $W_1,W_2$.
\item Compute $\|W_i\|_2$ and $\|W_2 W_1\|_2$.
\item Verify submultiplicativity and estimate Lipschitz bound.
\end{bullets}
}
\textbf{Implementation (From Scratch)}
\begin{codepy}
import numpy as np

def spectral_norm(A, iters=80):
    v = np.ones(A.shape[1])
    for _ in range(iters):
        u = A @ v
        if np.allclose(u, 0):
            return 0.0
        u = u / np.linalg.norm(u)
        v = A.T @ u
        if np.allclose(v, 0):
            return 0.0
        v = v / np.linalg.norm(v)
    return float(np.linalg.norm(A @ v))

def main():
    np.random.seed(2)
    W1 = np.random.randn(8, 5)
    W2 = np.random.randn(3, 8)
    L1 = spectral_norm(W1)
    L2 = spectral_norm(W2)
    L = spectral_norm(W2 @ W1)
    print("L:", round(L, 6), "L1*L2:", round(L1*L2, 6))
    assert L <= L1*L2 + 1e-8

if __name__ == "__main__":
    main()
\end{codepy}
\textbf{Implementation (Library Version)}
\begin{codepy}
import numpy as np

def main():
    np.random.seed(2)
    W1 = np.random.randn(8, 5)
    W2 = np.random.randn(3, 8)
    L1 = float(np.linalg.norm(W1, 2))
    L2 = float(np.linalg.norm(W2, 2))
    L = float(np.linalg.norm(W2 @ W1, 2))
    print("L:", round(L, 6), "L1*L2:", round(L1*L2, 6))
    assert L <= L1*L2 + 1e-8

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{Compare $\|W_2 W_1\|_2$ against $\|W_2\|_2\|W_1\|_2$.}
\INTERPRET{
Lipschitz constant of cascaded linear layers is at most product of layer norms.
}
\NEXTSTEPS{
For nonlinear networks, multiply by Lipschitz constants of activations.
}

\DomainPage{Quantitative Finance}
\SCENARIO{
Factor model $r=W f+\varepsilon$ with exposures $W$ and factors $f$.
Bound portfolio exposure transformation: $\|W_2 W_1\|_2\le \|W_2\|_2\|W_1\|_2$.
}
\ASSUMPTIONS{
\begin{bullets}
\item Euclidean risk measure for linear transformations.
\item Deterministic synthetic matrices and fixed seed.
\end{bullets}
}
\WHICHFORMULA{
Submultiplicativity of spectral norm bounds the amplification of factor shocks
through chained transformations (e.g., mapping factors to sectors to assets).
}
\varmapStart
\var{W_1}{Factor-to-sector mapping.}
\var{W_2}{Sector-to-asset mapping.}
\var{L}{Operator gain bound.}
\varmapEnd
\PIPELINE{
\begin{bullets}
\item Simulate $W_1,W_2$.
\item Compute $\|W_2 W_1\|_2$ and compare to product bound.
\item Interpret as worst-case risk amplification.
\end{bullets}
}
\textbf{Implementation (Full Pipeline)}
\begin{codepy}
import numpy as np

def main():
    np.random.seed(3)
    W1 = np.random.randn(6, 3)  # sectors x factors
    W2 = np.random.randn(10, 6) # assets x sectors
    L1 = float(np.linalg.norm(W1, 2))
    L2 = float(np.linalg.norm(W2, 2))
    L = float(np.linalg.norm(W2 @ W1, 2))
    print("L:", round(L, 6), "Bound:", round(L1*L2, 6))
    assert L <= L1*L2 + 1e-9

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{Operator norm and multiplicative bound.}
\INTERPRET{
Worst-case portfolio sensitivity to factor shocks is bounded by product of
mapping gains.
}
\NEXTSTEPS{
Incorporate factor covariance and use $\|W\Sigma_f^{1/2}\|_2$ for risk bounds.
}

\DomainPage{Deep Learning}
\SCENARIO{
Bound the Lipschitz constant of a $3$-layer linear network by the product of
spectral norms and verify numerically.
}
\ASSUMPTIONS{
\begin{bullets}
\item Use Euclidean norms and fixed random seed.
\item Ignore activations to isolate linear part.
\end{bullets}
}
\WHICHFORMULA{
$\|W_3 W_2 W_1\|_2\le \|W_3\|_2\|W_2\|_2\|W_1\|_2$.
}
\PIPELINE{
\begin{bullets}
\item Generate $W_1,W_2,W_3$.
\item Compute spectral norms and the bound.
\item Compare composite norm with bound.
\end{bullets}
}
\textbf{Implementation (End-to-End)}
\begin{codepy}
import numpy as np

def main():
    np.random.seed(4)
    W1 = np.random.randn(16, 12)
    W2 = np.random.randn(12, 10)
    W3 = np.random.randn(8, 12)
    n2 = lambda M: float(np.linalg.norm(M, 2))
    L1, L2, L3 = n2(W1), n2(W2), n2(W3)
    L = n2(W3 @ W2 @ W1)
    print("L:", round(L, 6), "Bound:", round(L1*L2*L3, 6))
    assert L <= L1*L2*L3 + 1e-8

if __name__ == "__main__":
    main()
\end{codepy}
\textbf{Analytical OLS Comparison}
\begin{codepy}
# Not applicable here; focus is purely on operator bounds.
def main():
    print("Use spectral norms to bound linear network Lipschitz constants.")

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{Composite spectral norm versus product of layer spectral norms.}
\INTERPRET{
Lipschitz constant of the linear stack is bounded by product of layer norms.
}
\NEXTSTEPS{
Apply spectral norm regularization to control Lipschitz constant during
training.
}

\DomainPage{Kaggle / Data Analytics}
\SCENARIO{
Standardize features with matrix $S$ and apply linear transform $W$.
Bound overall amplification $\|WS\|_2$ by $\|W\|_2\|S\|_2$.
}
\ASSUMPTIONS{
\begin{bullets}
\item Standardization is linear scaling per feature.
\item Euclidean metric for amplification.
\end{bullets}
}
\WHICHFORMULA{
Submultiplicativity of spectral norm bounds composite preprocessing and model
transformations.
}
\PIPELINE{
\begin{bullets}
\item Build diagonal scaler $S$ from standard deviations.
\item Generate random $W$ and compute norms.
\item Verify $\|WS\|_2\le \|W\|_2\|S\|_2$.
\end{bullets}
}
\textbf{Implementation (Complete EDA Pipeline)}
\begin{codepy}
import numpy as np

def main():
    np.random.seed(5)
    std = 0.5 + np.arange(1, 6) * 0.1
    S = np.diag(1.0 / std)
    W = np.random.randn(3, 5)
    L = float(np.linalg.norm(W @ S, 2))
    LW = float(np.linalg.norm(W, 2))
    LS = float(np.linalg.norm(S, 2))
    print("L:", round(L, 6), "Bound:", round(LW*LS, 6))
    assert L <= LW*LS + 1e-9

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{Spectral norms of $W$, $S$, and $WS$.}
\INTERPRET{
Preprocessing and modeling steps compose; worst-case amplification is bounded
by the product of their spectral norms.
}
\NEXTSTEPS{
Extend to pipelines with PCA or feature selection; track product of norms to
control numerical stability and sensitivity.
}

\end{document}
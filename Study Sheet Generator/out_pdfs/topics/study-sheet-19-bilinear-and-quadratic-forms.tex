% !TeX program = xelatex
\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{microtype,setspace,amsmath,amssymb,mathtools,amsthm,unicode-math}
\setstretch{1.05}
\setmainfont{Latin Modern Roman}
\setmonofont{Latin Modern Mono}
\setmathfont{Latin Modern Math}
\allowdisplaybreaks[4]
\setlength{\jot}{7pt}
\setlength{\emergencystretch}{8em}
\sloppy
\usepackage{xcolor,fancyhdr,enumitem,inconsolata,listings}
\pagestyle{fancy}\fancyhf{}\lhead{\nouppercase{\leftmark}}\rhead{\thepage}
\setlength{\headheight}{26pt}
\setlength{\parindent}{0pt}
\setlength{\parskip}{8pt plus 2pt minus 1pt}
\raggedbottom
\newenvironment{BreakableEquation}{\begin{equation}\begin{aligned}}{\end{aligned}\end{equation}}
\newenvironment{BreakableEquation*}{\begin{equation*}\begin{aligned}}{\end{aligned}\end{equation*}}
\newenvironment{tightalign}{\begingroup\small\allowdisplaybreaks\begin{align}}{\end{align}\endgroup}
\providecommand{\enumlistm}{enumitem}
\newenvironment{minted}[2][]{%
  \lstset{style=code,language=#2,#1}\begin{lstlisting}%
}{\end{lstlisting}}
\newcommand{\inputminted}[3][]{\begin{lstlisting}\end{lstlisting}}
\newlist{bullets}{itemize}{1}
\setlist[bullets]{label=--,leftmargin=1.6em,itemsep=2pt,topsep=4pt}
\newcommand{\varmapStart}{\textbf{VARIABLE MAPPING:}\par\begin{bullets}}
\newcommand{\var}[2]{\item $#1$ — #2}
\newcommand{\varmapEnd}{\end{bullets}}
\newcommand{\glossx}[6]{%
  \textbf{#1}\par
  \begin{bullets}
    \item \textbf{What:} #2
    \item \textbf{Why:} #3
    \item \textbf{How:} #4
    \item \textbf{ELI5:} #5
    \item \textbf{Pitfall/Example:} #6
  \end{bullets}
}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{claim}[theorem]{Claim}
\lstdefinestyle{code}{
  basicstyle=\ttfamily\footnotesize,
  backgroundcolor=\color{black!02},
  frame=single,
  numbers=left, numberstyle=\tiny, numbersep=8pt,
  breaklines=true, breakatwhitespace=true,
  tabsize=4, showstringspaces=false,
  upquote=true, keepspaces=true, columns=fullflexible,
  literate=
    {–}{{-}}1
    {—}{{-}}1
    {…}{{...}}1
    {≤}{{\ensuremath{\le}}}1
    {≥}{{\ensuremath{\ge}}}1
    {≠}{{\ensuremath{\ne}}}1
    {≈}{{\ensuremath{\approx}}}1
    {±}{{\ensuremath{\pm}}}1
    {→}{{\ensuremath{\to}}}1
    {←}{{\ensuremath{\leftarrow}}}1
    {∞}{{\ensuremath{\infty}}}1
    {√}{{\ensuremath{\sqrt{\ }}}}1
    {×}{{\ensuremath{\times}}}1
    {÷}{{\ensuremath{\div}}}1
}
\lstnewenvironment{codepy}[1][]%
  {\lstset{style=code,language=Python,#1}}%
  {}
\newcommand{\inlinecode}[1]{\lstinline[style=code]!#1!}
\newcommand{\LF}[2]{\par\noindent\textbf{#1:}~#2\par}
\newcommand{\WHAT}[1]{\LF{WHAT}{#1}}
\newcommand{\WHY}[1]{\LF{WHY}{#1}}
\newcommand{\HOW}[1]{\LF{HOW}{#1}}
\newcommand{\ELI}[1]{\LF{ELI5}{#1}}
\newcommand{\SCOPE}[1]{\LF{SCOPE}{#1}}
\newcommand{\CONFUSIONS}[1]{\LF{COMMON CONFUSIONS}{#1}}
\newcommand{\APPLICATIONS}[1]{\LF{APPLICATIONS}{#1}}
\newcommand{\FORMULA}[1]{\LF{FORMULA}{#1}}
\newcommand{\CANONICAL}[1]{\LF{CANONICAL FORM}{#1}}
\newcommand{\PRECONDS}[1]{\LF{PRECONDITIONS}{#1}}
\newcommand{\DERIVATION}[1]{\LF{DERIVATION}{#1}}
\newcommand{\EQUIV}[1]{\LF{EQUIVALENT FORMS}{#1}}
\newcommand{\LIMITS}[1]{\LF{LIMIT CASES}{#1}}
\newcommand{\INPUTS}[1]{\LF{INPUTS}{#1}}
\newcommand{\OUTPUTS}[1]{\LF{OUTPUTS}{#1}}
\newcommand{\RESULT}[1]{\LF{RESULT}{#1}}
\newcommand{\INTUITION}[1]{\LF{INTUITION}{#1}}
\newcommand{\PITFALLS}[1]{\LF{PITFALLS}{#1}}
\newcommand{\MODEL}[1]{\LF{CANONICAL MATH MODEL}{#1}}
\newcommand{\ASSUMPTIONS}[1]{\LF{ASSUMPTIONS}{#1}}
\newcommand{\WHICHFORMULA}[1]{\LF{WHICH FORMULA \& WHY}{#1}}
\newcommand{\GOVERN}[1]{\LF{GOVERNING EQUATION(S)}{#1}}
\newcommand{\UNITCHECK}[1]{\LF{UNIT CHECK}{#1}}
\newcommand{\EDGECASES}[1]{\LF{EDGE CASES}{#1}}
\newcommand{\ALTERNATE}[1]{\LF{ALTERNATE APPROACH (sketch)}{#1}}
\newcommand{\PROBLEM}[1]{\LF{PROBLEM}{#1}}
\newcommand{\API}[1]{\LF{API}{#1}}
\newcommand{\COMPLEXITY}[1]{\LF{COMPLEXITY}{#1}}
\newcommand{\FAILMODES}[1]{\LF{FAILURE MODES}{#1}}
\newcommand{\STABILITY}[1]{\LF{NUMERICAL STABILITY}{#1}}
\newcommand{\VALIDATION}[1]{\LF{VALIDATION}{#1}}
\newcommand{\EXPLANATION}[1]{\LF{EXPLANATION}{#1}}
\newcommand{\SCENARIO}[1]{\LF{SCENARIO}{#1}}
\newcommand{\PIPELINE}[1]{\LF{PIPELINE STEPS}{#1}}
\newcommand{\METRICS}[1]{\LF{METRICS}{#1}}
\newcommand{\INTERPRET}[1]{\LF{INTERPRETATION}{#1}}
\newcommand{\NEXTSTEPS}[1]{\LF{LIMITATIONS \& NEXT STEPS}{#1}}
\usepackage{titlesec}
\titleformat{\section}{\Large\bfseries}{\thesection}{0.6em}{}
\titlespacing*{\section}{0pt}{*2}{*1}
\usepackage{etoolbox}
\pretocmd{\section}{\clearpage}{}{}
\newcommand{\FormulaPage}[2]{%
  \clearpage
  \section*{Formula #1 — #2}%
  \addcontentsline{toc}{section}{Formula #1 — #2}%
}
\newcommand{\ProblemPage}[2]{%
  \clearpage
  \subsection*{Problem #1: #2}%
  \addcontentsline{toc}{subsection}{Problem #1: #2}%
}
\newcommand{\CodeDemoPage}[1]{%
  \clearpage
  \subsection*{Coding Demo: #1}%
  \addcontentsline{toc}{subsection}{Coding Demo: #1}%
}
\newcommand{\DomainPage}[1]{%
  \clearpage
  \subsection*{#1 (End-to-End)}%
  \addcontentsline{toc}{subsection}{#1 (End-to-End)}%
}
\begin{document}
\title{Comprehensive Study Sheet — Bilinear and Quadratic Forms}
\date{\today}
\maketitle
\tableofcontents
\clearpage
\section{Concept Overview}
\WHAT{
A bilinear form on a vector space $V$ over a field $\mathbb{F}$ is a map
$b:V\times V\to\mathbb{F}$ linear in each argument. A quadratic form is a map
$Q:V\to\mathbb{F}$ satisfying $Q(\alpha x)=\alpha^2 Q(x)$ and having an
associated symmetric bilinear form $b$ with $Q(x)=b(x,x)$. Relative to a basis,
there is a matrix $A$ with $b(x,y)=x^\top A y$ and $Q(x)=x^\top A x$. Over
$\mathbb{R}$, symmetry of $b$ corresponds to $A^\top=A$.
}
\WHY{
Bilinear and quadratic forms encode geometry and energy: inner products,
variances, distances, curvature, and optimization objectives. They underpin
diagonalization, spectral theory, positive definiteness, and classification of
conics and quadrics. Many algorithms reduce to minimizing or analyzing quadratic
forms.
}
\HOW{
1. Fix a basis of $V$. Define the coordinate matrix $A$ of $b$ by
$A_{ij}=b(e_i,e_j)$. This yields $b(x,y)=x^\top A y$ and $Q(x)=x^\top A x$.
2. For symmetric $b$ and $\mathrm{char}(\mathbb{F})\ne 2$, derive $b$ from
$Q$ via the polarization identity.
3. Study congruence $A\mapsto P^\top A P$ to understand basis changes and
canonical forms.
4. Use spectral theorems and Sylvester\'s criteria to classify definiteness
and inertia, and optimize Rayleigh quotients.
}
\ELI{
A bilinear form is a machine that takes two vectors and outputs a number,
linearly in each input. A quadratic form feeds the same vector twice into that
machine. Think of a flexible ruler that can measure lengths and angles depending
on how it is calibrated by a matrix $A$.
}
\SCOPE{
Fields with characteristic not equal to $2$ allow clean polarization
relations. Over $\mathbb{R}$, symmetry and definiteness have geometric meaning.
Non-symmetric bilinear forms exist but do not define quadratic forms directly.
Degenerate forms have kernels; positive definiteness excludes nonzero vectors
with zero value. Over $\mathbb{C}$, Hermitian (sesquilinear) forms are the
right generalization, not covered here.
}
\CONFUSIONS{
Bilinear vs. sesquilinear (complex conjugation); similarity
$P^{-1}AP$ vs. congruence $P^\top A P$; positive semidefinite vs. positive
definite; eigenvalues invariant under similarity, inertia invariant under
congruence. Rayleigh quotient uses symmetric $A$; for non-symmetric $A$,
$x^\top A x$ need not be real.
}
\APPLICATIONS{
List 3--4 major domains where this topic directly applies:
\begin{bullets}
\item Mathematical foundations (pure or applied): geometry of inner products,
conics, inertia.
\item Computational modeling or simulation: energy minimization, FEM stiffness.
\item Physical or engineering interpretations: elastic energies, quadratic
potentials.
\item Statistical or algorithmic implications: covariance, Mahalanobis distance,
least squares.
\end{bullets}
}
\textbf{ANALYTIC STRUCTURE.}
Quadratic forms are homogeneous of degree $2$ and convex iff their symmetric
matrix is positive semidefinite. Symmetric bilinear forms define inner products
when positive definite. Inertia $(p,q,r)$ decomposes space into orthogonal
positive, negative, and null subspaces.
\textbf{CANONICAL LINKS.}
Matrix representation connects all computations; polarization links $Q$ and $b$;
congruence drives canonical reduction; Sylvester\'s criterion and law of inertia
classify definiteness; Rayleigh quotient ties to extremal eigenvalues.
\textbf{PROBLEM-TYPE RECOGNITION HEURISTICS.}
\begin{bullets}
\item Expressions like $x^\top A x$ or $x^\top A y$.
\item Words: positive definite, inertia, Mahalanobis, variance, energy.
\item Presence of symmetric matrices, leading principal minors, Cholesky.
\item Optimization over unit vectors suggests Rayleigh quotients.
\end{bullets}
\textbf{SOLUTION STRATEGY BLUEPRINT.}
\begin{bullets}
\item Translate into $b(x,y)=x^\top A y$ and $Q(x)=x^\top A x$.
\item Identify symmetry and definiteness by criteria or eigenvalues.
\item Use congruence to diagonalize and expose inertia.
\item Apply polarization or Rayleigh quotient as appropriate.
\item Validate with dimensionless homogeneity and limit checks.
\end{bullets}
\textbf{CONCEPTUAL INVARIANTS.}
Inertia under congruence, signature of $Q$, Rayleigh quotient bounds within the
eigenvalue interval, homogeneity of degree $2$.
\textbf{EDGE INTUITION.}
As $\|x\|\to 0$, $Q(x)$ shrinks quadratically; as scaling $\alpha\to\infty$,
$Q(\alpha x)=\alpha^2 Q(x)$. Near degeneracy, small eigenvalues make $Q$ nearly
flat along corresponding directions.
\clearpage
\section{Glossary}
\glossx{Bilinear Form}{
A map $b:V\times V\to\mathbb{F}$ linear in each argument separately.
}{
Encodes pairwise interactions, enabling definitions of orthogonality and
energies when symmetric.
}{
Fix a basis, set $A_{ij}=b(e_i,e_j)$, then compute $b(x,y)=x^\top A y$.
}{
A rule that combines two vectors and outputs a number, scaling nicely when you
scale either input.
}{
Confusing similarity with congruence leads to wrong transformation laws.
}
\glossx{Quadratic Form}{
A homogeneous degree-$2$ map $Q:V\to\mathbb{F}$ representable as $x^\top A x$
with $A=A^\top$ over $\mathbb{R}$.
}{
Measures squared lengths or energies; central in optimization and geometry.
}{
Find symmetric $A$ so that $Q(x)=x^\top A x$. Recover $b$ via polarization.
}{
Like computing the area of a square from its side, but in many dimensions with
a possibly skewed ruler.
}{
Using non-symmetric $A$ changes $b$ but not $Q$ because $x^\top A x$ ignores
the skew-symmetric part.
}
\glossx{Congruence Transformation}{
Matrix map $A\mapsto P^\top A P$ induced by change of basis in $V$.
}{
Relates coordinate matrices of the same bilinear form in different bases and
preserves inertia.
}{
Given basis change matrix $P$, compute $A'=P^\top A P$; then $Q(x)=x^\top A x=
\hat{x}^\top A' \hat{x}$ with $x=P\hat{x}$.
}{
Changing the measuring grid deforms coordinates but not the underlying shape
measured by the quadratic form.
}{
Mistaking similarity $P^{-1}AP$ for congruence breaks invariants like inertia.
}
\glossx{Inertia and Sylvester\'s Law}{
Inertia $(p,q,r)$ counts positive, negative, zero eigenvalues of symmetric $A$.
Under congruence, $(p,q,r)$ is invariant.
}{
Classifies quadratic forms up to change of basis into canonical diagonal forms
with $+1$, $-1$, and $0$ entries.
}{
Diagonalize by congruence via symmetric elimination; count signs on the diagonal.
}{
Count how many ways $Q$ acts like $+x^2$, $-x^2$, or $0$ along independent
directions.
}{
Assuming eigenvalues are invariant under congruence is incorrect; only their
sign pattern is preserved.
}
\clearpage
\section{Symbol Ledger}
\varmapStart
\var{V}{Finite-dimensional vector space over field $\mathbb{F}$.}
\var{\mathbb{F}}{Base field, typically $\mathbb{R}$ with $\mathrm{char}\ne 2$.}
\var{n}{Dimension of $V$.}
\var{b}{Bilinear form $b:V\times V\to\mathbb{F}$.}
\var{Q}{Quadratic form $Q:V\to\mathbb{F}$ with $Q(x)=b(x,x)$ when symmetric.}
\var{A}{Matrix of $b$ or $Q$ in a chosen basis; often symmetric.}
\var{P}{Invertible change-of-basis matrix.}
\var{I}{Identity matrix of appropriate size.}
\var{x,y}{Vectors in $V$ with coordinate columns in $\mathbb{F}^n$.}
\var{\lambda_i}{Eigenvalues of symmetric $A$.}
\var{r(x)}{Rayleigh quotient $x^\top A x/(x^\top x)$.}
\var{\Delta_k}{Leading principal minor $\det A_{1:k,1:k}$.}
\var{p,q,r}{Inertia counts: positive, negative, and zero indices.}
\var{\Sigma}{Covariance matrix; symmetric positive semidefinite.}
\var{w}{Weight vector in portfolio context.}
\varmapEnd
\clearpage
\section{Formula Canon — One Formula Per Page}
\FormulaPage{1}{Matrix Representation of Bilinear and Quadratic Forms}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
For any bilinear form $b$ on $V$ and basis $\{e_i\}_{i=1}^n$, there is a unique
matrix $A\in\mathbb{F}^{n\times n}$ with $A_{ij}=b(e_i,e_j)$ such that
$b(x,y)=x^\top A y$. The associated quadratic form is $Q(x)=b(x,x)=x^\top A x$.
\WHAT{
Represents $b$ and $Q$ as coordinate polynomials via a matrix $A$, enabling
computation and classification using linear algebra.
}
\WHY{
A concrete matrix makes algebraic properties transparent: symmetry, rank,
definiteness, and decomposition reduce to standard matrix analysis.
}
\FORMULA{
\[
b(x,y)=x^\top A y,\qquad Q(x)=x^\top A x,\qquad A_{ij}=b(e_i,e_j).
\]
}
\CANONICAL{
Basis-dependent matrix $A$ uniquely encodes $b$; $Q$ depends only on the
symmetric part $A_s=\tfrac12(A+A^\top)$ because $x^\top A x=x^\top A_s x$.
}
\PRECONDS{
\begin{bullets}
\item Finite-dimensional $V$ with a fixed ordered basis.
\item Field $\mathbb{F}$ arbitrary for $b$ and $Q$ representation.
\item For $Q$ induced by $b$, $b$ must be symmetric to ensure $Q(x)=b(x,x)$
represents a quadratic form independent of cross-ordering.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
Let $b$ be bilinear and $\{e_i\}$ a basis. Then for $x=\sum_i x_i e_i$,
$y=\sum_j y_j e_j$ we have $b(x,y)=\sum_{i,j} x_i y_j b(e_i,e_j)$, hence
$b(x,y)=x^\top A y$ with $A_{ij}=b(e_i,e_j)$; $A$ is unique.
\end{lemma}
\begin{proof}
By bilinearity,
$b(x,y)=b\!\big(\sum_i x_i e_i,\sum_j y_j e_j\big)=\sum_{i,j} x_i y_j b(e_i,e_j)$.
Defining $A_{ij}=b(e_i,e_j)$ yields $b(x,y)=x^\top A y$. Uniqueness follows
since $A_{ij}=b(e_i,e_j)$ is determined by $b$ and the basis. \qedhere
\end{proof}
\DERIVATION{
\begin{align*}
\text{Step 1:}&\ \text{Expand } x=\sum_i x_i e_i,\ y=\sum_j y_j e_j.\\
\text{Step 2:}&\ b(x,y)=\sum_{i,j} x_i y_j b(e_i,e_j).\\
\text{Step 3:}&\ \text{Define } A_{ij}=b(e_i,e_j)\ \Rightarrow\ b(x,y)=x^\top A y.\\
\text{Step 4:}&\ Q(x)=b(x,x)=\sum_{i,j} x_i x_j A_{ij}=x^\top A x.\\
\text{Step 5:}&\ \text{Note } x^\top A x=x^\top \tfrac12(A+A^\top) x.
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Choose or identify a basis. Compute $A_{ij}=b(e_i,e_j)$.
\item Evaluate $b(x,y)$ by $x^\top A y$; evaluate $Q(x)$ by $x^\top A x$.
\item Reduce to symmetric part for quadratic questions.
\item Use eigenvalues, minors, or factorizations to classify $Q$.
\end{bullets}
\EQUIV{
\begin{bullets}
\item $Q(x)=x^\top A x=x^\top A_s x$ with $A_s=\tfrac12(A+A^\top)$.
\item If $A$ is symmetric, $b(x,y)=b(y,x)$.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item If $V$ is infinite-dimensional, coordinate matrices require a basis of
finite support; outside scope.
\item Over $\mathbb{C}$, Hermitian forms use conjugate transpose.
\end{bullets}
}
\INPUTS{$x=(1,2)^\top,\ y=(3,1)^\top,\ A=\begin{pmatrix}2&1\\3&0\end{pmatrix}$.}
\DERIVATION{
\begin{align*}
b(x,y)&=x^\top A y=\begin{pmatrix}1&2\end{pmatrix}
\begin{pmatrix}2&1\\3&0\end{pmatrix}\begin{pmatrix}3\\1\end{pmatrix}\\
&=\begin{pmatrix}1&2\end{pmatrix}\begin{pmatrix}2\cdot 3+1\cdot 1\\
3\cdot 3+0\cdot 1\end{pmatrix}
=\begin{pmatrix}1&2\end{pmatrix}\begin{pmatrix}7\\9\end{pmatrix}=25,\\
Q(x)&=x^\top A x=\begin{pmatrix}1&2\end{pmatrix}
\begin{pmatrix}2&1\\3&0\end{pmatrix}\begin{pmatrix}1\\2\end{pmatrix}\\
&=\begin{pmatrix}1&2\end{pmatrix}\begin{pmatrix}2+2\\3+0\end{pmatrix}
=\begin{pmatrix}1&2\end{pmatrix}\begin{pmatrix}4\\3\end{pmatrix}=10.
\end{align*}
}
\RESULT{
Computed $b(x,y)=25$ and $Q(x)=10$ by the matrix representation.
}
\UNITCHECK{
Homogeneous of degree $2$ for $Q$: $Q(\alpha x)=\alpha^2 Q(x)$ holds because
$x^\top A x$ is quadratic in $x$.
}
\PITFALLS{
\begin{bullets}
\item Using $A$ non-symmetric for $Q$ may obscure geometry; always symmetrize.
\item Confusing row and column vectors breaks $x^\top A y$ shape.
\end{bullets}
}
\INTUITION{
Entries of $A$ codify how basis vectors interact. Bilinearity spreads those
interactions to arbitrary vectors by linearity.
}
\CANONICAL{
\begin{bullets}
\item $b(x,y)=x^\top A y$ uniquely in a basis.
\item $Q$ is determined by the symmetric part of $A$.
\end{bullets}
}
\FormulaPage{2}{Polarization Identity}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
For $\mathrm{char}(\mathbb{F})\ne 2$ and symmetric bilinear $b$, the quadratic
form $Q(x)=b(x,x)$ determines $b$ via
$b(x,y)=\tfrac14\big(Q(x+y)-Q(x-y)\big)$.
\WHAT{
Recover symmetric bilinear form from its quadratic form.
}
\WHY{
Shows equivalence between symmetric bilinear forms and quadratic forms when
characteristic is not $2$, enabling one to work with $Q$ alone.
}
\FORMULA{
\[
b(x,y)=\frac{1}{4}\big(Q(x+y)-Q(x-y)\big),\quad Q(x)=b(x,x).
\]
}
\CANONICAL{
Valid over fields where $2$ is invertible. For $\mathbb{R}$, it holds for all
symmetric $b$ and their $Q$.
}
\PRECONDS{
\begin{bullets}
\item $\mathrm{char}(\mathbb{F})\ne 2$.
\item $b$ symmetric bilinear. Then $Q(x)=b(x,x)$ is a quadratic form.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
Let $b$ be symmetric bilinear and $Q(x)=b(x,x)$. Then for all $x,y$,
$Q(x+y)=Q(x)+2b(x,y)+Q(y)$ and $Q(x-y)=Q(x)-2b(x,y)+Q(y)$.
\end{lemma}
\begin{proof}
By bilinearity and symmetry,
$Q(x+y)=b(x+y,x+y)=b(x,x)+b(x,y)+b(y,x)+b(y,y)=Q(x)+2b(x,y)+Q(y)$.
Similarly,
$Q(x-y)=b(x-y,x-y)=Q(x)-2b(x,y)+Q(y)$. \qedhere
\end{proof}
\DERIVATION{
\begin{align*}
\text{Step 1:}&\ Q(x+y)-Q(x-y)=4b(x,y) \quad \text{by lemma.}\\
\text{Step 2:}&\ \text{Divide by }4\text{ to obtain }
b(x,y)=\tfrac14\big(Q(x+y)-Q(x-y)\big).\\
\text{Step 3:}&\ \text{Conversely, define }Q(x)=b(x,x)\text{ to complete the
equivalence.}
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Compute $Q$ or $b$ first.
\item Use polarization to get the other when symmetry and char conditions hold.
\item Reduce computations to sums and differences of $Q$.
\end{bullets}
\EQUIV{
\begin{bullets}
\item Alternative form:
$b(x,y)=\tfrac12\big(Q(x+y)-Q(x)-Q(y)\big)$ for symmetric $b$.
\item Over $\mathbb{C}$ with Hermitian forms:
$b(x,y)=\tfrac14\sum_{k=0}^{3} i^k Q(x+i^k y)$ with sesquilinearity.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item Fails in characteristic $2$ because $2$ is not invertible.
\item Requires symmetry; antisymmetric parts vanish in $Q$ and cannot be
recovered.
\end{bullets}
}
\INPUTS{$Q(x)=x^\top A x$ with $A=\begin{pmatrix}2&1\\1&3\end{pmatrix}$,
$x=(1,2)^\top$, $y=(2,-1)^\top$.}
\DERIVATION{
\begin{align*}
Q(x)&=\begin{pmatrix}1&2\end{pmatrix}\begin{pmatrix}2&1\\1&3\end{pmatrix}
\begin{pmatrix}1\\2\end{pmatrix}
=\begin{pmatrix}1&2\end{pmatrix}\begin{pmatrix}4\\7\end{pmatrix}=18,\\
Q(y)&=\begin{pmatrix}2&-1\end{pmatrix}\begin{pmatrix}2&1\\1&3\end{pmatrix}
\begin{pmatrix}2\\-1\end{pmatrix}
=\begin{pmatrix}2&-1\end{pmatrix}\begin{pmatrix}3\\-1\end{pmatrix}=7,\\
x+y&=(3,1)^\top,\quad Q(x+y)=\begin{pmatrix}3&1\end{pmatrix}
\begin{pmatrix}2&1\\1&3\end{pmatrix}\begin{pmatrix}3\\1\end{pmatrix}\\
&=\begin{pmatrix}3&1\end{pmatrix}\begin{pmatrix}7\\6\end{pmatrix}=27,\\
x-y&=(-1,3)^\top,\quad Q(x-y)=\begin{pmatrix}-1&3\end{pmatrix}
\begin{pmatrix}2&1\\1&3\end{pmatrix}\begin{pmatrix}-1\\3\end{pmatrix}\\
&=\begin{pmatrix}-1&3\end{pmatrix}\begin{pmatrix}1\\8\end{pmatrix}=23,\\
b(x,y)&=\tfrac14(27-23)=1.
\end{align*}
}
\RESULT{
Recovered $b(x,y)=1$ from $Q$ via polarization, matching $x^\top A y=1$.
}
\UNITCHECK{
Both sides are bilinear in $(x,y)$ and homogeneous of degree $2$ jointly.
}
\PITFALLS{
\begin{bullets}
\item Forgetting the factor $\tfrac14$ yields a factor-of-two error.
\item Using non-symmetric $A$ invalidates the identity.
\end{bullets}
}
\INTUITION{
$Q(x+y)$ and $Q(x-y)$ cancel the pure terms $Q(x)$ and $Q(y)$, isolating the
cross interaction $2b(x,y)$.
}
\CANONICAL{
\begin{bullets}
\item Symmetric bilinear forms and quadratic forms are equivalent data when
$\mathrm{char}\ne 2$.
\end{bullets}
}
\FormulaPage{3}{Change of Basis and Congruence}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Under basis change $x=P\hat{x}$, the matrix of a bilinear form transforms by
congruence: $A\mapsto A'=P^\top A P$ so that $x^\top A y=\hat{x}^\top A'\hat{y}$.
\WHAT{
Describes how coordinates of $b$ and $Q$ change with basis, preserving values.
}
\WHY{
Allows diagonalization of quadratic forms by congruence and shows invariance of
inertia and definiteness under basis change.
}
\FORMULA{
\[
A' = P^\top A P,\qquad x=P\hat{x},\ y=P\hat{y},\quad
x^\top A y=\hat{x}^\top A' \hat{y}.
\]
}
\CANONICAL{
$P$ invertible. If $A$ is symmetric, so is $A'$. Quadratic form invariants are
preserved under congruence, not similarity in general.
}
\PRECONDS{
\begin{bullets}
\item Invertible basis change $P$ relating old and new coordinates.
\item $A$ any matrix representing $b$ in the old basis.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
Let $x=P\hat{x}$, $y=P\hat{y}$. Then $x^\top A y=\hat{x}^\top (P^\top A P)\hat{y}$.
\end{lemma}
\begin{proof}
Compute $x^\top A y=(P\hat{x})^\top A (P\hat{y})=\hat{x}^\top P^\top A P\hat{y}$,
which equals $\hat{x}^\top A' \hat{y}$ with $A'=P^\top A P$. \qedhere
\end{proof}
\DERIVATION{
\begin{align*}
\text{Step 1:}&\ \text{Define }A'=P^\top A P.\\
\text{Step 2:}&\ x^\top A y=\hat{x}^\top P^\top A P \hat{y}=\hat{x}^\top A' \hat{y}.\\
\text{Step 3:}&\ Q(x)=x^\top A x=\hat{x}^\top A' \hat{x}\ \text{so }Q\text{ is
basis-invariant}.
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Choose $P$ to simplify $A$ by congruence.
\item Verify $A'$ symmetry and compute new coordinates.
\item Evaluate $Q$ in any basis using the corresponding matrix.
\end{bullets}
\EQUIV{
\begin{bullets}
\item Orthogonal change: if $P$ orthogonal, $A'=P^\top A P=P^{-1} A P$.
\item Similarity $P^{-1} A P$ preserves eigenvalues, not $Q$.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item Non-invertible $P$ does not define a valid basis change.
\item Similarity alone does not preserve quadratic forms.
\end{bullets}
}
\INPUTS{$A=\begin{pmatrix}4&2\\2&3\end{pmatrix}$,
$P=\begin{pmatrix}1&1\\0&1\end{pmatrix}$,
$x=(3,1)^\top$, $y=(1,2)^\top$.}
\DERIVATION{
\begin{align*}
A'&=P^\top A P=\begin{pmatrix}1&0\\1&1\end{pmatrix}
\begin{pmatrix}4&2\\2&3\end{pmatrix}\begin{pmatrix}1&1\\0&1\end{pmatrix}\\
&=\begin{pmatrix}4&2\\6&5\end{pmatrix}\begin{pmatrix}1&1\\0&1\end{pmatrix}
=\begin{pmatrix}4&6\\6&11\end{pmatrix},\\
\hat{x}&=P^{-1}x=\begin{pmatrix}1&-1\\0&1\end{pmatrix}\begin{pmatrix}3\\1\end{pmatrix}
=\begin{pmatrix}2\\1\end{pmatrix},\
\hat{y}=\begin{pmatrix}1&-1\\0&1\end{pmatrix}\begin{pmatrix}1\\2\end{pmatrix}
=\begin{pmatrix}-1\\2\end{pmatrix},\\
x^\top A y&=\begin{pmatrix}3&1\end{pmatrix}\begin{pmatrix}4&2\\2&3\end{pmatrix}
\begin{pmatrix}1\\2\end{pmatrix}
=\begin{pmatrix}3&1\end{pmatrix}\begin{pmatrix}8\\8\end{pmatrix}=32,\\
\hat{x}^\top A' \hat{y}&=\begin{pmatrix}2&1\end{pmatrix}
\begin{pmatrix}4&6\\6&11\end{pmatrix}\begin{pmatrix}-1\\2\end{pmatrix}\\
&=\begin{pmatrix}2&1\end{pmatrix}\begin{pmatrix}8\\16\end{pmatrix}=32.
\end{align*}
}
\RESULT{
Congruence preserves bilinear evaluations: both bases yield value $32$.
}
\UNITCHECK{
$Q$ invariant under congruence and homogeneous of degree $2$ in vectors.
}
\PITFALLS{
\begin{bullets}
\item Using similarity instead of congruence changes $Q$ values.
\item Forgetting to transform vectors when comparing coordinates.
\end{bullets}
}
\INTUITION{
Changing coordinates re-expresses the same geometric object; congruence updates
the coefficients accordingly.
}
\CANONICAL{
\begin{bullets}
\item Quadratic forms are basis-independent objects represented by congruent
matrices.
\end{bullets}
}
\FormulaPage{4}{Sylvester\'s Criterion for Positive Definiteness}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
A real symmetric matrix $A$ is positive definite iff all leading principal
minors are positive: $\Delta_k=\det A_{1:k,1:k}>0$ for $k=1,\dots,n$.
\WHAT{
Characterizes positive definiteness of $Q(x)=x^\top A x$ by determinants of
principal submatrices.
}
\WHY{
Provides an efficient algebraic test equivalent to existence of Cholesky
factorization and convexity of $Q$.
}
\FORMULA{
\[
A\succ 0 \iff \Delta_k>0\ \text{ for all }k=1,\dots,n.
\]
}
\CANONICAL{
For $A=A^\top\in\mathbb{R}^{n\times n}$. The criterion is necessary and
sufficient and independent of basis choice.
}
\PRECONDS{
\begin{bullets}
\item $A$ real symmetric.
\item Leading principal minors are well-defined.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
If $A$ is real symmetric and $\Delta_k>0$ for all $k$, then $A$ admits a
Cholesky factorization $A=R^\top R$ with $R$ upper triangular with positive
diagonal.
\end{lemma}
\begin{proof}
Construct $R$ recursively by the standard Cholesky algorithm; positivity of
$\Delta_k$ guarantees positive pivots and well-defined square roots. Conversely,
if $A=R^\top R$ with $R$ nonsingular, then each leading principal block equals
$R_{1:k,1:k}^\top R_{1:k,1:k}$ which has positive determinant, proving
$\Delta_k>0$. \qedhere
\end{proof}
\DERIVATION{
\begin{align*}
\text{Step 1:}&\ \Delta_k>0\ \forall k \Rightarrow \text{Cholesky }A=R^\top R.\\
\text{Step 2:}&\ x^\top A x=x^\top R^\top R x=\|R x\|_2^2>0\ \forall x\ne 0.\\
\text{Step 3:}&\ \text{Conversely, }A\succ 0\Rightarrow \Delta_k>0\ \text{since
all principal minors of }R^\top R\ \text{are positive.}
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Compute $\Delta_1,\dots,\Delta_n$.
\item If all positive, conclude $A\succ 0$ and $Q$ is strictly convex.
\item Otherwise inspect eigenvalues or pivot signs to classify inertia.
\end{bullets}
\EQUIV{
\begin{bullets}
\item $A\succ 0 \iff$ all eigenvalues positive.
\item $A\succ 0 \iff$ exists unique Cholesky $R$ with positive diagonal.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item For semidefiniteness, leading minors nonnegative are not sufficient;
need all principal minors nonnegative.
\item Non-symmetric $A$ invalidates the criterion.
\end{bullets}
}
\INPUTS{$A=\begin{pmatrix}2&-1&0\\-1&2&-1\\0&-1&2\end{pmatrix}$.}
\DERIVATION{
\begin{align*}
\Delta_1&=2>0,\\
\Delta_2&=\det\begin{pmatrix}2&-1\\-1&2\end{pmatrix}=4-1=3>0,\\
\Delta_3&=\det(A)=2\det\begin{pmatrix}2&-1\\-1&2\end{pmatrix}
-(-1)\det\begin{pmatrix}-1&-1\\0&2\end{pmatrix}\\
&=2\cdot 3+1\cdot(-2)=6-2=4>0.
\end{align*}
}
\RESULT{
All leading principal minors positive, hence $A\succ 0$ and
$Q(x)=x^\top A x>0$ for all $x\ne 0$.
}
\UNITCHECK{
Scaling $A\mapsto \alpha A$ with $\alpha>0$ scales minors by $\alpha^k$ and
preserves definiteness.
}
\PITFALLS{
\begin{bullets}
\item Checking only $\det A>0$ is insufficient in higher dimensions.
\item Numerical instability may misjudge near-singular pivots.
\end{bullets}
}
\INTUITION{
Positive pivots and minors mean every growing coordinate block stores positive
energy, preventing any nontrivial vector from yielding zero or negative energy.
}
\CANONICAL{
\begin{bullets}
\item Positive definiteness is equivalent to all leading principal minors
being positive.
\end{bullets}
}
\FormulaPage{5}{Sylvester\'s Law of Inertia}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Any real symmetric matrix $A$ is congruent to a diagonal matrix
$\mathrm{diag}(I_p,-I_q,0_r)$; the integers $(p,q,r)$ are uniquely determined
and invariant under congruence.
\WHAT{
Classifies quadratic forms up to change of basis by the triple $(p,q,r)$.
}
\WHY{
Reveals the essential geometry: number of positive, negative, and flat
directions, independent of coordinates.
}
\FORMULA{
\[
\exists P\ \text{invertible},\quad P^\top A P=\mathrm{diag}(I_p,-I_q,0_r),\
p+q+r=n.
\]
}
\CANONICAL{
Diagonalization by congruence using symmetric elimination. Inertia $(p,q,r)$
equals counts of signs of eigenvalues for symmetric $A$.
}
\PRECONDS{
\begin{bullets}
\item $A=A^\top\in\mathbb{R}^{n\times n}$.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
Let $A=A^\top$. There exists an invertible $P$ such that $P^\top A P$ is
diagonal with diagonal entries nonzero except possibly some zeros.
\end{lemma}
\begin{proof}
Apply symmetric Gaussian elimination: pivot on a nonzero diagonal or find a
nonzero off-diagonal entry and perform a $2\times 2$ congruence to create a
nonzero pivot, then clear off-diagonal entries by congruence-preserving row and
column operations in tandem. Iterate until diagonal. \qedhere
\end{proof}
\DERIVATION{
\begin{align*}
\text{Step 1:}&\ \text{Diagonalize }A\text{ by congruence }P^\top A P=D.\\
\text{Step 2:}&\ \text{Scale diagonal entries by congruence to }+1,-1,0.\\
\text{Step 3:}&\ \text{Let }p,q,r\text{ count }+1,-1,0\text{ entries.}\\
\text{Step 4:}&\ \text{Uniqueness: if }P_1^\top A P_1=D_1,\ P_2^\top A P_2=D_2,\\
&\ \text{then }D_2=(P_1^{-1}P_2)^\top D_1 (P_1^{-1}P_2)\ \text{preserves }(p,q,r).
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Perform congruence steps to diagonalize.
\item Count positive, negative, zero diagonal entries for inertia.
\item Use inertia to classify definiteness and level sets of $Q$.
\end{bullets}
\EQUIV{
\begin{bullets}
\item $(p,q,r)$ equals numbers of positive, negative, zero eigenvalues.
\item $Q(x)=x^\top A x$ is equivalent to $p$ copies of $+u_i^2$, $q$ of $-v_j^2$,
and $r$ flat directions.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item Inertia is not given by eigenvalue magnitudes, only signs.
\item Non-symmetric matrices need not have well-defined inertia under congruence.
\end{bullets}
}
\INPUTS{$A=\begin{pmatrix}0&1\\1&0\end{pmatrix}$.}
\DERIVATION{
\begin{align*}
P&=\tfrac{1}{\sqrt2}\begin{pmatrix}1&1\\1&-1\end{pmatrix}\ \text{orthogonal},\\
P^\top A P&=\tfrac{1}{2}\begin{pmatrix}1&1\\1&-1\end{pmatrix}
\begin{pmatrix}0&1\\1&0\end{pmatrix}\begin{pmatrix}1&1\\1&-1\end{pmatrix}\\
&=\tfrac{1}{2}\begin{pmatrix}1&-1\\-1&-1\end{pmatrix}
\begin{pmatrix}1&1\\1&-1\end{pmatrix}
=\begin{pmatrix}1&0\\0&-1\end{pmatrix}.
\end{align*}
}
\RESULT{
Inertia $(p,q,r)=(1,1,0)$. The quadratic form is equivalent to $u^2-v^2$.
}
\UNITCHECK{
Scaling by $\alpha>0$ does not change inertia; scaling by $-1$ swaps $p$ and $q$.
}
\PITFALLS{
\begin{bullets}
\item Mistaking similarity for congruence fails to preserve inertia.
\item Zero eigenvalues indicate flat directions, not indefinite curvature.
\end{bullets}
}
\INTUITION{
Any quadratic form is a sum of independent squares with signs; coordinates pick
which combinations appear, inertia tells the essence.
}
\CANONICAL{
\begin{bullets}
\item Congruence classification by inertia is complete for real symmetric
matrices.
\end{bullets}
}
\FormulaPage{6}{Rayleigh Quotient Extremal Characterization}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
For real symmetric $A$ with eigenvalues
$\lambda_{\min}\le\dots\le\lambda_{\max}$,
$r(x)=\dfrac{x^\top A x}{x^\top x}$ satisfies
$\lambda_{\min}\le r(x)\le \lambda_{\max}$, with equality at eigenvectors.
\WHAT{
Characterizes the range of $Q$ on the unit sphere and identifies extremizers as
eigenvectors.
}
\WHY{
Central to optimization, eigenvalue estimation, and understanding curvature of
quadratic objectives.
}
\FORMULA{
\[
\lambda_{\min}=\min_{\|x\|=1} x^\top A x,\quad
\lambda_{\max}=\max_{\|x\|=1} x^\top A x.
\]
}
\CANONICAL{
Applies to $A=A^\top$. Orthogonal diagonalization expresses the quotient as a
convex combination of eigenvalues.
}
\PRECONDS{
\begin{bullets}
\item $A$ real symmetric.
\item $x\ne 0$ to define $r(x)$.
\end{bullets}
}
\textbf{SUPPORTING LEMMAS.}
\begin{lemma}
Let $A=U\Lambda U^\top$ be orthogonal diagonalization. Then
$r(x)=\dfrac{\sum_i \lambda_i y_i^2}{\sum_i y_i^2}$ where $y=U^\top x$.
\end{lemma}
\begin{proof}
$r(x)=\dfrac{x^\top U\Lambda U^\top x}{x^\top x}=
\dfrac{y^\top \Lambda y}{y^\top y}=
\dfrac{\sum_i \lambda_i y_i^2}{\sum_i y_i^2}$ since $U$ is orthogonal.
\qedhere
\end{proof}
\DERIVATION{
\begin{align*}
\text{Step 1:}&\ \text{Diagonalize }A=U\Lambda U^\top.\\
\text{Step 2:}&\ r(x)=\frac{\sum_i \lambda_i y_i^2}{\sum_i y_i^2},\
y=U^\top x.\\
\text{Step 3:}&\ \text{Weights }w_i=\frac{y_i^2}{\sum_j y_j^2}\ \text{form a
simplex}.\\
\text{Step 4:}&\ r(x)=\sum_i w_i \lambda_i\in
[\lambda_{\min},\lambda_{\max}].\\
\text{Step 5:}&\ \text{Equality when }y\ \text{is aligned with an eigenvector}.
\end{align*}
}
\textbf{GENERAL PROBLEM-SOLVING TEMPLATE.}
\begin{bullets}
\item Compute or bound eigenvalues to bound $Q$ on the sphere.
\item Find maximizing or minimizing $x$ as eigenvectors of largest or smallest
eigenvalues.
\end{bullets}
\EQUIV{
\begin{bullets}
\item Courant-Fischer min-max principles generalize to intermediate eigenvalues.
\item For $A\succ 0$, $r(x)\in[\lambda_{\min},\lambda_{\max}] \subset (0,\infty)$.
\end{bullets}
}
\LIMITS{
\begin{bullets}
\item Non-symmetric $A$ can yield complex or unbounded Rayleigh quotients.
\item For $x=0$, $r$ undefined.
\end{bullets}
}
\INPUTS{$A=\begin{pmatrix}3&1\\1&2\end{pmatrix}$, $x=(1,2)^\top$.}
\DERIVATION{
\begin{align*}
x^\top A x&=\begin{pmatrix}1&2\end{pmatrix}\begin{pmatrix}3&1\\1&2\end{pmatrix}
\begin{pmatrix}1\\2\end{pmatrix}
=\begin{pmatrix}1&2\end{pmatrix}\begin{pmatrix}5\\5\end{pmatrix}=15,\\
x^\top x&=1^2+2^2=5,\quad r(x)=15/5=3.\\
\lambda_{\pm}&=\frac{5\pm\sqrt{5}}{2}\approx 3.618,\ 1.382,\\
\text{Check:}&\ 1.382\le 3\le 3.618\ \text{holds.}
\end{align*}
}
\RESULT{
$r(x)=3$ lies within the eigenvalue interval, as guaranteed.
}
\UNITCHECK{
Rayleigh quotient is scale-invariant: $r(\alpha x)=r(x)$ for $\alpha\ne 0$.
}
\PITFALLS{
\begin{bullets}
\item Using non-unit $x$ in the extremal statement without normalizing.
\item Confusing similarity and orthogonal diagonalization.
\end{bullets}
}
\INTUITION{
On the sphere, $Q$ blends eigenvalues according to how $x$ projects on
eigenvectors. Extremes occur at pure alignments.
}
\CANONICAL{
\begin{bullets}
\item Extremal values of a quadratic form on the sphere are eigenvalues.
\end{bullets}
}
\clearpage
\section{10 Exhaustive Problems and Solutions}
\ProblemPage{1}{Definiteness, Minors, and Diagonalization}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Given $A=\begin{pmatrix}2&1\\1&2\end{pmatrix}$, analyze $Q(x)=x^\top A x$:
determine definiteness via Sylvester, diagonalize by orthogonal $P$, and compute
$Q(1,2)$.
\PROBLEM{
Classify $Q$ by leading principal minors and diagonalize $A$ by an orthogonal
matrix. Evaluate $Q$ at a point.
}
\MODEL{
\[
Q(x)=x^\top A x,\quad A=A^\top\in\mathbb{R}^{2\times 2}.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Real symmetric $A$.
\item Standard Euclidean coordinates.
\end{bullets}
}
\varmapStart
\var{A}{Matrix of quadratic form.}
\var{x}{Vector argument of $Q$.}
\var{P}{Orthogonal diagonalizer of $A$.}
\var{\lambda_i}{Eigenvalues of $A$.}
\varmapEnd
\WHICHFORMULA{
Sylvester\'s criterion; orthogonal diagonalization of symmetric matrices; matrix
representation $Q(x)=x^\top A x$.
}
\GOVERN{
\[
\Delta_1=2>0,\ \Delta_2=\det A=3>0\ \Rightarrow\ A\succ 0.
\]
}
\INPUTS{$A=\begin{pmatrix}2&1\\1&2\end{pmatrix}$, $x=(1,2)^\top$.}
\DERIVATION{
\begin{align*}
\text{Step 1:}&\ \Delta_1=2>0,\ \Delta_2=4-1=3>0\Rightarrow A\succ 0.\\
\text{Step 2:}&\ \lambda=2\pm 1 \Rightarrow \lambda_1=1,\ \lambda_2=3.\\
\text{Step 3:}&\ \text{Eigenvectors: }v_1=(1,-1)^\top,\ v_2=(1,1)^\top.\\
\text{Step 4:}&\ P=\tfrac{1}{\sqrt2}\begin{pmatrix}1&1\\-1&1\end{pmatrix},\
P^\top A P=\mathrm{diag}(1,3).\\
\text{Step 5:}&\ Q(1,2)=\begin{pmatrix}1&2\end{pmatrix}
\begin{pmatrix}2&1\\1&2\end{pmatrix}\begin{pmatrix}1\\2\end{pmatrix}\\
&=\begin{pmatrix}1&2\end{pmatrix}\begin{pmatrix}4\\5\end{pmatrix}=14.
\end{align*}
}
\RESULT{
$A\succ 0$; orthogonal diagonalization gives eigenvalues $1,3$; $Q(1,2)=14$.
}
\UNITCHECK{
$Q(\alpha x)=\alpha^2 Q(x)$; positivity for nonzero $x$ confirmed by eigenvalues.
}
\EDGECASES{
\begin{bullets}
\item If off-diagonal were $-1$, the matrix would still be positive definite.
\item If diagonal were $(1,0)$, $A$ would be semidefinite with a flat direction.
\end{bullets}
}
\ALTERNATE{
Use Cholesky: $A=R^\top R$ with $R=\begin{pmatrix}\sqrt2&1/\sqrt2\\0&\sqrt{3/2}
\end{pmatrix}$ to see $Q(x)=\|R x\|^2$.
}
\VALIDATION{
\begin{bullets}
\item Compute eigenvalues numerically to verify positivity.
\item Check that $P$ is orthogonal and $P^\top A P$ is diagonal.
\end{bullets}
}
\INTUITION{
$Q$ measures squared length in a slightly skewed metric, strictly positive away
from the origin.
}
\CANONICAL{
\begin{bullets}
\item Positive definiteness via minors and eigenvalues are equivalent.
\end{bullets}
}
\ProblemPage{2}{Recovering a Bilinear Form from a Quadratic Form}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Given $Q(x_1,x_2)=3x_1^2+4x_1x_2+2x_2^2$, find the symmetric bilinear form $b$
with $Q(x)=b(x,x)$ and compute $b\big((1,2),(2,-1)\big)$.
\PROBLEM{
Use polarization to recover $b$ and evaluate it on given vectors.
}
\MODEL{
\[
Q(x)=x^\top A x \ \text{ with }\ A=\begin{pmatrix}3&2\\2&2\end{pmatrix},\quad
b(x,y)=\tfrac14\big(Q(x+y)-Q(x-y)\big).
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Real field, characteristic not $2$.
\item Symmetric $A$ yields symmetric $b$.
\end{bullets}
}
\varmapStart
\var{Q}{Quadratic form.}
\var{b}{Symmetric bilinear form.}
\var{A}{Symmetric matrix of $Q$.}
\var{x,y}{Evaluation vectors.}
\varmapEnd
\WHICHFORMULA{
Polarization identity and matrix representation $b(x,y)=x^\top A y$.
}
\GOVERN{
\[
b(x,y)=\tfrac14\big(Q(x+y)-Q(x-y)\big)=x^\top A y.
\]
}
\INPUTS{$x=(1,2)^\top$, $y=(2,-1)^\top$.}
\DERIVATION{
\begin{align*}
A&=\begin{pmatrix}3&2\\2&2\end{pmatrix},\quad b(x,y)=x^\top A y.\\
x^\top A y&=\begin{pmatrix}1&2\end{pmatrix}\begin{pmatrix}3&2\\2&2\end{pmatrix}
\begin{pmatrix}2\\-1\end{pmatrix}\\
&=\begin{pmatrix}1&2\end{pmatrix}\begin{pmatrix}6-2\\4-2\end{pmatrix}
=\begin{pmatrix}1&2\end{pmatrix}\begin{pmatrix}4\\2\end{pmatrix}=8.\\
\text{Polarization: }&\ x+y=(3,1),\ x-y=(-1,3).\\
Q(x+y)&=3\cdot 9+4\cdot 3\cdot 1+2\cdot 1=27+12+2=41,\\
Q(x-y)&=3\cdot 1+4\cdot (-1)\cdot 3+2\cdot 9=3-12+18=9,\\
b(x,y)&=\tfrac14(41-9)=8.
\end{align*}
}
\RESULT{
$b(x,y)=8$ from both matrix evaluation and polarization.
}
\UNITCHECK{
Bilinearity verified by linearity in each argument; homogeneity holds.
}
\EDGECASES{
\begin{bullets}
\item If $\mathrm{char}=2$, polarization fails due to division by $2$.
\item Non-symmetric $A$ would still yield the same $Q$ but wrong $b$.
\end{bullets}
}
\ALTERNATE{
Extract $A$ coefficients by matching $Q$ terms and compute $x^\top A y$.
}
\VALIDATION{
\begin{bullets}
\item Cross-check polarization against $x^\top A y$ numerically.
\end{bullets}
}
\INTUITION{
The cross term $4x_1x_2$ encodes interaction; polarization isolates it.
}
\CANONICAL{
\begin{bullets}
\item Symmetric $b$ and $Q$ are equivalent via polarization.
\end{bullets}
}
\ProblemPage{3}{Congruence Invariance Check}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Verify $x^\top A y=\hat{x}^\top (P^\top A P)\hat{y}$ for given $A,P,x,y$.
\PROBLEM{
Compute both sides and confirm equality, demonstrating congruence invariance.
}
\MODEL{
\[
A' = P^\top A P,\quad x=P\hat{x},\ y=P\hat{y}.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $P$ invertible.
\item Standard bilinear pairing via transpose.
\end{bullets}
}
\varmapStart
\var{A}{Original matrix.}
\var{P}{Change-of-basis matrix.}
\var{x,y}{Original vectors.}
\var{\hat{x},\hat{y}}{New-basis coordinates.}
\varmapEnd
\WHICHFORMULA{
Congruence transformation law from the change-of-basis formula.
}
\GOVERN{
\[
x^\top A y=\hat{x}^\top (P^\top A P)\hat{y}.
\]
}
\INPUTS{$A=\begin{pmatrix}1&2\\2&5\end{pmatrix}$,
$P=\begin{pmatrix}2&1\\1&1\end{pmatrix}$,
$x=(3,1)^\top$, $y=(1,2)^\top$.}
\DERIVATION{
\begin{align*}
A'&=P^\top A P=\begin{pmatrix}2&1\\1&1\end{pmatrix}
\begin{pmatrix}1&2\\2&5\end{pmatrix}\begin{pmatrix}2&1\\1&1\end{pmatrix}\\
&=\begin{pmatrix}4&7\\3&7\end{pmatrix}\begin{pmatrix}2&1\\1&1\end{pmatrix}
=\begin{pmatrix}15&11\\13&10\end{pmatrix},\\
\hat{x}&=P^{-1}x=\begin{pmatrix}1&-1\\-1&2\end{pmatrix}\begin{pmatrix}3\\1\end{pmatrix}
=\begin{pmatrix}2\\-1\end{pmatrix},\\
\hat{y}&=\begin{pmatrix}1&-1\\-1&2\end{pmatrix}\begin{pmatrix}1\\2\end{pmatrix}
=\begin{pmatrix}-1\\3\end{pmatrix},\\
x^\top A y&=\begin{pmatrix}3&1\end{pmatrix}\begin{pmatrix}1&2\\2&5\end{pmatrix}
\begin{pmatrix}1\\2\end{pmatrix}
=\begin{pmatrix}3&1\end{pmatrix}\begin{pmatrix}5\\12\end{pmatrix}=27,\\
\hat{x}^\top A' \hat{y}&=\begin{pmatrix}2&-1\end{pmatrix}
\begin{pmatrix}15&11\\13&10\end{pmatrix}\begin{pmatrix}-1\\3\end{pmatrix}\\
&=\begin{pmatrix}2&-1\end{pmatrix}\begin{pmatrix}-15+33\\-13+30\end{pmatrix}
=\begin{pmatrix}2&-1\end{pmatrix}\begin{pmatrix}18\\17\end{pmatrix}=27.
\end{align*}
}
\RESULT{
Equality holds: congruence preserves bilinear evaluations.
}
\UNITCHECK{
Dimensional consistency: both sides scalars; invariance under scaling of basis
by invertible $P$.
}
\EDGECASES{
\begin{bullets}
\item If $P$ is singular, inverse does not exist and relation fails.
\item Orthogonal $P$ simplifies to similarity.
\end{bullets}
}
\ALTERNATE{
Compute $x=P\hat{x}$ and $y=P\hat{y}$ first, then evaluate $x^\top A y$.
}
\VALIDATION{
\begin{bullets}
\item Numerically verify multiple random instances with deterministic seeds.
\end{bullets}
}
\INTUITION{
Changing viewpoint does not change the interaction value measured by $b$.
}
\CANONICAL{
\begin{bullets}
\item Congruence is the coordinate rule for quadratic forms.
\end{bullets}
}
\ProblemPage{4}{Portfolio Variance as a Quadratic Form}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Portfolio variance is $Q(w)=w^\top \Sigma w$. Show invariance under whitening
$w=C^\top \hat{w}$ with $C C^\top=\Sigma$ and compute a numeric example.
\PROBLEM{
Interpret variance as a quadratic form and verify that in whitened coordinates
it becomes Euclidean: $Q(w)=\|\hat{w}\|^2$.
}
\MODEL{
\[
\Sigma=C C^\top,\quad w=C^\top \hat{w}\Rightarrow Q(w)=\hat{w}^\top \hat{w}.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $\Sigma\succeq 0$ symmetric; use Cholesky $C$ if $\Sigma\succ 0$.
\item Real weights summing to one optional, not used here.
\end{bullets}
}
\varmapStart
\var{\Sigma}{Covariance matrix.}
\var{w}{Portfolio weights.}
\var{C}{Cholesky factor of $\Sigma$.}
\var{\hat{w}}{Whitened coordinates.}
\varmapEnd
\WHICHFORMULA{
Matrix representation $Q(w)=w^\top \Sigma w$ and congruence with $P=C$.
}
\GOVERN{
\[
w^\top \Sigma w=(C^\top \hat{w})^\top (C C^\top)(C^\top \hat{w})
=\hat{w}^\top (C C^\top C C^\top)\hat{w}=\hat{w}^\top \hat{w}.
\]
}
\INPUTS{$\Sigma=\begin{pmatrix}2&1\\1&2\end{pmatrix}$, $C$ its Cholesky,
$\hat{w}=(1,2)^\top$.}
\DERIVATION{
\begin{align*}
C&=\begin{pmatrix}\sqrt2&1/\sqrt2\\0&\sqrt{3/2}\end{pmatrix},\
w=C^\top \hat{w}=\begin{pmatrix}\sqrt2&0\\1/\sqrt2&\sqrt{3/2}\end{pmatrix}
\begin{pmatrix}1\\2\end{pmatrix}\\
&=\begin{pmatrix}\sqrt2\\ \tfrac{1}{\sqrt2}+2\sqrt{\tfrac{3}{2}}\end{pmatrix},\\
Q(w)&=w^\top \Sigma w=\hat{w}^\top \hat{w}=1^2+2^2=5.
\end{align*}
}
\RESULT{
Variance equals $5$ in either coordinates; whitening reduces $Q$ to Euclidean
norm squared.
}
\UNITCHECK{
$Q$ is homogeneous of degree $2$ in $w$; whitening is a congruence.
}
\EDGECASES{
\begin{bullets}
\item If $\Sigma$ singular, use pseudo-Cholesky or SVD; flat directions occur.
\item Off-diagonal zero reduces to independent assets.
\end{bullets}
}
\ALTERNATE{
Use eigen-decomposition $\Sigma=U\Lambda U^\top$, take $C=U\Lambda^{1/2}$.
}
\VALIDATION{
\begin{bullets}
\item Numerically verify $C C^\top=\Sigma$ and $Q(w)=\|\hat{w}\|^2$.
\end{bullets}
}
\INTUITION{
Variance measures squared length in the covariance-induced metric; whitening
switches to standard length units.
}
\CANONICAL{
\begin{bullets}
\item Covariance defines a quadratic form; congruence yields canonical form.
\end{bullets}
}
\ProblemPage{5}{Minimum Variance Portfolio via Quadratic Optimization}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Minimize $Q(w)=w^\top \Sigma w$ subject to $\mathbf{1}^\top w=1$.
\PROBLEM{
Derive the closed-form optimizer using Lagrange conditions and compute with a
numeric covariance.
}
\MODEL{
\[
\min_{w}\ w^\top \Sigma w\ \text{s.t.}\ \mathbf{1}^\top w=1.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $\Sigma\succ 0$ for uniqueness.
\item Real weights; no other constraints.
\end{bullets}
}
\varmapStart
\var{\Sigma}{Covariance matrix.}
\var{w}{Portfolio weights.}
\var{\lambda}{Lagrange multiplier.}
\var{\mathbf{1}}{All-ones vector.}
\varmapEnd
\WHICHFORMULA{
First-order conditions for quadratic forms:
$2\Sigma w-\lambda \mathbf{1}=0$, constraint $\mathbf{1}^\top w=1$.
}
\GOVERN{
\[
w^\star=\frac{\Sigma^{-1}\mathbf{1}}{\mathbf{1}^\top \Sigma^{-1}\mathbf{1}},
\quad Q(w^\star)=\frac{1}{\mathbf{1}^\top \Sigma^{-1}\mathbf{1}}.
\]
}
\INPUTS{$\Sigma=\begin{pmatrix}2&1\\1&2\end{pmatrix}$.}
\DERIVATION{
\begin{align*}
\Sigma^{-1}&=\frac{1}{3}\begin{pmatrix}2&-1\\-1&2\end{pmatrix},\
\Sigma^{-1}\mathbf{1}=\frac{1}{3}\begin{pmatrix}1\\1\end{pmatrix},\\
\mathbf{1}^\top \Sigma^{-1}\mathbf{1}&=\frac{2}{3},\
w^\star=\frac{\tfrac{1}{3}(1,1)^\top}{2/3}=\tfrac12(1,1)^\top,\\
Q(w^\star)&=(w^\star)^\top \Sigma w^\star=\tfrac14 (1,1)
\begin{pmatrix}2&1\\1&2\end{pmatrix}\begin{pmatrix}1\\1\end{pmatrix}\\
&=\tfrac14 (1,1)\begin{pmatrix}3\\3\end{pmatrix}=\tfrac{3}{2}.
\end{align*}
}
\RESULT{
$w^\star=(1/2,1/2)$ and minimal variance $Q(w^\star)=1.5$.
}
\UNITCHECK{
$w$ dimensionless weights; $Q$ has variance units; scaling constraint fixes
scale.
}
\EDGECASES{
\begin{bullets}
\item If $\Sigma$ singular, optimizer not unique; restrict to range space.
\item Adding bounds or shorting constraints requires QP solvers.
\end{bullets}
}
\ALTERNATE{
Solve KKT system
$\begin{pmatrix}2\Sigma&-\mathbf{1}\\ \mathbf{1}^\top&0\end{pmatrix}
\begin{pmatrix}w\\ \lambda\end{pmatrix}=\begin{pmatrix}0\\1\end{pmatrix}$.
}
\VALIDATION{
\begin{bullets}
\item Check $\mathbf{1}^\top w^\star=1$ and stationarity $2\Sigma w^\star=
\lambda \mathbf{1}$.
\end{bullets}
}
\INTUITION{
Quadratic objective with a linear sum constraint spreads weights evenly when
assets are symmetric.
}
\CANONICAL{
\begin{bullets}
\item Linear constraints with quadratic forms yield linear systems.
\end{bullets}
}
\ProblemPage{6}{Expectation of a Quadratic Form in a Random Vector}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Let $X\in\mathbb{R}^n$ have mean $\mu$ and covariance $\Sigma$. Then
$\mathbb{E}[X^\top A X]=\mathrm{tr}(A\Sigma)+\mu^\top A \mu$ for symmetric $A$.
\PROBLEM{
Compute the expectation for a dice-based random vector with given $A$.
}
\MODEL{
\[
\mathbb{E}[X^\top A X]=\mathrm{tr}(A\Sigma)+\mu^\top A \mu.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $X$ has finite second moments.
\item $A=A^\top$ to ensure real values.
\end{bullets}
}
\varmapStart
\var{X}{Random vector.}
\var{\mu}{Mean of $X$.}
\var{\Sigma}{Covariance of $X$.}
\var{A}{Symmetric matrix.}
\varmapEnd
\WHICHFORMULA{
Linearity of trace and expectation; identity
$x^\top A x=\mathrm{tr}(A x x^\top)$.
}
\GOVERN{
\[
\mathbb{E}[X^\top A X]=\mathrm{tr}\!\big(A\ \mathbb{E}[X X^\top]\big)
=\mathrm{tr}\!\big(A(\Sigma+\mu\mu^\top)\big).
\]
}
\INPUTS{$X=(D_1-3.5,\ D_2-3.5)^\top$ with independent fair dice,
$A=\begin{pmatrix}2&1\\1&3\end{pmatrix}$.}
\DERIVATION{
\begin{align*}
\mu&=(0,0)^\top,\ \Sigma=\mathrm{diag}(\sigma^2,\sigma^2),\ \sigma^2=\tfrac{35}{12}.\\
\mathbb{E}[X^\top A X]&=\mathrm{tr}(A\Sigma)+\mu^\top A \mu
=\mathrm{tr}(A\Sigma)\\
&=\sigma^2(2+3)=5\sigma^2=5\cdot \tfrac{35}{12}=\tfrac{175}{12}.
\end{align*}
}
\RESULT{
$\mathbb{E}[X^\top A X]=175/12\approx 14.5833$.
}
\UNITCHECK{
Variance units multiply the dimensionless coefficients in $A$ yielding units of
squared dice faces.
}
\EDGECASES{
\begin{bullets}
\item If $X$ has correlated components, off-diagonal $\Sigma$ contributes via
$\mathrm{tr}(A\Sigma)$.
\item If $\mu\ne 0$, the $\mu^\top A \mu$ term adds bias energy.
\end{bullets}
}
\ALTERNATE{
Compute directly by expanding $X^\top A X$ and taking expectations termwise.
}
\VALIDATION{
\begin{bullets}
\item Monte Carlo with fixed seed reproduces the analytic value.
\end{bullets}
}
\INTUITION{
Average quadratic energy splits into variability energy plus bias energy.
}
\CANONICAL{
\begin{bullets}
\item Expectation pulls trace through: $\mathbb{E}[\mathrm{tr}(A X X^\top)]
=\mathrm{tr}(A \mathbb{E}[X X^\top])$.
\end{bullets}
}
\ProblemPage{7}{Proof-Style: Polarization Identity}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Prove $b(x,y)=\tfrac14(Q(x+y)-Q(x-y))$ for symmetric bilinear $b$ with
$Q(x)=b(x,x)$ over $\mathbb{R}$.
\PROBLEM{
Provide a closed proof using bilinearity and symmetry.
}
\MODEL{
\[
Q(x\pm y)=Q(x)\pm 2b(x,y)+Q(y).
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $\mathrm{char}(\mathbb{R})\ne 2$.
\item $b$ symmetric bilinear.
\end{bullets}
}
\varmapStart
\var{b}{Symmetric bilinear form.}
\var{Q}{Quadratic form $Q(x)=b(x,x)$.}
\var{x,y}{Vectors in $V$.}
\varmapEnd
\WHICHFORMULA{
Polarization identity from Formula 2.
}
\GOVERN{
\[
b(x,y)=\tfrac14\big(Q(x+y)-Q(x-y)\big).
\]
}
\INPUTS{Symbolic proof; numeric check with $A=\begin{pmatrix}2&1\\1&3\end{pmatrix}$.}
\DERIVATION{
\begin{align*}
Q(x+y)&=b(x+y,x+y)=b(x,x)+b(x,y)+b(y,x)+b(y,y)\\
&=Q(x)+2b(x,y)+Q(y),\\
Q(x-y)&=b(x-y,x-y)=Q(x)-2b(x,y)+Q(y),\\
Q(x+y)-Q(x-y)&=4b(x,y)\Rightarrow b(x,y)=\tfrac14(\cdots).\\
\text{Numeric: }&\ A=\begin{pmatrix}2&1\\1&3\end{pmatrix},\
x=(1,2),\ y=(2,-1),\\
x^\top A y&=1,\ \tfrac14(Q(x+y)-Q(x-y))=1.
\end{align*}
}
\RESULT{
Identity proven rigorously; numeric check matches.
}
\UNITCHECK{
Both sides bilinear; scaling $x\mapsto \alpha x$ scales both by $\alpha$.
}
\EDGECASES{
\begin{bullets}
\item In characteristic $2$, division by $4$ invalid.
\end{bullets}
}
\ALTERNATE{
Use $b(x,y)=\tfrac12(Q(x+y)-Q(x)-Q(y))$ derived from the parallelogram identity.
}
\VALIDATION{
\begin{bullets}
\item Test on random symmetric matrices with fixed seeds.
\end{bullets}
}
\INTUITION{
Add and subtract to cancel pure terms and isolate the cross interaction.
}
\CANONICAL{
\begin{bullets}
\item Symmetric bilinear and quadratic data are equivalent.
\end{bullets}
}
\ProblemPage{8}{Proof-Style: Symmetry and Matrix Representation}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Show that $b$ is symmetric iff its matrix is symmetric in some or any basis.
\PROBLEM{
Prove equivalence between $b(x,y)=b(y,x)$ and $A=A^\top$ where
$A_{ij}=b(e_i,e_j)$.
}
\MODEL{
\[
b(x,y)=x^\top A y,\quad b(y,x)=y^\top A x.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Finite-dimensional $V$ with basis $\{e_i\}$.
\end{bullets}
}
\varmapStart
\var{b}{Bilinear form.}
\var{A}{Matrix in basis $\{e_i\}$.}
\var{x,y}{Coordinate vectors.}
\varmapEnd
\WHICHFORMULA{
Matrix representation from Formula 1.
}
\GOVERN{
\[
b(x,y)=b(y,x)\ \forall x,y \iff x^\top A y=y^\top A x\ \forall x,y.
\]
}
\INPUTS{Symbolic proof; numeric check with a non-symmetric and symmetric $A$.}
\DERIVATION{
\begin{align*}
\Rightarrow:&\ b(x,y)=b(y,x)\Rightarrow x^\top A y=y^\top A x=(x^\top A y)^\top
=x^\top A^\top y,\\
&\ \forall x,y:\ x^\top (A-A^\top) y=0\Rightarrow A=A^\top.\\
\Leftarrow:&\ A=A^\top\Rightarrow b(x,y)=x^\top A y=y^\top A^\top x=b(y,x).\\
\text{Numeric: }&\ A_s=\begin{pmatrix}2&1\\1&3\end{pmatrix}\ \text{symmetric},\
b(x,y)=b(y,x).\\
&\ A_{ns}=\begin{pmatrix}2&2\\1&3\end{pmatrix}\ \text{not symmetric},\
b(x,y)\ne b(y,x)\ \text{in general}.
\end{align*}
}
\RESULT{
Symmetry of $b$ is equivalent to symmetry of its representing matrix.
}
\UNITCHECK{
Coordinate-free property matches coordinate symmetry across bases.
}
\EDGECASES{
\begin{bullets}
\item Different bases yield congruent symmetric matrices when $b$ is symmetric.
\end{bullets}
}
\ALTERNATE{
Use $b(e_i,e_j)=b(e_j,e_i)$ to show $A_{ij}=A_{ji}$ directly.
}
\VALIDATION{
\begin{bullets}
\item Random tests: $A\leftarrow \tfrac12(A+A^\top)$ yields symmetry and
reciprocity in $b$.
\end{bullets}
}
\INTUITION{
Symmetry in inputs mirrors transpose symmetry in coordinates.
}
\CANONICAL{
\begin{bullets}
\item Symmetry is a coordinate-invariant property captured by $A=A^\top$.
\end{bullets}
}
\ProblemPage{9}{Cauchy\textendash Schwarz from Positive Definiteness}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
If $A\succ 0$ defines inner product $\langle x,y\rangle_A=x^\top A y$, then
$\langle x,y\rangle_A^2\le \langle x,x\rangle_A\langle y,y\rangle_A$.
\PROBLEM{
Prove the inequality via positivity of a quadratic form in one variable.
}
\MODEL{
\[
0\le \langle x-t y,x-t y\rangle_A=\langle x,x\rangle_A
-2t\langle x,y\rangle_A+t^2\langle y,y\rangle_A.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item $A\succ 0$ symmetric definite.
\end{bullets}
}
\varmapStart
\var{A}{SPD matrix.}
\var{x,y}{Vectors.}
\var{t}{Real scalar.}
\varmapEnd
\WHICHFORMULA{
Nonnegativity of $Q(t)=\langle x-ty,x-ty\rangle_A$ for all $t$.
}
\GOVERN{
\[
\text{Discriminant } \Delta=4\langle x,y\rangle_A^2-4\langle x,x\rangle_A
\langle y,y\rangle_A\le 0.
\]
}
\INPUTS{$A=\begin{pmatrix}2&1\\1&3\end{pmatrix}$, $x=(1,2)$, $y=(2,-1)$.}
\DERIVATION{
\begin{align*}
\langle x,x\rangle_A&=x^\top A x=18,\ \langle y,y\rangle_A=7,\
\langle x,y\rangle_A=1,\\
\Delta&=4(1)^2-4(18)(7)=-500<0\Rightarrow \text{inequality holds},\\
\text{Check: }&\ 1^2\le 18\cdot 7 \ \text{true}.
\end{align*}
}
\RESULT{
Cauchy\textendash Schwarz holds in the $A$-inner product.
}
\UNITCHECK{
Both sides homogeneous of degree $2$ in $(x,y)$ jointly.
}
\EDGECASES{
\begin{bullets}
\item If $A\succeq 0$ with nullspace, inequality still holds but equality can
occur for noncollinear vectors in the nullspace directions.
\end{bullets}
}
\ALTERNATE{
Diagonalize $A=U\Lambda U^\top$ and reduce to Euclidean Cauchy\textendash Schwarz
by $z=\Lambda^{1/2} U^\top x$.
}
\VALIDATION{
\begin{bullets}
\item Random SPD tests with fixed seed verify nonpositive discriminants.
\end{bullets}
}
\INTUITION{
Nonnegativity of a square forces the middle term not to exceed the product of
lengths.
}
\CANONICAL{
\begin{bullets}
\item Positive definiteness equips $V$ with an inner product and its geometry.
\end{bullets}
}
\ProblemPage{10}{Classifying a Conic via Congruence}
\textbf{CANONICAL MATHEMATICAL STATEMENT.}
Classify $Q(x,y)=4x^2+4xy+y^2$ by congruence and find an orthogonal transform
reducing it to canonical form.
\PROBLEM{
Diagonalize the symmetric matrix and identify the conic type.
}
\MODEL{
\[
A=\begin{pmatrix}4&2\\2&1\end{pmatrix},\ Q(x)=\begin{pmatrix}x&y\end{pmatrix}
A\begin{pmatrix}x\\y\end{pmatrix}.
\]
}
\ASSUMPTIONS{
\begin{bullets}
\item Real plane, orthogonal changes allowed.
\end{bullets}
}
\varmapStart
\var{A}{Symmetric matrix of the conic.}
\var{P}{Orthogonal diagonalizer.}
\var{\lambda_1,\lambda_2}{Eigenvalues.}
\varmapEnd
\WHICHFORMULA{
Orthogonal diagonalization and inertia from Formula 5.
}
\GOVERN{
\[
P^\top A P=\mathrm{diag}(\lambda_1,\lambda_2),\quad \lambda_i>0\Rightarrow
\text{ellipse}.
\]
}
\INPUTS{$A=\begin{pmatrix}4&2\\2&1\end{pmatrix}$.}
\DERIVATION{
\begin{align*}
\lambda&=\frac{5\pm\sqrt{25-0}}{2}=\frac{5\pm 5}{2}\Rightarrow
\lambda_1=5,\ \lambda_2=0?\\
\text{Correct }\lambda:&\ \det(A-\lambda I)=(4-\lambda)(1-\lambda)-4\\
&=\lambda^2-5\lambda= \lambda(\lambda-5),\\
\Rightarrow&\ \lambda_1=5,\ \lambda_2=0,\\
\text{But }&\ Q(x,y)=4x^2+4xy+y^2=(2x+y)^2,\\
&\ \text{rank }1\Rightarrow \text{parabolic cylinder in 2D: a double line}.\\
P&=\frac{1}{\sqrt{5}}\begin{pmatrix}2&-1\\1&2\end{pmatrix},\
P^\top A P=\mathrm{diag}(5,0).
\end{align*}
}
\RESULT{
$Q$ reduces to $5u^2+0\cdot v^2$, a degenerate ellipse: a pair of coinciding
lines; inertia $(1,0,1)$.
}
\UNITCHECK{
Homogeneous quadratic; orthogonal transform preserves degeneracy and sign.
}
\EDGECASES{
\begin{bullets}
\item If the zero eigenvalue perturbs to positive, an ellipse appears.
\item If one eigenvalue becomes negative, a hyperbola results.
\end{bullets}
}
\ALTERNATE{
Complete the square: $4x^2+4xy+y^2=(2x+y)^2$ confirms rank 1 directly.
}
\VALIDATION{
\begin{bullets}
\item Verify $P^\top P=I$ and $P^\top A P$ is diagonal with entries $5,0$.
\end{bullets}
}
\INTUITION{
The conic is a stretched square of a single linear form; geometry collapses onto
one axis.
}
\CANONICAL{
\begin{bullets}
\item Congruence reduces $Q$ to sum of signed squares; inertia classifies type.
\end{bullets}
}
\clearpage
\section{Coding Demonstrations}
\CodeDemoPage{Polarization Identity Verification}
\PROBLEM{
Numerically verify the polarization identity $b(x,y)=\tfrac14(Q(x+y)-Q(x-y))$
for random symmetric matrices and vectors.
}
\API{
\begin{bullets}
\item \inlinecode{def read_input(s) -> tuple} — parse seed and size.
\item \inlinecode{def solve_case(args) -> float} — max abs error over trials.
\item \inlinecode{def validate() -> None} — assert small error.
\item \inlinecode{def main() -> None} — run validation and a demo.
\end{bullets}
}
\INPUTS{
Seed integer and dimension $n$ for random generation; number of trials $m$.
}
\OUTPUTS{
Maximum absolute discrepancy between $x^\top A y$ and polarization value.
}
\FORMULA{
\[
\text{err}=\max_{k}\left|x_k^\top A y_k-\tfrac14(Q(x_k+y_k)-Q(x_k-y_k))\right|.
\]
}
\textbf{SOLUTION A — From Scratch (Mathematically Explicit Implementation)}
\begin{codepy}
import numpy as np

def read_input(s):
    parts = [int(x) for x in s.split()]
    if len(parts) == 0:
        return (0, 3, 50)
    if len(parts) == 1:
        return (parts[0], 3, 50)
    if len(parts) == 2:
        return (parts[0], parts[1], 50)
    return (parts[0], parts[1], parts[2])

def rand_sym(n, rng):
    M = rng.randn(n, n)
    return (M + M.T) / 2.0

def solve_case(args):
    seed, n, m = args
    rng = np.random.RandomState(seed)
    A = rand_sym(n, rng)
    err = 0.0
    for _ in range(m):
        x = rng.randn(n)
        y = rng.randn(n)
        Q = lambda v: float(v.T @ A @ v)
        b_mat = float(x.T @ A @ y)
        b_pol = 0.25 * (Q(x + y) - Q(x - y))
        err = max(err, abs(b_mat - b_pol))
    return err

def validate():
    err = solve_case((0, 3, 200))
    assert err < 1e-10

def main():
    validate()
    print("max_error", solve_case(read_input("0 5 100")))

if __name__ == "__main__":
    main()
\end{codepy}
\textbf{SOLUTION B — Library-Based (Validated Computational Shortcut)}
\begin{codepy}
import numpy as np

def read_input(s):
    return tuple(int(x) for x in s.split()) if s else (0, 3, 50)

def solve_case(args):
    seed, n, m = args
    rng = np.random.default_rng(seed)
    M = rng.standard_normal((n, n))
    A = (M + M.T) / 2.0
    err = 0.0
    for _ in range(m):
        x = rng.standard_normal(n)
        y = rng.standard_normal(n)
        Q = lambda v: float(v.T @ A @ v)
        b_mat = float(x.T @ A @ y)
        b_pol = 0.25 * (Q(x + y) - Q(x - y))
        err = max(err, abs(b_mat - b_pol))
    return err

def validate():
    assert solve_case((0, 4, 200)) < 1e-10

def main():
    validate()
    print("max_error", solve_case(read_input("1 6 200")))

if __name__ == "__main__":
    main()
\end{codepy}
\COMPLEXITY{
Time $\mathcal{O}(m n^2)$ per run; space $\mathcal{O}(n^2)$ to store $A$.
}
\FAILMODES{
\begin{bullets}
\item Non-symmetric $A$ breaks identity; symmetrize input if needed.
\item Overflow for huge values; use scaling.
\end{bullets}
}
\STABILITY{
\begin{bullets}
\item Differences of similar $Q$ values can cancel; but errors are near
machine epsilon for random data.
\end{bullets}
}
\VALIDATION{
\begin{bullets}
\item Assert error below $1e{-}10$ on multiple seeds.
\end{bullets}
}
\RESULT{
Both implementations yield negligible error, confirming polarization.
}
\EXPLANATION{
The code evaluates both definitions of $b$ and compares, directly mapping to
Formula 2.
}
\EXTENSION{
Vectorize over trials to leverage BLAS for speed.
}
\CodeDemoPage{Sylvester Criterion and Eigenvalue Cross-Check}
\PROBLEM{
Implement Sylvester\'s criterion to test positive definiteness and cross-check
with eigenvalues.
}
\API{
\begin{bullets}
\item \inlinecode{def read_input(s)} — parse seed and size.
\item \inlinecode{def solve_case(args)} — return minors and eigen signs.
\item \inlinecode{def validate()} — assert equivalence on random SPD.
\item \inlinecode{def main()} — run validation and a demo.
\end{bullets}
}
\INPUTS{
Seed and size $n$; constructs $A=R^\top R$ to ensure SPD.
}
\OUTPUTS{
List of leading minors, boolean SPD via minors, min eigenvalue.
}
\FORMULA{
\[
A\succ 0\iff \Delta_k>0\ \forall k\ \text{ and }\ \lambda_{\min}>0.
\]
}
\textbf{SOLUTION A — From Scratch (Mathematically Explicit Implementation)}
\begin{codepy}
import numpy as np

def read_input(s):
    parts = [int(x) for x in s.split()] if s else [0, 4]
    return (parts[0], parts[1])

def make_spd(n, seed):
    rng = np.random.RandomState(seed)
    R = rng.randn(n, n) + n * np.eye(n)
    return R.T @ R

def sylvester_minors(A):
    n = A.shape[0]
    minors = []
    for k in range(1, n + 1):
        minors.append(float(np.linalg.det(A[:k, :k])))
    return minors

def solve_case(args):
    seed, n = args
    A = make_spd(n, seed)
    minors = sylvester_minors(A)
    spd = all(m > 0 for m in minors)
    eigmin = float(np.linalg.eigvalsh(A)[0])
    return minors, spd, eigmin

def validate():
    minors, spd, eigmin = solve_case((0, 5))
    assert spd and eigmin > 1e-9

def main():
    validate()
    minors, spd, eigmin = solve_case(read_input("1 4"))
    print("spd", spd, "eigmin", round(eigmin, 6))

if __name__ == "__main__":
    main()
\end{codepy}
\textbf{SOLUTION B — Library-Based (Validated Computational Shortcut)}
\begin{codepy}
import numpy as np

def read_input(s):
    return tuple(int(x) for x in s.split()) if s else (0, 4)

def solve_case(args):
    seed, n = args
    rng = np.random.default_rng(seed)
    R = rng.standard_normal((n, n)) + n * np.eye(n)
    A = R.T @ R
    minors = [float(np.linalg.det(A[:k, :k])) for k in range(1, n + 1)]
    spd = all(m > 0 for m in minors)
    eigmin = float(np.linalg.eigvalsh(A)[0])
    return minors, spd, eigmin

def validate():
    minors, spd, eigmin = solve_case((2, 6))
    assert spd and eigmin > 1e-9

def main():
    validate()
    minors, spd, eigmin = solve_case(read_input("3 5"))
    print("spd", spd, "eigmin", round(eigmin, 6))

if __name__ == "__main__":
    main()
\end{codepy}
\COMPLEXITY{
Time $\mathcal{O}(n^3)$ for determinants and eigenvalues; space $\mathcal{O}(n^2)$.
}
\FAILMODES{
\begin{bullets}
\item Near-singular matrices cause numeric sign flips; add diagonal jitter.
\item Determinant overflow for large $n$; use log-determinants if needed.
\end{bullets}
}
\STABILITY{
\begin{bullets}
\item Use symmetric eigensolver \inlinecode{eigvalsh} for accuracy.
\end{bullets}
}
\VALIDATION{
\begin{bullets}
\item Construct SPD by $R^\top R$ to guarantee positive minors and eigenvalues.
\end{bullets}
}
\RESULT{
Minors positive and min eigenvalue positive, confirming Sylvester.
}
\EXPLANATION{
Implements Formula 4 directly and cross-checks with spectral property.
}
\CodeDemoPage{Inertia Invariance Under Congruence}
\PROBLEM{
Compute inertia $(p,q,r)$ via eigenvalue signs and verify invariance under
congruence $A'\!=\!P^\top A P$.
}
\API{
\begin{bullets}
\item \inlinecode{def read_input(s)} — parse seed and size.
\item \inlinecode{def solve_case(args)} — return inertias of $A$ and $A'$.
\item \inlinecode{def validate()} — assert equality of inertias.
\item \inlinecode{def main()} — run validation and demo.
\end{bullets}
}
\INPUTS{
Seed and size $n$; random symmetric $A$ with mixed signs; random invertible $P$.
}
\OUTPUTS{
Inertia tuples for $A$ and $A'$.
}
\FORMULA{
\[
\mathrm{Inertia}(A)=\mathrm{Inertia}(P^\top A P).
\]
}
\textbf{SOLUTION A — From Scratch (Mathematically Explicit Implementation)}
\begin{codepy}
import numpy as np

def read_input(s):
    return tuple(int(x) for x in s.split()) if s else (0, 5)

def make_sym(seed, n):
    rng = np.random.RandomState(seed)
    M = rng.randn(n, n)
    A = (M + M.T) / 2.0
    A += np.diag(rng.randn(n))  # mix signs
    return A

def inertia(A, tol=1e-10):
    ev = np.linalg.eigvalsh(A)
    p = int(np.sum(ev > tol))
    q = int(np.sum(ev < -tol))
    r = int(len(ev) - p - q)
    return (p, q, r)

def solve_case(args):
    seed, n = args
    A = make_sym(seed, n)
    rng = np.random.RandomState(seed + 1)
    P = rng.randn(n, n) + np.eye(n)
    A2 = P.T @ A @ P
    return inertia(A), inertia(A2)

def validate():
    i1, i2 = solve_case((0, 6))
    assert i1 == i2

def main():
    validate()
    i1, i2 = solve_case(read_input("3 5"))
    print("inertia_A", i1, "inertia_congr", i2)

if __name__ == "__main__":
    main()
\end{codepy}
\textbf{SOLUTION B — Library-Based (Validated Computational Shortcut)}
\begin{codepy}
import numpy as np

def read_input(s):
    return tuple(int(x) for x in s.split()) if s else (1, 5)

def inertia(A, tol=1e-10):
    ev = np.linalg.eigvalsh(A)
    p = int((ev > tol).sum())
    q = int((ev < -tol).sum())
    r = int(len(ev) - p - q)
    return (p, q, r)

def solve_case(args):
    seed, n = args
    rng = np.random.default_rng(seed)
    M = rng.standard_normal((n, n))
    A = (M + M.T) / 2.0 + np.diag(rng.standard_normal(n))
    P = rng.standard_normal((n, n)) + np.eye(n)
    A2 = P.T @ A @ P
    return inertia(A), inertia(A2)

def validate():
    i1, i2 = solve_case((2, 7))
    assert i1 == i2

def main():
    validate()
    i1, i2 = solve_case(read_input("4 6"))
    print("inertia_A", i1, "inertia_congr", i2)

if __name__ == "__main__":
    main()
\end{codepy}
\COMPLEXITY{
Time $\mathcal{O}(n^3)$ for eigenvalues; space $\mathcal{O}(n^2)$.
}
\FAILMODES{
\begin{bullets}
\item Near-zero eigenvalues sensitive to tol; choose tolerance explicitly.
\item Non-invertible $P$ invalid; add identity to ensure invertibility.
\end{bullets}
}
\STABILITY{
\begin{bullets}
\item Symmetric eigensolvers are numerically stable; use \inlinecode{eigvalsh}.
\end{bullets}
}
\VALIDATION{
\begin{bullets}
\item Equality of inertia tuples confirms Sylvester\'s law.
\end{bullets}
}
\RESULT{
Inertia preserved under congruence across random tests.
}
\EXPLANATION{
Maps directly to Formula 5: congruent symmetric matrices share inertia.
}
\EXTENSION{
Implement LDL\^T decomposition to compute inertia without eigenvalues.
}
\clearpage
\section{Applied Domains — Detailed End-to-End Scenarios}
\DomainPage{Machine Learning}
\SCENARIO{
Ridge regression minimizes a quadratic objective
$J(\beta)=\|X\beta-y\|_2^2+\lambda\|\beta\|_2^2$, a quadratic form in $\beta$.
Solve for $\beta$ and verify positive definiteness of the Hessian.
}
\ASSUMPTIONS{
\begin{bullets}
\item $X\in\mathbb{R}^{n\times d}$, $\lambda>0$.
\item Data deterministic via fixed seed.
\end{bullets}
}
\WHICHFORMULA{
Normal equations $(X^\top X+\lambda I)\beta=X^\top y$ with SPD matrix
$A=X^\top X+\lambda I$; $J(\beta)=\beta^\top A \beta-2\beta^\top X^\top y+y^\top y$.
}
\varmapStart
\var{X}{Design matrix.}
\var{y}{Target vector.}
\var{\beta}{Coefficients.}
\var{\lambda}{Regularization strength.}
\var{A}{Hessian $X^\top X+\lambda I$.}
\varmapEnd
\PIPELINE{
\begin{bullets}
\item Generate $X,y$; build $A$.
\item Solve linear system for $\beta$.
\item Verify SPD via Sylvester and eigenvalues.
\end{bullets}
}
\textbf{Implementation (From Scratch)}
\begin{codepy}
import numpy as np

def generate(n=100, d=3, lam=1.0, seed=0):
    rng = np.random.RandomState(seed)
    X = rng.randn(n, d)
    beta_true = rng.randn(d)
    y = X @ beta_true + 0.1 * rng.randn(n)
    A = X.T @ X + lam * np.eye(d)
    b = X.T @ y
    return X, y, A, b, beta_true

def ridge(A, b):
    return np.linalg.solve(A, b)

def rmse(y, yhat):
    return float(np.sqrt(np.mean((y - yhat) ** 2)))

def main():
    X, y, A, b, beta_true = generate()
    beta = ridge(A, b)
    yhat = X @ beta
    e = rmse(y, yhat)
    mineig = float(np.linalg.eigvalsh(A)[0])
    print("rmse", round(e, 4), "min_eig", round(mineig, 6))

if __name__ == "__main__":
    main()
\end{codepy}
\textbf{Implementation (Library Version)}
\begin{codepy}
import numpy as np
from numpy.linalg import solve, eigvalsh

def main():
    rng = np.random.default_rng(0)
    n, d, lam = 120, 4, 1.0
    X = rng.standard_normal((n, d))
    beta_true = rng.standard_normal(d)
    y = X @ beta_true + 0.1 * rng.standard_normal(n)
    A = X.T @ X + lam * np.eye(d)
    b = X.T @ y
    beta = solve(A, b)
    print("min_eig", round(float(eigvalsh(A)[0]), 6),
          "norm_beta", round(float(np.linalg.norm(beta)), 4))

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{RMSE and minimum eigenvalue of $A$; $A\succ 0$ implies unique solution.}
\INTERPRET{Quadratic form $J$ is strictly convex; solution is its unique minimizer.}
\NEXTSTEPS{Add feature scaling or generalized Tikhonov via $A=X^\top X+\lambda M$.}
\DomainPage{Quantitative Finance}
\SCENARIO{
Compute and decompose portfolio variance $Q(w)=w^\top \Sigma w$ and verify
risk contributions as a bilinear interaction.
}
\ASSUMPTIONS{
\begin{bullets}
\item $\Sigma$ estimated with fixed seed.
\item Weights sum to one.
\end{bullets}
}
\WHICHFORMULA{
Variance as quadratic form; marginal contribution $(\Sigma w)_i$; Euler
decomposition $Q(w)=\sum_i w_i (\Sigma w)_i$.
}
\varmapStart
\var{\Sigma}{Covariance matrix.}
\var{w}{Weights with $\sum w_i=1$.}
\var{Q}{Quadratic variance.}
\var{mc}{Marginal contributions $\Sigma w$.}
\varmapEnd
\PIPELINE{
\begin{bullets}
\item Simulate returns; estimate $\Sigma$.
\item Choose $w$; compute $Q$ and contributions.
\item Verify sum of contributions equals $Q$.
\end{bullets}
}
\textbf{Implementation (Full Pipeline)}
\begin{codepy}
import numpy as np

def simulate(n=1000, d=3, seed=0):
    rng = np.random.RandomState(seed)
    A = rng.randn(d, d)
    cov = A @ A.T
    R = rng.multivariate_normal(np.zeros(d), cov, size=n)
    return R

def variance_decomp(R, w):
    S = np.cov(R, rowvar=False, bias=True)
    Q = float(w.T @ S @ w)
    mc = S @ w
    contrib = w * mc
    return Q, S, mc, contrib

def main():
    R = simulate()
    w = np.array([0.5, 0.3, 0.2])
    Q, S, mc, contrib = variance_decomp(R, w)
    print("var", round(Q, 4), "sum_contrib", round(float(contrib.sum()), 4))

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{Total variance and sum of Euler contributions; equality holds.}
\INTERPRET{Variance is a bilinear energy; contributions partition its value.}
\NEXTSTEPS{Optimize $w$ for minimum variance under constraints.}
\DomainPage{Deep Learning}
\SCENARIO{
Quadratic loss $L(\beta)=\tfrac12\|X\beta-y\|^2$ has Hessian $H=X^\top X$.
Demonstrate gradient descent convergence governed by eigenvalues of $H$.
}
\ASSUMPTIONS{
\begin{bullets}
\item Fixed step $\eta<2/\lambda_{\max}(H)$.
\item Deterministic seed.
\end{bullets}
}
\WHICHFORMULA{
Update $\beta_{k+1}=\beta_k-\eta H\beta_k+\eta X^\top y$ converges linearly
with rate $\max_i |1-\eta \lambda_i|$.
}
\PIPELINE{
\begin{bullets}
\item Generate $X,y$.
\item Run GD with safe step size.
\item Compare to closed-form solution.
\end{bullets}
}
\textbf{Implementation (End-to-End)}
\begin{codepy}
import numpy as np

def generate(n=200, d=5, seed=0):
    rng = np.random.RandomState(seed)
    X = rng.randn(n, d)
    beta_true = rng.randn(d)
    y = X @ beta_true + 0.1 * rng.randn(n)
    H = X.T @ X
    g = X.T @ y
    return X, y, H, g

def gd(H, g, eta, iters=200):
    d = H.shape[0]
    b = np.zeros(d)
    for _ in range(iters):
        b = b - eta * (H @ b) + eta * g
    return b

def main():
    X, y, H, g = generate()
    eig = np.linalg.eigvalsh(H)
    eta = 1.0 / (eig[-1] + 1e-9)
    b_gd = gd(H, g, eta, iters=500)
    b_cf = np.linalg.solve(H, g)
    err = float(np.linalg.norm(b_gd - b_cf))
    print("gd_err", round(err, 6), "cond", round(float(eig[-1]/eig[0]), 3))

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{Distance $\|\beta_{\mathrm{GD}}-\beta_{\mathrm{CF}}\|$ and condition number.}
\INTERPRET{Quadratic structure ensures linear convergence with step bound by
eigenvalues.}
\NEXTSTEPS{Use conjugate gradients exploiting quadratic form structure.}
\DomainPage{Kaggle / Data Analytics}
\SCENARIO{
Compute Mahalanobis distances $d(x)^2=(x-\mu)^\top \Sigma^{-1}(x-\mu)$ to flag
outliers; this is a quadratic form in centered coordinates.
}
\ASSUMPTIONS{
\begin{bullets}
\item Full-rank covariance; deterministic generation.
\end{bullets}
}
\WHICHFORMULA{
Quadratic form with $A=\Sigma^{-1}$; whitening transforms distances to Euclidean.
}
\PIPELINE{
\begin{bullets}
\item Generate correlated data.
\item Estimate $\mu,\Sigma$.
\item Compute distances and basic stats.
\end{bullets}
}
\textbf{Implementation (Complete EDA Pipeline)}
\begin{codepy}
import numpy as np

def generate(n=300, d=3, seed=0):
    rng = np.random.RandomState(seed)
    A = rng.randn(d, d)
    S = A @ A.T
    X = rng.multivariate_normal(np.zeros(d), S, size=n)
    return X

def mahalanobis(X):
    mu = X.mean(axis=0)
    S = np.cov(X, rowvar=False, bias=True)
    A = np.linalg.inv(S)
    D2 = np.array([(x - mu).T @ A @ (x - mu) for x in X])
    return D2, mu, S

def main():
    X = generate()
    D2, mu, S = mahalanobis(X)
    print("mean_D2", round(float(D2.mean()), 3),
          "min_D2", round(float(D2.min()), 3))

if __name__ == "__main__":
    main()
\end{codepy}
\METRICS{Mean and minimum Mahalanobis squared distances.}
\INTERPRET{Distances measure standardized deviation under the covariance metric.}
\NEXTSTEPS{Threshold using chi-square quantiles for anomaly detection.}
\end{document}